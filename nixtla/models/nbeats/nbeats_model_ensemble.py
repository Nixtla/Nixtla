# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/models_nbeats__nbeats_model_ensemble.ipynb (unless otherwise specified).

__all__ = ['common_grid', 'Yearly', 'Quarterly', 'EnsembleNBEATS']

# Cell
from dataclasses import dataclass

import pytorch_lightning as pl
from itertools import product
from pathlib import Path
from typing import Callable, Dict, Iterable, Union, List

import pandas as pd
from tqdm import tqdm
import pylab as plt
from pylab import rcParams

from .nbeats_model import NBEATS
from ...data.datasets.m4 import M4Info, M4, M4Evaluation
from ...data.tsdataset import TimeSeriesDataset
from ...data.tsloader import TimeSeriesLoader
from ...experiments.utils import create_datasets, get_mask_dfs

# Internal Cell
def _parameter_grid(grid):
    specs_list = list(product(*list(grid.values())))
    model_specs_df = pd.DataFrame(specs_list, columns=list(grid.keys()))

    return model_specs_df

# Cell
common_grid = {}

# Architecture parameters
common_grid['activation'] = ['ReLU'] # Oreshkin
common_grid['n_x'] = [0] # No exogenous variables
common_grid['n_s'] = [0] # No static variables
common_grid['n_x_hidden'] = [0] # No exogenous variables
common_grid['n_s_hidden'] = [0] # No static variables
common_grid['stack_types'] = [['trend', 'seasonality']] # NBEATS-I original architecture
common_grid['n_blocks'] = [[3, 3]] # Trend blocks, Seasonal blocks - Oreshkin
common_grid['n_layers'] = [[4, 4]] # Trend-block layers, Seasonal-block - Oreshkin
common_grid['shared_weights'] = [True] # Oreshkin
common_grid['n_harmonics'] = [1] # Oreshkin
common_grid['n_polynomials'] = [2] # Trend polynomial degree
common_grid['n_theta_hidden'] = [[common_grid['n_layers'][0][0] * [256],
                                  common_grid['n_layers'][0][1] * [2048]]] # Oreshkin
common_grid['initialization'] = ['lecun_normal'] # Arbitrary

# Optimization parameters
common_grid['learning_rate'] = [0.001] # Oreshkin
common_grid['lr_decay'] = [0] # No lr_decay in the original implementation
common_grid['lr_decay_step_size'] = [1_000] # No lr_decay in the original implementation
common_grid['loss_train'] = ['MAPE', 'SMAPE'] # MASE not available. Oreshkin
common_grid['loss_hypar'] = [0.5] # ???
# common_grid['loss_valid'] = common_grid['loss_train'] # Oreskin NOT INCLUDED TO AVOID DUPLICITY
common_grid['dropout_prob_theta'] = [0] # No dropout in the original implementation
common_grid['weight_decay'] = [0] # # No weight_decay in the original implementation
common_grid['batch_size'] = [1024] # Oreshkin
common_grid['batch_normalization'] = [False] # No batch_normalization in the original implementation

common_grid['max_steps'] = [1_000] # Oreshkin
common_grid['random_seed'] = list(range(1)) # Change to range(10). Oreshkin

# Data Parameters
common_grid['complete_inputs'] = [True] # ???
common_grid['mode'] = ['simple'] # Step = 1 window
common_grid['lookbacks'] = list(range(2, 8)) # Change to range(2, 8). Oreshkin

# Cell
@dataclass
class Yearly:
    group = M4Info['Yearly']

    grid_freq = {}
    grid_freq['max_epochs'] = [3] # In combination with max_n_steps, it trains max_n_epochs*max_n_steps
    grid_freq['n_time_in'] = [8]
    grid_freq['n_time_out'] = [group.horizon]
    grid_freq['idx_to_sample_freq'] = [1] # Nixtlats sota_results esrnn yearly
    grid_freq['val_idx_to_sample_freq'] = [group.horizon * 1] # ?
    grid_freq['frequency'] = ['Q'] # ???
    grid_freq['seasonality'] = [4] # ???
    grid_freq['window_sampling_limit'] = \
        [int(1.5 * lookback) for lookback in common_grid['lookbacks']]

    grid = {**common_grid,
            **grid_freq}

# Cell
@dataclass
class Quarterly:
    group = M4Info['Quarterly']

    grid_freq = {}
    grid_freq['max_epochs'] = [3]
    grid_freq['n_time_in'] = [4]
    grid_freq['n_time_out'] = [group.horizon]
    grid_freq['idx_to_sample_freq'] = [1] # ???
    grid_freq['val_idx_to_sample_freq'] = [group.horizon * 1] # ???
    grid_freq['frequency'] = ['Y'] # ???
    grid_freq['seasonality'] = [1] # ???
    grid_freq['window_sampling_limit'] = \
        [int(1.5 * lookback) for lookback in common_grid['lookbacks']]

    grid = {**common_grid,
            **grid_freq}

# Cell
class EnsembleNBEATS:
    # TODO: Update test TimeSeriesDataset instantiation wit parameter last_samplable_window
    def __init__(self,
                 frequencies: List[type],
                 loader: callable,
                 num_workers: int):
        self.frequencies = frequencies
        self.loader = loader

        self.num_workers = num_workers

    def fit(self):
        results = {}

        for freq in self.frequencies:
            print(f'\n{freq.group.name}')
            Y_df, _, S_df = M4.load(directory='data', group=freq.group.name)
            freq_grid = _parameter_grid(freq.grid)

            forecasts = []

            for idx_ensemble, row_ensemble in tqdm(freq_grid.iterrows(), position=0, leave=True):
                hparams = row_ensemble.to_dict()

                train_dataset = \
                    TimeSeriesDataset(Y_df=Y_df, S_df=S_df,
                                      ds_in_test=freq.group.horizon,
                                      mode=hparams['mode'],
                                      window_sampling_limit=hparams['window_sampling_limit'], # To limit backprop time
                                      input_size=hparams['n_time_in'],
                                      output_size=hparams['n_time_out'],
                                      idx_to_sample_freq=hparams['idx_to_sample_freq'],
                                      complete_inputs=hparams['complete_inputs'],
                                      skip_nonsamplable=True)

                train_loader = TimeSeriesLoader(dataset=train_dataset,
                                                batch_size=hparams['batch_size'],
                                                eq_batch_size=True,
                                                num_workers=self.num_workers,
                                                shuffle=False)

                test_dataset = \
                    TimeSeriesDataset(Y_df=Y_df, S_df=S_df,
                                      ds_in_test=0,
                                      mode=hparams['mode'],
                                      window_sampling_limit=hparams['window_sampling_limit'], # To limit backprop time
                                      input_size=hparams['n_time_in'],
                                      output_size=freq.group.horizon,
                                      idx_to_sample_freq=hparams['idx_to_sample_freq'],
                                      complete_inputs=hparams['complete_inputs'],
                                      complete_outputs=True,
                                    #   last_samplable_window=True,
                                      skip_nonsamplable=False)

                test_loader = TimeSeriesLoader(dataset=test_dataset,
                                               batch_size=1024,
                                               eq_batch_size=False,
                                               num_workers=self.num_workers,
                                               shuffle=False)

                model = NBEATS(n_time_in=int(hparams['n_time_in']),
                               n_time_out=int(hparams['n_time_out']),
                               n_x=hparams['n_x'],
                               n_s=hparams['n_s'],
                               n_s_hidden=int(hparams['n_s_hidden']),
                               n_x_hidden=int(hparams['n_x_hidden']),
                               shared_weights=hparams['shared_weights'],
                               initialization=hparams['initialization'],
                               activation=hparams['activation'],
                               stack_types=hparams['stack_types'],
                               n_blocks=hparams['n_blocks'],
                               n_layers=hparams['n_layers'],
                               n_theta_hidden=hparams['n_theta_hidden'],
                               n_harmonics=int(hparams['n_harmonics']),
                               n_polynomials=int(hparams['n_polynomials']),
                               batch_normalization = hparams['batch_normalization'],
                               dropout_prob_theta=hparams['dropout_prob_theta'],
                               learning_rate=float(hparams['learning_rate']),
                               lr_decay=float(hparams['lr_decay']),
                               lr_decay_step_size=float(hparams['lr_decay_step_size']),
                               weight_decay=hparams['weight_decay'],
                               loss_train=hparams['loss_train'],
                               loss_hypar=float(hparams['loss_hypar']),
                               loss_valid='SMAPE',
                               frequency=hparams['frequency'],
                               seasonality=int(hparams['seasonality']),
                               random_seed=int(hparams['random_seed']))

                print(f'\nModel distinctive attributes: loss: {model.loss_train}, random_seeds: {model.random_seed}')

                trainer = pl.Trainer(max_epochs=hparams['max_epochs'],
                                     max_steps=hparams['max_steps'],
                                     gradient_clip_val=0,
                                     progress_bar_refresh_rate=1,
                                     log_every_n_steps=100,
                                    #  val_check_interval=0,
                                    #  check_val_every_n_epoch=hparams['max_epochs'],
                                     gpus=-1,
                                     auto_select_gpus=True)

                trainer.fit(model, train_loader)

                outputs = trainer.predict(model, test_loader)

                print("outputs[0][0].shape", outputs[0][0].shape)
                print("outputs[0][1].shape", outputs[0][1].shape)
                print("outputs[0][2].shape", outputs[0][2].shape)



            results[freq.group.name] = forecasts

        return results
