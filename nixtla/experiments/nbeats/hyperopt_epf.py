# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/experiments_nbeats__hyperopt_epf.ipynb (unless otherwise specified).

__all__ = ['forecast_evaluation_table', 'protect_nan_reported_loss', 'get_last_n_timestamps_mask_df', 'balance_data',
           'scale_data', 'declare_mask_df_Kin', 'prepare_data_Kin', 'train_val_split', 'declare_mask_df_Cristian',
           'prepare_data_Cristian', 'BENCHMARK_DF', 'run_val_nbeatsx', 'get_experiment_space', 'parse_trials', 'main',
           'parse_args']

# Cell
import time
import os
# Limit number of threads in numpy and others to avoid throttling
os.environ["OMP_NUM_THREADS"] = "2" # export OMP_NUM_THREADS=4
os.environ["OPENBLAS_NUM_THREADS"] = "2" # export OPENBLAS_NUM_THREADS=4
os.environ["MKL_NUM_THREADS"] = "3" # export MKL_NUM_THREADS=6
os.environ["VECLIB_MAXIMUM_THREADS"] = "2" # export VECLIB_MAXIMUM_THREADS=4
os.environ["NUMEXPR_NUM_THREADS"] = "3" # export NUMEXPR_NUM_THREADS=6

import numpy as np
import pandas as pd
import argparse
import pickle
import glob
import itertools
import random
from datetime import datetime
from functools import partial

from ...data.scalers import Scaler
from ...data.datasets.epf import EPF, EPFInfo
from ...data.tsdataset import TimeSeriesDataset
from ...data.tsloader_fast import TimeSeriesLoader
from ...losses.numpy import mae, mape, smape, rmse, pinball_loss

# Models
from ...models.nbeats.nbeats import Nbeats

import warnings
warnings.filterwarnings("ignore")

from hyperopt import fmin, tpe, hp, Trials, STATUS_OK

# Cell
# TODO: Think and test new mask_df and datasets with random validation

############################################################################################
#### COMMON
############################################################################################

BENCHMARK_DF = pd.DataFrame({'id': [1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4],
                             'unique_id': ['NP', 'NP', 'NP', 'NP', 'PJM', 'PJM', 'PJM', 'PJM',
                                           'BE', 'BE', 'BE', 'BE', 'FR', 'FR', 'FR', 'FR'],
                             'metric': ['MAE', 'MAPE', 'SMAPE', 'RMSE', 'MAE', 'MAPE', 'SMAPE', 'RMSE',
                                        'MAE', 'MAPE', 'SMAPE', 'RMSE', 'MAE', 'MAPE', 'SMAPE', 'RMSE'],
                             'DNN' : [1.67, 5.38, 4.85, 3.33, 2.78, 28.66, 11.22, 4.64, 5.82,
                                      26.11, 13.33, 16.13, 3.91, 14.77, 10.98, 11.74]})

def forecast_evaluation_table(model, ts_loader):
    y_total, y_hat_total, _ = model.predict(ts_loader=ts_loader, eval_mode=True)

    y_tot = y_total.reshape(-1)
    y_total_nans_perc = np.sum((np.isnan(y_tot)))  / len(y_tot)
    print("y_total nan perc", y_total_nans_perc)
    print("y_total.shape \t\t(#n_windows, #lt) \t", y_total.shape)
    print("y_hat_total.shape  \t(#n_windows, #lt) \t", y_hat_total.shape)
    print("\n")

    # Reshape for univariate and panel model compatibility
    n_series = ts_loader.ts_dataset.n_series
    n_fcds = len(y_total) // n_series
    output_size = y_hat_total.shape[1]
    y_total = y_total.reshape(n_series, n_fcds, output_size)
    y_hat_total = y_hat_total.reshape(n_series, n_fcds, output_size)

    print("y_total.shape \t\t(#n_series, #n_fcds, #lt) \t", y_total.shape)
    print("y_hat_total.shape  \t(#n_series, #n_fcds, #lt) \t", y_hat_total.shape)
    print("\n")

    performances = []
    for i, meta_data in enumerate(ts_loader.ts_dataset.meta_data):
        market = meta_data['unique_id']

        y = y_total[i,:,:].reshape(-1)
        y_hat = y_hat_total[i,:,:].reshape(-1)

        _mae   = np.round(mae(y=y, y_hat=y_hat),5)
        _mape  = np.round(mape(y=y, y_hat=y_hat),5)
        _smape = np.round(smape(y=y, y_hat=y_hat),5)
        _rmse  = np.round(rmse(y=y, y_hat=y_hat),5)

        performance_df = pd.DataFrame({'unique_id': [market]*4,
                                       'metric': ['MAE', 'MAPE', 'SMAPE', 'RMSE'],
                                       'NBEATSx': [_mae, _mape, _smape, _rmse]})

        performances += [performance_df]

    performances_df = pd.concat(performances)

    #benchmark_df.sort_values(['id'], inplace=True)
    #benchmark_df.reset_index(drop=True, inplace=True)
    benchmark_df = BENCHMARK_DF.merge(performances_df, on=['unique_id', 'metric'], how='left')
    benchmark_df['perc_diff'] = 100 * (benchmark_df['NBEATSx']-benchmark_df['DNN'])/benchmark_df['DNN']
    benchmark_df['improvement'] = benchmark_df['perc_diff'] < 0
    benchmark_df = benchmark_df.dropna()
    average_perc_diff = benchmark_df['perc_diff'].mean()
    print("average_perc_diff", average_perc_diff)

    return benchmark_df, average_perc_diff

def protect_nan_reported_loss(model):
    # TODO: Pytorch numerical error hacky protection, protect from losses.numpy.py
    reported_loss = model.final_outsample_loss
    if np.isnan(model.final_insample_loss):
        reported_loss = 500
    if model.final_insample_loss<=0:
        reported_loss = 500

    if np.isnan(model.final_outsample_loss):
        reported_loss = 500
    if model.final_outsample_loss<=0:
        reported_loss = 500
    return reported_loss


def get_last_n_timestamps_mask_df(Y_df, n_timestamps):
    # Creates outsample_mask
    # train_mask: 1 last_n_timestamps, 0 timestamps until last_n_timestamps

    last_df = Y_df.copy()[['unique_id', 'ds']]
    last_df.sort_values(by=['unique_id', 'ds'], inplace=True, ascending=False)
    last_df.reset_index(drop=True, inplace=True)

    last_df = last_df.groupby('unique_id').head(n_timestamps)
    last_df['mask'] = 1

    last_df = last_df[['unique_id', 'ds', 'mask']]

    mask_df = Y_df.merge(last_df, on=['unique_id', 'ds'], how='left')
    mask_df['mask'] = mask_df['mask'].fillna(0)    # The first len(Y)-n_hours used as train

    mask_df = mask_df[['unique_id', 'ds', 'mask']]
    mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)

    assert len(mask_df)==len(Y_df), \
        f'The mask_df length {len(mask_df)} is not equal to Y_df length {len(Y_df)}'

    return mask_df

def balance_data(Y_df, X_df):
    # Create balanced placeholder dataframe
    balance_ids = {'unique_id': Y_df.unique_id.unique(),
                   'ds': Y_df.ds.unique()}

    product_list = list(itertools.product(*list(balance_ids.values())))
    balance_df = pd.DataFrame(product_list, columns=list(balance_ids.keys()))

    # Create mask for weighted losses for the las 2 years of each unique_id
    weights_df = get_last_n_timestamps_mask_df(Y_df=Y_df, n_timestamps=365*2*24)
    weights_df['weights'] = weights_df['mask']
    del weights_df['mask']

    # Balance with merge
    Y_balanced_df = balance_df.merge(Y_df, on=['unique_id', 'ds'], how='left')
    X_balanced_df = balance_df.merge(X_df, on=['unique_id', 'ds'], how='left')
    weights_balanced_df = balance_df.merge(weights_df, on=['unique_id', 'ds'], how='left')
    #print(weights_balanced_df.groupby(['unique_id', 'weights']).agg({'ds': ['min', 'max']}))

    print('\n')
    print('Y_df.shape \t', Y_df.shape)
    print('X_df.shape \t', X_df.shape)
    print('Y_balanced_df.shape \t', Y_balanced_df.shape)
    print('X_balanced_df.shape \t', X_balanced_df.shape)
    print('weights_balanced_df.shape \t', weights_balanced_df.shape)

    return Y_balanced_df, X_balanced_df, weights_balanced_df

def scale_data(Y_df, X_df, mask, normalizer_y, normalizer_x):
    y_shift = None
    y_scale = None

    mask = mask.astype(int)

    if normalizer_y is not None:
        scaler_y = Scaler(normalizer=normalizer_y)
        Y_df['y'] = scaler_y.scale(x=Y_df['y'].values, mask=mask)
    else:
        scaler_y = None
    # Exogenous are always scaled to help learning
    if normalizer_x is not None:
        scaler_x = Scaler(normalizer=normalizer_x)
        X_df['Exogenous1'] = scaler_x.scale(x=X_df['Exogenous1'].values, mask=mask)

        scaler_x = Scaler(normalizer=normalizer_x)
        X_df['Exogenous2'] = scaler_x.scale(x=X_df['Exogenous2'].values, mask=mask)

    filter_variables = ['unique_id', 'ds', 'Exogenous1', 'Exogenous2', 'week_day'] + \
                       [col for col in X_df if (col.startswith('day'))]
                       #[col for col in X_df if (col.startswith('_hour_'))]
    X_df = X_df[filter_variables]

    return Y_df, X_df, scaler_y



############################################################################################
#### KIN
############################################################################################

def declare_mask_df_Kin(mc, Y_df, X_df, S_df):
    mask_df = Y_df[['unique_id', 'ds', 'y']].copy()
    mask_df['available_maskY'] = (1-Y_df.y.isnull().values)
    mask_df['available_maskX'] = (1-X_df['Exogenous1'].isnull().values)
    mask_df['available_mask'] = mask_df['available_maskY'] * mask_df['available_maskX']

    del mask_df['y']
    del mask_df['available_maskX']
    del mask_df['available_maskY']

    # Train Validation splits
    #                        ds
    #                       min                 max
    # unique_id mask
    # BE        0.0  2013-01-04 2015-01-03 23:00:00
    #           1.0  2011-01-09 2013-01-03 23:00:00
    # FR        0.0  2013-01-04 2015-01-03 23:00:00
    #           1.0  2011-01-09 2013-01-03 23:00:00
    # NP        0.0  2014-12-28 2016-12-26 23:00:00
    #           1.0  2013-01-01 2014-12-27 23:00:00
    # PJM       0.0  2014-12-28 2016-12-26 23:00:00
    #           1.0  2013-01-01 2014-12-27 23:00:00
    mask_df['sample_mask1'] = (mask_df['ds'] <= pd.to_datetime('2013-01-03 23:00:00')) * 1
    mask_df['sample_mask2'] = (mask_df['ds'] <= pd.to_datetime('2014-12-27 23:00:00')) * 1
    mask_df['sample_mask'] = mask_df['sample_mask2']
    return mask_df

def prepare_data_Kin(mc, Y_df, X_df, S_df):

    #-------------------------------------------- Data Wrangling --------------------------------------------#
    Y_balanced_df, X_balanced_df, weights_balanced_df = balance_data(Y_df, X_df)
    del Y_df, X_df

    #------------------------------------- Available and Validation Mask ------------------------------------#
    # Create available_mask and sample_mask
    mask_df = declare_mask_df_Kin(mc=mc, Y_df=Y_balanced_df, X_df=X_balanced_df, S_df=S_df)

    #---------------------------------------------- Scale Data ----------------------------------------------#

    # Scale data # TODO: write sample_mask conditional/groupby(['unique_id]) scaling
    train_mask = mask_df.available_mask.values * mask_df.available_mask.values
    Y_scaled_df, X_scaled_df, scaler_y = scale_data(Y_df=Y_balanced_df, X_df=X_balanced_df, mask=train_mask,
                                      normalizer_y=mc['normalizer_y'], normalizer_x=mc['normalizer_x'])
    del Y_balanced_df
    del X_balanced_df

    #-------------------------------------------- Declare Loaders -------------------------------------------#

    ts_dataset = TimeSeriesDataset(Y_df=Y_scaled_df, X_df=X_scaled_df, S_df=S_df, mask_df=mask_df)

    train_ts_loader = TimeSeriesLoader(ts_dataset=ts_dataset,
                                       model='nbeats',
                                       offset=0, #offset,
                                       window_sampling_limit=ts_dataset.max_len,
                                       input_size=int(mc['input_size_multiplier']*mc['output_size']),
                                       output_size=int(mc['output_size']),
                                       idx_to_sample_freq=int(mc['idx_to_sample_freq']),
                                       batch_size=int(mc['batch_size']),
                                       is_train_loader=True,
                                       shuffle=True, random_seed=int(mc['random_seed']))

    print("train_ts_loader.ts_windows.shape", train_ts_loader.ts_windows.shape)
    print(f"len(train_loader.windows_sampling_idx) * 24 = \
       \t {len(train_ts_loader.windows_sampling_idx)} * 24 = {len(train_ts_loader.windows_sampling_idx) * 24}")
    print("\n")

    val_ts_loader = TimeSeriesLoader(ts_dataset=ts_dataset,
                                     model='nbeats',
                                     offset=0, #offset,
                                     window_sampling_limit=ts_dataset.max_len,
                                     input_size=int(mc['input_size_multiplier']*mc['output_size']),
                                     output_size=int(mc['output_size']),
                                     idx_to_sample_freq=int(mc['idx_to_sample_freq']),
                                     batch_size=1024,
                                     is_train_loader=False, # Samples the opposite of train_outsample_mask
                                     shuffle=False, random_seed=int(mc['random_seed']))

    print("val_ts_loader.ts_windows.shape", val_ts_loader.ts_windows.shape)
    print(f"len(val_loader.windows_sampling_idx) * 24 = \
       \t {len(val_ts_loader.windows_sampling_idx)} * 24 = {len(val_ts_loader.windows_sampling_idx) * 24}")
    print("\n")

    mc['t_cols'] = ts_dataset.t_cols
    return mc, train_ts_loader, val_ts_loader, scaler_y




############################################################################################
#### CRISTIAN
############################################################################################

def train_val_split(len_series, offset, window_sampling_limit, n_val_weeks, ds_per_day):
    last_ds = len_series - offset
    first_ds = max(last_ds - window_sampling_limit, 0)

    last_day = int(last_ds/ds_per_day)
    first_day = int(first_ds/ds_per_day)

    days = set(range(first_day, last_day)) # All days, to later get train days
    # Sample weeks from here, -7 to avoid sampling from last week
    # To not sample first week and have inputs
    sampling_days = set(range(first_day + 7, last_day - 7))
    validation_days = set({}) # Val days set

    # For loop for n of weeks in validation
    for i in range(n_val_weeks):
        # Sample random day, init of week
        init_day = random.sample(sampling_days, 1)[0]
        # Select days of sampled init of week
        sampled_days = list(range(init_day, min(init_day+7, last_day)))
        # Add days to validation days
        validation_days.update(sampled_days)
        # Remove days from sampling_days, including overlapping resulting previous week
        days_to_remove = set(range(init_day-6, min(init_day+7, last_day)))
        sampling_days = sampling_days.difference(days_to_remove)

    train_days = days.difference(validation_days)

    train_days = sorted(list(train_days))
    validation_days = sorted(list(validation_days))

    train_idx = []
    for day in train_days:
        hours_idx = range(day*ds_per_day,(day+1)*ds_per_day)
        train_idx += hours_idx

    val_idx = []
    for day in validation_days:
        hours_idx = range(day*ds_per_day,(day+1)*ds_per_day)
        val_idx += hours_idx

    assert all([idx < last_ds for idx in val_idx]), 'Leakage!!!!'

    return train_idx, val_idx

def declare_mask_df_Cristian(mc, Y_df, X_df, S_df):
    # train_mask: 1 to keep, 0 to hide
    train_outsample_mask = np.ones(len(Y_df), dtype=int)
    # if random_validation:
    #     print('Random validation activated')
    #     np.random.seed(1)
    #     random.seed(1)
    #     _, val_idx = train_val_split(len_series=len(Y_df), offset=0,
    #                             window_sampling_limit=window_sampling_limit,
    #                             n_val_weeks=n_val_weeks, ds_per_day=24)
    #     train_outsample_mask[val_idx] = 0
    #else:
    print('Random validation de-activated')
    train_outsample_mask[-(mc['n_val_weeks'] * 7 * mc['output_size']):] = 0

    print(f'Train {sum(train_outsample_mask)} hours = {np.round(sum(train_outsample_mask)/(24*365),2)} years')
    print(f'Validation {sum(1-train_outsample_mask)} hours = {np.round(sum(1-train_outsample_mask)/(24*365),2)} years')

    # NO MAMES ESTA BASURA DE VECTOR BASURA
    y_validation_vector = Y_df['y'].values[(1-train_outsample_mask)==1] # To compute validation loss in true scale

    mask_df = Y_df[['unique_id', 'ds', 'y']].copy()
    mask_df['available_mask'] = np.ones(len(Y_df))
    mask_df['sample_mask'] = train_outsample_mask
    return mask_df

def prepare_data_Cristian(mc, Y_df, X_df, S_df):

    #window_sampling_limit = int(mc['window_sampling_limit_multiplier']) * int(mc['output_size'])

    #--------------------------------------- Train and Validation Mask --------------------------------------#
    mask_df = declare_mask_df_Cristian(mc=mc, Y_df=Y_df, X_df=X_df, S_df=S_df)

    #---------------------------------------------- Scale Data ----------------------------------------------#

    # Transform data with scale transformation (Y_df, X_df, offset, normalizer_x, normalizer_y
    # Avoid change original data
    Y_scaled_df = Y_df.copy()
    X_scaled_df = X_df.copy()

    # NO ME GUSTA QUE SE LLAME OUTSAMPLE, OUTSAMPLE SE USA COMO SINONIMO DE VALIDATION
    train_mask = mask_df.available_mask.values * mask_df.available_mask.values
    Y_scaled_df, X_scaled_df, scaler_y = scale_data(Y_df=Y_scaled_df,
                                                     X_df=X_scaled_df,
                                                     mask=train_mask,
                                                     normalizer_y = mc['normalizer_y'],
                                                     normalizer_x = mc['normalizer_x'])

    #-------------------------------------------- Declare Loaders -------------------------------------------#

    #ts_dataset = TimeSeriesDataset(Y_df=Y_scaled_df_scaled, X_df=X_df_scaled, ts_train_mask=train_outsample_mask)
    ts_dataset = TimeSeriesDataset(Y_df=Y_scaled_df, X_df=X_scaled_df, S_df=S_df, mask_df=mask_df)

    train_ts_loader = TimeSeriesLoader(model='nbeats',
                                       ts_dataset=ts_dataset,
                                       window_sampling_limit=ts_dataset.max_len,#window_sampling_limit,
                                       offset=0,
                                       input_size=int(mc['input_size_multiplier'] * mc['output_size']),
                                       output_size=int(mc['output_size']),
                                       idx_to_sample_freq=int(mc['idx_to_sample_freq']),
                                       batch_size=int(mc['batch_size']),
                                       is_train_loader=True,
                                       random_seed=int(mc['random_seed']),
                                       shuffle=True)

    val_ts_loader = TimeSeriesLoader(model='nbeats',
                                     ts_dataset=ts_dataset,
                                     window_sampling_limit=ts_dataset.max_len,
                                     offset=0,
                                     input_size=int(mc['input_size_multiplier'] * mc['output_size']),
                                     output_size=int(mc['output_size']),
                                     idx_to_sample_freq=24, #TODO: pensar esto
                                     batch_size=int(mc['batch_size']),
                                     is_train_loader=False,
                                     random_seed=int(mc['random_seed']),
                                     shuffle=False)

    mc['t_cols'] = ts_dataset.t_cols
    return mc, train_ts_loader, val_ts_loader, scaler_y


# Cell
# run_val_nbeatsx(hyperparameters, Y_df, X_df, data_augmentation, random_validation, trials, trials_file_name)


def run_val_nbeatsx(mc, Y_df, X_df, S_df, trials, trials_file_name, final_evaluation=False, return_model=False):

    # Save trials, can analyze progress
    if trials is not None:
        save_every_n_step = 5
        current_step = len(trials.trials)
        if (current_step % save_every_n_step==0):
            with open(trials_file_name, "wb") as f:
                pickle.dump(trials, f)

    start_time = time.time()

    #---------------------------------------- Parse  Hyperparameters ----------------------------------------#
    # CONSIDERO ESTO INNECESARIO
    # if data_augmentation:
    #     mc['idx_to_sample_freq'] = 1
    # else:
    #     mc['idx_to_sample_freq'] = 24

    # mc['input_size_multiplier'] = 7
    # mc['output_size'] = 24
    # mc['window_sampling_limit_multiplier'] = 365*4 # para matchear weron
    # mc['shared_weights'] = False
    # mc['n_harmonics'] = 1 # no afecta
    # mc['n_polynomials'] = 4 # no afecta
    # mc['x_s_n_hidden'] = 0 # no afecta
    # mc['weight_decay'] = 0
    # mc['lr_decay'] = 0.5 #TODO: DIFERENTE A WERON
    # mc['n_lr_decay_steps'] = 3 #TODO: DIFERENTE A WERON
    # mc['n_iterations'] = 30000 # para matchear weron 1,000*(365*4)/256
    # mc['early_stopping'] = 10 # TODO: save model para matchear weron con 20
    # mc['eval_steps'] = 100 # para matchear weron (365*24*3)/256
    # mc['train_every_n_steps'] = 1
    # mc['frequency'] = 'H' # no afecta
    # mc['seasonality'] = 24 # no afecta
    # mc['loss_hypar'] = None
    # mc['val_loss'] = mc['loss']

    # mc['n_hidden'] = len(mc['stack_types']) * [ [int(mc['n_hidden_1']), int(mc['n_hidden_2'])] ]

    # include_var_dict = {'y': [],
    #                     'Exogenous1': [],
    #                     'Exogenous2': [],
    #                     'week_day': []}

    # if mc['incl_pr1']: include_var_dict['y'].append(-2)
    # if mc['incl_pr2']: include_var_dict['y'].append(-3)
    # if mc['incl_pr3']: include_var_dict['y'].append(-4)
    # if mc['incl_pr7']: include_var_dict['y'].append(-8)

    # if mc['incl_ex1_0']: include_var_dict['Exogenous1'].append(-1)
    # if mc['incl_ex1_1']: include_var_dict['Exogenous1'].append(-2)
    # if mc['incl_ex1_7']: include_var_dict['Exogenous1'].append(-8)

    # if mc['incl_ex2_0']: include_var_dict['Exogenous2'].append(-1)
    # if mc['incl_ex2_1']: include_var_dict['Exogenous2'].append(-2)
    # if mc['incl_ex2_7']: include_var_dict['Exogenous2'].append(-8)

    # if mc['incl_day']: include_var_dict['week_day'].append(-1)
    # mc['include_var_dict'] = include_var_dict

    mc['include_var_dict'] = {'y': [-2, -3, -8],
                              'Exogenous1': [-1, -2, -8],
                              'Exogenous2': [-1, -2, -8],
                              'week_day': [-1]}

    n_hidden = int(mc['n_hidden'])
    mc['n_hidden_list'] =  2*[[n_hidden, n_hidden]]
    # print(75*'=')
    # print(pd.Series(mc))
    # print(75*'=' + '\n')

    #------------------------------------------------- Data -------------------------------------------------#

    mc, train_ts_loader, val_ts_loader, scaler_y = prepare_data_Cristian(mc=mc, Y_df=Y_df, X_df=X_df, S_df=None)
    # mc, train_ts_loader, val_ts_loader, scaler_y = prepare_data_Kin(mc=mc, Y_df=Y_df, X_df=X_df, S_df=S_df)

    #---------------------------------- Instantiate model, fit and predict ----------------------------------#

    # Instantiate and train model
    model = Nbeats(input_size_multiplier=mc['input_size_multiplier'],
                   output_size=int(mc['output_size']),
                   shared_weights=mc['shared_weights'],
                   initialization=mc['initialization'],
                   activation=mc['activation'],
                   stack_types=mc['stack_types'],
                   n_blocks=mc['n_blocks'],
                   n_layers=mc['n_layers'],
                   #n_hidden=2*[2*[int(mc['n_hidden'])]], # TODO; Revisar n_hidden1, n_hidden2 <------
                   n_hidden=mc['n_hidden_list'],
                   #n_hidden=2*[[256,256]],
                   n_harmonics=int(mc['n_harmonics']),
                   n_polynomials=int(mc['n_polynomials']),
                   x_s_n_hidden = int(mc['x_s_n_hidden']),
                   exogenous_n_channels=int(mc['exogenous_n_channels']),
                   include_var_dict=mc['include_var_dict'],
                   t_cols=mc['t_cols'],
                   batch_normalization = mc['batch_normalization'],
                   dropout_prob_theta=mc['dropout_prob_theta'],
                   dropout_prob_exogenous=mc['dropout_prob_exogenous'],
                   learning_rate=float(mc['learning_rate']),
                   lr_decay=float(mc['lr_decay']),
                   n_lr_decay_steps=float(mc['n_lr_decay_steps']),
                   weight_decay=mc['weight_decay'],
                   l1_theta=mc['l1_theta'],
                   n_iterations=int(mc['n_iterations']),
                   early_stopping=int(mc['early_stopping']),
                   loss=mc['loss'],
                   loss_hypar=float(mc['loss_hypar']),
                   val_loss=mc['val_loss'],
                   frequency=mc['frequency'],
                   seasonality=int(mc['seasonality']),
                   random_seed=int(mc['random_seed']))

    # Fit model
    model.fit(train_ts_loader=train_ts_loader, val_ts_loader=val_ts_loader, eval_steps=mc['eval_steps'])

    print('Best Model Evaluation')
    benchmark_df, average_perc_diff = forecast_evaluation_table(model=model, ts_loader=val_ts_loader)
    print(benchmark_df)
    print('\n')

    # CONDITIONAL ON CORRECT PREDICTION
    # Average percentage difference of MAE, SMAPE, MAPE, RMSE
    reported_loss = protect_nan_reported_loss(model)
    run_time = time.time() - start_time

    if reported_loss < 100:
        reported_loss = average_perc_diff

    # # Scale to original scale
    # TODO: reescalar o no reescalar a la y?
    # TODO: el modelo tendría que reescalar al interior, esto es medio feo
    # if mc['normalizer_y'] is not None:
    #     y_true, y_hat, _ = model.predict(ts_loader=val_loader, eval_mode=True)
    #     y_true = scaler_y.inv_scale(x=y_true)
    #     y_hat = scaler_y.inv_scale(x=y_hat)
    #     val_mae = mae(y=y_true, y_hat=y_hat)
    #     reported_loss = val_mae

    results =  {'loss': reported_loss,
                'loss_name': mc['val_loss'],
                'mc': mc,
                'final_insample_loss': model.final_insample_loss,
                'final_outsample_loss': model.final_outsample_loss,
                'trajectories': model.trajectories,
                'run_time': run_time,
                'status': STATUS_OK}

    if final_evaluation:
        print('Best Model Hyperpars')
        print(75*'=')
        print(pd.Series(mc))
        print(75*'='+'\n')

    if return_model:
        return model, train_ts_loader, val_ts_loader

    return results


# Cell
# TODO: eliminate n_harmonics, n_polynomials think on kwargs maybe?
# TODO: think on n_consistency for exogenous_n_channels -> n_xt_channels
# TODO: x_s_n_hidden -> n_xs_hidden
# TODO: input_size_multiplier -> Change for n_xt?
# TODO: n_hidden -> n_theta_list
def get_experiment_space(args):
    if args.space=='nbeats_cristian':
        space = {# Architecture parameters
                 'input_size_multiplier': hp.choice('input_size_multiplier', [7]),
                 'output_size': hp.choice('output_size', [24]),
                 'shared_weights': hp.choice('shared_weights', [False]),
                 'activation': hp.choice('activation', ['relu','softplus','tanh','selu','lrelu','prelu','sigmoid']),
                 'initialization':  hp.choice('initialization', ['orthogonal','he_uniform','he_normal',
                                                                 'glorot_uniform','glorot_normal','lecun_normal']),
                 'stack_types': hp.choice('stack_types', [ ['identity'],
                                                            1*['identity']+['exogenous_wavenet'],
                                                                ['exogenous_wavenet']+1*['identity'],
                                                            1*['identity']+['exogenous_tcn'],
                                                                ['exogenous_tcn']+1*['identity'] ]),
                 'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),
                 'n_layers': hp.choice('n_layers', [ [2, 2] ]),
                 'n_hidden': hp.quniform('n_hidden', 50, 500, 1),
                 'n_harmonics': hp.choice('n_harmonics', [1]),
                 'n_polynomials': hp.choice('n_polynomials', [2]),
                 'exogenous_n_channels': hp.quniform('exogenous_n_channels', 1, 10, 1),
                 'x_s_n_hidden': hp.choice('x_s_n_hidden', [0]),
                 # Regularization and optimization parameters
                 'batch_normalization': hp.choice('batch_normalization', [True, False]),
                 'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 1),
                 'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),
                 'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.1)),
                 'lr_decay': hp.choice('lr_decay', [0.5]),
                 'n_lr_decay_steps': hp.choice('n_lr_decay_steps', [3]),
                 'weight_decay': hp.loguniform('weight_decay', np.log(5e-4), np.log(0.01)),
                 'n_iterations': hp.choice('n_iterations', [args.max_epochs]),
                 'early_stopping': hp.choice('early_stopping', [40]),
                 'eval_steps': hp.choice('eval_steps', [100]),
                 #'n_val_weeks': hp.choice('n_val_weeks', [52]), # NUEVO <---------
                 'n_val_weeks': hp.choice('n_val_weeks', [52*2]), # NUEVO <---------
                 'loss': hp.choice('loss', ['MAE']),
                 'loss_hypar': hp.choice('loss_hypar', [0.5]),
                 'val_loss': hp.choice('val_loss', [args.val_loss]),
                 'l1_theta': hp.choice('l1_theta', [0, hp.loguniform('lambdal1', np.log(1e-5), np.log(1))]),
                 # Data parameters
                 'normalizer_y': hp.choice('normalizer_y', [None, 'norm', 'norm1',
                                                            'std', 'median', 'invariant']), # NUEVO <---------
                 'normalizer_x': hp.choice('normalizer_x', [None, 'norm', 'norm1',
                                                            'std', 'median', 'invariant']), # NUEVO <---------
                 'frequency': hp.choice('frequency', ['H']),
                 'seasonality': hp.choice('seasonality', [24]),
                 'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [24]), # NUEVO args <----------
                 'batch_size': hp.choice('batch_size', [128, 256, 512]), # NUEVO <---------
                 'random_seed': hp.quniform('random_seed', 1, 1000, 1)}
                 # CONSIDERO ESTO INNECESARIO
                 # 'n_hidden_1': hp.quniform('n_hidden_1', 50, 500, 1),
                 # 'n_hidden_2': hp.quniform('n_hidden_2', 50, 500, 1),
                 # 'include_var_dict': hp.choice('include_var_dict', [{'y': [-2, -3, -8],
                 #                                                     'Exogenous1': [-1, -2, -8],
                 #                                                     'Exogenous2': [-1, -2, -8],
                 #                                                     'week_day': [-1]}]),
                 # 'incl_pr1': hp.choice('incl_pr1', [True]),
                 # 'incl_pr2': hp.choice('incl_pr2', [True, False]),
                 # 'incl_pr3': hp.choice('incl_pr3', [True, False]),
                 # 'incl_pr7': hp.choice('incl_pr7', [True, False]),
                 # 'incl_ex1_0': hp.choice('incl_ex1_0', [True, False]),
                 # 'incl_ex1_1': hp.choice('incl_ex1_1', [True, False]),
                 # 'incl_ex1_7': hp.choice('incl_ex1_7', [True, False]),
                 # 'incl_ex2_0': hp.choice('incl_ex2_0', [True, False]),
                 # 'incl_ex2_1': hp.choice('incl_ex2_1', [True, False]),
                 # 'incl_ex2_7': hp.choice('incl_ex2_7', [True, False]),
                 # 'incl_day': hp.choice('incl_day', [True, False]),
                 # 'args.data_augmentation'
                 # 'n_val_weeks': hp.choice('n_val_weeks', [args.n_val_weeks]}

    elif args.space=='nbeats_collapsed':
        space= {# Architecture parameters
                'input_size_multiplier': hp.choice('input_size_multiplier', [7]),
                'output_size': hp.choice('output_size', [24]),
                'shared_weights': hp.choice('shared_weights', [False]),
                'activation': hp.choice('activation', ['relu','softplus','tanh','selu','lrelu','prelu','sigmoid']),
                'initialization':  hp.choice('initialization', ['orthogonal','he_uniform','he_normal',
                                                                'glorot_uniform','glorot_normal','lecun_normal']),
                'stack_types': hp.choice('stack_types', [['exogenous_wavenet']+1*['identity'],
                                                         ['exogenous_tcn']+1*['identity'] ]),
                'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),
                'n_layers': hp.choice('n_layers', [ [2, 2] ]),
                'n_hidden': hp.quniform('n_hidden', 50, 500, 1),
                'n_harmonics': hp.choice('n_harmonics', [1]),
                'n_polynomials': hp.choice('n_polynomials', [2]),
                'exogenous_n_channels': hp.quniform('exogenous_n_channels', 1, 10, 1),
                'x_s_n_hidden': hp.choice('x_s_n_hidden', [0]),
                # Regularization and optimization parameters
                'batch_normalization': hp.choice('batch_normalization', [False]),
                'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 1),
                'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),
                'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.1)),
                'lr_decay': hp.uniform('lr_decay', 0.3, 1.0),
                'n_lr_decay_steps': hp.choice('n_lr_decay_steps', [3]),
                'weight_decay': hp.loguniform('weight_decay', np.log(5e-5), np.log(5e-3)),
                'n_iterations': hp.choice('n_iterations', [args.max_epochs]),
                'early_stopping': hp.choice('early_stopping', [40]),
                'eval_steps': hp.choice('eval_steps', [10]),
                'n_val_weeks': hp.choice('n_val_weeks', [52*2]), # NUEVO <---------
                'loss': hp.choice('loss', ['PINBALL']),
                'loss_hypar': hp.uniform('loss_hypar', 0.48, 0.51),
                'val_loss': hp.choice('val_loss', [args.val_loss]),
                'l1_theta': hp.choice('l1_theta', [0, hp.loguniform('lambdal1', np.log(1e-5), np.log(1))]),
                # Data parameters
                'normalizer_y': hp.choice('normalizer_y', [None]),
                'normalizer_x': hp.choice('normalizer_x', ['median']),
                'frequency': hp.choice('frequency', ['H']),
                'seasonality': hp.choice('seasonality', [24]),
                'include_var_dict': hp.choice('include_var_dict', [{'y': [-2, -3, -8],
                                                                    'Exogenous1': [-1, -2, -8],
                                                                    'Exogenous2': [-1, -2, -8],
                                                                    'week_day': [-1]}]),
                'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [24]),
                'batch_size': hp.choice('batch_size', [256]),
                'random_seed': hp.quniform('random_seed', 10, 20, 1)}

    else:
        print(f'Experiment space {args.space} not available')

    return space

def parse_trials(trials):
    # Initialize
    trials_dict = {'tid': [], 'loss': [], 'trajectories': [], 'mc': []}
    for tidx in range(len(trials)):
        # Main
        trials_dict['tid']  += [trials.trials[tidx]['tid']]
        trials_dict['loss'] += [trials.trials[tidx]['result']['loss']]
        trials_dict['trajectories'] += [trials.trials[tidx]['result']['trajectories']]

        # Model Configs
        mc = trials.trials[tidx]['result']['mc']
        trials_dict['mc'] += [mc]

    trials_df = pd.DataFrame(trials_dict)
    return trials_df

def main(args):
    #---------------------------------------------- Directories ----------------------------------------------#

    dataset = eval(args.dataset)
    dataset_str = dataset[0]
    for market in dataset[1:]:
        dataset_str += f'{market}_'
    output_dir = f'./results/{dataset_str}/{args.space}/'
    os.makedirs(output_dir, exist_ok = True)
    assert os.path.exists(output_dir), f'Output dir {output_dir} does not exist'

    hyperopt_file = output_dir + f'hyperopt_{args.experiment_id}.p'
    result_test_file = output_dir + f'result_test_{args.experiment_id}.p'

    #---------------------------------------------- Read  Data ----------------------------------------------#
    print('\n'+75*'-')
    print(28*'-', 'Preparing Dataset', 28*'-')
    print(75*'-'+'\n')

    #TEST_DATE = {'NP': '2016-12-27',
    #             'PJM':'2016-12-27',
    #             'BE':'2015-01-04',
    #             'FR': '2015-01-04',
    #             'DE':'2016-01-04'}
    #test_date = TEST_DATE[args.dataset]
    #Y_insample_df, X_insample_df, Y_outsample_df, X_outsample_df, _ = load_epf(directory='../data/',
    #                                                                           market=args.dataset,
    #                                                                           first_date_test=test_date,
    #                                                                           days_in_test=728)
    Y_df, X_df, S_df = EPF.load_groups(directory='data', groups=dataset)

    #-------------------------------------- Hyperparameter Optimization --------------------------------------#

    if not os.path.isfile(hyperopt_file):
        print('\n'+75*'-')
        print(22*'-', 'Start Hyperparameter  tunning', 22*'-')
        print(75*'-'+'\n')

        space = get_experiment_space(args)

        trials = Trials()
        fmin_objective = partial(run_val_nbeatsx, Y_df=Y_df, X_df=X_df, S_df=S_df,
                                 trials=trials, trials_file_name=hyperopt_file)
        fmin(fmin_objective, space=space, algo=tpe.suggest, max_evals=args.hyperopt_iters, trials=trials, verbose=True)

        # Save output
        with open(hyperopt_file, "wb") as f:
            pickle.dump(trials, f)

    print('\n'+75*'-')
    print(20*'-', 'Hyperparameter  tunning  finished', 20*'-')
    print(75*'-'+'\n')

    #----------------------------------------- Selected 'Best' Model -----------------------------------------#

    # Read and parse trials pickle
    trials = pickle.load(open(hyperopt_file, 'rb'))
    trials_df = parse_trials(trials)

    # Get best mc
    idx = trials_df.loss.idxmin()
    best_mc = trials_df.loc[idx]['mc']

    run_val_nbeatsx(best_mc, Y_df=Y_df, X_df=X_df, S_df=S_df,
                    trials=trials, trials_file_name=hyperopt_file, final_evaluation=True)

def parse_args():
    desc = "NBEATSx overfit"
    parser = argparse.ArgumentParser(description=desc)

    parser.add_argument('--dataset', type=str, required=True, help='NP')
    parser.add_argument('--space', type=str, required=True, help='Experiment hyperparameter space')
    parser.add_argument('--hyperopt_iters', type=int, help='hyperopt_iters')
    parser.add_argument('--max_epochs', type=int, required=False, default=2000, help='max train epochs')
    parser.add_argument('--val_loss', type=str, required=False, default=None, help='validation loss')
    parser.add_argument('--experiment_id', default=None, required=False, type=str, help='string to identify experiment')
    return parser.parse_args()


# Cell
if __name__ == '__main__':

    # parse arguments
    args = parse_args()
    if args is None:
        exit()

    main(args)

# CUDA_VISIBLE_DEVICES=0 PYTHONPATH=. python src/overfit_nbeatsx.py --dataset "['NP']" --space 'nbeats_collapsed' --hyperopt_iters 200 --val_loss 'MAE'   --experiment_id 'MAE_eval_2021_01_15'
# CUDA_VISIBLE_DEVICES=0 PYTHONPATH=. python src/overfit_nbeatsx.py --dataset "['NP']" --space 'nbeats_collapsed' --hyperopt_iters 200 --val_loss 'MAPE'  --experiment_id 'MAPE_eval_2021_01_15'
# CUDA_VISIBLE_DEVICES=1 PYTHONPATH=. python src/overfit_nbeatsx.py --dataset "['NP']" --space 'nbeats_collapsed' --hyperopt_iters 200 --val_loss 'SMAPE' --experiment_id 'SMAPE_eval_2021_01_15'
# CUDA_VISIBLE_DEVICES=1 PYTHONPATH=. python src/overfit_nbeatsx.py --dataset "['NP']" --space 'nbeats_collapsed' --hyperopt_iters 200 --val_loss 'RMSE'  --experiment_id 'RMSE_eval_2021_01_15'