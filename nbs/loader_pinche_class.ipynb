{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.ts_loader_pinche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch as t\n",
    "import copy\n",
    "from fastcore.foundation import patch\n",
    "from nixtla.data.ts_dataset import TimeSeriesDataset\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TimeSeriesLoader(object):\n",
    "    def __init__(self,\n",
    "                 ts_dataset:TimeSeriesDataset,\n",
    "                 model:str,\n",
    "                 offset:int,\n",
    "                 window_sampling_limit: int, \n",
    "                 input_size: int,\n",
    "                 output_size: int,\n",
    "                 idx_to_sample_freq: int, #TODO: not active yet\n",
    "                 batch_size: int,\n",
    "                 train_loader: bool):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.ts_dataset = ts_dataset # Pass by reference\n",
    "        self.model = model\n",
    "        self.offset = offset\n",
    "        self.window_sampling_limit = window_sampling_limit\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.idx_to_sample_freq = idx_to_sample_freq\n",
    "        self.batch_size = batch_size\n",
    "        self.train_loader = train_loader\n",
    "        self._is_train = True\n",
    "\n",
    "        assert offset < self.ts_dataset.max_len, 'Offset must be smaller than max_len'\n",
    "\n",
    "        self.window_sampling_idx = self._update_windows_sampling_idx()\n",
    "        \n",
    "        #TODO: cambiar estos prints\n",
    "        # print('X: time series features, of shape (#series,#times,#features): \\t' + str(X.shape))\n",
    "        # print('Y: target series (in X), of shape (#series,#times): \\t \\t' + str(Y.shape))\n",
    "        # print('S: static features, of shape (#series,#features): \\t \\t' + str(S.shape))\n",
    "\n",
    "    def _update_windows_sampling_idx(self):\n",
    "        # Filter sampling_mask with offset and window_sampling_limit\n",
    "        last_ds = self.ts_dataset.max_len - self.offset\n",
    "        first_ds = max(last_ds - self.window_sampling_limit, 0)\n",
    "        filtered_outsample_mask = self.ts_dataset.ts_tensor[:, self.ts_dataset.t_cols.index('outsample_mask'), first_ds:last_ds]\n",
    "        filtered_ts_train_mask = self.ts_dataset.ts_train_mask[first_ds:last_ds]\n",
    "\n",
    "        # Get indices of train/validation windows\n",
    "        if self.train_loader:\n",
    "            train_mask =  filtered_outsample_mask * filtered_ts_train_mask\n",
    "            indices = np.argwhere(train_mask > 0)\n",
    "        else:\n",
    "            val_mask = filtered_outsample_mask * (1-filtered_ts_train_mask)\n",
    "            indices = np.argwhere(val_mask > 0)\n",
    "\n",
    "        #To change relative position of filtered tensor to global position\n",
    "        indices[:, 1] += first_ds\n",
    "\n",
    "        #Loop for each serie to extract window_sampling_idx\n",
    "        window_sampling_idx = []\n",
    "        for i in range(self.ts_dataset.n_series):\n",
    "            ts_idx = indices[indices[:, 0] == i]\n",
    "            window_sampling_idx.append(list(ts_idx[:, 1]))\n",
    "        return window_sampling_idx\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            if self._is_train:\n",
    "                sampled_ts_indices = np.random.randint(self.ts_dataset.n_series, size=self.batch_size)\n",
    "            else:\n",
    "                sampled_ts_indices = range(self.ts_dataset.n_series)\n",
    "\n",
    "            batch_dict = defaultdict(list)\n",
    "            for index in sampled_ts_indices:\n",
    "                batch_i = self.__get_item__(index)\n",
    "                for key in batch_i:\n",
    "                    batch_dict[key].append(batch_i[key])\n",
    "\n",
    "            batch = defaultdict(list)\n",
    "            for key in batch_dict:\n",
    "                batch[key] = np.stack(batch_dict[key])\n",
    "\n",
    "            yield batch\n",
    "\n",
    "    def __get_item__(self, index):\n",
    "        if self.model == 'nbeats':\n",
    "            return self._nbeats_batch(index)\n",
    "        elif self.model == 'esrnn':\n",
    "            assert 1<0, 'hacer esrnn'\n",
    "        else:\n",
    "            assert 1<0, 'error'\n",
    "\n",
    "    def _nbeats_batch(self, index):\n",
    "        insample = np.zeros((self.ts_dataset.n_channels, self.input_size), dtype=float)\n",
    "        insample_mask = np.zeros(self.input_size)\n",
    "        outsample = np.zeros((self.ts_dataset.n_channels, self.output_size), dtype=float)\n",
    "        outsample_mask = np.zeros(self.output_size)\n",
    "            \n",
    "        ts = self.ts_dataset.ts_tensor[index]\n",
    "        len_ts = self.ts_dataset.len_series[index]\n",
    "        init_ts = max(self.ts_dataset.max_len-len_ts, self.ts_dataset.max_len-self.offset-self.window_sampling_limit) #TODO: precomputar en loader\n",
    "\n",
    "        assert self.ts_dataset.max_len-self.offset > init_ts, f'Offset too big for serie {index}'\n",
    "        if self._is_train:\n",
    "            cut_point = np.random.choice(self.window_sampling_idx[index],1)[0] # Sampling from available cuts for ts \"index\"\n",
    "        else:\n",
    "            cut_point = max(self.ts_dataset.max_len-self.offset, self.input_size)\n",
    "        \n",
    "        insample_window = ts[:-2, max(0, cut_point - self.input_size):cut_point] # se saca mask channel del final\n",
    "        insample_mask_start = min(self.input_size, cut_point - init_ts) #In case cut_point is close to init_ts, because series are padded\n",
    "        # print('ts', ts)\n",
    "        # print('insample', insample)\n",
    "        # print('insample_window', insample_window)\n",
    "        # print('self.window_sampling_idx[index]',self.window_sampling_idx[index])\n",
    "        # print('cut_point', cut_point)\n",
    "        # print('----')\n",
    "        insample[:, -insample_window.shape[1]:] = insample_window\n",
    "        insample_mask[-insample_mask_start:] = 1.0\n",
    "\n",
    "        if self._is_train:\n",
    "            #se saca mask channel del final\n",
    "            outsample_window = ts[:-2, cut_point:min(self.ts_dataset.max_len - self.offset, cut_point + self.output_size)]\n",
    "        else:\n",
    "            #se saca mask channel del final\n",
    "            outsample_window = ts[:-2, cut_point:min(self.ts_dataset.max_len, cut_point + self.output_size)]\n",
    "\n",
    "        # First mask is to filter after offset, second mask to filter ts validation\n",
    "        outsample[:, :outsample_window.shape[1]] = outsample_window \n",
    "        outsample_mask[:outsample_window.shape[1]] = 1.0\n",
    "        outsample_mask[:outsample_window.shape[1]] = outsample_mask[:outsample_window.shape[1]] * \\\n",
    "                                                     self.ts_dataset.ts_train_mask[cut_point:(cut_point+outsample_window.shape[1])]\n",
    "\n",
    "        insample_y = insample[0, :]\n",
    "        insample_x_t = insample[1:, :]\n",
    "\n",
    "        outsample_y = outsample[0, :]\n",
    "        outsample_x_t = outsample[1:, :]\n",
    "\n",
    "        x_s = self.ts_dataset.x_s[index, :]\n",
    "\n",
    "        sample = {'insample_y':insample_y, 'insample_x_t':insample_x_t, 'insample_mask':insample_mask,\n",
    "                  'outsample_y':outsample_y, 'outsample_x_t':outsample_x_t, 'outsample_mask':outsample_mask,\n",
    "                  'x_s':x_s}\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def update_offset(self, offset):\n",
    "        if offset == self.offset:\n",
    "            return # Avoid extra computation\n",
    "        self.offset = offset\n",
    "\n",
    "    def get_meta_data_var(self, var):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return self.ts_dataset.get_meta_data_var(var)\n",
    "\n",
    "    def get_n_variables(self):\n",
    "        return self.ts_dataset.n_x_t, self.ts_dataset.n_s_t\n",
    "\n",
    "    def get_n_series(self):\n",
    "        return self.ts_dataset.n_series\n",
    "\n",
    "    def get_max_len(self):\n",
    "        return self.ts_dataset.max_len\n",
    "\n",
    "    def get_n_channels(self):\n",
    "        return self.ts_dataset.n_channels\n",
    "\n",
    "    def get_frequency(self):\n",
    "        return self.ts_dataset.frequency\n",
    "\n",
    "    def train(self):\n",
    "        self._is_train = True\n",
    "\n",
    "    def eval(self):\n",
    "        self._is_train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing dataframes ...\nCreating ts tensor ...\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         3.73856371e+04, 3.84319699e+04, 4.03453300e+04],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         3.91049000e+05, 4.54041000e+05, 5.52942000e+05],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.80507000e+03, 2.13865000e+03, 2.68214000e+03],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         4.88040000e+00, 5.38680000e+00, 5.74060000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         2.32400000e+01, 2.52900000e+01, 2.73360000e+01],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         9.33115600e+06, 1.01999940e+07, 1.17027350e+07],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00]]])"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "from nixtla.data.datasets.tourism import Tourism, TourismInfo\n",
    "tourism_dataset = Tourism.load(directory='data', group=TourismInfo.groups[0])\n",
    "tourism_dataset.ts_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_loader = TimeSeriesLoader(ts_dataset= tourism_dataset,\n",
    "                             model='nbeats',\n",
    "                             offset = 0,\n",
    "                             window_sampling_limit=50, \n",
    "                             input_size=8,\n",
    "                             output_size=4,\n",
    "                             idx_to_sample_freq=1,\n",
    "                             batch_size=1024,\n",
    "                             train_loader=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader = iter(ts_loader)\n",
    "batch = next(dataloader)\n",
    "insample_y = batch['insample_y']\n",
    "insample_x_t = batch['insample_x_t']\n",
    "insample_mask = batch['insample_mask']\n",
    "outsample_x_t = batch['outsample_x_t']\n",
    "outsample_y = batch['outsample_y']\n",
    "outsample_mask = batch['outsample_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[     0.,      0., 522300., ..., 551400., 556200., 602800.],\n",
       "       [  6168.,   7198.,   8833., ...,  15460.,  20253.,  19508.],\n",
       "       [     0.,      0.,      0., ..., 106644., 105738., 108639.],\n",
       "       ...,\n",
       "       [  1012.,    949.,    858., ...,   2086.,   2639.,   2515.],\n",
       "       [ 15866.,  17340.,  19628., ...,  46490.,  66842.,  81357.],\n",
       "       [ 21865.,  24574.,  27116., ...,  34435.,  36540.,  41579.]])"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "insample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[615200., 713500., 752400., 763400.],\n",
       "       [ 22882.,      0.,      0.,      0.],\n",
       "       [109548., 121985., 104198., 100421.],\n",
       "       ...,\n",
       "       [  2913.,   2487.,   2495.,   2690.],\n",
       "       [ 88909.,  68585.,  49149.,  61899.],\n",
       "       [ 41118.,  44175.,      0.,      0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "outsample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "outsample_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.94140625"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "outsample_mask.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "52b0028e7074a0058398558ea661882eddbb489e66a28504cc0449d2f54d6290"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}