{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.datasets.m3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M3 dataset\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nixtla.data.datasets.utils import download_file, Info, TimeSeriesDataclass\n",
    "from nixtla.data.ts_dataset import TimeSeriesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "SOURCE_URL = 'https://forecasters.org/data/m3comp/M3C.xls'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tourism meta information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class Yearly:\n",
    "    seasonality: int = 1\n",
    "    horizon: int = 6\n",
    "    freq: str = 'Y'\n",
    "    sheet_name: str = 'M3Year'\n",
    "    name: str = 'Yearly'\n",
    "\n",
    "@dataclass\n",
    "class Quarterly:\n",
    "    seasonality: int = 4\n",
    "    horizon: int = 8\n",
    "    freq: str = 'Q'\n",
    "    sheet_name: str = 'M3Quart'\n",
    "    name: str = 'Quarterly'\n",
    "\n",
    "@dataclass\n",
    "class Monthly:\n",
    "    seasonality: int = 12\n",
    "    horizon: int = 18\n",
    "    freq: str = 'M'\n",
    "    sheet_name: str = 'M3Month'\n",
    "    name: str = 'Monthly'\n",
    "\n",
    "@dataclass\n",
    "class Other:\n",
    "    seasonality: int = 1\n",
    "    horizon: int = 8\n",
    "    freq: str = 'D'\n",
    "    sheet_name: str = 'M3Other'\n",
    "    name: str = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "M3Info = Info(groups=('Yearly', 'Quarterly', 'Monthly', 'Other'),\n",
    "              class_groups=(Yearly, Quarterly, Monthly, Other))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _return_year(ts):\n",
    "    year = ts.iloc[0]\n",
    "    year = year if year != 0 else 1970\n",
    "\n",
    "    return year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class M3(TimeSeriesDataclass):\n",
    "\n",
    "    @staticmethod\n",
    "    def load(directory: str,\n",
    "             group: str,\n",
    "             training: bool = True,\n",
    "             return_tensor: bool = True) -> Union[TimeSeriesDataset, TimeSeriesDataclass]:\n",
    "        \"\"\"\n",
    "        Downloads and loads M3 data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        directory: str\n",
    "            Directory where data will be downloaded.\n",
    "        group: str\n",
    "            Group name.\n",
    "            Allowed groups: 'Yearly', 'Quarterly', 'Monthly', 'Other'.\n",
    "        training: bool\n",
    "            Wheter return training or testing data. Default True.\n",
    "        return_tensor: bool\n",
    "            Wheter return TimeSeriesDataset (tensors, True) or\n",
    "            TimeSeriesDataclass (dataframes)\n",
    "        \"\"\"\n",
    "        path = Path(directory) / 'm3' / 'datasets'\n",
    "\n",
    "        M3.download(directory)\n",
    "\n",
    "        class_group = M3Info.get_group(group)\n",
    "\n",
    "        df = pd.read_excel(path / 'M3C.xls', sheet_name=class_group.sheet_name)\n",
    "\n",
    "        df = df.rename(columns={'Series': 'unique_id',\n",
    "                                'Category': 'category',\n",
    "                                'Starting Year': 'year',\n",
    "                                'Starting Month': 'month'})\n",
    "\n",
    "        df['unique_id'] = [class_group.name[0] + str(i + 1) for i in range(len(df))]\n",
    "        S = df.filter(items=['unique_id', 'category'])\n",
    "\n",
    "        id_vars = list(df.columns[:6])\n",
    "        df = pd.melt(df, id_vars=id_vars, var_name='ds', value_name='y')\n",
    "        df = df.dropna().sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "\n",
    "        freq = pd.tseries.frequencies.to_offset(class_group.freq)\n",
    "\n",
    "        if group == 'Other':\n",
    "            df['year'] = 1970\n",
    "\n",
    "        df['ds'] = df.groupby('unique_id')['year'] \\\n",
    "                     .transform(lambda df: pd.date_range(f'{_return_year(df)}-01-01',\n",
    "                                                         periods=df.shape[0],\n",
    "                                                         freq=freq))\n",
    "\n",
    "        df = df.filter(items=['unique_id', 'ds', 'y'])\n",
    "\n",
    "        if training:\n",
    "            df = df.groupby('unique_id').apply(lambda df: df.head(-class_group.horizon)).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.groupby('unique_id').tail(class_group.horizon)\n",
    "            df['ds'] = df.groupby('unique_id').cumcount() + 1\n",
    "        \n",
    "        if return_tensor:\n",
    "            S['category'] = S['category'].astype('category').cat.codes\n",
    "            return TimeSeriesDataset(y_df=df, X_s_df=S, X_t_df=None)\n",
    "        else:\n",
    "            return TimeSeriesDataclass(Y=df, S=S, X=None, idx_categorical_static=[0], group=group)\n",
    "\n",
    "    @staticmethod\n",
    "    def download(directory: Path) -> None:\n",
    "        \"\"\"Download M3 Dataset.\"\"\"\n",
    "        path = Path(directory) / 'm3' / 'datasets'\n",
    "        if not path.exists():\n",
    "            download_file(path, SOURCE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "\n",
    "1. Hay series mensuales que no tienen a침o de inicio, en este caso iniciar치n en 1970.\n",
    "2. Las series `Other` no tienen fecha como tal. Ser치n consideradas como diarias y empezar치n en 1970."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yearly\n",
      "Processing dataframes ...\n",
      "Creating ts tensor ...\n",
      "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.8076e+03,\n",
      "          4.3879e+03, 4.9370e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.8920e+03,\n",
      "          9.3440e+03, 9.5510e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.1660e+03,\n",
      "          2.2100e+03, 2.5400e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.9186e+03,\n",
      "          4.1738e+03, 4.1376e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.7190e+03,\n",
      "          2.6340e+03, 2.4850e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.1867e+03,\n",
      "          3.7547e+03, 3.8520e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]]])\n",
      "Quarterly\n",
      "Processing dataframes ...\n",
      "Creating ts tensor ...\n",
      "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.5921e+03,\n",
      "          5.4816e+03, 5.5115e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.7312e+03,\n",
      "          6.9847e+03, 6.8205e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.7340e+03,\n",
      "          3.7840e+03, 3.9740e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.7662e+03,\n",
      "          7.9711e+03, 8.1595e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.3112e+03,\n",
      "          6.4079e+03, 7.5107e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.0092e+03,\n",
      "          4.3792e+03, 6.4265e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]]])\n",
      "Monthly\n",
      "Processing dataframes ...\n",
      "Creating ts tensor ...\n",
      "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.8800e+03,\n",
      "          2.6400e+03, 2.4000e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.4600e+03,\n",
      "          6.2600e+03, 6.1400e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.2300e+03,\n",
      "          6.2300e+03, 5.4800e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.9490e+03,\n",
      "          6.0515e+03, 5.9895e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.6445e+03,\n",
      "          5.6715e+03, 5.7120e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.2188e+03,\n",
      "          5.2616e+03, 5.2471e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]]])\n",
      "Other\n",
      "Processing dataframes ...\n",
      "Creating ts tensor ...\n",
      "tensor([[[3.0604e+03, 3.0212e+03, 3.3011e+03,  ..., 4.5093e+03,\n",
      "          4.5472e+03, 4.5425e+03],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        [[6.1538e+03, 6.1257e+03, 6.1414e+03,  ..., 5.7520e+03,\n",
      "          5.6740e+03, 5.7313e+03],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.9100e+03,\n",
      "          1.9260e+03, 1.9400e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.9750e+03,\n",
      "          2.9450e+03, 2.8650e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.1240e+03,\n",
      "          2.0920e+03, 2.0530e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.9476e+04,\n",
      "          2.9073e+04, 2.8648e+04],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "for group in M3Info.groups:\n",
    "    print(group)\n",
    "    m3_dataset = M3.load(directory='data', group=group)\n",
    "    print(m3_dataset.ts_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nixtla",
   "language": "python",
   "name": "nixtla"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
