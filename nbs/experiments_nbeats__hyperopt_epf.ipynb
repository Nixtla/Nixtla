{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c68c0f618c4f787b1e02bfee278e48d25b62407cb335aab5258cedc4db4ced"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiments.nbeats.hyperopt_epf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import time\n",
    "import os\n",
    "# Limit number of threads in numpy and others to avoid throttling\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\" # export OMP_NUM_THREADS=4\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"2\" # export OPENBLAS_NUM_THREADS=4 \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"3\" # export MKL_NUM_THREADS=6\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"2\" # export VECLIB_MAXIMUM_THREADS=4\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"3\" # export NUMEXPR_NUM_THREADS=6\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import pickle\n",
    "import glob\n",
    "import itertools\n",
    "import random\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "from nixtla.data.scalers import Scaler\n",
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset\n",
    "from nixtla.data.tsloader_fast import TimeSeriesLoader\n",
    "from nixtla.losses.numpy import mae, mape, smape, rmse, pinball_loss\n",
    "\n",
    "# Models\n",
    "from nixtla.models.nbeats.nbeats import Nbeats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK"
   ]
  },
  {
   "source": [
    "# DATA WRANGLING AND EVALUATION UTILS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# TODO: Think and test new mask_df and datasets with random validation\n",
    "\n",
    "############################################################################################\n",
    "#### COMMON\n",
    "############################################################################################\n",
    "\n",
    "BENCHMARK_DF = pd.DataFrame({'id': [1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4],\n",
    "                             'unique_id': ['NP', 'NP', 'NP', 'NP', 'PJM', 'PJM', 'PJM', 'PJM',\n",
    "                                           'BE', 'BE', 'BE', 'BE', 'FR', 'FR', 'FR', 'FR'],\n",
    "                             'metric': ['MAE', 'MAPE', 'SMAPE', 'RMSE', 'MAE', 'MAPE', 'SMAPE', 'RMSE',\n",
    "                                        'MAE', 'MAPE', 'SMAPE', 'RMSE', 'MAE', 'MAPE', 'SMAPE', 'RMSE'],    \n",
    "                             'DNN' : [1.67, 5.38, 4.85, 3.33, 2.78, 28.66, 11.22, 4.64, 5.82,\n",
    "                                      26.11, 13.33, 16.13, 3.91, 14.77, 10.98, 11.74]})\n",
    "\n",
    "def forecast_evaluation_table(y_total, y_hat_total, meta_data):\n",
    "    performances = []\n",
    "    for i, market_data in enumerate(meta_data):\n",
    "        market = market_data['unique_id']\n",
    "        \n",
    "        y = y_total[i,:,:].reshape(-1)\n",
    "        y_hat = y_hat_total[i,:,:].reshape(-1)\n",
    "\n",
    "        _mae   = np.round(mae(y=y, y_hat=y_hat),5)\n",
    "        _mape  = np.round(mape(y=y, y_hat=y_hat),5)\n",
    "        _smape = np.round(smape(y=y, y_hat=y_hat),5)\n",
    "        _rmse  = np.round(rmse(y=y, y_hat=y_hat),5)\n",
    "\n",
    "        performance_df = pd.DataFrame({'unique_id': [market]*4,\n",
    "                                       'metric': ['MAE', 'MAPE', 'SMAPE', 'RMSE'],\n",
    "                                       'NBEATSx': [_mae, _mape, _smape, _rmse]})\n",
    "\n",
    "        performances += [performance_df]\n",
    "\n",
    "    performances_df = pd.concat(performances)\n",
    "\n",
    "    benchmark_df = BENCHMARK_DF.merge(performances_df, on=['unique_id', 'metric'], how='left')\n",
    "    benchmark_df['perc_diff'] = 100 * (benchmark_df['NBEATSx']-benchmark_df['DNN'])/benchmark_df['DNN']\n",
    "    benchmark_df['improvement'] = benchmark_df['perc_diff'] < 0\n",
    "    benchmark_df = benchmark_df.dropna()\n",
    "    average_perc_diff = benchmark_df['perc_diff'].mean()\n",
    "\n",
    "    y_tot = y_total.reshape(-1)\n",
    "    y_total_nans_perc = np.sum((np.isnan(y_tot)))  / len(y_tot)\n",
    "    print(f'y_total {len(y_tot)} nan_perc {y_total_nans_perc}')    \n",
    "    print(\"average_perc_diff\", average_perc_diff)\n",
    "    if y_total_nans_perc <= 0.95: average_perc_diff=100\n",
    "\n",
    "    return benchmark_df, average_perc_diff\n",
    "\n",
    "def protect_nan_reported_loss(model):\n",
    "    # TODO: Pytorch numerical error hacky protection, protect from losses.numpy.py\n",
    "    reported_loss = model.final_outsample_loss\n",
    "    if np.isnan(model.final_insample_loss):\n",
    "        reported_loss = 500\n",
    "    if model.final_insample_loss<=0:\n",
    "        reported_loss = 500\n",
    "\n",
    "    if np.isnan(model.final_outsample_loss):\n",
    "        reported_loss = 500    \n",
    "    if model.final_outsample_loss<=0:\n",
    "        reported_loss = 500    \n",
    "    return reported_loss\n",
    "\n",
    "\n",
    "def get_last_n_timestamps_mask_df(Y_df, n_timestamps):\n",
    "    # Creates outsample_mask\n",
    "    # train_mask: 1 last_n_timestamps, 0 timestamps until last_n_timestamps\n",
    "\n",
    "    last_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    last_df.sort_values(by=['unique_id', 'ds'], inplace=True, ascending=False)\n",
    "    last_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    last_df = last_df.groupby('unique_id').head(n_timestamps)\n",
    "    last_df['mask'] = 1\n",
    "\n",
    "    last_df = last_df[['unique_id', 'ds', 'mask']]\n",
    "\n",
    "    mask_df = Y_df.merge(last_df, on=['unique_id', 'ds'], how='left')\n",
    "    mask_df['mask'] = mask_df['mask'].fillna(0)    # The first len(Y)-n_hours used as train\n",
    "\n",
    "    mask_df = mask_df[['unique_id', 'ds', 'mask']]\n",
    "    mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "\n",
    "    assert len(mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "\n",
    "    return mask_df\n",
    "\n",
    "def balance_data(Y_df, X_df):\n",
    "    # Create balanced placeholder dataframe\n",
    "    balance_ids = {'unique_id': Y_df.unique_id.unique(),\n",
    "                   'ds': Y_df.ds.unique()}\n",
    "\n",
    "    product_list = list(itertools.product(*list(balance_ids.values())))\n",
    "    balance_df = pd.DataFrame(product_list, columns=list(balance_ids.keys()))\n",
    "    \n",
    "    # Balance with merge\n",
    "    Y_balanced_df = balance_df.merge(Y_df, on=['unique_id', 'ds'], how='left')\n",
    "    X_balanced_df = balance_df.merge(X_df, on=['unique_id', 'ds'], how='left')\n",
    "\n",
    "    print('\\n')\n",
    "    print('Y_df.shape \\t', Y_df.shape)\n",
    "    print('X_df.shape \\t', X_df.shape)\n",
    "    print('Y_balanced_df.shape \\t', Y_balanced_df.shape)\n",
    "    print('X_balanced_df.shape \\t', X_balanced_df.shape)\n",
    "\n",
    "    return Y_balanced_df, X_balanced_df\n",
    "\n",
    "def scale_data(Y_df, X_df, mask, normalizer_y, normalizer_x):\n",
    "    y_shift = None\n",
    "    y_scale = None\n",
    "\n",
    "    mask = mask.astype(int)\n",
    "    \n",
    "    if normalizer_y is not None:\n",
    "        scaler_y = Scaler(normalizer=normalizer_y)\n",
    "        Y_df['y'] = scaler_y.scale(x=Y_df['y'].values, mask=mask)\n",
    "    else:\n",
    "        scaler_y = None\n",
    "    # Exogenous are always scaled to help learning\n",
    "    if normalizer_x is not None:\n",
    "        scaler_x = Scaler(normalizer=normalizer_x)\n",
    "        X_df['Exogenous1'] = scaler_x.scale(x=X_df['Exogenous1'].values, mask=mask)\n",
    "\n",
    "        scaler_x = Scaler(normalizer=normalizer_x)\n",
    "        X_df['Exogenous2'] = scaler_x.scale(x=X_df['Exogenous2'].values, mask=mask)\n",
    "\n",
    "    filter_variables = ['unique_id', 'ds', 'Exogenous1', 'Exogenous2', 'week_day'] + \\\n",
    "                       [col for col in X_df if (col.startswith('day'))]\n",
    "                       #[col for col in X_df if (col.startswith('_hour_'))]\n",
    "    X_df = X_df[filter_variables]\n",
    "\n",
    "    return Y_df, X_df, scaler_y\n",
    "\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "#### KIN\n",
    "############################################################################################\n",
    "\n",
    "def prepare_data_Kin(mc, Y_df, X_df, S_df, n_timestamps_pred=365*1*24, offset=0): #offset=365*1*24 tira un aÃ±o\n",
    "    # n_timestamps_pred defines number of hours ahead to predict\n",
    "    # offset defines the shift of the data to simulate rolling window\n",
    "    assert offset % n_timestamps_pred == 0, 'Avoid overlap of predictions, redefine n_timestamps_pred or offset'\n",
    "    Y_df = Y_df.head(len(Y_df)-offset)\n",
    "    X_df = X_df.head(len(X_df)-offset)\n",
    "\n",
    "\n",
    "    ################################################################################\n",
    "    Y_df.y = Y_df.y + 5 ### <----------------------- MAPE, SMAPE hypothesis\n",
    "    ################################################################################\n",
    "\n",
    "    #-------------------------------------------- Data Wrangling --------------------------------------------#\n",
    "    Y_balanced_df, X_balanced_df = balance_data(Y_df, X_df)\n",
    "    del Y_df, X_df\n",
    "\n",
    "    #------------------------------------- Available and Validation Mask ------------------------------------#\n",
    "    # mask: 1 last_n_timestamps, 0 timestamps until last_n_timestamps\n",
    "    last_timestamps_df = get_last_n_timestamps_mask_df(Y_df=Y_balanced_df, n_timestamps=n_timestamps_pred)\n",
    "    mask_df = last_timestamps_df.copy()\n",
    "    mask_df['available_mask'] = (1-Y_balanced_df.y.isnull().values)\n",
    "    mask_df['sample_mask'] = (1-last_timestamps_df['mask'])\n",
    "    del last_timestamps_df\n",
    "\n",
    "    # Checking split\n",
    "    #y_train = Y_balanced_df[mask_df.sample_mask==1].y.values\n",
    "    #y_val = Y_balanced_df[mask_df.sample_mask==0].y.values\n",
    "    #plt.plot(y_train, label='train', color='blue')\n",
    "    #plt.plot(np.array(range(len(y_val))) + len(y_train), y_val, label='val', color='purple')\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    #---------------------------------------------- Scale Data ----------------------------------------------#\n",
    "\n",
    "    # Scale data # TODO: write sample_mask conditional/groupby(['unique_id]) scaling\n",
    "    train_mask = mask_df.available_mask.values * mask_df.available_mask.values\n",
    "    Y_scaled_df, X_scaled_df, scaler_y = scale_data(Y_df=Y_balanced_df, X_df=X_balanced_df, mask=train_mask,\n",
    "                                                    normalizer_y=mc['normalizer_y'], normalizer_x=mc['normalizer_x'])\n",
    "    del Y_balanced_df\n",
    "    del X_balanced_df\n",
    "\n",
    "    #-------------------------------------------- Declare Loaders -------------------------------------------#\n",
    "\n",
    "    ts_dataset = TimeSeriesDataset(Y_df=Y_scaled_df, X_df=X_scaled_df, S_df=S_df, mask_df=mask_df)\n",
    "\n",
    "    train_ts_loader = TimeSeriesLoader(ts_dataset=ts_dataset,\n",
    "                                       model='nbeats',\n",
    "                                       offset=0,\n",
    "                                       window_sampling_limit=ts_dataset.max_len,\n",
    "                                       input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                                       output_size=int(mc['output_size']),\n",
    "                                       idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                       batch_size=int(mc['batch_size']),\n",
    "                                       is_train_loader=True,\n",
    "                                       shuffle=True, random_seed=int(mc['random_seed']))\n",
    "\n",
    "    print(\"train_ts_loader.ts_windows.shape\", train_ts_loader.ts_windows.shape)\n",
    "    print(f\"len(train_loader.windows_sampling_idx) * 24 = \\\n",
    "          {len(train_ts_loader.windows_sampling_idx)} * 24 = {len(train_ts_loader.windows_sampling_idx) * 24}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    val_ts_loader = TimeSeriesLoader(ts_dataset=ts_dataset,\n",
    "                                     model='nbeats',\n",
    "                                     offset=0,\n",
    "                                     window_sampling_limit=ts_dataset.max_len,\n",
    "                                     input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                                     output_size=int(mc['output_size']),\n",
    "                                     idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                     batch_size=1024,\n",
    "                                     is_train_loader=False, # Samples the opposite of train_outsample_mask\n",
    "                                     shuffle=False, random_seed=int(mc['random_seed']))\n",
    "\n",
    "    print(\"val_ts_loader.ts_windows.shape\", val_ts_loader.ts_windows.shape)\n",
    "    print(f\"len(val_loader.windows_sampling_idx) * 24 = \\\n",
    "          {len(val_ts_loader.windows_sampling_idx)} * 24 = {len(val_ts_loader.windows_sampling_idx) * 24}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    mc['t_cols'] = ts_dataset.t_cols\n",
    "    return mc, train_ts_loader, val_ts_loader, scaler_y\n",
    "\n",
    "\n",
    "def model_fit_predict_roll(mc, Y_df, X_df, S_df):\n",
    "\n",
    "    X_raw_df = X_df.copy()\n",
    "    Y_raw_df = Y_df.copy()\n",
    "\n",
    "    #-------------------------------------- Rolling prediction on test --------------------------------------#\n",
    "\n",
    "    y_true_list = []\n",
    "    y_hat_list = []\n",
    "    offsets = [365*1*24, 0]\n",
    "    n_splits = len(offsets)\n",
    "    for split, offset in enumerate(offsets):\n",
    "        print(10*'-', f'Split {split+1}/{n_splits}', 10*'-')\n",
    "\n",
    "        #----------------------------------------------- Data -----------------------------------------------#\n",
    "        mc, train_ts_loader, val_ts_loader, scaler_y = prepare_data_Kin(mc=mc, Y_df=Y_raw_df, X_df=X_raw_df,\n",
    "                                                              S_df=S_df, n_timestamps_pred=365*1*24,\n",
    "                                                              offset=offset)\n",
    "\n",
    "        #--------------------------------------- Finetune and predict ---------------------------------------#\n",
    "        # Instantiate and train model\n",
    "        model = Nbeats(input_size_multiplier=mc['input_size_multiplier'],\n",
    "                    output_size=int(mc['output_size']),\n",
    "                    shared_weights=mc['shared_weights'],\n",
    "                    initialization=mc['initialization'],\n",
    "                    activation=mc['activation'],\n",
    "                    stack_types=mc['stack_types'],\n",
    "                    n_blocks=mc['n_blocks'],\n",
    "                    n_layers=mc['n_layers'],\n",
    "                    #n_hidden=2*[2*[int(mc['n_hidden'])]], # TODO; Revisar n_hidden1, n_hidden2 <------\n",
    "                    n_hidden=mc['n_hidden_list'],\n",
    "                    #n_hidden=2*[[256,256]],\n",
    "                    n_harmonics=int(mc['n_harmonics']),\n",
    "                    n_polynomials=int(mc['n_polynomials']),\n",
    "                    x_s_n_hidden = 0,#int(mc['x_s_n_hidden']),\n",
    "                    exogenous_n_channels=int(mc['exogenous_n_channels']),\n",
    "                    include_var_dict=mc['include_var_dict'],\n",
    "                    t_cols=mc['t_cols'],\n",
    "                    batch_normalization = mc['batch_normalization'],\n",
    "                    dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                    dropout_prob_exogenous=mc['dropout_prob_exogenous'],\n",
    "                    learning_rate=float(mc['learning_rate']),\n",
    "                    lr_decay=float(mc['lr_decay']),\n",
    "                    n_lr_decay_steps=float(mc['n_lr_decay_steps']),\n",
    "                    weight_decay=mc['weight_decay'],\n",
    "                    l1_theta=mc['l1_theta'],\n",
    "                    n_iterations=int(mc['n_iterations']),\n",
    "                    early_stopping=int(mc['early_stopping']),\n",
    "                    loss=mc['loss'],\n",
    "                    loss_hypar=float(mc['loss_hypar']),\n",
    "                    val_loss=mc['val_loss'],\n",
    "                    frequency=mc['frequency'],\n",
    "                    seasonality=int(mc['seasonality']),\n",
    "                    random_seed=int(mc['random_seed']))\n",
    "\n",
    "        model.fit(train_ts_loader=train_ts_loader, val_ts_loader=val_ts_loader, eval_steps=mc['eval_steps'])\n",
    "        y_true, y_hat, _ = model.predict(ts_loader=val_ts_loader, eval_mode=True)\n",
    "        y_true_list.append(y_true)\n",
    "        y_hat_list.append(y_hat)\n",
    "    \n",
    "    y_total = np.vstack(y_hat_list)\n",
    "    y_hat_total = np.vstack(y_true_list)\n",
    "\n",
    "    print(f'y_total.shape (#n_windows, #lt) {y_total.shape}')\n",
    "    print(f'y_hat_total.shape (#n_windows, #lt) {y_hat_total.shape}')\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Reshape for univariate and panel model compatibility\n",
    "    n_series = train_ts_loader.ts_dataset.n_series\n",
    "    n_fcds = len(y_total) // n_series\n",
    "    output_size = y_hat_total.shape[1]\n",
    "    y_total = y_total.reshape(n_series, n_fcds, output_size)\n",
    "    y_hat_total = y_hat_total.reshape(n_series, n_fcds, output_size)\n",
    "\n",
    "    print(\"y_total.shape (#n_series, #n_fcds, #lt) \", y_total.shape)\n",
    "    print(\"y_hat_total.shape (#n_series, #n_fcds, #lt) \", y_hat_total.shape)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    meta_data = val_ts_loader.ts_dataset.meta_data\n",
    "\n",
    "    return y_total, y_hat_total, meta_data, model\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "#### CRISTIAN\n",
    "############################################################################################\n",
    "\n",
    "def train_val_split(len_series, offset, window_sampling_limit, n_val_weeks, ds_per_day):\n",
    "    last_ds = len_series - offset\n",
    "    first_ds = max(last_ds - window_sampling_limit, 0)\n",
    "\n",
    "    last_day = int(last_ds/ds_per_day)\n",
    "    first_day = int(first_ds/ds_per_day)\n",
    "\n",
    "    days = set(range(first_day, last_day)) # All days, to later get train days\n",
    "    # Sample weeks from here, -7 to avoid sampling from last week\n",
    "    # To not sample first week and have inputs\n",
    "    sampling_days = set(range(first_day + 7, last_day - 7))\n",
    "    validation_days = set({}) # Val days set\n",
    "    \n",
    "    # For loop for n of weeks in validation\n",
    "    for i in range(n_val_weeks):\n",
    "        # Sample random day, init of week\n",
    "        init_day = random.sample(sampling_days, 1)[0]\n",
    "        # Select days of sampled init of week\n",
    "        sampled_days = list(range(init_day, min(init_day+7, last_day)))\n",
    "        # Add days to validation days\n",
    "        validation_days.update(sampled_days)\n",
    "        # Remove days from sampling_days, including overlapping resulting previous week\n",
    "        days_to_remove = set(range(init_day-6, min(init_day+7, last_day)))\n",
    "        sampling_days = sampling_days.difference(days_to_remove)\n",
    "\n",
    "    train_days = days.difference(validation_days)\n",
    "\n",
    "    train_days = sorted(list(train_days))\n",
    "    validation_days = sorted(list(validation_days))\n",
    "\n",
    "    train_idx = []\n",
    "    for day in train_days:\n",
    "        hours_idx = range(day*ds_per_day,(day+1)*ds_per_day)\n",
    "        train_idx += hours_idx\n",
    "\n",
    "    val_idx = []\n",
    "    for day in validation_days:\n",
    "        hours_idx = range(day*ds_per_day,(day+1)*ds_per_day)\n",
    "        val_idx += hours_idx\n",
    "\n",
    "    assert all([idx < last_ds for idx in val_idx]), 'Leakage!!!!'\n",
    "    \n",
    "    return train_idx, val_idx\n",
    "\n",
    "def declare_mask_df_Cristian(mc, Y_df, X_df, S_df, offset):\n",
    "    # train_mask: 1 to keep, 0 to hide\n",
    "    train_outsample_mask = np.ones(len(Y_df), dtype=int)\n",
    "    # if random_validation:\n",
    "    #     print('Random validation activated')\n",
    "    #     np.random.seed(1)\n",
    "    #     random.seed(1)\n",
    "    #     _, val_idx = train_val_split(len_series=len(Y_df), offset=0,\n",
    "    #                             window_sampling_limit=window_sampling_limit,\n",
    "    #                             n_val_weeks=n_val_weeks, ds_per_day=24)\n",
    "    #     train_outsample_mask[val_idx] = 0\n",
    "    #else:\n",
    "    print('Random validation de-activated')\n",
    "    train_outsample_mask[-offset:] = 0\n",
    "\n",
    "    print(f'Train {sum(train_outsample_mask)} hours = {np.round(sum(train_outsample_mask)/(24*365),2)} years')\n",
    "    print(f'Validation {sum(1-train_outsample_mask)} hours = {np.round(sum(1-train_outsample_mask)/(24*365),2)} years')\n",
    "\n",
    "    mask_df = Y_df[['unique_id', 'ds', 'y']].copy()\n",
    "    mask_df['available_mask'] = np.ones(len(Y_df))\n",
    "    mask_df['sample_mask'] = train_outsample_mask\n",
    "    return mask_df\n",
    "\n",
    "def prepare_data_Cristian(mc, Y_df, X_df, S_df):\n",
    "\n",
    "    #window_sampling_limit = int(mc['window_sampling_limit_multiplier']) * int(mc['output_size'])\n",
    "        \n",
    "    #--------------------------------------- Train and Validation Mask --------------------------------------#\n",
    "    mask_df = declare_mask_df_Cristian(mc=mc, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "                                       offset=(mc['n_val_weeks'] * 7 * mc['output_size']))\n",
    "\n",
    "    #---------------------------------------------- Scale Data ----------------------------------------------#\n",
    "    \n",
    "    # Transform data with scale transformation (Y_df, X_df, offset, normalizer_x, normalizer_y\n",
    "    # Avoid change original data\n",
    "    Y_scaled_df = Y_df.copy()\n",
    "    X_scaled_df = X_df.copy()\n",
    "\n",
    "    # NO ME GUSTA QUE SE LLAME OUTSAMPLE, OUTSAMPLE SE USA COMO SINONIMO DE VALIDATION\n",
    "    train_mask = mask_df.available_mask.values * mask_df.available_mask.values\n",
    "    Y_scaled_df, X_scaled_df, scaler_y = scale_data(Y_df=Y_scaled_df,\n",
    "                                                     X_df=X_scaled_df,\n",
    "                                                     mask=train_mask,\n",
    "                                                     normalizer_y = mc['normalizer_y'],\n",
    "                                                     normalizer_x = mc['normalizer_x'])\n",
    "\n",
    "    #-------------------------------------------- Declare Loaders -------------------------------------------#\n",
    "\n",
    "    #ts_dataset = TimeSeriesDataset(Y_df=Y_scaled_df_scaled, X_df=X_df_scaled, ts_train_mask=train_outsample_mask)\n",
    "    ts_dataset = TimeSeriesDataset(Y_df=Y_scaled_df, X_df=X_scaled_df, S_df=S_df, mask_df=mask_df)\n",
    "\n",
    "    train_ts_loader = TimeSeriesLoader(model='nbeats',\n",
    "                                       ts_dataset=ts_dataset,\n",
    "                                       window_sampling_limit=ts_dataset.max_len,#window_sampling_limit,\n",
    "                                       offset=0,\n",
    "                                       input_size=int(mc['input_size_multiplier'] * mc['output_size']),\n",
    "                                       output_size=int(mc['output_size']),\n",
    "                                       idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                       batch_size=int(mc['batch_size']),\n",
    "                                       is_train_loader=True,\n",
    "                                       random_seed=int(mc['random_seed']),\n",
    "                                       shuffle=True)\n",
    "\n",
    "    val_ts_loader = TimeSeriesLoader(model='nbeats',\n",
    "                                     ts_dataset=ts_dataset,\n",
    "                                     window_sampling_limit=ts_dataset.max_len,\n",
    "                                     offset=0,\n",
    "                                     input_size=int(mc['input_size_multiplier'] * mc['output_size']),\n",
    "                                     output_size=int(mc['output_size']),\n",
    "                                     idx_to_sample_freq=24, #TODO: pensar esto\n",
    "                                     batch_size=int(mc['batch_size']),\n",
    "                                     is_train_loader=False,\n",
    "                                     random_seed=int(mc['random_seed']),\n",
    "                                     shuffle=False)\n",
    "\n",
    "    mc['t_cols'] = ts_dataset.t_cols\n",
    "    return mc, train_ts_loader, val_ts_loader, scaler_y\n"
   ]
  },
  {
   "source": [
    "# RUN VALIDATION CODE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# run_val_nbeatsx(hyperparameters, Y_df, X_df, data_augmentation, random_validation, trials, trials_file_name)\n",
    "def run_val_nbeatsx(mc, Y_df, X_df, S_df, trials, trials_file_name, final_evaluation=False, return_predictions=False):\n",
    "\n",
    "    # Save trials, can analyze progress\n",
    "    if trials is not None:\n",
    "        save_every_n_step = 5\n",
    "        current_step = len(trials.trials)\n",
    "        if (current_step % save_every_n_step==0):\n",
    "            with open(trials_file_name, \"wb\") as f:\n",
    "                pickle.dump(trials, f)\n",
    "\n",
    "    start_time = time.time()    \n",
    "    \n",
    "    #---------------------------------------- Parse  Hyperparameters ----------------------------------------#\n",
    "    mc['include_var_dict'] = {'y': [-2, -3, -8],\n",
    "                              'Exogenous1': [-1, -2, -8],\n",
    "                              'Exogenous2': [-1, -2, -8],\n",
    "                              'week_day': [-1]}\n",
    "\n",
    "    n_hidden = int(mc['n_hidden'])\n",
    "    mc['n_hidden_list'] =  2*[[n_hidden, n_hidden]]\n",
    "    \n",
    "    #---------------------------------- Instantiate model, fit and predict ----------------------------------#\n",
    "\n",
    "    y_total, y_hat_total, meta_data, model = model_fit_predict_roll(mc, Y_df, X_df, S_df)\n",
    "\n",
    "    print('Best Model Evaluation')\n",
    "    benchmark_df, average_perc_diff = forecast_evaluation_table(y_total=y_total, y_hat_total=y_hat_total, \n",
    "                                                                meta_data=meta_data)\n",
    "    print(benchmark_df)\n",
    "    print('\\n')\n",
    "    \n",
    "    # CONDITIONAL ON CORRECT PREDICTION \n",
    "    # Average percentage difference of MAE, SMAPE, MAPE, RMSE\n",
    "    reported_loss = protect_nan_reported_loss(model)\n",
    "    run_time = time.time() - start_time\n",
    "\n",
    "    if reported_loss < 100:\n",
    "        reported_loss = average_perc_diff\n",
    "\n",
    "    results =  {'loss': reported_loss,\n",
    "                'loss_name': mc['val_loss'],\n",
    "                'mc': mc,\n",
    "                'final_insample_loss': model.final_insample_loss,\n",
    "                'final_outsample_loss': model.final_outsample_loss,\n",
    "                'trajectories': model.trajectories,\n",
    "                'run_time': run_time,\n",
    "                'status': STATUS_OK}\n",
    "    \n",
    "    if final_evaluation:\n",
    "        print('Best Model Hyperpars')\n",
    "        print(75*'=')\n",
    "        print(pd.Series(mc))\n",
    "        print(75*'='+'\\n')\n",
    "    \n",
    "    if return_predictions:\n",
    "        return y_total, y_hat_total, meta_data\n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "source": [
    "# EXPERIMENT SPACES"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# TODO: eliminate n_harmonics, n_polynomials think on kwargs maybe?\n",
    "# TODO: think on n_consistency for exogenous_n_channels -> n_xt_channels\n",
    "# TODO: x_s_n_hidden -> n_xs_hidden\n",
    "# TODO: input_size_multiplier -> Change for n_xt?\n",
    "# TODO: n_hidden -> n_theta_list\n",
    "def get_experiment_space(args):\n",
    "    if args.space=='nbeats_cristian':\n",
    "        space = {# Architecture parameters\n",
    "                 'input_size_multiplier': hp.choice('input_size_multiplier', [7]),\n",
    "                 'output_size': hp.choice('output_size', [24]),\n",
    "                 'shared_weights': hp.choice('shared_weights', [False]),\n",
    "                 'activation': hp.choice('activation', ['relu','softplus','tanh','selu','lrelu','prelu','sigmoid']),\n",
    "                 'initialization':  hp.choice('initialization', ['orthogonal','he_uniform','he_normal',\n",
    "                                                                 'glorot_uniform','glorot_normal','lecun_normal']),\n",
    "                 'stack_types': hp.choice('stack_types', [ ['identity'],\n",
    "                                                            1*['identity']+['exogenous_wavenet'],\n",
    "                                                                ['exogenous_wavenet']+1*['identity'],\n",
    "                                                            1*['identity']+['exogenous_tcn'],\n",
    "                                                                ['exogenous_tcn']+1*['identity'] ]),\n",
    "                 'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),\n",
    "                 'n_layers': hp.choice('n_layers', [ [2, 2] ]),\n",
    "                 'n_hidden': hp.quniform('n_hidden', 50, 500, 1),\n",
    "                 'n_harmonics': hp.choice('n_harmonics', [1]),\n",
    "                 'n_polynomials': hp.choice('n_polynomials', [2]),\n",
    "                 'exogenous_n_channels': hp.quniform('exogenous_n_channels', 1, 10, 1),\n",
    "                 'x_s_n_hidden': hp.choice('x_s_n_hidden', [0]),\n",
    "                 # Regularization and optimization parameters\n",
    "                 'batch_normalization': hp.choice('batch_normalization', [True, False]),\n",
    "                 'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 1),\n",
    "                 'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),\n",
    "                 'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.1)),\n",
    "                 'lr_decay': hp.choice('lr_decay', [0.5]),\n",
    "                 'n_lr_decay_steps': hp.choice('n_lr_decay_steps', [3]),\n",
    "                 'weight_decay': hp.loguniform('weight_decay', np.log(5e-4), np.log(0.01)),\n",
    "                 'n_iterations': hp.choice('n_iterations', [args.max_epochs]),\n",
    "                 'early_stopping': hp.choice('early_stopping', [8]),\n",
    "                 'eval_steps': hp.choice('eval_steps', [50]),\n",
    "                 #'n_val_weeks': hp.choice('n_val_weeks', [52]), # NUEVO <---------\n",
    "                 'n_val_weeks': hp.choice('n_val_weeks', [52*2]), # NUEVO <---------\n",
    "                 'loss': hp.choice('loss', ['MAE']),\n",
    "                 'loss_hypar': hp.choice('loss_hypar', [0.5]),\n",
    "                 'val_loss': hp.choice('val_loss', [args.val_loss]),\n",
    "                 'l1_theta': hp.choice('l1_theta', [0, hp.loguniform('lambdal1', np.log(1e-5), np.log(1))]),\n",
    "                 # Data parameters\n",
    "                 'normalizer_y': hp.choice('normalizer_y', [None, 'norm', 'norm1', \n",
    "                                                            'std', 'median', 'invariant']), # NUEVO <---------\n",
    "                 'normalizer_x': hp.choice('normalizer_x', [None, 'norm', 'norm1',\n",
    "                                                            'std', 'median', 'invariant']), # NUEVO <---------\n",
    "                 'frequency': hp.choice('frequency', ['H']),\n",
    "                 'seasonality': hp.choice('seasonality', [24]),\n",
    "                 'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [24]), # NUEVO args <----------\n",
    "                 'batch_size': hp.choice('batch_size', [128, 256, 512]), # NUEVO <---------\n",
    "                 'random_seed': hp.quniform('random_seed', 1, 1000, 1)}\n",
    "                 # CONSIDERO ESTO INNECESARIO\n",
    "                 # 'n_hidden_1': hp.quniform('n_hidden_1', 50, 500, 1),\n",
    "                 # 'n_hidden_2': hp.quniform('n_hidden_2', 50, 500, 1),\n",
    "                 # 'include_var_dict': hp.choice('include_var_dict', [{'y': [-2, -3, -8],\n",
    "                 #                                                     'Exogenous1': [-1, -2, -8],\n",
    "                 #                                                     'Exogenous2': [-1, -2, -8],\n",
    "                 #                                                     'week_day': [-1]}]),                 \n",
    "                 # 'incl_pr1': hp.choice('incl_pr1', [True]),\n",
    "                 # 'incl_pr2': hp.choice('incl_pr2', [True, False]),\n",
    "                 # 'incl_pr3': hp.choice('incl_pr3', [True, False]),\n",
    "                 # 'incl_pr7': hp.choice('incl_pr7', [True, False]),\n",
    "                 # 'incl_ex1_0': hp.choice('incl_ex1_0', [True, False]),\n",
    "                 # 'incl_ex1_1': hp.choice('incl_ex1_1', [True, False]),\n",
    "                 # 'incl_ex1_7': hp.choice('incl_ex1_7', [True, False]),\n",
    "                 # 'incl_ex2_0': hp.choice('incl_ex2_0', [True, False]),\n",
    "                 # 'incl_ex2_1': hp.choice('incl_ex2_1', [True, False]),\n",
    "                 # 'incl_ex2_7': hp.choice('incl_ex2_7', [True, False]),\n",
    "                 # 'incl_day': hp.choice('incl_day', [True, False]),\n",
    "                 # 'args.data_augmentation'\n",
    "                 # 'n_val_weeks': hp.choice('n_val_weeks', [args.n_val_weeks]}\n",
    "\n",
    "    elif args.space=='nbeats_collapsed':\n",
    "        space= {# Architecture parameters\n",
    "                'input_size_multiplier': hp.choice('input_size_multiplier', [7]),\n",
    "                'output_size': hp.choice('output_size', [24]),\n",
    "                'shared_weights': hp.choice('shared_weights', [False]),\n",
    "                'activation': hp.choice('activation', ['relu','softplus','tanh','selu','lrelu','prelu','sigmoid']),\n",
    "                'initialization':  hp.choice('initialization', ['orthogonal','he_uniform','he_normal',\n",
    "                                                                'glorot_uniform','glorot_normal','lecun_normal']),\n",
    "                'stack_types': hp.choice('stack_types', [['exogenous_wavenet']+1*['identity'],\n",
    "                                                         ['exogenous_tcn']+1*['identity'] ]),\n",
    "                'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),\n",
    "                'n_layers': hp.choice('n_layers', [ [2, 2] ]),\n",
    "                'n_hidden': hp.quniform('n_hidden', 50, 500, 1),\n",
    "                'n_harmonics': hp.choice('n_harmonics', [1]),\n",
    "                'n_polynomials': hp.choice('n_polynomials', [2]),\n",
    "                'exogenous_n_channels': hp.quniform('exogenous_n_channels', 1, 10, 1),\n",
    "                'x_s_n_hidden': hp.choice('x_s_n_hidden', [0]),\n",
    "                # Regularization and optimization parameters\n",
    "                'batch_normalization': hp.choice('batch_normalization', [False]),\n",
    "                'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 1),\n",
    "                'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),\n",
    "                'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.1)),\n",
    "                'lr_decay': hp.uniform('lr_decay', 0.3, 1.0),\n",
    "                'n_lr_decay_steps': hp.choice('n_lr_decay_steps', [3]),\n",
    "                'weight_decay': hp.loguniform('weight_decay', np.log(5e-5), np.log(5e-3)),\n",
    "                'n_iterations': hp.choice('n_iterations', [args.max_epochs]),\n",
    "                'early_stopping': hp.choice('early_stopping', [8]),\n",
    "                'eval_steps': hp.choice('eval_steps', [50]),\n",
    "                'n_val_weeks': hp.choice('n_val_weeks', [52*2]), # NUEVO <---------\n",
    "                'loss': hp.choice('loss', ['PINBALL']),\n",
    "                'loss_hypar': hp.uniform('loss_hypar', 0.48, 0.51),\n",
    "                'val_loss': hp.choice('val_loss', [args.val_loss]),\n",
    "                'l1_theta': hp.choice('l1_theta', [0, hp.loguniform('lambdal1', np.log(1e-5), np.log(1))]),\n",
    "                # Data parameters\n",
    "                'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "                'normalizer_x': hp.choice('normalizer_x', ['median']),\n",
    "                'frequency': hp.choice('frequency', ['H']),\n",
    "                'seasonality': hp.choice('seasonality', [24]),\n",
    "                'include_var_dict': hp.choice('include_var_dict', [{'y': [-2, -3, -8],\n",
    "                                                                    'Exogenous1': [-1, -2, -8],\n",
    "                                                                    'Exogenous2': [-1, -2, -8],\n",
    "                                                                    'week_day': [-1]}]),\n",
    "                'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [24]),\n",
    "                'batch_size': hp.choice('batch_size', [256]),\n",
    "                'random_seed': hp.quniform('random_seed', 10, 20, 1)}\n",
    "    \n",
    "    else:\n",
    "        print(f'Experiment space {args.space} not available')\n",
    "\n",
    "    return space\n",
    "\n",
    "def parse_trials(trials):\n",
    "    # Initialize\n",
    "    trials_dict = {'tid': [], 'loss': [], 'trajectories': [], 'mc': []}\n",
    "    for tidx in range(len(trials)):\n",
    "        # Main\n",
    "        trials_dict['tid']  += [trials.trials[tidx]['tid']]\n",
    "        trials_dict['loss'] += [trials.trials[tidx]['result']['loss']]\n",
    "        trials_dict['trajectories'] += [trials.trials[tidx]['result']['trajectories']]\n",
    "\n",
    "        # Model Configs\n",
    "        mc = trials.trials[tidx]['result']['mc']\n",
    "        trials_dict['mc'] += [mc]\n",
    "    \n",
    "    trials_df = pd.DataFrame(trials_dict)\n",
    "    return trials_df\n",
    "\n",
    "def main(args):\n",
    "    #---------------------------------------------- Directories ----------------------------------------------#\n",
    "    \n",
    "    dataset = eval(args.dataset)\n",
    "    dataset_str = dataset[0]\n",
    "    for market in dataset[1:]:\n",
    "        dataset_str += f'{market}_'\n",
    "    output_dir = f'./results/{dataset_str}/{args.space}/'\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    assert os.path.exists(output_dir), f'Output dir {output_dir} does not exist'\n",
    "\n",
    "    hyperopt_file = output_dir + f'hyperopt_{args.experiment_id}.p'\n",
    "    result_test_file = output_dir + f'result_test_{args.experiment_id}.p'\n",
    "\n",
    "    #---------------------------------------------- Read  Data ----------------------------------------------#\n",
    "    print('\\n'+75*'-')\n",
    "    print(28*'-', 'Preparing Dataset', 28*'-')\n",
    "    print(75*'-'+'\\n')\n",
    "\n",
    "    #TEST_DATE = {'NP': '2016-12-27',\n",
    "    #             'PJM':'2016-12-27',\n",
    "    #             'BE':'2015-01-04',\n",
    "    #             'FR': '2015-01-04',\n",
    "    #             'DE':'2016-01-04'}\n",
    "    #test_date = TEST_DATE[args.dataset]\n",
    "    #Y_insample_df, X_insample_df, Y_outsample_df, X_outsample_df, _ = load_epf(directory='../data/',\n",
    "    #                                                                           market=args.dataset,\n",
    "    #                                                                           first_date_test=test_date,\n",
    "    #                                                                           days_in_test=728)\n",
    "    Y_df, X_df, S_df = EPF.load_groups(directory='data', groups=dataset)\n",
    "\n",
    "    #-------------------------------------- Hyperparameter Optimization --------------------------------------#\n",
    "\n",
    "    if not os.path.isfile(hyperopt_file):\n",
    "        print('\\n'+75*'-')\n",
    "        print(22*'-', 'Start Hyperparameter  tunning', 22*'-')\n",
    "        print(75*'-'+'\\n')\n",
    "\n",
    "        space = get_experiment_space(args)\n",
    "\n",
    "        trials = Trials()\n",
    "        fmin_objective = partial(run_val_nbeatsx, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "                                 trials=trials, trials_file_name=hyperopt_file)\n",
    "        fmin(fmin_objective, space=space, algo=tpe.suggest, max_evals=args.hyperopt_iters, trials=trials, verbose=True)\n",
    "\n",
    "        # Save output\n",
    "        with open(hyperopt_file, \"wb\") as f:\n",
    "            pickle.dump(trials, f)\n",
    "\n",
    "    print('\\n'+75*'-')\n",
    "    print(20*'-', 'Hyperparameter  tunning  finished', 20*'-')\n",
    "    print(75*'-'+'\\n')\n",
    "\n",
    "    #----------------------------------------- Selected 'Best' Model -----------------------------------------#\n",
    "\n",
    "    # Read and parse trials pickle\n",
    "    trials = pickle.load(open(hyperopt_file, 'rb'))\n",
    "    trials_df = parse_trials(trials)\n",
    "\n",
    "    # Get best mc\n",
    "    idx = trials_df.loss.idxmin()\n",
    "    best_mc = trials_df.loc[idx]['mc']\n",
    "    \n",
    "    run_val_nbeatsx(best_mc, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "                    trials=trials, trials_file_name=hyperopt_file, final_evaluation=True)\n",
    "\n",
    "def parse_args():\n",
    "    desc = \"NBEATSx overfit\"\n",
    "    parser = argparse.ArgumentParser(description=desc)\n",
    "\n",
    "    parser.add_argument('--dataset', type=str, required=True, help='NP')\n",
    "    parser.add_argument('--space', type=str, required=True, help='Experiment hyperparameter space')\n",
    "    parser.add_argument('--hyperopt_iters', type=int, help='hyperopt_iters')\n",
    "    parser.add_argument('--max_epochs', type=int, required=False, default=2000, help='max train epochs')\n",
    "    parser.add_argument('--val_loss', type=str, required=False, default=None, help='validation loss')\n",
    "    parser.add_argument('--experiment_id', default=None, required=False, type=str, help='string to identify experiment')\n",
    "    return parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # parse arguments\n",
    "    args = parse_args()\n",
    "    if args is None:\n",
    "        exit()\n",
    "    \n",
    "    main(args)\n",
    "\n",
    "# CUDA_VISIBLE_DEVICES=0 PYTHONPATH=. python src/overfit_nbeatsx.py --dataset \"['NP']\" --space 'nbeats_collapsed' --hyperopt_iters 200 --val_loss 'MAE'  --experiment_id 'AvgDiff_2021_01_16'\n",
    "# CUDA_VISIBLE_DEVICES=0 PYTHONPATH=. python src/overfit_nbeatsx.py --dataset \"['NP']\" --space 'nbeats_collapsed' --hyperopt_iters 200 --val_loss 'MAE'  --experiment_id 'AvgDiff_2021_01_16'\n",
    "# CUDA_VISIBLE_DEVICES=1 PYTHONPATH=. python src/overfit_nbeatsx.py --dataset \"['NP']\" --space 'nbeats_collapsed' --hyperopt_iters 200 --val_loss 'MAE'  --experiment_id 'AvgDiff_2021_01_16'\n",
    "# CUDA_VISIBLE_DEVICES=1 PYTHONPATH=. python src/overfit_nbeatsx.py --dataset \"['NP']\" --space 'nbeats_collapsed' --hyperopt_iters 200 --val_loss 'MAE'  --experiment_id 'AvgDiff_2021_01_16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = pd.Series({'dataset': \"['NP']\",\n",
    "                  #'dataset': \"['NP', 'PJM', 'BE', 'FR']\",\n",
    "                  'val_loss': 'MAE',\n",
    "                  'space': 'nbeats_collapsed',\n",
    "                  'hyperopt_iters': 1, 'max_epochs': 100,\n",
    "                  'experiment_id': 'debug3', 'gpu_id': 1})\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu_id)\n",
    "print('cuda devices,', os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "main(args)"
   ]
  },
  {
   "source": [
    "# TEST SINGLE MODEL CONFIG"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------- Split 1/2 ----------\n",
      "\n",
      "\n",
      "Y_df.shape \t (26184, 3)\n",
      "X_df.shape \t (26184, 12)\n",
      "Y_balanced_df.shape \t (26184, 3)\n",
      "X_balanced_df.shape \t (26184, 12)\n",
      "Train Validation splits\n",
      "                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0.0         2014-12-28 2015-12-27 23:00:00\n",
      "          1.0         2013-01-01 2014-12-27 23:00:00\n",
      "Total data \t\t\t26184 time stamps\n",
      "Available percentage=100.0, \t26184 time stamps\n",
      "Train percentage=66.54, \t17424.0 time stamps\n",
      "Predict percentage=33.46, \t8760.0 time stamps\n",
      "\n",
      "\n",
      "train_ts_loader.ts_windows.shape torch.Size([1092, 13, 192])\n",
      "len(train_loader.windows_sampling_idx) * 24 =           726 * 24 = 17424\n",
      "\n",
      "\n",
      "val_ts_loader.ts_windows.shape torch.Size([1092, 13, 192])\n",
      "len(val_loader.windows_sampling_idx) * 24 =           365 * 24 = 8760\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================== Start fitting ==============================\n",
      "Step: 50, Time: 6.771, Insample PINBALL: 0.83295, Outsample MAE: 1.68507\n",
      "Step: 100, Time: 13.171, Insample PINBALL: 0.73585, Outsample MAE: 1.67231\n",
      "Step: 150, Time: 19.475, Insample PINBALL: 0.71068, Outsample MAE: 1.62613\n",
      "Step: 200, Time: 25.791, Insample PINBALL: 0.64851, Outsample MAE: 1.63434\n",
      "Step: 250, Time: 32.887, Insample PINBALL: 0.67537, Outsample MAE: 1.53856\n",
      "Step: 300, Time: 39.388, Insample PINBALL: 0.60266, Outsample MAE: 1.63841\n",
      "Step: 350, Time: 46.036, Insample PINBALL: 0.60869, Outsample MAE: 1.52657\n",
      "Step: 400, Time: 52.654, Insample PINBALL: 0.57934, Outsample MAE: 1.52303\n",
      "Step: 450, Time: 59.145, Insample PINBALL: 0.62594, Outsample MAE: 1.54321\n",
      "Step: 500, Time: 65.527, Insample PINBALL: 0.58196, Outsample MAE: 1.52917\n",
      "Step: 550, Time: 71.857, Insample PINBALL: 0.58363, Outsample MAE: 1.55391\n",
      "Step: 600, Time: 78.237, Insample PINBALL: 0.58623, Outsample MAE: 1.55603\n",
      "Step: 650, Time: 84.607, Insample PINBALL: 0.61939, Outsample MAE: 1.52687\n",
      "Step: 700, Time: 91.181, Insample PINBALL: 0.54434, Outsample MAE: 1.52878\n",
      "Step: 750, Time: 97.547, Insample PINBALL: 0.56620, Outsample MAE: 1.53233\n",
      "Step: 800, Time: 103.835, Insample PINBALL: 0.54430, Outsample MAE: 1.52610\n",
      "\n",
      "\n",
      "-------------------  Stopped training by early stopping -------------------\n",
      "Step: 800, Time: 103.929, Insample PINBALL: 0.57934, Outsample MAE: 1.52303\n",
      "==============================  End fitting  ==============================\n",
      "\n",
      "\n",
      "---------- Split 2/2 ----------\n",
      "\n",
      "\n",
      "Y_df.shape \t (34944, 3)\n",
      "X_df.shape \t (34944, 12)\n",
      "Y_balanced_df.shape \t (34944, 3)\n",
      "X_balanced_df.shape \t (34944, 12)\n",
      "Train Validation splits\n",
      "                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0.0         2015-12-28 2016-12-26 23:00:00\n",
      "          1.0         2013-01-01 2015-12-27 23:00:00\n",
      "Total data \t\t\t34944 time stamps\n",
      "Available percentage=100.0, \t34944 time stamps\n",
      "Train percentage=74.93, \t26184.0 time stamps\n",
      "Predict percentage=25.07, \t8760.0 time stamps\n",
      "\n",
      "\n",
      "train_ts_loader.ts_windows.shape torch.Size([1457, 13, 192])\n",
      "len(train_loader.windows_sampling_idx) * 24 =           1091 * 24 = 26184\n",
      "\n",
      "\n",
      "val_ts_loader.ts_windows.shape torch.Size([1457, 13, 192])\n",
      "len(val_loader.windows_sampling_idx) * 24 =           365 * 24 = 8760\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================== Start fitting ==============================\n",
      "Step: 50, Time: 5.821, Insample PINBALL: 0.95260, Outsample MAE: 1.90655\n",
      "Step: 100, Time: 11.634, Insample PINBALL: 0.73452, Outsample MAE: 1.65319\n",
      "Step: 150, Time: 17.526, Insample PINBALL: 0.64597, Outsample MAE: 1.55713\n",
      "Step: 200, Time: 23.524, Insample PINBALL: 0.66105, Outsample MAE: 1.56002\n",
      "Step: 250, Time: 29.617, Insample PINBALL: 0.58927, Outsample MAE: 1.50384\n",
      "Step: 300, Time: 35.627, Insample PINBALL: 0.74734, Outsample MAE: 1.49368\n",
      "Step: 350, Time: 41.621, Insample PINBALL: 0.55596, Outsample MAE: 1.47494\n",
      "Step: 400, Time: 47.591, Insample PINBALL: 0.54709, Outsample MAE: 1.49771\n",
      "Step: 450, Time: 53.658, Insample PINBALL: 0.50175, Outsample MAE: 1.48523\n",
      "Step: 500, Time: 59.688, Insample PINBALL: 0.52581, Outsample MAE: 1.42807\n",
      "Step: 550, Time: 65.775, Insample PINBALL: 0.75684, Outsample MAE: 1.42173\n",
      "Step: 600, Time: 71.829, Insample PINBALL: 0.58764, Outsample MAE: 1.47707\n",
      "Step: 650, Time: 77.778, Insample PINBALL: 0.64072, Outsample MAE: 1.41275\n",
      "Step: 700, Time: 83.646, Insample PINBALL: 0.55579, Outsample MAE: 1.40295\n",
      "Step: 750, Time: 89.493, Insample PINBALL: 0.64239, Outsample MAE: 1.41025\n",
      "Step: 800, Time: 95.594, Insample PINBALL: 0.54755, Outsample MAE: 1.39157\n",
      "Step: 850, Time: 101.646, Insample PINBALL: 0.68626, Outsample MAE: 1.39312\n",
      "Step: 900, Time: 107.640, Insample PINBALL: 0.64641, Outsample MAE: 1.40253\n",
      "Step: 950, Time: 113.855, Insample PINBALL: 0.69130, Outsample MAE: 1.40451\n",
      "Step: 1000, Time: 120.183, Insample PINBALL: 0.59650, Outsample MAE: 1.37851\n",
      "Step: 1000, Time: 120.280, Insample PINBALL: 0.59650, Outsample MAE: 1.37851\n",
      "==============================  End fitting  ==============================\n",
      "\n",
      "\n",
      "y_total.shape (#n_windows, #lt) (730, 24)\n",
      "y_hat_total.shape (#n_windows, #lt) (730, 24)\n",
      "\n",
      "\n",
      "y_total.shape (#n_series, #n_fcds, #lt)  (1, 730, 24)\n",
      "y_hat_total.shape (#n_series, #n_fcds, #lt)  (1, 730, 24)\n",
      "\n",
      "\n",
      "Best Model Evaluation\n",
      "y_total 17520 nan_perc 0.0\n",
      "average_perc_diff -2.1554244993438947\n",
      "   id unique_id metric   DNN  NBEATSx  perc_diff  improvement\n",
      "0   1        NP    MAE  1.67  1.45077 -13.127544         True\n",
      "1   1        NP   MAPE  5.38  5.24445  -2.519517         True\n",
      "2   1        NP  SMAPE  4.85  5.14491   6.080619        False\n",
      "3   1        NP   RMSE  3.33  3.36146   0.944744        False\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mc0 = {# Architecture parameters\n",
    "      'input_size_multiplier': 7,\n",
    "      'output_size': 24,\n",
    "      'shared_weights': False,\n",
    "      'activation': 'selu',\n",
    "      'initialization': 'glorot_normal',\n",
    "      'stack_types': ['exogenous_tcn']+1*['identity'],\n",
    "      'n_blocks': [1, 1],\n",
    "      'n_layers': [2, 2],\n",
    "      'n_hidden': 179,\n",
    "      #'n_hidden_list': 2*[[462,462]],\n",
    "      'n_polynomials': 2,\n",
    "      'n_harmonics': 1,\n",
    "      'exogenous_n_channels': 6,\n",
    "      'x_s_n_hidden': 0, # TODO: referencia dinÃ¡mica vs datasets\n",
    "      # Regularization and optimization parameters\n",
    "      'batch_normalization': False,\n",
    "      'dropout_prob_theta': 0.35,\n",
    "      'dropout_prob_exogenous': 0.35,\n",
    "      'learning_rate': 0.00155,\n",
    "      'lr_decay': 0.44,\n",
    "      'n_lr_decay_steps': 3,\n",
    "      'weight_decay': 0.00037,\n",
    "      'n_iterations': 1000,\n",
    "      'early_stopping': 8,\n",
    "      'eval_steps': 50,\n",
    "      'n_val_weeks': 52*2,\n",
    "      'loss': 'PINBALL',\n",
    "      'loss_hypar': 0.5025, #0.49,\n",
    "      'val_loss': 'MAE',\n",
    "      'l1_theta': 0,\n",
    "      # Data parameters\n",
    "      'normalizer_y': None,\n",
    "      'normalizer_x': 'median',\n",
    "      'frequency':'H',\n",
    "      'seasonality': 24,\n",
    "      'idx_to_sample_freq': 24,\n",
    "      'include_var_dict': {'y': [-2, -3, -8],\n",
    "                           'Exogenous1': [-1, -2, -8],\n",
    "                           'Exogenous2': [-1, -2, -8],\n",
    "                           'week_day': [-1]},\n",
    "      'batch_size': 256,\n",
    "      'random_seed': 15}\n",
    "\n",
    "# mc1 = {# Architecture parameters\n",
    "#       'input_size_multiplier': 7,\n",
    "#       'output_size': 24,\n",
    "#       'shared_weights': False,\n",
    "#       'activation': 'lrelu',\n",
    "#       'initialization': 'glorot_normal',\n",
    "#       'stack_types': ['exogenous_tcn']+1*['identity'],\n",
    "#       'n_blocks': [1, 1],\n",
    "#       'n_layers': [2, 2],\n",
    "#       'n_hidden': 400,\n",
    "#       #'n_hidden_list': 2*[[462,462]],\n",
    "#       'n_polynomials': 2,\n",
    "#       'n_harmonics': 1,\n",
    "#       'exogenous_n_channels': 3,\n",
    "#       'x_s_n_hidden': 0, # TODO: referencia dinÃ¡mica vs datasets\n",
    "#       # Regularization and optimization parameters\n",
    "#       'batch_normalization': False,\n",
    "#       'dropout_prob_theta': 0.28,\n",
    "#       'dropout_prob_exogenous': 0.44,\n",
    "#       'learning_rate': 0.0005,\n",
    "#       'lr_decay': 0.7,\n",
    "#       'n_lr_decay_steps': 3,\n",
    "#       'weight_decay': 0.00015,\n",
    "#       'n_iterations': 2000,\n",
    "#       'early_stopping': 40,\n",
    "#       'eval_steps': 10,\n",
    "#       'n_val_weeks': 52*2,\n",
    "#       'loss': 'PINBALL',\n",
    "#       'loss_hypar': 0.49, #0.5,\n",
    "#       'val_loss': 'MAE',\n",
    "#       'l1_theta': 0,\n",
    "#       # Data parameters\n",
    "#       'normalizer_y': None,\n",
    "#       'normalizer_x': 'median',\n",
    "#       'frequency':'H',\n",
    "#       'seasonality': 24,\n",
    "#       'idx_to_sample_freq': 24,\n",
    "#       'include_var_dict': {'y': [-2, -3, -8],\n",
    "#                            'Exogenous1': [-1, -2, -8],\n",
    "#                            'Exogenous2': [-1, -2, -8],\n",
    "#                            'week_day': [-1]},\n",
    "#       'batch_size': 256,\n",
    "#       'random_seed': 12}\n",
    "\n",
    "mc = mc0\n",
    "dataset = ['NP']\n",
    "# dataset = ['BE']\n",
    "# dataset = ['NP', 'PJM']\n",
    "# dataset = ['NP', 'PJM', 'BE', 'FR']\n",
    "if len(dataset)>0: mc['x_s_n_hidden'] = 2\n",
    "\n",
    "Y_df, X_df, S_df = EPF.load_groups(directory='data', groups=dataset)\n",
    "y_total, y_hat_total, meta_data = run_val_nbeatsx(mc=mc, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "                                                  trials=None, trials_file_name=None, final_evaluation=False, return_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, market_data in enumerate(meta_data):\n",
    "    market = market_data['unique_id']\n",
    "    y_hat_plot = y_hat_total[i,:,:].reshape(-1)\n",
    "    y_true_plot = y_total[i,:,:].reshape(-1)\n",
    "    \n",
    "    performance = np.round(mape(y=y_true_plot, y_hat=y_hat_plot), 5)\n",
    "    plt.plot(range(len(y_true_plot)), y_true_plot, label='true', alpha=0.7)\n",
    "    plt.plot(range(len(y_hat_plot)), y_hat_plot, label='pred', alpha=0.7)\n",
    "    plt.title(f\"{market} predictions \\n all lead time  MAE={performance}\")\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Electricity Price')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}