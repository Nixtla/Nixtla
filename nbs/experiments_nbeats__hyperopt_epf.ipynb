{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c68c0f618c4f787b1e02bfee278e48d25b62407cb335aab5258cedc4db4ced"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiments.nbeats.hyperopt_epf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import time\n",
    "import os\n",
    "# Limit number of threads in numpy and others to avoid throttling\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\" # export OMP_NUM_THREADS=4\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"2\" # export OPENBLAS_NUM_THREADS=4 \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"3\" # export MKL_NUM_THREADS=6\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"2\" # export VECLIB_MAXIMUM_THREADS=4\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"3\" # export NUMEXPR_NUM_THREADS=6\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import pickle\n",
    "import glob\n",
    "import itertools\n",
    "import random\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "from nixtla.data.scalers import Scaler\n",
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset\n",
    "from nixtla.data.tsloader_fast import TimeSeriesLoader\n",
    "from nixtla.losses.numpy import mae, mape, smape, rmse, pinball_loss\n",
    "\n",
    "# Models\n",
    "from nixtla.models.nbeats.nbeats import Nbeats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK"
   ]
  },
  {
   "source": [
    "# DATA WRANGLING AND EVALUATION UTILS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# TODO: Think and test new mask_df and datasets with random validation\n",
    "\n",
    "############################################################################################\n",
    "#### COMMON\n",
    "############################################################################################\n",
    "\n",
    "BENCHMARK_DF = pd.DataFrame({'id': [1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5],\n",
    "                             'unique_id': ['NP', 'NP', 'NP', 'NP', 'PJM', 'PJM', 'PJM', 'PJM',\n",
    "                                           'BE', 'BE', 'BE', 'BE', 'FR', 'FR', 'FR', 'FR',\n",
    "                                           'DE', 'DE', 'DE', 'DE'],\n",
    "                             'metric': ['MAE', 'MAPE', 'SMAPE', 'RMSE', 'MAE', 'MAPE', 'SMAPE', 'RMSE',\n",
    "                                        'MAE', 'MAPE', 'SMAPE', 'RMSE', 'MAE', 'MAPE', 'SMAPE', 'RMSE',\n",
    "                                        'MAE', 'MAPE', 'SMAPE', 'RMSE',],\n",
    "                             'DNN_ens' : [1.67, 5.38, 4.85, 3.33, 2.78, 28.66, 11.22, 4.64, \n",
    "                                          5.82, 26.11, 13.33, 16.13, 3.91, 14.77, 10.98, 11.74,\n",
    "                                          3.44, 95.76, 14.19, 6.00],\n",
    "                             'DNN_best': [1.72, 5.46, 5.00, 3.34, 2.95, 29.10, 11.81, 4.82,\n",
    "                                          6.07, 24.08, 13.87, 15.88, 4.19, 15.13, 11.65, 11.41,\n",
    "                                          3.60, 83.1, 14.74, 6.13]})\n",
    "\n",
    "def forecast_evaluation_table(y_total, y_hat_total, meta_data):\n",
    "    performances = []\n",
    "    for i, market_data in enumerate(meta_data):\n",
    "        market = market_data['unique_id']\n",
    "        \n",
    "        y = y_total[i,:,:].reshape(-1)\n",
    "        y_hat = y_hat_total[i,:,:].reshape(-1)\n",
    "\n",
    "        _mae   = np.round(mae(y=y, y_hat=y_hat),5)\n",
    "        _mape  = np.round(mape(y=y, y_hat=y_hat),5)\n",
    "        _smape = np.round(smape(y=y, y_hat=y_hat),5)\n",
    "        _rmse  = np.round(rmse(y=y, y_hat=y_hat),5)\n",
    "\n",
    "        performance_df = pd.DataFrame({'unique_id': [market]*4,\n",
    "                                       'metric': ['MAE', 'MAPE', 'SMAPE', 'RMSE'],\n",
    "                                       'NBEATSx': [_mae, _mape, _smape, _rmse]})\n",
    "\n",
    "        performances += [performance_df]\n",
    "\n",
    "    performances_df = pd.concat(performances)\n",
    "\n",
    "    benchmark_df = BENCHMARK_DF.merge(performances_df, on=['unique_id', 'metric'], how='left')\n",
    "    benchmark_df['perc_diff'] = 100 * (benchmark_df['NBEATSx']-benchmark_df['DNN_ens'])/benchmark_df['DNN_ens']\n",
    "    benchmark_df['improvement'] = benchmark_df['perc_diff'] < 0\n",
    "    benchmark_df = benchmark_df.dropna()\n",
    "    average_perc_diff = benchmark_df['perc_diff'].mean()\n",
    "\n",
    "    \n",
    "    y_tot = y_total.reshape(-1)\n",
    "    y_hat_tot = y_hat_total.reshape(-1)\n",
    "    y_total_nans_perc = np.sum((np.isnan(y_tot)))  / len(y_tot)\n",
    "    y_hat_total_nans_perc = np.sum((np.isnan(y_hat_tot)))  / len(y_hat_tot)\n",
    "    print(f'y_total {len(y_tot)} nan_perc {y_total_nans_perc}')\n",
    "    print(f'y_hat_total {len(y_tot)} nan_perc {y_total_nans_perc}')\n",
    "    print(\"average_perc_diff\", average_perc_diff)\n",
    "\n",
    "    reported_loss = _mae\n",
    "    if y_total_nans_perc >= 0.05: reported_loss=500\n",
    "    if y_hat_total_nans_perc >= 0.05: reported_loss=500\n",
    "    #improvement_loss = 200 * (1-np.mean(benchmark_df.improvement)) + average_perc_diff\n",
    "    print(f'reported_loss {reported_loss}')\n",
    "\n",
    "    return benchmark_df, y_total_nans_perc, y_hat_total_nans_perc, reported_loss\n",
    "\n",
    "def protect_nan_reported_loss(model):\n",
    "    # TODO: Pytorch numerical error hacky protection, protect from losses.numpy.py\n",
    "    reported_loss = model.final_outsample_loss\n",
    "    if np.isnan(model.final_insample_loss):\n",
    "        reported_loss = 500\n",
    "    if model.final_insample_loss<=0:\n",
    "        reported_loss = 500\n",
    "\n",
    "    if np.isnan(model.final_outsample_loss):\n",
    "        reported_loss = 500    \n",
    "    if model.final_outsample_loss<=0:\n",
    "        reported_loss = 500    \n",
    "    return reported_loss\n",
    "\n",
    "\n",
    "def get_last_n_timestamps_mask_df(Y_df, n_timestamps):\n",
    "    # Creates outsample_mask\n",
    "    # train_mask: 1 last_n_timestamps, 0 timestamps until last_n_timestamps\n",
    "\n",
    "    last_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    last_df.sort_values(by=['unique_id', 'ds'], inplace=True, ascending=False)\n",
    "    last_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    last_df = last_df.groupby('unique_id').head(n_timestamps)\n",
    "    last_df['mask'] = 1\n",
    "\n",
    "    last_df = last_df[['unique_id', 'ds', 'mask']]\n",
    "\n",
    "    mask_df = Y_df.merge(last_df, on=['unique_id', 'ds'], how='left')\n",
    "    mask_df['mask'] = mask_df['mask'].fillna(0)    # The first len(Y)-n_hours used as train\n",
    "\n",
    "    mask_df = mask_df[['unique_id', 'ds', 'mask']]\n",
    "    mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "\n",
    "    assert len(mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "\n",
    "    return mask_df\n",
    "\n",
    "def balance_data(Y_df, X_df):\n",
    "    # Create balanced placeholder dataframe\n",
    "    balance_ids = {'unique_id': Y_df.unique_id.unique(),\n",
    "                   'ds': Y_df.ds.unique()}\n",
    "\n",
    "    product_list = list(itertools.product(*list(balance_ids.values())))\n",
    "    balance_df = pd.DataFrame(product_list, columns=list(balance_ids.keys()))\n",
    "    \n",
    "    # Balance with merge\n",
    "    Y_balanced_df = balance_df.merge(Y_df, on=['unique_id', 'ds'], how='left')\n",
    "    X_balanced_df = balance_df.merge(X_df, on=['unique_id', 'ds'], how='left')\n",
    "\n",
    "    print('\\n')\n",
    "    print(f'Y_df.shape \\t{Y_df.shape}')\n",
    "    print(f'X_df.shape \\t{X_df.shape}')\n",
    "    print(f'Y_balanced_df.shape \\t{Y_balanced_df.shape}')\n",
    "    print(f'X_balanced_df.shape \\t{X_balanced_df.shape}')\n",
    "\n",
    "    return Y_balanced_df, X_balanced_df\n",
    "\n",
    "def scale_data(Y_df, X_df, mask, normalizer_y, normalizer_x):\n",
    "    y_shift = None\n",
    "    y_scale = None\n",
    "\n",
    "    mask = mask.astype(int)\n",
    "    \n",
    "    if normalizer_y is not None:\n",
    "        scaler_y = Scaler(normalizer=normalizer_y)\n",
    "        Y_df['y'] = scaler_y.scale(x=Y_df['y'].values, mask=mask)\n",
    "    else:\n",
    "        scaler_y = None\n",
    "    # Exogenous are always scaled to help learning\n",
    "    if normalizer_x is not None:\n",
    "        scaler_x = Scaler(normalizer=normalizer_x)\n",
    "        X_df['Exogenous1'] = scaler_x.scale(x=X_df['Exogenous1'].values, mask=mask)\n",
    "\n",
    "        scaler_x = Scaler(normalizer=normalizer_x)\n",
    "        X_df['Exogenous2'] = scaler_x.scale(x=X_df['Exogenous2'].values, mask=mask)\n",
    "\n",
    "    filter_variables = ['unique_id', 'ds', 'Exogenous1', 'Exogenous2', 'week_day'] + \\\n",
    "                       [col for col in X_df if (col.startswith('day'))]\n",
    "                       #[col for col in X_df if (col.startswith('_hour_'))]\n",
    "    X_df = X_df[filter_variables]\n",
    "\n",
    "    return Y_df, X_df, scaler_y\n",
    "\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "#### KIN\n",
    "############################################################################################\n",
    "\n",
    "def prepare_data_Kin(mc, Y_df, X_df, S_df, n_timestamps_pred=365*1*24, offset=0): #offset=365*1*24 tira un aÃ±o\n",
    "    # n_timestamps_pred defines number of hours ahead to predict\n",
    "    # offset defines the shift of the data to simulate rolling window\n",
    "    assert offset % n_timestamps_pred == 0, 'Avoid overlap of predictions, redefine n_timestamps_pred or offset'\n",
    "    Y_df = Y_df.head(len(Y_df)-offset)\n",
    "    X_df = X_df.head(len(X_df)-offset)\n",
    "\n",
    "    assert len(Y_df.unique_id.unique())==1, 'Data prepartation for more than one market not implemented yet'\n",
    "\n",
    "    #-------------------------------------------- Data Wrangling --------------------------------------------#\n",
    "    Y_balanced_df, X_balanced_df = balance_data(Y_df, X_df)\n",
    "    del Y_df, X_df\n",
    "\n",
    "    #------------------------------------- Available and Validation Mask ------------------------------------#\n",
    "    # mask: 1 last_n_timestamps, 0 timestamps until last_n_timestamps\n",
    "    last_timestamps_df = get_last_n_timestamps_mask_df(Y_df=Y_balanced_df, n_timestamps=n_timestamps_pred)\n",
    "    mask_df = last_timestamps_df.copy()\n",
    "    mask_df['available_mask'] = (1-Y_balanced_df.y.isnull().values)\n",
    "    mask_df['sample_mask'] = (1-last_timestamps_df['mask'])\n",
    "    del last_timestamps_df\n",
    "\n",
    "    # Plotting train validation splits\n",
    "    #y_train = Y_balanced_df[mask_df.sample_mask==1].y.values\n",
    "    #y_val = Y_balanced_df[mask_df.sample_mask==0].y.values\n",
    "    #plt.plot(y_train, label='train', color='blue')\n",
    "    #plt.plot(np.array(range(len(y_val))) + len(y_train), y_val, label='val', color='purple')\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    #---------------------------------------------- Scale Data ----------------------------------------------#\n",
    "\n",
    "    # Scale data # TODO: write sample_mask conditional/groupby(['unique_id]) scaling\n",
    "    train_mask = mask_df.available_mask.values * mask_df.available_mask.values\n",
    "    Y_scaled_df, X_scaled_df, scaler_y = scale_data(Y_df=Y_balanced_df, X_df=X_balanced_df, mask=train_mask,\n",
    "                                                    normalizer_y=mc['normalizer_y'], normalizer_x=mc['normalizer_x'])\n",
    "    del Y_balanced_df\n",
    "    del X_balanced_df\n",
    "\n",
    "    #-------------------------------------------- Declare Loaders -------------------------------------------#\n",
    "\n",
    "    ts_dataset = TimeSeriesDataset(Y_df=Y_scaled_df, X_df=X_scaled_df, S_df=S_df, mask_df=mask_df)\n",
    "\n",
    "    train_ts_loader = TimeSeriesLoader(ts_dataset=ts_dataset,\n",
    "                                       model='nbeats',\n",
    "                                       offset=0,\n",
    "                                       window_sampling_limit=ts_dataset.max_len,\n",
    "                                       input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                                       output_size=int(mc['output_size']),\n",
    "                                       idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                       batch_size=int(mc['batch_size']),\n",
    "                                       is_train_loader=True,\n",
    "                                       shuffle=True, random_seed=int(mc['random_seed']))\n",
    "\n",
    "    print(\"train_ts_loader.ts_windows.shape\", train_ts_loader.ts_windows.shape)\n",
    "    print(f\"len(train_sampling_windows) * 24 = {len(train_ts_loader.windows_sampling_idx)} * 24 = {len(train_ts_loader.windows_sampling_idx) * 24}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    val_ts_loader = TimeSeriesLoader(ts_dataset=ts_dataset,\n",
    "                                     model='nbeats',\n",
    "                                     offset=0,\n",
    "                                     window_sampling_limit=ts_dataset.max_len,\n",
    "                                     input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                                     output_size=int(mc['output_size']),\n",
    "                                     idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                     batch_size=1024,\n",
    "                                     is_train_loader=False, # Samples the opposite of train_outsample_mask\n",
    "                                     shuffle=False, random_seed=int(mc['random_seed']))\n",
    "\n",
    "    print(\"val_ts_loader.ts_windows.shape\", val_ts_loader.ts_windows.shape)\n",
    "    print(f\"len(val_sampling_windows) * 24 = {len(val_ts_loader.windows_sampling_idx)} * 24 = {len(val_ts_loader.windows_sampling_idx) * 24}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    mc['t_cols'] = ts_dataset.t_cols\n",
    "    return mc, train_ts_loader, val_ts_loader, scaler_y\n",
    "\n",
    "\n",
    "def model_fit_predict_roll(mc, Y_df, X_df, S_df, n_timestamps_pred, offsets):\n",
    "\n",
    "    X_raw_df = X_df.copy()\n",
    "    Y_raw_df = Y_df.copy()\n",
    "\n",
    "    #-------------------------------------- Rolling prediction on test --------------------------------------#\n",
    "\n",
    "    y_true_list = []\n",
    "    y_hat_list = []\n",
    "    #offsets = [365*1*24, 0]\n",
    "    n_splits = len(offsets)\n",
    "    for split, offset in enumerate(offsets):\n",
    "        print(10*'-', f'Split {split+1}/{n_splits}', 10*'-')\n",
    "\n",
    "        #----------------------------------------------- Data -----------------------------------------------#\n",
    "        mc, train_ts_loader, val_ts_loader, scaler_y = prepare_data_Kin(mc=mc, Y_df=Y_raw_df, X_df=X_raw_df,\n",
    "                                                              S_df=S_df, n_timestamps_pred=n_timestamps_pred,\n",
    "                                                              offset=offset)\n",
    "\n",
    "        #--------------------------------------- Finetune and predict ---------------------------------------#\n",
    "        # Instantiate and train model\n",
    "        model = Nbeats(input_size_multiplier=mc['input_size_multiplier'],\n",
    "                    output_size=int(mc['output_size']),\n",
    "                    shared_weights=mc['shared_weights'],\n",
    "                    initialization=mc['initialization'],\n",
    "                    activation=mc['activation'],\n",
    "                    stack_types=mc['stack_types'],\n",
    "                    n_blocks=mc['n_blocks'],\n",
    "                    n_layers=mc['n_layers'],\n",
    "                    #n_hidden=2*[2*[int(mc['n_hidden'])]], # TODO; Revisar n_hidden1, n_hidden2 <------\n",
    "                    n_hidden=mc['n_hidden_list'],\n",
    "                    #n_hidden=2*[[256,256]],\n",
    "                    n_harmonics=int(mc['n_harmonics']),\n",
    "                    n_polynomials=int(mc['n_polynomials']),\n",
    "                    x_s_n_hidden = 0,#int(mc['x_s_n_hidden']),\n",
    "                    exogenous_n_channels=int(mc['exogenous_n_channels']),\n",
    "                    include_var_dict=mc['include_var_dict'],\n",
    "                    t_cols=mc['t_cols'],\n",
    "                    batch_normalization = mc['batch_normalization'],\n",
    "                    dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                    dropout_prob_exogenous=mc['dropout_prob_exogenous'],\n",
    "                    learning_rate=float(mc['learning_rate']),\n",
    "                    lr_decay=float(mc['lr_decay']),\n",
    "                    n_lr_decay_steps=float(mc['n_lr_decay_steps']),\n",
    "                    weight_decay=mc['weight_decay'],\n",
    "                    l1_theta=mc['l1_theta'],\n",
    "                    n_iterations=int(mc['n_iterations']),\n",
    "                    early_stopping=int(mc['early_stopping']),\n",
    "                    loss=mc['loss'],\n",
    "                    loss_hypar=float(mc['loss_hypar']),\n",
    "                    val_loss=mc['val_loss'],\n",
    "                    frequency=mc['frequency'],\n",
    "                    seasonality=int(mc['seasonality']),\n",
    "                    random_seed=int(mc['random_seed']))\n",
    "\n",
    "        model.fit(train_ts_loader=train_ts_loader, val_ts_loader=val_ts_loader, eval_steps=mc['eval_steps'])\n",
    "        y_true, y_hat, _ = model.predict(ts_loader=val_ts_loader, eval_mode=True)\n",
    "        y_true_list.append(y_true)\n",
    "        y_hat_list.append(y_hat)\n",
    "    \n",
    "    y_total = np.vstack(y_true_list)\n",
    "    y_hat_total = np.vstack(y_hat_list)\n",
    "\n",
    "    print(f'y_total.shape (#n_windows, #lt) {y_total.shape}')\n",
    "    print(f'y_hat_total.shape (#n_windows, #lt) {y_hat_total.shape}')\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Reshape for univariate and panel model compatibility\n",
    "    n_series = train_ts_loader.ts_dataset.n_series\n",
    "    n_fcds = len(y_total) // n_series\n",
    "    output_size = y_hat_total.shape[1]\n",
    "    y_total = y_total.reshape(n_series, n_fcds, output_size)\n",
    "    y_hat_total = y_hat_total.reshape(n_series, n_fcds, output_size)\n",
    "\n",
    "    print(\"y_total.shape (#n_series, #n_fcds, #lt) \", y_total.shape)\n",
    "    print(\"y_hat_total.shape (#n_series, #n_fcds, #lt) \", y_hat_total.shape)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    meta_data = val_ts_loader.ts_dataset.meta_data\n",
    "\n",
    "    return y_total, y_hat_total, meta_data, model\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "#### CRISTIAN\n",
    "############################################################################################\n",
    "\n",
    "def train_val_split(len_series, offset, window_sampling_limit, n_val_weeks, ds_per_day):\n",
    "    last_ds = len_series - offset\n",
    "    first_ds = max(last_ds - window_sampling_limit, 0)\n",
    "\n",
    "    last_day = int(last_ds/ds_per_day)\n",
    "    first_day = int(first_ds/ds_per_day)\n",
    "\n",
    "    days = set(range(first_day, last_day)) # All days, to later get train days\n",
    "    # Sample weeks from here, -7 to avoid sampling from last week\n",
    "    # To not sample first week and have inputs\n",
    "    sampling_days = set(range(first_day + 7, last_day - 7))\n",
    "    validation_days = set({}) # Val days set\n",
    "    \n",
    "    # For loop for n of weeks in validation\n",
    "    for i in range(n_val_weeks):\n",
    "        # Sample random day, init of week\n",
    "        init_day = random.sample(sampling_days, 1)[0]\n",
    "        # Select days of sampled init of week\n",
    "        sampled_days = list(range(init_day, min(init_day+7, last_day)))\n",
    "        # Add days to validation days\n",
    "        validation_days.update(sampled_days)\n",
    "        # Remove days from sampling_days, including overlapping resulting previous week\n",
    "        days_to_remove = set(range(init_day-6, min(init_day+7, last_day)))\n",
    "        sampling_days = sampling_days.difference(days_to_remove)\n",
    "\n",
    "    train_days = days.difference(validation_days)\n",
    "\n",
    "    train_days = sorted(list(train_days))\n",
    "    validation_days = sorted(list(validation_days))\n",
    "\n",
    "    train_idx = []\n",
    "    for day in train_days:\n",
    "        hours_idx = range(day*ds_per_day,(day+1)*ds_per_day)\n",
    "        train_idx += hours_idx\n",
    "\n",
    "    val_idx = []\n",
    "    for day in validation_days:\n",
    "        hours_idx = range(day*ds_per_day,(day+1)*ds_per_day)\n",
    "        val_idx += hours_idx\n",
    "\n",
    "    assert all([idx < last_ds for idx in val_idx]), 'Leakage!!!!'\n",
    "    \n",
    "    return train_idx, val_idx\n",
    "\n",
    "def declare_mask_df_Cristian(mc, Y_df, X_df, S_df, offset):\n",
    "    # train_mask: 1 to keep, 0 to hide\n",
    "    train_outsample_mask = np.ones(len(Y_df), dtype=int)\n",
    "    # if random_validation:\n",
    "    #     print('Random validation activated')\n",
    "    #     np.random.seed(1)\n",
    "    #     random.seed(1)\n",
    "    #     _, val_idx = train_val_split(len_series=len(Y_df), offset=0,\n",
    "    #                             window_sampling_limit=window_sampling_limit,\n",
    "    #                             n_val_weeks=n_val_weeks, ds_per_day=24)\n",
    "    #     train_outsample_mask[val_idx] = 0\n",
    "    #else:\n",
    "    print('Random validation de-activated')\n",
    "    train_outsample_mask[-offset:] = 0\n",
    "\n",
    "    print(f'Train {sum(train_outsample_mask)} hours = {np.round(sum(train_outsample_mask)/(24*365),2)} years')\n",
    "    print(f'Validation {sum(1-train_outsample_mask)} hours = {np.round(sum(1-train_outsample_mask)/(24*365),2)} years')\n",
    "\n",
    "    mask_df = Y_df[['unique_id', 'ds', 'y']].copy()\n",
    "    mask_df['available_mask'] = np.ones(len(Y_df))\n",
    "    mask_df['sample_mask'] = train_outsample_mask\n",
    "    return mask_df\n",
    "\n",
    "def prepare_data_Cristian(mc, Y_df, X_df, S_df):\n",
    "\n",
    "    #window_sampling_limit = int(mc['window_sampling_limit_multiplier']) * int(mc['output_size'])\n",
    "        \n",
    "    #--------------------------------------- Train and Validation Mask --------------------------------------#\n",
    "    mask_df = declare_mask_df_Cristian(mc=mc, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "                                       offset=(mc['n_val_weeks'] * 7 * mc['output_size']))\n",
    "\n",
    "    #---------------------------------------------- Scale Data ----------------------------------------------#\n",
    "    \n",
    "    # Transform data with scale transformation (Y_df, X_df, offset, normalizer_x, normalizer_y\n",
    "    # Avoid change original data\n",
    "    Y_scaled_df = Y_df.copy()\n",
    "    X_scaled_df = X_df.copy()\n",
    "\n",
    "    # NO ME GUSTA QUE SE LLAME OUTSAMPLE, OUTSAMPLE SE USA COMO SINONIMO DE VALIDATION\n",
    "    train_mask = mask_df.available_mask.values * mask_df.available_mask.values\n",
    "    Y_scaled_df, X_scaled_df, scaler_y = scale_data(Y_df=Y_scaled_df,\n",
    "                                                     X_df=X_scaled_df,\n",
    "                                                     mask=train_mask,\n",
    "                                                     normalizer_y = mc['normalizer_y'],\n",
    "                                                     normalizer_x = mc['normalizer_x'])\n",
    "\n",
    "    #-------------------------------------------- Declare Loaders -------------------------------------------#\n",
    "\n",
    "    #ts_dataset = TimeSeriesDataset(Y_df=Y_scaled_df_scaled, X_df=X_df_scaled, ts_train_mask=train_outsample_mask)\n",
    "    ts_dataset = TimeSeriesDataset(Y_df=Y_scaled_df, X_df=X_scaled_df, S_df=S_df, mask_df=mask_df)\n",
    "\n",
    "    train_ts_loader = TimeSeriesLoader(model='nbeats',\n",
    "                                       ts_dataset=ts_dataset,\n",
    "                                       window_sampling_limit=ts_dataset.max_len,#window_sampling_limit,\n",
    "                                       offset=0,\n",
    "                                       input_size=int(mc['input_size_multiplier'] * mc['output_size']),\n",
    "                                       output_size=int(mc['output_size']),\n",
    "                                       idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                       batch_size=int(mc['batch_size']),\n",
    "                                       is_train_loader=True,\n",
    "                                       random_seed=int(mc['random_seed']),\n",
    "                                       shuffle=True)\n",
    "\n",
    "    val_ts_loader = TimeSeriesLoader(model='nbeats',\n",
    "                                     ts_dataset=ts_dataset,\n",
    "                                     window_sampling_limit=ts_dataset.max_len,\n",
    "                                     offset=0,\n",
    "                                     input_size=int(mc['input_size_multiplier'] * mc['output_size']),\n",
    "                                     output_size=int(mc['output_size']),\n",
    "                                     idx_to_sample_freq=24, #TODO: pensar esto\n",
    "                                     batch_size=int(mc['batch_size']),\n",
    "                                     is_train_loader=False,\n",
    "                                     random_seed=int(mc['random_seed']),\n",
    "                                     shuffle=False)\n",
    "\n",
    "    mc['t_cols'] = ts_dataset.t_cols\n",
    "    return mc, train_ts_loader, val_ts_loader, scaler_y\n"
   ]
  },
  {
   "source": [
    "# RUN VALIDATION CODE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# run_val_nbeatsx(hyperparameters, Y_df, X_df, data_augmentation, random_validation, trials, trials_file_name)\n",
    "def run_val_nbeatsx(mc, Y_df, X_df, S_df, trials, trials_file_name, final_evaluation=False, return_predictions=False):\n",
    "\n",
    "    # Save trials, can analyze progress\n",
    "    if trials is not None:\n",
    "        save_every_n_step = 5\n",
    "        current_step = len(trials.trials)\n",
    "        if (current_step % save_every_n_step==0):\n",
    "            with open(trials_file_name, \"wb\") as f:\n",
    "                pickle.dump(trials, f)\n",
    "\n",
    "    start_time = time.time()    \n",
    "    \n",
    "    #---------------------------------------- Parse  Hyperparameters ----------------------------------------#\n",
    "    mc['include_var_dict'] = {'y': [-2, -3, -8],\n",
    "                              'Exogenous1': [-1, -2, -8],\n",
    "                              'Exogenous2': [-1, -2, -8],\n",
    "                              'week_day': [-1]}\n",
    "\n",
    "    n_hidden = int(mc['n_hidden'])\n",
    "    mc['n_hidden_list'] =  2*[[n_hidden, n_hidden]]\n",
    "    \n",
    "    #---------------------------------- Instantiate model, fit and predict ----------------------------------#\n",
    "\n",
    "    # y_total, y_hat_total, meta_data, model = model_fit_predict_roll(mc=mc, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "    #                                                                offsets=[0], n_timestamps_pred=365*2*24)\n",
    "\n",
    "    # y_total, y_hat_total, meta_data, model = model_fit_predict_roll(mc=mc, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "    #                                                                 offsets=[365*1*24, 0], n_timestamps_pred=365*1*24)\n",
    "\n",
    "    y_total, y_hat_total, meta_data, model = model_fit_predict_roll(mc=mc, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "                                                                   offsets=[9*73*24, 8*73*24, 7*73*24, 6*73*24, 5*73*24,\n",
    "                                                                            4*73*24, 3*73*24, 2*73*24, 1*73*24, 0],\n",
    "                                                                   n_timestamps_pred=73*24)\n",
    "\n",
    "    print('Best Model Evaluation')\n",
    "    benchmark_df, y_total_nans_perc, y_hat_total_nans_perc, reported_loss = forecast_evaluation_table(y_total=y_total, \n",
    "                                                                                                      y_hat_total=y_hat_total, \n",
    "                                                                                                      meta_data=meta_data)\n",
    "    print(benchmark_df)\n",
    "    print('\\n')\n",
    "    \n",
    "    # CONDITIONAL ON CORRECT PREDICTION \n",
    "    # Average percentage difference of MAE, SMAPE, MAPE, RMSE\n",
    "    #reported_loss = protect_nan_reported_loss(model)\n",
    "    run_time = time.time() - start_time\n",
    "\n",
    "    results =  {'loss': reported_loss,\n",
    "                'loss_name': mc['val_loss'],\n",
    "                'mc': mc,\n",
    "                'final_insample_loss': model.final_insample_loss,\n",
    "                'final_outsample_loss': model.final_outsample_loss,\n",
    "                'trajectories': model.trajectories,\n",
    "                'run_time': run_time,\n",
    "                'status': STATUS_OK}\n",
    "    \n",
    "    if final_evaluation:\n",
    "        print('Best Model Hyperpars')\n",
    "        print(75*'=')\n",
    "        print(pd.Series(mc))\n",
    "        print(75*'='+'\\n')\n",
    "    \n",
    "    if return_predictions:\n",
    "        return y_total, y_hat_total, meta_data\n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "source": [
    "# EXPERIMENT SPACES"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# TODO: eliminate n_harmonics, n_polynomials think on kwargs maybe?\n",
    "# TODO: think on n_consistency for exogenous_n_channels -> n_xt_channels\n",
    "# TODO: x_s_n_hidden -> n_xs_hidden\n",
    "# TODO: input_size_multiplier -> Change for n_xt?\n",
    "# TODO: n_hidden -> n_theta_list\n",
    "def get_experiment_space(args):\n",
    "    if args.space=='nbeats_cristian':\n",
    "        space = {# Architecture parameters\n",
    "                 'input_size_multiplier': hp.choice('input_size_multiplier', [7]),\n",
    "                 'output_size': hp.choice('output_size', [24]),\n",
    "                 'shared_weights': hp.choice('shared_weights', [False]),\n",
    "                 'activation': hp.choice('activation', ['relu','softplus','tanh','selu','lrelu','prelu','sigmoid']),\n",
    "                 'initialization':  hp.choice('initialization', ['orthogonal','he_uniform','he_normal',\n",
    "                                                                 'glorot_uniform','glorot_normal','lecun_normal']),\n",
    "                 'stack_types': hp.choice('stack_types', [ ['identity'],\n",
    "                                                            1*['identity']+['exogenous_wavenet'],\n",
    "                                                                ['exogenous_wavenet']+1*['identity'],\n",
    "                                                            1*['identity']+['exogenous_tcn'],\n",
    "                                                                ['exogenous_tcn']+1*['identity'] ]),\n",
    "                 'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),\n",
    "                 'n_layers': hp.choice('n_layers', [ [2, 2] ]),\n",
    "                 'n_hidden': hp.quniform('n_hidden', 50, 500, 1),\n",
    "                 'n_harmonics': hp.choice('n_harmonics', [1]),\n",
    "                 'n_polynomials': hp.choice('n_polynomials', [2]),\n",
    "                 'exogenous_n_channels': hp.quniform('exogenous_n_channels', 1, 10, 1),\n",
    "                 'x_s_n_hidden': hp.choice('x_s_n_hidden', [0]),\n",
    "                 # Regularization and optimization parameters\n",
    "                 'batch_normalization': hp.choice('batch_normalization', [True, False]),\n",
    "                 'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 1),\n",
    "                 'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),\n",
    "                 'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.1)),\n",
    "                 'lr_decay': hp.choice('lr_decay', [0.5]),\n",
    "                 'n_lr_decay_steps': hp.choice('n_lr_decay_steps', [3]),\n",
    "                 'weight_decay': hp.loguniform('weight_decay', np.log(5e-4), np.log(0.01)),\n",
    "                 'n_iterations': hp.choice('n_iterations', [args.max_epochs]),\n",
    "                 'early_stopping': hp.choice('early_stopping', [8]),\n",
    "                 'eval_steps': hp.choice('eval_steps', [50]),\n",
    "                 #'n_val_weeks': hp.choice('n_val_weeks', [52]), # NUEVO <---------\n",
    "                 'n_val_weeks': hp.choice('n_val_weeks', [52*2]), # NUEVO <---------\n",
    "                 'loss': hp.choice('loss', ['MAE']),\n",
    "                 'loss_hypar': hp.choice('loss_hypar', [0.5]),\n",
    "                 'val_loss': hp.choice('val_loss', [args.val_loss]),\n",
    "                 'l1_theta': hp.choice('l1_theta', [0, hp.loguniform('lambdal1', np.log(1e-5), np.log(1))]),\n",
    "                 # Data parameters\n",
    "                 'normalizer_y': hp.choice('normalizer_y', [None, 'norm', 'norm1', \n",
    "                                                            'std', 'median', 'invariant']), # NUEVO <---------\n",
    "                 'normalizer_x': hp.choice('normalizer_x', [None, 'norm', 'norm1',\n",
    "                                                            'std', 'median', 'invariant']), # NUEVO <---------\n",
    "                 'frequency': hp.choice('frequency', ['H']),\n",
    "                 'seasonality': hp.choice('seasonality', [24]),\n",
    "                 'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [24]), # NUEVO args <----------\n",
    "                 'batch_size': hp.choice('batch_size', [128, 256, 512]), # NUEVO <---------\n",
    "                 'random_seed': hp.quniform('random_seed', 1, 1000, 1)}\n",
    "                 # CONSIDERO ESTO INNECESARIO\n",
    "                 # 'n_hidden_1': hp.quniform('n_hidden_1', 50, 500, 1),\n",
    "                 # 'n_hidden_2': hp.quniform('n_hidden_2', 50, 500, 1),\n",
    "                 # 'include_var_dict': hp.choice('include_var_dict', [{'y': [-2, -3, -8],\n",
    "                 #                                                     'Exogenous1': [-1, -2, -8],\n",
    "                 #                                                     'Exogenous2': [-1, -2, -8],\n",
    "                 #                                                     'week_day': [-1]}]),                 \n",
    "                 # 'incl_pr1': hp.choice('incl_pr1', [True]),\n",
    "                 # 'incl_pr2': hp.choice('incl_pr2', [True, False]),\n",
    "                 # 'incl_pr3': hp.choice('incl_pr3', [True, False]),\n",
    "                 # 'incl_pr7': hp.choice('incl_pr7', [True, False]),\n",
    "                 # 'incl_ex1_0': hp.choice('incl_ex1_0', [True, False]),\n",
    "                 # 'incl_ex1_1': hp.choice('incl_ex1_1', [True, False]),\n",
    "                 # 'incl_ex1_7': hp.choice('incl_ex1_7', [True, False]),\n",
    "                 # 'incl_ex2_0': hp.choice('incl_ex2_0', [True, False]),\n",
    "                 # 'incl_ex2_1': hp.choice('incl_ex2_1', [True, False]),\n",
    "                 # 'incl_ex2_7': hp.choice('incl_ex2_7', [True, False]),\n",
    "                 # 'incl_day': hp.choice('incl_day', [True, False]),\n",
    "                 # 'args.data_augmentation'\n",
    "                 # 'n_val_weeks': hp.choice('n_val_weeks', [args.n_val_weeks]}\n",
    "\n",
    "    elif args.space=='nbeats_collapsed':\n",
    "        space= {# Architecture parameters\n",
    "                'input_size_multiplier': hp.choice('input_size_multiplier', [7]),\n",
    "                'output_size': hp.choice('output_size', [24]),\n",
    "                'shared_weights': hp.choice('shared_weights', [False]),\n",
    "                'activation': hp.choice('activation', ['relu','softplus','tanh','selu','lrelu','prelu','sigmoid']),\n",
    "                'initialization':  hp.choice('initialization', ['orthogonal','he_uniform','he_normal',\n",
    "                                                                'glorot_uniform','glorot_normal','lecun_normal']),\n",
    "                'stack_types': hp.choice('stack_types', [['exogenous_wavenet']+1*['identity'],\n",
    "                                                         ['exogenous_tcn']+1*['identity'] ]),\n",
    "                'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),\n",
    "                'n_layers': hp.choice('n_layers', [ [2, 2] ]),\n",
    "                'n_hidden': hp.quniform('n_hidden', 50, 500, 1),\n",
    "                'n_harmonics': hp.choice('n_harmonics', [1]),\n",
    "                'n_polynomials': hp.choice('n_polynomials', [2]),\n",
    "                'exogenous_n_channels': hp.quniform('exogenous_n_channels', 1, 10, 1),\n",
    "                'x_s_n_hidden': hp.choice('x_s_n_hidden', [0]),\n",
    "                # Regularization and optimization parameters\n",
    "                'batch_normalization': hp.choice('batch_normalization', [False]),\n",
    "                'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 1),\n",
    "                'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),\n",
    "                'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.1)),\n",
    "                'lr_decay': hp.uniform('lr_decay', 0.3, 1.0),\n",
    "                'n_lr_decay_steps': hp.choice('n_lr_decay_steps', [3]),\n",
    "                'weight_decay': hp.loguniform('weight_decay', np.log(5e-5), np.log(5e-3)),\n",
    "                'n_iterations': hp.choice('n_iterations', [args.max_epochs]),\n",
    "                'early_stopping': hp.choice('early_stopping', [8]),\n",
    "                'eval_steps': hp.choice('eval_steps', [50]),\n",
    "                'n_val_weeks': hp.choice('n_val_weeks', [52*2]), # NUEVO <---------\n",
    "                #'loss': hp.choice('loss', ['PINBALL']),\n",
    "                #'loss_hypar': hp.uniform('loss_hypar', 0.48, 0.51),\n",
    "                'loss': hp.choice('loss', ['MAE']),\n",
    "                'loss_hypar': hp.choice('loss_hypar', [0.5]),                \n",
    "                'val_loss': hp.choice('val_loss', [args.val_loss]),\n",
    "                'l1_theta': hp.choice('l1_theta', [0, hp.loguniform('lambdal1', np.log(1e-5), np.log(1))]),\n",
    "                # Data parameters\n",
    "                'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "                'normalizer_x': hp.choice('normalizer_x', ['median']),\n",
    "                'frequency': hp.choice('frequency', ['H']),\n",
    "                'seasonality': hp.choice('seasonality', [24]),\n",
    "                'include_var_dict': hp.choice('include_var_dict', [{'y': [-2, -3, -8],\n",
    "                                                                    'Exogenous1': [-1, -2, -8],\n",
    "                                                                    'Exogenous2': [-1, -2, -8],\n",
    "                                                                    'week_day': [-1]}]),\n",
    "                'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [24]),\n",
    "                'batch_size': hp.choice('batch_size', [256]),\n",
    "                'random_seed': hp.quniform('random_seed', 10, 20, 1)}\n",
    "    \n",
    "    elif args.space=='nbeats_collapsed2':\n",
    "        space= {# Architecture parameters\n",
    "                'input_size_multiplier': hp.choice('input_size_multiplier', [7]),\n",
    "                'output_size': hp.choice('output_size', [24]),\n",
    "                'shared_weights': hp.choice('shared_weights', [False]),\n",
    "                'activation': hp.choice('activation', ['selu']),\n",
    "                'initialization':  hp.choice('initialization', ['glorot_normal','he_normal']),\n",
    "                'stack_types': hp.choice('stack_types', [2*['identity'],\n",
    "                                                         1*['identity']+1*['exogenous_tcn'],\n",
    "                                                         1*['exogenous_tcn']+1*['identity'] ]),\n",
    "                'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),\n",
    "                'n_layers': hp.choice('n_layers', [ [2, 2] ]),\n",
    "                #'n_hidden': hp.choice('n_hidden', [400]),\n",
    "                'n_hidden': hp.choice('n_hidden', [400]),\n",
    "                'n_harmonics': hp.choice('n_harmonics', [1]),\n",
    "                'n_polynomials': hp.choice('n_polynomials', [2]),\n",
    "                'exogenous_n_channels': hp.quniform('exogenous_n_channels', 1, 10, 1),\n",
    "                'x_s_n_hidden': hp.choice('x_s_n_hidden', [0]),\n",
    "                # Regularization and optimization parameters\n",
    "                'batch_normalization': hp.choice('batch_normalization', [False]),\n",
    "                'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 0.5),\n",
    "                'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),\n",
    "                'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.001)),\n",
    "                'lr_decay': hp.uniform('lr_decay', 0.3, 0.5),\n",
    "                'n_lr_decay_steps': hp.choice('n_lr_decay_steps', [3]),\n",
    "                'weight_decay': hp.loguniform('weight_decay', np.log(5e-5), np.log(5e-3)),\n",
    "                'n_iterations': hp.choice('n_iterations', [args.max_epochs]),\n",
    "                'early_stopping': hp.choice('early_stopping', [16]),\n",
    "                'eval_steps': hp.choice('eval_steps', [50]),\n",
    "                'n_val_weeks': hp.choice('n_val_weeks', [52*2]), # NUEVO <---------\n",
    "                #'loss': hp.choice('loss', ['PINBALL']),\n",
    "                #'loss_hypar': hp.uniform('loss_hypar', 0.48, 0.51),\n",
    "                'loss': hp.choice('loss', ['MAE']),\n",
    "                'loss_hypar': hp.choice('loss_hypar', [0.5]),                \n",
    "                'val_loss': hp.choice('val_loss', [args.val_loss]),\n",
    "                'l1_theta': hp.choice('l1_theta', [0]),\n",
    "                # Data parameters\n",
    "                'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "                'normalizer_x': hp.choice('normalizer_x', ['median']),\n",
    "                'frequency': hp.choice('frequency', ['H']),\n",
    "                'seasonality': hp.choice('seasonality', [24]),\n",
    "                'include_var_dict': hp.choice('include_var_dict', [{'y': [-2, -3, -8],\n",
    "                                                                    'Exogenous1': [-1, -2, -8],\n",
    "                                                                    'Exogenous2': [-1, -2, -8],\n",
    "                                                                    'week_day': [-1]}]),\n",
    "                'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [24]),\n",
    "                'batch_size': hp.choice('batch_size', [256]),\n",
    "                'random_seed': hp.quniform('random_seed', 10, 20, 1)}                \n",
    "    \n",
    "    else:\n",
    "        print(f'Experiment space {args.space} not available')\n",
    "\n",
    "    return space\n",
    "\n",
    "def parse_trials(trials):\n",
    "    # Initialize\n",
    "    trials_dict = {'tid': [], 'loss': [], 'trajectories': [], 'mc': []}\n",
    "    for tidx in range(len(trials)):\n",
    "        # Main\n",
    "        trials_dict['tid']  += [trials.trials[tidx]['tid']]\n",
    "        trials_dict['loss'] += [trials.trials[tidx]['result']['loss']]\n",
    "        trials_dict['trajectories'] += [trials.trials[tidx]['result']['trajectories']]\n",
    "\n",
    "        # Model Configs\n",
    "        mc = trials.trials[tidx]['result']['mc']\n",
    "        trials_dict['mc'] += [mc]\n",
    "    \n",
    "    trials_df = pd.DataFrame(trials_dict)\n",
    "    return trials_df\n",
    "\n",
    "def main(args):\n",
    "    #---------------------------------------------- Directories ----------------------------------------------#\n",
    "    \n",
    "    dataset = eval(args.dataset)\n",
    "    dataset_str = dataset[0]\n",
    "    for market in dataset[1:]:\n",
    "        dataset_str += f'{market}_'\n",
    "    output_dir = f'./results/{dataset_str}/{args.space}/'\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    assert os.path.exists(output_dir), f'Output dir {output_dir} does not exist'\n",
    "\n",
    "    hyperopt_file = output_dir + f'hyperopt_{args.experiment_id}.p'\n",
    "    result_test_file = output_dir + f'result_test_{args.experiment_id}.p'\n",
    "\n",
    "    #---------------------------------------------- Read  Data ----------------------------------------------#\n",
    "    print('\\n'+75*'-')\n",
    "    print(28*'-', 'Preparing Dataset', 28*'-')\n",
    "    print(75*'-'+'\\n')\n",
    "\n",
    "    #TEST_DATE = {'NP': '2016-12-27',\n",
    "    #             'PJM':'2016-12-27',\n",
    "    #             'BE':'2015-01-04',\n",
    "    #             'FR': '2015-01-04',\n",
    "    #             'DE':'2016-01-04'}\n",
    "    #test_date = TEST_DATE[args.dataset]\n",
    "    #Y_insample_df, X_insample_df, Y_outsample_df, X_outsample_df, _ = load_epf(directory='../data/',\n",
    "    #                                                                           market=args.dataset,\n",
    "    #                                                                           first_date_test=test_date,\n",
    "    #                                                                           days_in_test=728)\n",
    "    Y_df, X_df, S_df = EPF.load_groups(directory='data', groups=dataset)\n",
    "\n",
    "    #-------------------------------------- Hyperparameter Optimization --------------------------------------#\n",
    "\n",
    "    if not os.path.isfile(hyperopt_file):\n",
    "        print('\\n'+75*'-')\n",
    "        print(22*'-', 'Start Hyperparameter  tunning', 22*'-')\n",
    "        print(75*'-'+'\\n')\n",
    "\n",
    "        space = get_experiment_space(args)\n",
    "\n",
    "        trials = Trials()\n",
    "        fmin_objective = partial(run_val_nbeatsx, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "                                 trials=trials, trials_file_name=hyperopt_file)\n",
    "        fmin(fmin_objective, space=space, algo=tpe.suggest, max_evals=args.hyperopt_iters, trials=trials, verbose=True)\n",
    "\n",
    "        # Save output\n",
    "        with open(hyperopt_file, \"wb\") as f:\n",
    "            pickle.dump(trials, f)\n",
    "\n",
    "    print('\\n'+75*'-')\n",
    "    print(20*'-', 'Hyperparameter  tunning  finished', 20*'-')\n",
    "    print(75*'-'+'\\n')\n",
    "\n",
    "    #----------------------------------------- Selected 'Best' Model -----------------------------------------#\n",
    "\n",
    "    # Read and parse trials pickle\n",
    "    trials = pickle.load(open(hyperopt_file, 'rb'))\n",
    "    trials_df = parse_trials(trials)\n",
    "\n",
    "    # Get best mc\n",
    "    idx = trials_df.loss.idxmin()\n",
    "    best_mc = trials_df.loc[idx]['mc']\n",
    "    \n",
    "    run_val_nbeatsx(best_mc, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "                    trials=trials, trials_file_name=hyperopt_file, final_evaluation=True)\n",
    "\n",
    "def parse_args():\n",
    "    desc = \"NBEATSx overfit\"\n",
    "    parser = argparse.ArgumentParser(description=desc)\n",
    "\n",
    "    parser.add_argument('--dataset', type=str, required=True, help='NP')\n",
    "    parser.add_argument('--space', type=str, required=True, help='Experiment hyperparameter space')\n",
    "    parser.add_argument('--hyperopt_iters', type=int, help='hyperopt_iters')\n",
    "    parser.add_argument('--max_epochs', type=int, required=False, default=2000, help='max train epochs')\n",
    "    parser.add_argument('--val_loss', type=str, required=False, default=None, help='validation loss')\n",
    "    parser.add_argument('--experiment_id', default=None, required=False, type=str, help='string to identify experiment')\n",
    "    return parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # parse arguments\n",
    "    args = parse_args()\n",
    "    if args is None:\n",
    "        exit()\n",
    "    \n",
    "    main(args)\n",
    "\n",
    "# CUDA_VISIBLE_DEVICES=0 PYTHONPATH=. python src/overfit_nbeatsx.py --dataset \"['NP']\" --space 'nbeats_collapsed' --hyperopt_iters 200 --val_loss 'MAE'  --experiment_id 'AvgDiff_2021_01_16'\n",
    "# CUDA_VISIBLE_DEVICES=0 PYTHONPATH=. python src/overfit_nbeatsx.py --dataset \"['NP']\" --space 'nbeats_collapsed' --hyperopt_iters 200 --val_loss 'MAE'  --experiment_id 'AvgDiff_2021_01_16'\n",
    "# CUDA_VISIBLE_DEVICES=1 PYTHONPATH=. python src/overfit_nbeatsx.py --dataset \"['NP']\" --space 'nbeats_collapsed' --hyperopt_iters 200 --val_loss 'MAE'  --experiment_id 'AvgDiff_2021_01_16'\n",
    "# CUDA_VISIBLE_DEVICES=1 PYTHONPATH=. python src/overfit_nbeatsx.py --dataset \"['NP']\" --space 'nbeats_collapsed' --hyperopt_iters 200 --val_loss 'MAE'  --experiment_id 'AvgDiff_2021_01_16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BALL: 2.35189, Outsample MAE: 2.41382\n",
      "Step: 120, Time: 13.875, Insample PINBALL: 2.96290, Outsample MAE: 2.54486\n",
      "Step: 130, Time: 15.054, Insample PINBALL: 2.80893, Outsample MAE: 2.48387\n",
      "Step: 140, Time: 16.199, Insample PINBALL: 1.63827, Outsample MAE: 2.56648\n",
      "Step: 150, Time: 17.374, Insample PINBALL: 1.40603, Outsample MAE: 2.49040\n",
      "Step: 160, Time: 18.510, Insample PINBALL: 1.45346, Outsample MAE: 2.51064\n",
      "Step: 170, Time: 19.685, Insample PINBALL: 1.34347, Outsample MAE: 2.42591\n",
      "Step: 180, Time: 20.833, Insample PINBALL: 1.37040, Outsample MAE: 2.40786\n",
      "Step: 190, Time: 22.051, Insample PINBALL: 1.26569, Outsample MAE: 2.29023\n",
      "Step: 200, Time: 23.203, Insample PINBALL: 1.19863, Outsample MAE: 2.33174\n",
      "Step: 200, Time: 23.221, Insample PINBALL: 1.19863, Outsample MAE: 2.33174\n",
      "==============================  End fitting  ==============================\n",
      "\n",
      "\n",
      "---------- Split 5/10 ----------\n",
      "\n",
      "\n",
      "Y_df.shape \t(26184, 3)\n",
      "X_df.shape \t(26184, 12)\n",
      "Y_balanced_df.shape \t(26184, 3)\n",
      "X_balanced_df.shape \t(26184, 12)\n",
      "Train Validation splits\n",
      "                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0.0         2015-10-16 2015-12-27 23:00:00\n",
      "          1.0         2013-01-01 2015-10-15 23:00:00\n",
      "Total data \t\t\t26184 time stamps\n",
      "Available percentage=100.0, \t26184 time stamps\n",
      "Train percentage=93.31, \t24432.0 time stamps\n",
      "Predict percentage=6.69, \t1752.0 time stamps\n",
      "\n",
      "\n",
      "train_ts_loader.ts_windows.shape torch.Size([1092, 13, 192])\n",
      "len(train_sampling_windows) * 24 = 1018 * 24 = 24432\n",
      "\n",
      "\n",
      "val_ts_loader.ts_windows.shape torch.Size([1092, 13, 192])\n",
      "len(val_sampling_windows) * 24 = 73 * 24 = 1752\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================== Start fitting ==============================\n",
      "Step: 10, Time: 1.224, Insample PINBALL: 81.67664, Outsample MAE: 62.95833\n",
      "Step: 20, Time: 2.440, Insample PINBALL: 110.66177, Outsample MAE: 71.55428\n",
      "Step: 30, Time: 3.702, Insample PINBALL: 149.19402, Outsample MAE: 149.31963\n",
      "Step: 40, Time: 4.940, Insample PINBALL: 8.36442, Outsample MAE: 3.88675\n",
      "Step: 50, Time: 6.157, Insample PINBALL: 6.70104, Outsample MAE: 3.04524\n",
      "Step: 60, Time: 7.378, Insample PINBALL: 5.12137, Outsample MAE: 2.87719\n",
      "Step: 70, Time: 8.595, Insample PINBALL: 3.60504, Outsample MAE: 2.40627\n",
      "Step: 80, Time: 9.811, Insample PINBALL: 2.81218, Outsample MAE: 2.46010\n",
      "Step: 90, Time: 11.044, Insample PINBALL: 3.04725, Outsample MAE: 2.30128\n",
      "Step: 100, Time: 12.279, Insample PINBALL: 2.93980, Outsample MAE: 2.39035\n",
      "Step: 110, Time: 13.509, Insample PINBALL: 2.19490, Outsample MAE: 2.12431\n",
      "Step: 120, Time: 14.735, Insample PINBALL: 1.95553, Outsample MAE: 2.03847\n",
      "Step: 130, Time: 15.959, Insample PINBALL: 1.66854, Outsample MAE: 2.06793\n",
      "Step: 140, Time: 17.184, Insample PINBALL: 1.47431, Outsample MAE: 2.06256\n",
      "Step: 150, Time: 18.413, Insample PINBALL: 1.22358, Outsample MAE: 2.02673\n",
      "Step: 160, Time: 19.633, Insample PINBALL: 1.32533, Outsample MAE: 1.99413\n",
      "Step: 170, Time: 20.856, Insample PINBALL: 1.13954, Outsample MAE: 1.98660\n",
      "Step: 180, Time: 22.082, Insample PINBALL: 1.15277, Outsample MAE: 1.99710\n",
      "Step: 190, Time: 23.312, Insample PINBALL: 1.13285, Outsample MAE: 2.04064\n",
      "Step: 200, Time: 24.529, Insample PINBALL: 1.01217, Outsample MAE: 1.99147\n",
      "Step: 200, Time: 24.548, Insample PINBALL: 1.01217, Outsample MAE: 1.99147\n",
      "==============================  End fitting  ==============================\n",
      "\n",
      "\n",
      "---------- Split 6/10 ----------\n",
      "\n",
      "\n",
      "Y_df.shape \t(27936, 3)\n",
      "X_df.shape \t(27936, 12)\n",
      "Y_balanced_df.shape \t(27936, 3)\n",
      "X_balanced_df.shape \t(27936, 12)\n",
      "Train Validation splits\n",
      "                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0.0         2015-12-28 2016-03-09 23:00:00\n",
      "          1.0         2013-01-01 2015-12-27 23:00:00\n",
      "Total data \t\t\t27936 time stamps\n",
      "Available percentage=100.0, \t27936 time stamps\n",
      "Train percentage=93.73, \t26184.0 time stamps\n",
      "Predict percentage=6.27, \t1752.0 time stamps\n",
      "\n",
      "\n",
      "train_ts_loader.ts_windows.shape torch.Size([1165, 13, 192])\n",
      "len(train_sampling_windows) * 24 = 1091 * 24 = 26184\n",
      "\n",
      "\n",
      "val_ts_loader.ts_windows.shape torch.Size([1165, 13, 192])\n",
      "len(val_sampling_windows) * 24 = 73 * 24 = 1752\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================== Start fitting ==============================\n",
      "Step: 10, Time: 1.055, Insample PINBALL: 86.53400, Outsample MAE: 37.28264\n",
      "Step: 20, Time: 2.133, Insample PINBALL: 26.85617, Outsample MAE: 42.62775\n",
      "Step: 30, Time: 3.210, Insample PINBALL: 11.59040, Outsample MAE: 5.50429\n",
      "Step: 40, Time: 4.686, Insample PINBALL: 6.30535, Outsample MAE: 5.40833\n",
      "Step: 50, Time: 5.936, Insample PINBALL: 5.18460, Outsample MAE: 5.07028\n",
      "Step: 60, Time: 7.226, Insample PINBALL: 4.78976, Outsample MAE: 5.10188\n",
      "Step: 70, Time: 8.521, Insample PINBALL: 2.82248, Outsample MAE: 4.86734\n",
      "Step: 80, Time: 9.811, Insample PINBALL: 2.23853, Outsample MAE: 4.97593\n",
      "Step: 90, Time: 10.975, Insample PINBALL: 2.15185, Outsample MAE: 4.96931\n",
      "Step: 100, Time: 12.084, Insample PINBALL: 1.82353, Outsample MAE: 4.83538\n",
      "Step: 110, Time: 13.190, Insample PINBALL: 1.72209, Outsample MAE: 4.65986\n",
      "Step: 120, Time: 14.288, Insample PINBALL: 1.66546, Outsample MAE: 4.60784\n",
      "Step: 130, Time: 15.419, Insample PINBALL: 1.57392, Outsample MAE: 4.53044\n",
      "Step: 140, Time: 16.666, Insample PINBALL: 1.22506, Outsample MAE: 4.52589\n",
      "Step: 150, Time: 17.810, Insample PINBALL: 1.09423, Outsample MAE: 4.50419\n",
      "Step: 160, Time: 18.952, Insample PINBALL: 1.07112, Outsample MAE: 4.46410\n",
      "Step: 170, Time: 20.013, Insample PINBALL: 1.16970, Outsample MAE: 4.42965\n",
      "Step: 180, Time: 21.191, Insample PINBALL: 1.26827, Outsample MAE: 4.40724\n",
      "Step: 190, Time: 22.260, Insample PINBALL: 1.10366, Outsample MAE: 4.37439\n",
      "Step: 200, Time: 23.324, Insample PINBALL: 0.95554, Outsample MAE: 4.38021\n",
      "Step: 200, Time: 23.343, Insample PINBALL: 0.95554, Outsample MAE: 4.38021\n",
      "==============================  End fitting  ==============================\n",
      "\n",
      "\n",
      "---------- Split 7/10 ----------\n",
      "\n",
      "\n",
      "Y_df.shape \t(29688, 3)\n",
      "X_df.shape \t(29688, 12)\n",
      "Y_balanced_df.shape \t(29688, 3)\n",
      "X_balanced_df.shape \t(29688, 12)\n",
      "Train Validation splits\n",
      "                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0.0         2016-03-10 2016-05-21 23:00:00\n",
      "          1.0         2013-01-01 2016-03-09 23:00:00\n",
      "Total data \t\t\t29688 time stamps\n",
      "Available percentage=100.0, \t29688 time stamps\n",
      "Train percentage=94.1, \t27936.0 time stamps\n",
      "Predict percentage=5.9, \t1752.0 time stamps\n",
      "\n",
      "\n",
      "train_ts_loader.ts_windows.shape torch.Size([1238, 13, 192])\n",
      "len(train_sampling_windows) * 24 = 1164 * 24 = 27936\n",
      "\n",
      "\n",
      "val_ts_loader.ts_windows.shape torch.Size([1238, 13, 192])\n",
      "len(val_sampling_windows) * 24 = 73 * 24 = 1752\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================== Start fitting ==============================\n",
      "Step: 10, Time: 1.269, Insample PINBALL: 55.36561, Outsample MAE: 50.98143\n",
      "Step: 20, Time: 2.566, Insample PINBALL: 56.09253, Outsample MAE: 202.80627\n",
      "Step: 30, Time: 3.739, Insample PINBALL: 30.94217, Outsample MAE: 34.50789\n",
      "Step: 40, Time: 4.868, Insample PINBALL: 26.42531, Outsample MAE: 19.72097\n",
      "Step: 50, Time: 6.002, Insample PINBALL: 7.16253, Outsample MAE: 1.84980\n",
      "Step: 60, Time: 7.135, Insample PINBALL: 6.00522, Outsample MAE: 1.74302\n",
      "Step: 70, Time: 8.263, Insample PINBALL: 4.22032, Outsample MAE: 1.52175\n",
      "Step: 80, Time: 9.394, Insample PINBALL: 3.36154, Outsample MAE: 1.57196\n",
      "Step: 90, Time: 10.556, Insample PINBALL: 2.48582, Outsample MAE: 1.49728\n",
      "Step: 100, Time: 11.688, Insample PINBALL: 1.97036, Outsample MAE: 1.52264\n",
      "Step: 110, Time: 12.824, Insample PINBALL: 2.00632, Outsample MAE: 1.45172\n",
      "Step: 120, Time: 14.066, Insample PINBALL: 2.15559, Outsample MAE: 1.49255\n",
      "Step: 130, Time: 15.210, Insample PINBALL: 1.85965, Outsample MAE: 1.50246\n",
      "Step: 140, Time: 16.349, Insample PINBALL: 1.48804, Outsample MAE: 1.33882\n",
      "Step: 150, Time: 17.485, Insample PINBALL: 1.35190, Outsample MAE: 1.38484\n",
      "Step: 160, Time: 18.629, Insample PINBALL: 1.30818, Outsample MAE: 1.31958\n",
      "Step: 170, Time: 19.767, Insample PINBALL: 1.48124, Outsample MAE: 1.32066\n",
      "Step: 180, Time: 20.909, Insample PINBALL: 1.25945, Outsample MAE: 1.29234\n",
      "Step: 190, Time: 22.052, Insample PINBALL: 1.25770, Outsample MAE: 1.28742\n",
      "Step: 200, Time: 23.202, Insample PINBALL: 1.05410, Outsample MAE: 1.29650\n",
      "Step: 200, Time: 23.220, Insample PINBALL: 1.05410, Outsample MAE: 1.29650\n",
      "==============================  End fitting  ==============================\n",
      "\n",
      "\n",
      "---------- Split 8/10 ----------\n",
      "\n",
      "\n",
      "Y_df.shape \t(31440, 3)\n",
      "X_df.shape \t(31440, 12)\n",
      "Y_balanced_df.shape \t(31440, 3)\n",
      "X_balanced_df.shape \t(31440, 12)\n",
      "Train Validation splits\n",
      "                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0.0         2016-05-22 2016-08-02 23:00:00\n",
      "          1.0         2013-01-01 2016-05-21 23:00:00\n",
      "Total data \t\t\t31440 time stamps\n",
      "Available percentage=100.0, \t31440 time stamps\n",
      "Train percentage=94.43, \t29688.0 time stamps\n",
      "Predict percentage=5.57, \t1752.0 time stamps\n",
      "\n",
      "\n",
      "train_ts_loader.ts_windows.shape torch.Size([1311, 13, 192])\n",
      "len(train_sampling_windows) * 24 = 1237 * 24 = 29688\n",
      "\n",
      "\n",
      "val_ts_loader.ts_windows.shape torch.Size([1311, 13, 192])\n",
      "len(val_sampling_windows) * 24 = 73 * 24 = 1752\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================== Start fitting ==============================\n",
      "Step: 10, Time: 1.171, Insample PINBALL: 60.11369, Outsample MAE: 48.71842\n",
      "Step: 20, Time: 2.348, Insample PINBALL: 131.87970, Outsample MAE: 127.26796\n",
      "Step: 30, Time: 3.520, Insample PINBALL: 20.85147, Outsample MAE: 11.09476\n",
      "Step: 40, Time: 4.692, Insample PINBALL: 9.89816, Outsample MAE: 2.66239\n",
      "Step: 50, Time: 5.871, Insample PINBALL: 9.89275, Outsample MAE: 6.17285\n",
      "Step: 60, Time: 7.048, Insample PINBALL: 4.89154, Outsample MAE: 1.66454\n",
      "Step: 70, Time: 8.346, Insample PINBALL: 3.72211, Outsample MAE: 1.58439\n",
      "Step: 80, Time: 9.894, Insample PINBALL: 2.88796, Outsample MAE: 1.67905\n",
      "Step: 90, Time: 11.347, Insample PINBALL: 2.37474, Outsample MAE: 1.48038\n",
      "Step: 100, Time: 12.810, Insample PINBALL: 2.07475, Outsample MAE: 1.60971\n",
      "Step: 110, Time: 14.293, Insample PINBALL: 1.98125, Outsample MAE: 1.43262\n",
      "Step: 120, Time: 15.643, Insample PINBALL: 2.22808, Outsample MAE: 1.32649\n",
      "Step: 130, Time: 16.882, Insample PINBALL: 1.68454, Outsample MAE: 1.23164\n",
      "Step: 140, Time: 18.282, Insample PINBALL: 1.36789, Outsample MAE: 1.17373\n",
      "Step: 150, Time: 19.555, Insample PINBALL: 1.34075, Outsample MAE: 1.15775\n",
      "Step: 160, Time: 20.739, Insample PINBALL: 1.36756, Outsample MAE: 1.17769\n",
      "Step: 170, Time: 21.922, Insample PINBALL: 1.31575, Outsample MAE: 1.18615\n",
      "Step: 180, Time: 23.105, Insample PINBALL: 1.22634, Outsample MAE: 1.19374\n",
      "Step: 190, Time: 24.283, Insample PINBALL: 1.22720, Outsample MAE: 1.13284\n",
      "Step: 200, Time: 25.655, Insample PINBALL: 1.28252, Outsample MAE: 1.09286\n",
      "Step: 200, Time: 25.678, Insample PINBALL: 1.28252, Outsample MAE: 1.09286\n",
      "==============================  End fitting  ==============================\n",
      "\n",
      "\n",
      "---------- Split 9/10 ----------\n",
      "\n",
      "\n",
      "Y_df.shape \t(33192, 3)\n",
      "X_df.shape \t(33192, 12)\n",
      "Y_balanced_df.shape \t(33192, 3)\n",
      "X_balanced_df.shape \t(33192, 12)\n",
      "Train Validation splits\n",
      "                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0.0         2016-08-03 2016-10-14 23:00:00\n",
      "          1.0         2013-01-01 2016-08-02 23:00:00\n",
      "Total data \t\t\t33192 time stamps\n",
      "Available percentage=100.0, \t33192 time stamps\n",
      "Train percentage=94.72, \t31440.0 time stamps\n",
      "Predict percentage=5.28, \t1752.0 time stamps\n",
      "\n",
      "\n",
      "train_ts_loader.ts_windows.shape torch.Size([1384, 13, 192])\n",
      "len(train_sampling_windows) * 24 = 1310 * 24 = 31440\n",
      "\n",
      "\n",
      "val_ts_loader.ts_windows.shape torch.Size([1384, 13, 192])\n",
      "len(val_sampling_windows) * 24 = 73 * 24 = 1752\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================== Start fitting ==============================\n",
      "Step: 10, Time: 2.049, Insample PINBALL: 83.17995, Outsample MAE: 91.40139\n",
      "Step: 20, Time: 3.230, Insample PINBALL: 72.85514, Outsample MAE: 32.37902\n",
      "Step: 30, Time: 4.434, Insample PINBALL: 11.34678, Outsample MAE: 8.76559\n",
      "Step: 40, Time: 5.736, Insample PINBALL: 10.55410, Outsample MAE: 5.04085\n",
      "Step: 50, Time: 6.757, Insample PINBALL: 7.54003, Outsample MAE: 13.90006\n",
      "Step: 60, Time: 7.879, Insample PINBALL: 6.69674, Outsample MAE: 2.62885\n",
      "Step: 70, Time: 9.127, Insample PINBALL: 3.81904, Outsample MAE: 1.66848\n",
      "Step: 80, Time: 10.186, Insample PINBALL: 3.94644, Outsample MAE: 1.74663\n",
      "Step: 90, Time: 11.317, Insample PINBALL: 2.29288, Outsample MAE: 1.78954\n",
      "Step: 100, Time: 12.616, Insample PINBALL: 2.11259, Outsample MAE: 1.50109\n",
      "Step: 110, Time: 13.642, Insample PINBALL: 1.99022, Outsample MAE: 1.36053\n",
      "Step: 120, Time: 14.648, Insample PINBALL: 1.81278, Outsample MAE: 1.53661\n",
      "Step: 130, Time: 15.748, Insample PINBALL: 1.63652, Outsample MAE: 1.31913\n",
      "Step: 140, Time: 16.762, Insample PINBALL: 1.43063, Outsample MAE: 1.25531\n",
      "Step: 150, Time: 17.772, Insample PINBALL: 1.25997, Outsample MAE: 1.21917\n",
      "Step: 160, Time: 18.868, Insample PINBALL: 1.24322, Outsample MAE: 1.48660\n",
      "Step: 170, Time: 19.896, Insample PINBALL: 1.25923, Outsample MAE: 1.29481\n",
      "Step: 180, Time: 20.936, Insample PINBALL: 1.20612, Outsample MAE: 1.18069\n",
      "Step: 190, Time: 22.814, Insample PINBALL: 1.20388, Outsample MAE: 1.21873\n",
      "Step: 200, Time: 24.902, Insample PINBALL: 1.31557, Outsample MAE: 1.19974\n",
      "Step: 204, Time: 24.947, Insample PINBALL: 1.31557, Outsample MAE: 1.19974\n",
      "==============================  End fitting  ==============================\n",
      "\n",
      "\n",
      "---------- Split 10/10 ----------\n",
      "\n",
      "\n",
      "Y_df.shape \t(34944, 3)\n",
      "X_df.shape \t(34944, 12)\n",
      "Y_balanced_df.shape \t(34944, 3)\n",
      "X_balanced_df.shape \t(34944, 12)\n",
      "Train Validation splits\n",
      "                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0.0         2016-10-15 2016-12-26 23:00:00\n",
      "          1.0         2013-01-01 2016-10-14 23:00:00\n",
      "Total data \t\t\t34944 time stamps\n",
      "Available percentage=100.0, \t34944 time stamps\n",
      "Train percentage=94.99, \t33192.0 time stamps\n",
      "Predict percentage=5.01, \t1752.0 time stamps\n",
      "\n",
      "\n",
      "train_ts_loader.ts_windows.shape torch.Size([1457, 13, 192])\n",
      "len(train_sampling_windows) * 24 = 1383 * 24 = 33192\n",
      "\n",
      "\n",
      "val_ts_loader.ts_windows.shape torch.Size([1457, 13, 192])\n",
      "len(val_sampling_windows) * 24 = 73 * 24 = 1752\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================== Start fitting ==============================\n",
      "Step: 10, Time: 1.488, Insample PINBALL: 556.60712, Outsample MAE: 1643.46643\n",
      "Step: 20, Time: 2.896, Insample PINBALL: 50.43919, Outsample MAE: 349.57748\n",
      "Step: 30, Time: 4.284, Insample PINBALL: 12.77989, Outsample MAE: 13.20540\n",
      "Step: 40, Time: 5.536, Insample PINBALL: 11.16337, Outsample MAE: 15.52220\n",
      "Step: 50, Time: 6.842, Insample PINBALL: 8.91893, Outsample MAE: 3.10891\n",
      "Step: 60, Time: 8.057, Insample PINBALL: 8.38441, Outsample MAE: 3.85436\n",
      "Step: 70, Time: 9.249, Insample PINBALL: 6.28023, Outsample MAE: 2.35246\n",
      "Step: 80, Time: 10.493, Insample PINBALL: 3.50944, Outsample MAE: 2.73902\n",
      "Step: 90, Time: 11.648, Insample PINBALL: 2.74695, Outsample MAE: 2.02190\n",
      "Step: 100, Time: 12.793, Insample PINBALL: 2.82278, Outsample MAE: 1.98863\n",
      "Step: 110, Time: 13.965, Insample PINBALL: 2.23082, Outsample MAE: 1.79309\n",
      "Step: 120, Time: 15.053, Insample PINBALL: 2.07946, Outsample MAE: 1.73136\n",
      "Step: 130, Time: 16.189, Insample PINBALL: 2.07156, Outsample MAE: 1.91035\n",
      "Step: 140, Time: 17.265, Insample PINBALL: 1.53461, Outsample MAE: 1.74731\n",
      "Step: 150, Time: 18.337, Insample PINBALL: 1.30885, Outsample MAE: 1.81572\n",
      "Step: 160, Time: 19.469, Insample PINBALL: 1.34818, Outsample MAE: 1.68476\n",
      "Step: 170, Time: 20.547, Insample PINBALL: 1.24621, Outsample MAE: 1.69426\n",
      "Step: 180, Time: 21.865, Insample PINBALL: 1.27221, Outsample MAE: 1.67071\n",
      "Step: 190, Time: 23.448, Insample PINBALL: 1.26160, Outsample MAE: 1.67483\n",
      "Step: 200, Time: 24.757, Insample PINBALL: 1.31509, Outsample MAE: 1.64706\n",
      "Step: 204, Time: 24.797, Insample PINBALL: 1.31509, Outsample MAE: 1.64706\n",
      "==============================  End fitting  ==============================\n",
      "\n",
      "\n",
      "y_total.shape (#n_windows, #lt) (730, 24)\n",
      "y_hat_total.shape (#n_windows, #lt) (730, 24)\n",
      "\n",
      "\n",
      "y_total.shape (#n_series, #n_fcds, #lt)  (1, 730, 24)\n",
      "y_hat_total.shape (#n_series, #n_fcds, #lt)  (1, 730, 24)\n",
      "\n",
      "\n",
      "Best Model Evaluation\n",
      "y_total 17520 nan_perc 0.0\n",
      "y_hat_total 17520 nan_perc 0.0\n",
      "average_perc_diff 47.594135745529805\n",
      "reported_loss 1.9376399517059326\n",
      "   id unique_id metric  DNN_ens  DNN_best  NBEATSx  perc_diff  improvement\n",
      "0   1        NP    MAE     1.67      1.72  1.93764  16.026344        False\n",
      "1   1        NP   MAPE     5.38      5.46  8.75813  62.790520        False\n",
      "2   1        NP  SMAPE     4.85      5.00  8.71214  79.631753        False\n",
      "3   1        NP   RMSE     3.33      3.34  4.39320  31.927926        False\n",
      "\n",
      "\n",
      "Best Model Hyperpars\n",
      "===========================================================================\n",
      "activation                                                            prelu\n",
      "batch_normalization                                                   False\n",
      "batch_size                                                              256\n",
      "dropout_prob_exogenous                                             0.443286\n",
      "dropout_prob_theta                                                 0.065952\n",
      "early_stopping                                                           40\n",
      "eval_steps                                                               10\n",
      "exogenous_n_channels                                                      9\n",
      "frequency                                                                 H\n",
      "idx_to_sample_freq                                                       24\n",
      "include_var_dict          {'y': [-2, -3, -8], 'Exogenous1': [-1, -2, -8]...\n",
      "initialization                                               glorot_uniform\n",
      "input_size_multiplier                                                     7\n",
      "l1_theta                                                         0.00113881\n",
      "learning_rate                                                     0.0123582\n",
      "loss                                                                PINBALL\n",
      "loss_hypar                                                         0.487869\n",
      "lr_decay                                                           0.377239\n",
      "n_blocks                                                             (1, 1)\n",
      "n_harmonics                                                               1\n",
      "n_hidden                                                                372\n",
      "n_iterations                                                            200\n",
      "n_layers                                                             (2, 2)\n",
      "n_lr_decay_steps                                                          3\n",
      "n_polynomials                                                             2\n",
      "n_val_weeks                                                             104\n",
      "normalizer_x                                                         median\n",
      "normalizer_y                                                           None\n",
      "output_size                                                              24\n",
      "random_seed                                                              12\n",
      "seasonality                                                              24\n",
      "shared_weights                                                        False\n",
      "stack_types                                   (exogenous_wavenet, identity)\n",
      "val_loss                                                                MAE\n",
      "weight_decay                                                    0.000715303\n",
      "x_s_n_hidden                                                              0\n",
      "n_hidden_list                                      [[372, 372], [372, 372]]\n",
      "t_cols                    [y, Exogenous1, Exogenous2, week_day, day_0, d...\n",
      "dtype: object\n",
      "===========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = pd.Series({'dataset': \"['NP']\",\n",
    "                  #'dataset': \"['NP', 'PJM', 'BE', 'FR']\",\n",
    "                  'val_loss': 'MAE',\n",
    "                  'space': 'nbeats_collapsed',\n",
    "                  'hyperopt_iters': 1, 'max_epochs': 2,\n",
    "                  'experiment_id': 'debug3', 'gpu_id': 1})\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu_id)\n",
    "print('cuda devices,', os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "main(args)"
   ]
  },
  {
   "source": [
    "# TEST SINGLE MODEL CONFIG"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mc0 = {# Architecture parameters\n",
    "      'input_size_multiplier': 7,\n",
    "      'output_size': 24,\n",
    "      'shared_weights': False,\n",
    "      'activation': 'selu',\n",
    "      'initialization': 'he_normal',\n",
    "      'stack_types': ['exogenous_tcn']+1*['identity'],\n",
    "      'n_blocks': [1, 1],\n",
    "      'n_layers': [2, 2],\n",
    "      'n_hidden': 364,\n",
    "      #'n_hidden_list': 2*[[462,462]],\n",
    "      'n_polynomials': 2,\n",
    "      'n_harmonics': 1,\n",
    "      'exogenous_n_channels': 3,\n",
    "      'x_s_n_hidden': 0, # TODO: referencia dinÃ¡mica vs datasets\n",
    "      # Regularization and optimization parameters\n",
    "      'batch_normalization': False,\n",
    "      'dropout_prob_theta': 0.2,\n",
    "      'dropout_prob_exogenous': 0.2,\n",
    "      'learning_rate': 0.0005, #0.002,\n",
    "      'lr_decay': 0.64,\n",
    "      'n_lr_decay_steps': 3,\n",
    "      'weight_decay': 0.00015,\n",
    "      'n_iterations': 1000,\n",
    "      'early_stopping': 8,\n",
    "      'eval_steps': 50,\n",
    "      'n_val_weeks': 52*2,\n",
    "      'loss': 'PINBALL',\n",
    "      'loss_hypar': 0.5, #0.49,\n",
    "      'val_loss': 'MAE',\n",
    "      'l1_theta': 0,\n",
    "      # Data parameters\n",
    "      'normalizer_y': None,\n",
    "      'normalizer_x': 'median',\n",
    "      'frequency':'H',\n",
    "      'seasonality': 24,\n",
    "      'idx_to_sample_freq': 24,\n",
    "      'include_var_dict': {'y': [-2, -3, -8],\n",
    "                           'Exogenous1': [-1, -2, -8],\n",
    "                           'Exogenous2': [-1, -2, -8],\n",
    "                           'week_day': [-1]},\n",
    "      'batch_size': 256,\n",
    "      'random_seed': 10}\n",
    "\n",
    "mc1 = {# Architecture parameters\n",
    "      'input_size_multiplier': 7,\n",
    "      'output_size': 24,\n",
    "      'shared_weights': False,\n",
    "      'activation': 'lrelu',\n",
    "      'initialization': 'glorot_normal',\n",
    "      'stack_types': ['exogenous_tcn']+1*['identity'],\n",
    "      'n_blocks': [1, 1],\n",
    "      'n_layers': [2, 2],\n",
    "      'n_hidden': 400,\n",
    "      #'n_hidden_list': 2*[[462,462]],\n",
    "      'n_polynomials': 2,\n",
    "      'n_harmonics': 1,\n",
    "      'exogenous_n_channels': 6,\n",
    "      'x_s_n_hidden': 0, # TODO: referencia dinÃ¡mica vs datasets\n",
    "      # Regularization and optimization parameters\n",
    "      'batch_normalization': False,\n",
    "      'dropout_prob_theta': 0.28,\n",
    "      'dropout_prob_exogenous': 0.44,\n",
    "      'learning_rate': 0.0005,\n",
    "      'lr_decay': 0.7,\n",
    "      'n_lr_decay_steps': 3,\n",
    "      'weight_decay': 0.00015,\n",
    "      'n_iterations': 2000,\n",
    "      'early_stopping': 10,\n",
    "      'eval_steps': 100,\n",
    "      'n_val_weeks': 52*2,\n",
    "      'loss': 'PINBALL',\n",
    "      'loss_hypar': 0.49, #0.5,\n",
    "      'val_loss': 'MAE',\n",
    "      'l1_theta': 0,\n",
    "      # Data parameters\n",
    "      'normalizer_y': None,\n",
    "      'normalizer_x': 'median',\n",
    "      'frequency':'H',\n",
    "      'seasonality': 24,\n",
    "      'idx_to_sample_freq': 24,\n",
    "      'include_var_dict': {'y': [-2, -3, -8],\n",
    "                           'Exogenous1': [-1, -2, -8],\n",
    "                           'Exogenous2': [-1, -2, -8],\n",
    "                           'week_day': [-1]},\n",
    "      'batch_size': 256,\n",
    "      'random_seed': 12}\n",
    "\n",
    "\n",
    "mc2 = {# Architecture parameters\n",
    "      'input_size_multiplier': 7,\n",
    "      'output_size': 24,\n",
    "      'shared_weights': False,\n",
    "      'activation': 'sigmoid',\n",
    "      'initialization': 'he_normal',\n",
    "      'stack_types': ['exogenous_wavenet']+1*['identity'],\n",
    "      'n_blocks': [1, 1],\n",
    "      'n_layers': [2, 2],\n",
    "      'n_hidden': 400,\n",
    "      # 'n_hidden_list': 2*[[201, 419]],\n",
    "      'n_polynomials': 2,\n",
    "      'n_harmonics': 1,\n",
    "      'exogenous_n_channels': 10,\n",
    "      'x_s_n_hidden': 0, # TODO: referencia dinÃ¡mica vs datasets\n",
    "      # Regularization and optimization parameters\n",
    "      'batch_normalization': False,\n",
    "      'dropout_prob_theta': 0.101428,\n",
    "      'dropout_prob_exogenous': 0.427743,\n",
    "      'learning_rate': 0.00242141,\n",
    "      'lr_decay': 0.5,\n",
    "      'n_lr_decay_steps': 3,\n",
    "      'weight_decay': 0.00015,\n",
    "      'n_iterations': 10000,\n",
    "      'early_stopping': 100,\n",
    "      'eval_steps': 100,\n",
    "      'n_val_weeks': 52*2,\n",
    "      'loss': 'MAE',\n",
    "      'loss_hypar': 0.5,\n",
    "      'val_loss': 'MAE',\n",
    "      'l1_theta': 0,\n",
    "      # Data parameters\n",
    "      'normalizer_y': None,\n",
    "      'normalizer_x': 'median',\n",
    "      'frequency':'H',\n",
    "      'seasonality': 24,\n",
    "      'idx_to_sample_freq': 1, #24,\n",
    "      # 'include_var_dict': {'y': [-2, -3, -8],\n",
    "      #                      'Exogenous1': [-1, -2, -8],\n",
    "      #                      'Exogenous2': [-1, -2, -8],\n",
    "      #                      'week_day': [-1]},\n",
    "      'include_var_dict': {'y': [-2, -4, -8],\n",
    "                           'Exogenous1': [-1, -2, -8],\n",
    "                           'Exogenous2': [-2],\n",
    "                           'week_day': []},\n",
    "      'batch_size': 256,\n",
    "      'random_seed': 442}\n",
    "\n",
    "\n",
    "mcFR = {# Architecture parameters\n",
    "        'input_size_multiplier': 7,\n",
    "        'output_size': 24,\n",
    "        'shared_weights': False,\n",
    "        'activation': 'selu',\n",
    "        'initialization': 'glorot_normal',\n",
    "        'stack_types': 1*['identity'],\n",
    "        'n_blocks': [1, 1],\n",
    "        'n_layers': [2, 2],\n",
    "        'n_hidden': 128,\n",
    "        #'n_hidden_list': 2*[[462,462]],\n",
    "        'n_polynomials': 2,\n",
    "        'n_harmonics': 1,\n",
    "        'exogenous_n_channels': 3,\n",
    "        'x_s_n_hidden': 0, # TODO: referencia dinÃ¡mica vs datasets\n",
    "        # Regularization and optimization parameters\n",
    "        'batch_normalization': False,\n",
    "        'dropout_prob_theta': 0.2,\n",
    "        'dropout_prob_exogenous': 0.3,\n",
    "        'learning_rate': 0.0006, #0.002,\n",
    "        'lr_decay': 0.5,\n",
    "        'n_lr_decay_steps': 3,\n",
    "        'weight_decay': 0.00015,\n",
    "        'n_iterations': 30000,\n",
    "        'early_stopping': 10,\n",
    "        'eval_steps': 100,\n",
    "        'n_val_weeks': 52*2,\n",
    "        'loss': 'PINBALL',\n",
    "        'loss_hypar': 0.5, #0.49,\n",
    "        'val_loss': 'MAE',\n",
    "        'l1_theta': 0,\n",
    "        # Data parameters\n",
    "        'normalizer_y': None,\n",
    "        'normalizer_x': 'median',\n",
    "        'frequency':'H',\n",
    "        'seasonality': 24,\n",
    "        'idx_to_sample_freq': 24,\n",
    "        'include_var_dict': {'y': [-2, -3, -8],\n",
    "                             'Exogenous1': [-1, -2, -8],\n",
    "                             'Exogenous2': [-1, -2, -8],\n",
    "                             'week_day': [-1]},\n",
    "        'batch_size': 256,\n",
    "        'random_seed': 742}\n",
    "\n",
    "\n",
    "\n",
    "mc = mcFR\n",
    "dataset = ['FR']\n",
    "# dataset = ['BE']\n",
    "# dataset = ['BE', 'FR']\n",
    "# dataset = ['NP', 'PJM']\n",
    "# dataset = ['NP', 'PJM', 'BE', 'FR']\n",
    "if len(dataset)>0: mc['x_s_n_hidden'] = 2\n",
    "\n",
    "Y_df, X_df, S_df = EPF.load_groups(directory='data', groups=dataset)\n",
    "\n",
    "plt.plot(Y_df.y.values)\n",
    "plt.show()\n",
    "\n",
    "y_total, y_hat_total, meta_data = run_val_nbeatsx(mc=mc, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "                                                  trials=None, trials_file_name=None, final_evaluation=False, return_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, market_data in enumerate(meta_data):\n",
    "    market = market_data['unique_id']\n",
    "    y_hat_plot = y_hat_total[i,:,:].reshape(-1)\n",
    "    y_true_plot = y_total[i,:,:].reshape(-1)\n",
    "    \n",
    "    performance = np.round(mae(y=y_true_plot, y_hat=y_hat_plot), 5)\n",
    "    plt.plot(range(len(y_true_plot)), y_true_plot, label='true', alpha=0.7)\n",
    "    plt.plot(range(len(y_hat_plot)), y_hat_plot, label='pred', alpha=0.7)\n",
    "    plt.title(f\"{market} predictions \\n all lead time  MAE={performance}\")\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Electricity Price')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}