{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c68c0f618c4f787b1e02bfee278e48d25b62407cb335aab5258cedc4db4ced"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiments.hyperopt_epf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import time\n",
    "import os\n",
    "# Limit number of threads in numpy and others to avoid throttling\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\" # export OMP_NUM_THREADS=4\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"2\" # export OPENBLAS_NUM_THREADS=4 \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"3\" # export MKL_NUM_THREADS=6\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"2\" # export VECLIB_MAXIMUM_THREADS=4\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"3\" # export NUMEXPR_NUM_THREADS=6\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import pickle\n",
    "import glob\n",
    "import itertools\n",
    "import random\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "from nixtla.data.scalers import Scaler\n",
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset\n",
    "from nixtla.data.tsloader_fast import TimeSeriesLoader\n",
    "from nixtla.losses.numpy import mae, mape, smape, rmse, pinball_loss\n",
    "\n",
    "# Models\n",
    "from nixtla.models.nbeats.nbeats import Nbeats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def run_val_nbeatsx(mc, train_loader, val_loader, trials, trials_file_name, final_evaluation=False):\n",
    "    # Save trials, can analyze progress\n",
    "    save_every_n_step = 5\n",
    "    current_step = len(trials.trials)\n",
    "    if (current_step % save_every_n_step==0):\n",
    "        with open(trials_file_name, \"wb\") as f:\n",
    "            pickle.dump(trials, f)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = Nbeats(input_size=int(mc['input_size']),\n",
    "                   output_size=int(mc['output_size']),\n",
    "                   shared_weights=int(mc['shared_weights']),\n",
    "                   activation=mc['activation'],\n",
    "                   initialization=mc['initialization'],\n",
    "                   stack_types=mc['stack_types'], #2*['identity'],\n",
    "                   n_blocks=mc['n_blocks'], #2*[1],\n",
    "                   n_layers=mc['n_layers'], #2*[2],\n",
    "                   n_hidden=2*[2*[int(mc['n_hidden'])]], #2*[[256,256]]\n",
    "                   n_polynomials=mc['n_polynomials'], #2,\n",
    "                   n_harmonics=int(mc['n_harmonics']), #1,\n",
    "                   exogenous_n_channels=int(mc['exogenous_n_channels']), #9,\n",
    "                   include_var_dict={'y': [-2, -3, -8],\n",
    "                                     'Exogenous1': [-1, -2, -8],\n",
    "                                     'Exogenous2': [-1, -2, -8],\n",
    "                                     'week_day': [-1]},\n",
    "                   t_cols=train_loader.ts_dataset.t_cols,\n",
    "                   batch_normalization=mc['batch_normalization'], #False,\n",
    "                   dropout_prob_theta=float(mc['dropout_prob_theta']), #0.01,\n",
    "                   dropout_prob_exogenous=float(mc['dropout_prob_exogenous']), #0.01,\n",
    "                   x_s_n_hidden=int(mc['x_s_n_hidden']), #0,\n",
    "                   learning_rate=float(mc['learning_rate']), #0.007,\n",
    "                   lr_decay=float(mc['lr_decay']), #0.5,\n",
    "                   n_lr_decay_steps=int(mc['n_lr_decay_steps']), #3,\n",
    "                   weight_decay=float(mc['weight_decay']), #0.0000001,\n",
    "                   l1_theta=float(mc['l1_theta']), #0.0001,\n",
    "                   n_iterations=int(mc['n_iterations']), #200,\n",
    "                   early_stopping=int(mc['early_stopping']), #40,\n",
    "                   loss=mc['loss'], #'MAE',\n",
    "                   loss_hypar=float(mc['loss_hypar']), #0.5,\n",
    "                   frequency=mc['frequency'], #'H',\n",
    "                   random_seed=int(mc['random_seed']), #1,\n",
    "                   seasonality=int(mc['seasonality'])) #24)\n",
    "\n",
    "    model.fit(train_ts_loader=train_loader, val_ts_loader=val_loader, verbose=True, eval_steps=10) # aqui val_loader==Test\n",
    "\n",
    "    results =  {'loss': model.final_outsample_loss, #val_mae <--------\n",
    "                'mc': mc,\n",
    "                'final_insample_loss': model.final_insample_loss,\n",
    "                'final_outsample_loss': model.final_outsample_loss,\n",
    "                'trajectories': model.trajectories,\n",
    "                'run_time': time.time() - start_time,\n",
    "                'status': STATUS_OK}\n",
    "\n",
    "    if final_evaluation:\n",
    "        y_true, y_hat = model.predict_all(ts_loader=val_loader)\n",
    "        print('Best Model Evaluation')\n",
    "        print(forecast_evaluation_table(y_true, y_hat))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hyperopt_space_nbeatsx_pinball(args):\n",
    "    space = {#----------------------------------------------  Fixed   ----------------------------------------------#\n",
    "             # Architecture parameters\n",
    "             'frequency': hp.choice('frequency', ['H']),\n",
    "             'seasonality': hp.choice('seasonality', [24]),\n",
    "             'input_size': hp.choice('input_size', [7*24]),\n",
    "             'output_size': hp.choice('output_size', [24]),\n",
    "             'shared_weights': hp.choice('shared_weights', [False]),\n",
    "             'n_polynomials': hp.choice('n_polynomials', [2]), #<----- TODO: Eliminate unnecesary hypar\n",
    "             'n_harmonics': hp.choice('n_harmonics', [1]), #<--------- TODO: Eliminate unnecesary hypar\n",
    "             'x_s_n_hidden': hp.choice('x_s_n_hidden', [0]), #<------- TODO: Change for n_xs_hidden\n",
    "             # Regularization and optimization parameters\n",
    "             'lr_decay': hp.choice('lr_decay', [0.5]),\n",
    "             'n_lr_decay_steps': hp.choice('n_lr_decay_steps', [3]),\n",
    "             'n_iterations': hp.choice('n_iterations', [100]), #<--------- Important\n",
    "             'early_stopping': hp.choice('early_stopping', [40]),\n",
    "             'loss': hp.choice('loss', ['PINBALL']),\n",
    "             'include_var_dict': hp.choice('include_var_dict', [{'y': [-2, -3, -8],\n",
    "                                                                 'Exogenous1': [-1, -2, -8],\n",
    "                                                                 'Exogenous2': [-1, -2, -8],\n",
    "                                                                 'week_day': [-1]}]),\n",
    "             #---------------------------------------------- Explored ----------------------------------------------#\n",
    "             # Architecture parameters\n",
    "             'activation': hp.choice('activation', ['relu','softplus','tanh','selu','lrelu','prelu','sigmoid']),\n",
    "             'initialization':  hp.choice('initialization', ['orthogonal','he_uniform','he_normal',\n",
    "                                                             'glorot_uniform','glorot_normal','lecun_normal']),\n",
    "             'stack_types': hp.choice('stack_types', [ ['identity'],\n",
    "                                                       1*['identity']+['exogenous_wavenet'],\n",
    "                                                       ['exogenous_wavenet']+1*['identity'],\n",
    "                                                       1*['identity']+['exogenous_tcn'],\n",
    "                                                       ['exogenous_tcn']+1*['identity'] ]),\n",
    "             'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),\n",
    "             'n_layers': hp.choice('n_layers', [ [2, 2] ]),\n",
    "             'n_hidden': hp.quniform('n_hidden_1', 50, 500, 1),\n",
    "             'exogenous_n_channels': hp.quniform('exogenous_n_channels', 1, 10, 1),\n",
    "             # Regularization and optimization parameters\n",
    "             'batch_normalization': hp.choice('batch_normalization', [True, False]),\n",
    "             'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 1),\n",
    "             'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),\n",
    "             'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.1)),\n",
    "             'weight_decay': hp.loguniform('weight_decay', np.log(5e-4), np.log(0.01)),\n",
    "             'l1_theta': hp.choice('l1_theta', [0, hp.loguniform('lambdal1', np.log(1e-5), np.log(1))]),\n",
    "             'loss_hypar': hp.uniform('dropout_prob', 0.45, 0.55),\n",
    "             'random_seed': hp.quniform('random_seed', 1, 20, 1)}\n",
    "    return space\n",
    "\n",
    "def parse_trials(trials):\n",
    "    # Initialize\n",
    "    trials_dict = {'tid': [], 'loss': [], 'trajectories': [], 'mc': []}\n",
    "    for tidx in range(len(trials)):\n",
    "        # Main\n",
    "        trials_dict['tid']  += [trials.trials[tidx]['tid']]\n",
    "        trials_dict['loss'] += [trials.trials[tidx]['result']['loss']]\n",
    "        trials_dict['trajectories'] += [trials.trials[tidx]['result']['trajectories']]\n",
    "\n",
    "        # Model Configs\n",
    "        mc = trials.trials[tidx]['result']['mc']\n",
    "        trials_dict['mc'] += [mc]\n",
    "    \n",
    "    trials_df = pd.DataFrame(trials_dict)\n",
    "    return trials_df\n",
    "\n",
    "def parse_args():\n",
    "    desc = \"NBEATSx overfit\"\n",
    "    parser = argparse.ArgumentParser(description=desc)\n",
    "\n",
    "    parser.add_argument('--dataset', type=str, required=True, help='NP')\n",
    "    parser.add_argument('--model', type=str, required=True, help='Models')\n",
    "    parser.add_argument('--hyperopt_iters', type=int, help='hyperopt_iters')\n",
    "    parser.add_argument('--experiment_id', default=None, required=False, type=str, help='string to identify experiment')\n",
    "    parser.add_argument('--gpu_id', type=int, default=0, required=False, help='GPU')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def forecast_evaluation_table(y_true, y_hat):\n",
    "    #print(\"y_true.shape\", y_true.shape)\n",
    "    #print(\"y_hat.shape\", y_hat.shape)\n",
    "\n",
    "    n_days = len(y_true) // 24\n",
    "    y_true = y_true[-n_days*24:, :]\n",
    "    y_hat  = y_hat[-n_days*24:, :]\n",
    "\n",
    "    #print(\"y_true.shape (#fcds, #lt)\", y_true.shape)\n",
    "    #print(\"y_hat.shape  (#fcds, #lt)\", y_hat.shape)\n",
    "\n",
    "    #y_true = y_true.reshape(n_days, 24, 24)\n",
    "    #y_hat = y_hat.reshape(n_days, 24, 24)\n",
    "\n",
    "    #y_true = y_true[:,0,:]\n",
    "    #y_hat = y_hat[:,0,:]\n",
    "\n",
    "    #print(\"y_true.shape (#fcd_day, #fcd_hour, #lt)\", y_true.shape)\n",
    "    #print(\"y_hat.shape (#fcd_day, #fcd_hour, #lt)\", y_hat.shape)\n",
    "    \n",
    "    _pinball50 = np.round(pinball_loss(y_true, y_hat, tau=0.5),2)\n",
    "    _mae   = np.round(mae(y_true, y_hat),2)\n",
    "    _mape  = np.round(mape(y_true, y_hat),2)\n",
    "    _smape = np.round(smape(y_true, y_hat),2)\n",
    "    _rmse  = np.round(rmse(y_true, y_hat),2)\n",
    "\n",
    "    evaluations = pd.DataFrame({'metric': ['pinball50', 'mae', 'mape', 'smape', 'rmse'],\n",
    "                                'nbeatsx': [_pinball50, _mae, _mape, _smape, _rmse]})\n",
    "\n",
    "    return evaluations\n",
    "\n",
    "def main(args):\n",
    "    #---------------------------------------------- Directories ----------------------------------------------#\n",
    "    output_dir = f'./results/{args.dataset}/{args.model}/'\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    assert os.path.exists(output_dir), f'Output dir {output_dir} does not exist'\n",
    "\n",
    "    hyperopt_file = output_dir + f'hyperopt_{args.experiment_id}.p'\n",
    "    result_test_file = output_dir + f'result_test_{args.experiment_id}.p'\n",
    "\n",
    "    #os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu_id)\n",
    "    #import torch\n",
    "    #from src.utils.experiment.run_nbeatsx import run_val_nbeatsx, run_test_nbeatsx\n",
    "    #print('cuda devices,', os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "\n",
    "    #---------------------------------------------- Read  Data ----------------------------------------------#\n",
    "    print('\\n'+75*'-')\n",
    "    print(28*'-', 'Preparing Dataset', 28*'-')\n",
    "    print(75*'-'+'\\n')\n",
    "\n",
    "    #TEST_DATE = {'NP': '2016-12-27',\n",
    "    #             'PJM':'2016-12-27',\n",
    "    #             'BE':'2015-01-04',\n",
    "    #             'FR': '2015-01-04',\n",
    "    #             'DE':'2016-01-04'}\n",
    "    #test_date = TEST_DATE[args.dataset]\n",
    "    #Y_insample_df, Xt_insample_df, Y_outsample_df, Xt_outsample_df, _ = load_epf(directory='../data/',\n",
    "    #                                                                             market=args.dataset,\n",
    "    #                                                                             first_date_test=test_date,\n",
    "    #                                                                             days_in_test=728)\n",
    "    Y_df, Xt_df = EPF.load(directory='../data/', group=args.dataset)\n",
    "\n",
    "    # To not modify original data\n",
    "    Xt_scaled_df = Xt_df.copy()\n",
    "\n",
    "    # Transform data with scale transformation\n",
    "    offset = 365 * 24\n",
    "    scaler = Scaler(normalizer='norm')\n",
    "    Xt_scaled_df['Exogenous1'] = scaler.scale(x=Xt_scaled_df['Exogenous1'].values, offset=offset)\n",
    "\n",
    "    scaler = Scaler(normalizer='norm')\n",
    "    Xt_scaled_df['Exogenous2'] = scaler.scale(x=Xt_scaled_df['Exogenous2'].values, offset=offset)\n",
    "\n",
    "    # train_mask: 1 to keep, 0 to mask\n",
    "    train_outsample_mask = np.ones(len(Y_df))\n",
    "    train_outsample_mask[-offset:] = 0\n",
    "\n",
    "    ts_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=None, X_df=Xt_scaled_df, \n",
    "                                   ts_train_mask=train_outsample_mask)\n",
    "\n",
    "    train_loader = TimeSeriesLoader(ts_dataset=ts_dataset,\n",
    "                                    model='nbeats',\n",
    "                                    offset=0, #offset,\n",
    "                                    window_sampling_limit=365*4*24, \n",
    "                                    input_size=7*24,\n",
    "                                    output_size=24,\n",
    "                                    idx_to_sample_freq=1,\n",
    "                                    batch_size=256,\n",
    "                                    is_train_loader=True,\n",
    "                                    shuffle=True)\n",
    "\n",
    "    val_loader = TimeSeriesLoader(ts_dataset=ts_dataset,\n",
    "                                  model='nbeats',\n",
    "                                  offset=0, #offset,\n",
    "                                  window_sampling_limit=365*4*24, \n",
    "                                  input_size=7*24,\n",
    "                                  output_size=24,\n",
    "                                  idx_to_sample_freq=1,\n",
    "                                  batch_size=1024,\n",
    "                                  is_train_loader=False, # Samples the opposite of train_outsample_mask\n",
    "                                  shuffle=False)                                       \n",
    "\n",
    "    print(f'Dataset: {args.dataset}')\n",
    "    #print(\"Xt_df.columns\", Xt_df.columns)\n",
    "    print(f'Train mask percentage: {np.round(np.sum(train_outsample_mask)/len(train_outsample_mask),2)}')\n",
    "    print('X: time series features, of shape (#hours, #times,#features): \\t' + str(Xt_df.shape))\n",
    "    print('Y: target series (in X), of shape (#hours, #times): \\t \\t' + str(Y_df.shape))\n",
    "    print(f'{len(Xt_df)} hours = {np.round(len(Xt_df)/(24*365),2)} years')\n",
    "    # print('S: static features, of shape (#series,#features): \\t \\t' + str(S.shape))\n",
    "    #Y_df.head()\n",
    "    print('\\n')\n",
    "\n",
    "    #-------------------------------------- Hyperparameter Optimization --------------------------------------#\n",
    "\n",
    "    if not os.path.isfile(hyperopt_file):\n",
    "        print('\\n'+75*'-')\n",
    "        print(22*'-', 'Start Hyperparameter  tunning', 22*'-')\n",
    "        print(75*'-'+'\\n')\n",
    "\n",
    "        space = hyperopt_space_nbeatsx_pinball(args)\n",
    "\n",
    "        trials = Trials()\n",
    "        #fmin_objective = partial(run_val_nbeatsx, y_df=y_insample_df, X_t_df=X_t_insample_df, val_ds=365,\n",
    "        #                         trials=trials, trials_file_name=hyperopt_file)\n",
    "        fmin_objective = partial(run_val_nbeatsx, train_loader=train_loader, val_loader=val_loader, \n",
    "                                 trials=trials, trials_file_name=hyperopt_file)\n",
    "        fmin(fmin_objective, space=space, algo=tpe.suggest, max_evals=args.hyperopt_iters, trials=trials, verbose=True)\n",
    "\n",
    "        # Save output\n",
    "        with open(hyperopt_file, \"wb\") as f:\n",
    "            pickle.dump(trials, f)\n",
    "\n",
    "    print('\\n'+75*'-')\n",
    "    print(20*'-', 'Hyperparameter  tunning  finished', 20*'-')\n",
    "    print(75*'-'+'\\n')\n",
    "\n",
    "    # Read and parse trials pickle\n",
    "    trials = pickle.load(open(hyperopt_file, 'rb'))\n",
    "    trials_df = parse_trials(trials)\n",
    "\n",
    "    # Get best mc\n",
    "    idx = trials_df.loss.idxmin()\n",
    "    best_mc = trials_df.loc[idx]['mc']\n",
    "    \n",
    "    run_val_nbeatsx(best_mc, train_loader=train_loader, val_loader=val_loader, \n",
    "                    trials=trials, trials_file_name=hyperopt_file, final_evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------- Preparing Dataset ----------------------------\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Processing dataframes ...\n",
      "Creating ts tensor ...\n",
      "Dataset: NP\n",
      "Train mask percentage: 0.75\n",
      "X: time series features, of shape (#hours, #times,#features): \t(34944, 12)\n",
      "Y: target series (in X), of shape (#hours, #times): \t \t(34944, 3)\n",
      "34944 hours = 3.99 years\n",
      "\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "-------------------- Hyperparameter  tunning  finished --------------------\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "============================== Start fitting ==============================\n",
      "Step: 10, Time: 0.261, Insample PINBALL: 1.76868, Outsample PINBALL: 1.87202\n",
      "Step: 20, Time: 1.335, Insample PINBALL: 1.68232, Outsample PINBALL: 1.86951\n",
      "Step: 30, Time: 2.280, Insample PINBALL: 1.80993, Outsample PINBALL: 1.86856\n",
      "Step: 40, Time: 3.249, Insample PINBALL: 1.73132, Outsample PINBALL: 1.86854\n",
      "Step: 50, Time: 4.238, Insample PINBALL: 1.74816, Outsample PINBALL: 1.86739\n",
      "Step: 60, Time: 5.206, Insample PINBALL: 1.76965, Outsample PINBALL: 1.86591\n",
      "Step: 70, Time: 6.123, Insample PINBALL: 1.68237, Outsample PINBALL: 1.86316\n",
      "Step: 80, Time: 7.039, Insample PINBALL: 1.71533, Outsample PINBALL: 1.85929\n",
      "Step: 90, Time: 8.016, Insample PINBALL: 1.78162, Outsample PINBALL: 1.85564\n",
      "Step: 100, Time: 8.920, Insample PINBALL: 1.67909, Outsample PINBALL: 1.85183\n",
      "Step: 103, Time: 9.655, Insample PINBALL: 1.67909, Outsample PINBALL: 1.85183\n",
      "==============================  End fitting  ==============================\n",
      "\n",
      "\n",
      "Best Model Evaluation\n",
      "      metric  nbeatsx\n",
      "0  pinball50     1.87\n",
      "1        mae     3.74\n",
      "2       mape    13.20\n",
      "3      smape    12.82\n",
      "4       rmse     8.03\n"
     ]
    }
   ],
   "source": [
    "args = pd.Series({'dataset': 'NP', 'model': 'nbeats', 'hyperopt_iters': 2, \n",
    "                  'experiment_id': 'debug4', 'gpu_id': 1})\n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}