{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c68c0f618c4f787b1e02bfee278e48d25b62407cb335aab5258cedc4db4ced"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiments.nbeats.hyperopt_epf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import time\n",
    "import os\n",
    "# Limit number of threads in numpy and others to avoid throttling\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\" # export OMP_NUM_THREADS=4\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"2\" # export OPENBLAS_NUM_THREADS=4 \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"3\" # export MKL_NUM_THREADS=6\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"2\" # export VECLIB_MAXIMUM_THREADS=4\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"3\" # export NUMEXPR_NUM_THREADS=6\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import pickle\n",
    "import glob\n",
    "import itertools\n",
    "import random\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "from nixtla.data.scalers import Scaler\n",
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset\n",
    "from nixtla.data.tsloader_fast import TimeSeriesLoader\n",
    "from nixtla.losses.numpy import mae, mape, smape, rmse, pinball_loss\n",
    "\n",
    "# Models\n",
    "from nixtla.models.nbeats.nbeats import Nbeats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK"
   ]
  },
  {
   "source": [
    "# DATA WRANGLING AND EVALUATION UTILS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# TODO: Think and test new mask_df and datasets with random validation\n",
    "\n",
    "############################################################################################\n",
    "#### COMMON\n",
    "############################################################################################\n",
    "\n",
    "BENCHMARK_DF = pd.DataFrame({'id': [1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5],\n",
    "                             'unique_id': ['NP', 'NP', 'NP', 'NP', 'PJM', 'PJM', 'PJM', 'PJM',\n",
    "                                           'BE', 'BE', 'BE', 'BE', 'FR', 'FR', 'FR', 'FR',\n",
    "                                           'DE', 'DE', 'DE', 'DE'],\n",
    "                             'metric': ['MAE', 'MAPE', 'SMAPE', 'RMSE', 'MAE', 'MAPE', 'SMAPE', 'RMSE',\n",
    "                                        'MAE', 'MAPE', 'SMAPE', 'RMSE', 'MAE', 'MAPE', 'SMAPE', 'RMSE',\n",
    "                                        'MAE', 'MAPE', 'SMAPE', 'RMSE',],\n",
    "                             'DNN_ens' : [1.67, 5.38, 4.85, 3.33, 2.78, 28.66, 11.22, 4.64, \n",
    "                                          5.82, 26.11, 13.33, 16.13, 3.91, 14.77, 10.98, 11.74,\n",
    "                                          3.44, 95.76, 14.19, 6.00],\n",
    "                             'DNN_best': [1.72, 5.46, 5.00, 3.34, 2.95, 29.10, 11.81, 4.82,\n",
    "                                          6.07, 24.08, 13.87, 15.88, 4.19, 15.13, 11.65, 11.41,\n",
    "                                          3.60, 83.1, 14.74, 6.13]})\n",
    "\n",
    "def forecast_evaluation_table(y_total, y_hat_total, meta_data):\n",
    "    performances = []\n",
    "    for i, market_data in enumerate(meta_data):\n",
    "        market = market_data['unique_id']\n",
    "        \n",
    "        y = y_total[i,:,:].reshape(-1)\n",
    "        y_hat = y_hat_total[i,:,:].reshape(-1)\n",
    "\n",
    "        _mae   = np.round(mae(y=y, y_hat=y_hat),5)\n",
    "        _mape  = np.round(mape(y=y, y_hat=y_hat),5)\n",
    "        _smape = np.round(smape(y=y, y_hat=y_hat),5)\n",
    "        _rmse  = np.round(rmse(y=y, y_hat=y_hat),5)\n",
    "\n",
    "        performance_df = pd.DataFrame({'unique_id': [market]*4,\n",
    "                                       'metric': ['MAE', 'MAPE', 'SMAPE', 'RMSE'],\n",
    "                                       'NBEATSx': [_mae, _mape, _smape, _rmse]})\n",
    "\n",
    "        performances += [performance_df]\n",
    "\n",
    "    performances_df = pd.concat(performances)\n",
    "\n",
    "    benchmark_df = BENCHMARK_DF.merge(performances_df, on=['unique_id', 'metric'], how='left')\n",
    "    benchmark_df['perc_diff'] = 100 * (benchmark_df['NBEATSx']-benchmark_df['DNN_ens'])/benchmark_df['DNN_ens']\n",
    "    benchmark_df['improvement'] = benchmark_df['perc_diff'] < 0\n",
    "    benchmark_df = benchmark_df.dropna()\n",
    "    average_perc_diff = benchmark_df['perc_diff'].mean()\n",
    "\n",
    "    \n",
    "    y_tot = y_total.reshape(-1)\n",
    "    y_hat_tot = y_hat_total.reshape(-1)\n",
    "    y_total_nans_perc = np.sum((np.isnan(y_tot)))  / len(y_tot)\n",
    "    y_hat_total_nans_perc = np.sum((np.isnan(y_hat_tot)))  / len(y_hat_tot)\n",
    "    print(f'y_total {len(y_tot)} nan_perc {y_total_nans_perc}')\n",
    "    print(f'y_hat_total {len(y_tot)} nan_perc {y_total_nans_perc}')\n",
    "    print(\"average_perc_diff\", average_perc_diff)\n",
    "\n",
    "    reported_loss = _mae\n",
    "    if y_total_nans_perc > 0: reported_loss=500\n",
    "    if y_hat_total_nans_perc > 0: reported_loss=500\n",
    "    #improvement_loss = 200 * (1-np.mean(benchmark_df.improvement)) + average_perc_diff\n",
    "    print(f'reported_loss {reported_loss}')\n",
    "\n",
    "    return benchmark_df, y_total_nans_perc, y_hat_total_nans_perc, reported_loss\n",
    "\n",
    "def protect_nan_reported_loss(model):\n",
    "    # TODO: Pytorch numerical error hacky protection, protect from losses.numpy.py\n",
    "    reported_loss = model.final_outsample_loss\n",
    "    if np.isnan(model.final_insample_loss):\n",
    "        reported_loss = 500\n",
    "    if model.final_insample_loss<=0:\n",
    "        reported_loss = 500\n",
    "\n",
    "    if np.isnan(model.final_outsample_loss):\n",
    "        reported_loss = 500    \n",
    "    if model.final_outsample_loss<=0:\n",
    "        reported_loss = 500    \n",
    "    return reported_loss\n",
    "\n",
    "def get_mask_df(Y_df, n_timestamps):\n",
    "    # Creates outsample_mask\n",
    "    # train 1 validation 0\n",
    "\n",
    "    last_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    last_df.sort_values(by=['unique_id', 'ds'], inplace=True, ascending=False)\n",
    "    last_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    last_df = last_df.groupby('unique_id').head(n_timestamps)\n",
    "    last_df['sample_mask'] = 0\n",
    "\n",
    "    last_df = last_df[['unique_id', 'ds', 'sample_mask']]\n",
    "\n",
    "    mask_df = Y_df.merge(last_df, on=['unique_id', 'ds'], how='left')\n",
    "    mask_df['sample_mask'] = mask_df['sample_mask'].fillna(1)    # The first len(Y)-n_hours used as train\n",
    "\n",
    "    mask_df = mask_df[['unique_id', 'ds', 'sample_mask']]\n",
    "    mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "    mask_df['available_mask'] = 1\n",
    "\n",
    "    assert len(mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "\n",
    "    return mask_df\n",
    "\n",
    "def balance_data(Y_df, X_df):\n",
    "    # Create balanced placeholder dataframe\n",
    "    balance_ids = {'unique_id': Y_df.unique_id.unique(),\n",
    "                   'ds': Y_df.ds.unique()}\n",
    "\n",
    "    product_list = list(itertools.product(*list(balance_ids.values())))\n",
    "    balance_df = pd.DataFrame(product_list, columns=list(balance_ids.keys()))\n",
    "    \n",
    "    # Balance with merge\n",
    "    Y_balanced_df = balance_df.merge(Y_df, on=['unique_id', 'ds'], how='left')\n",
    "    X_balanced_df = balance_df.merge(X_df, on=['unique_id', 'ds'], how='left')\n",
    "\n",
    "    print('\\n')\n",
    "    print(f'Y_df.shape \\t{Y_df.shape}')\n",
    "    print(f'X_df.shape \\t{X_df.shape}')\n",
    "    print(f'Y_balanced_df.shape \\t{Y_balanced_df.shape}')\n",
    "    print(f'X_balanced_df.shape \\t{X_balanced_df.shape}')\n",
    "\n",
    "    return Y_balanced_df, X_balanced_df\n",
    "\n",
    "def scale_data(Y_df, X_df, mask_df, normalizer_y, normalizer_x):\n",
    "    y_shift = None\n",
    "    y_scale = None\n",
    "\n",
    "    # mask = mask.astype(int)\n",
    "    mask = mask_df['available_mask'].values * mask_df['sample_mask'].values\n",
    "    \n",
    "    if normalizer_y is not None:\n",
    "        scaler_y = Scaler(normalizer=normalizer_y)\n",
    "        Y_df['y'] = scaler_y.scale(x=Y_df['y'].values, mask=mask)\n",
    "    else:\n",
    "        scaler_y = None\n",
    "    # Exogenous are always scaled to help learning\n",
    "    if normalizer_x is not None:\n",
    "        scaler_x = Scaler(normalizer=normalizer_x)\n",
    "        X_df['Exogenous1'] = scaler_x.scale(x=X_df['Exogenous1'].values, mask=mask)\n",
    "\n",
    "        scaler_x = Scaler(normalizer=normalizer_x)\n",
    "        X_df['Exogenous2'] = scaler_x.scale(x=X_df['Exogenous2'].values, mask=mask)\n",
    "\n",
    "    filter_variables = ['unique_id', 'ds', 'Exogenous1', 'Exogenous2', 'week_day'] + \\\n",
    "                       [col for col in X_df if (col.startswith('day'))]\n",
    "                       #[col for col in X_df if (col.startswith('_hour_'))]\n",
    "    X_df = X_df[filter_variables]\n",
    "\n",
    "    return Y_df, X_df, scaler_y\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "#### KIN\n",
    "############################################################################################\n",
    "\n",
    "def prepare_data(mc, Y_df, X_df, S_df, n_timestamps_pred=365*1*24, offset=0): #offset=365*1*24 tira un año\n",
    "    # n_timestamps_pred defines number of hours ahead to predict\n",
    "    # offset defines the shift of the data to simulate rolling window\n",
    "    assert offset % n_timestamps_pred == 0, 'Avoid overlap of predictions, redefine n_timestamps_pred or offset'\n",
    "    Y_df = Y_df.head(len(Y_df)-offset)\n",
    "    X_df = X_df.head(len(X_df)-offset)\n",
    "\n",
    "    assert len(Y_df.unique_id.unique())==1, 'Data prepartation for more than one market not implemented yet'\n",
    "\n",
    "    #-------------------------------------------- Data Wrangling --------------------------------------------#\n",
    "    Y_balanced_df, X_balanced_df = balance_data(Y_df, X_df)\n",
    "    del Y_df, X_df\n",
    "\n",
    "    #------------------------------------- Available and Validation Mask ------------------------------------#\n",
    "    # mask: 1 last_n_timestamps, 0 timestamps until last_n_timestamps\n",
    "    mask_df = get_mask_df(Y_df=Y_balanced_df, n_timestamps=n_timestamps_pred)\n",
    "    val_mask_df = mask_df.copy()\n",
    "    val_mask_df['sample_mask'] = 1 - val_mask_df['sample_mask']\n",
    "    # mask_df['available_mask'] = (1-Y_balanced_df.y.isnull().values)\n",
    "    # mask_df['sample_mask'] = (1-last_timestamps_df['mask'])\n",
    "\n",
    "    # Plotting train validation splits\n",
    "    #y_train = Y_balanced_df[mask_df.sample_mask==1].y.values\n",
    "    #y_val = Y_balanced_df[mask_df.sample_mask==0].y.values\n",
    "    #plt.plot(y_train, label='train', color='blue')\n",
    "    #plt.plot(np.array(range(len(y_val))) + len(y_train), y_val, label='val', color='purple')\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    #---------------------------------------------- Scale Data ----------------------------------------------#\n",
    "\n",
    "    # Scale data # TODO: write sample_mask conditional/groupby(['unique_id]) scaling\n",
    "    Y_scaled_df, X_scaled_df, scaler_y = scale_data(Y_df=Y_balanced_df, X_df=X_balanced_df, mask_df=mask_df,\n",
    "                                                    normalizer_y=mc['normalizer_y'], normalizer_x=mc['normalizer_x'])\n",
    "    del Y_balanced_df\n",
    "    del X_balanced_df\n",
    "\n",
    "    #-------------------------------------------- Declare Loaders -------------------------------------------#\n",
    "\n",
    "    train_ts_dataset = TimeSeriesDataset(Y_df=Y_scaled_df, X_df=X_scaled_df, S_df=S_df, mask_df=mask_df)\n",
    "    val_ts_dataset = TimeSeriesDataset(Y_df=Y_scaled_df, X_df=X_scaled_df, S_df=S_df, mask_df=val_mask_df)\n",
    "    \n",
    "    train_ts_loader = TimeSeriesLoader(ts_dataset=train_ts_dataset,\n",
    "                                       model='nbeats',\n",
    "                                       offset=0,\n",
    "                                       #window_sampling_limit=ts_dataset.max_len,\n",
    "                                       window_sampling_limit=int(mc['window_sampling_limit']),\n",
    "                                       input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                                       output_size=int(mc['output_size']),\n",
    "                                       idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                       batch_size=int(mc['batch_size']),\n",
    "                                       complete_inputs=mc['complete_inputs'],\n",
    "                                       complete_sample=False,\n",
    "                                       shuffle=True)\n",
    "\n",
    "    print(\"train_ts_loader.ts_windows.shape\", train_ts_loader.ts_windows.shape)\n",
    "    print(f\"len(train_sampling_windows) * 24 = {len(train_ts_loader.windows_sampling_idx)} * 24 = {len(train_ts_loader.windows_sampling_idx) * 24}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    val_ts_loader = TimeSeriesLoader(ts_dataset=val_ts_dataset,\n",
    "                                       model='nbeats',\n",
    "                                       offset=0,\n",
    "                                       #window_sampling_limit=ts_dataset.max_len,\n",
    "                                       window_sampling_limit=int(mc['window_sampling_limit']),\n",
    "                                       input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                                       output_size=int(mc['output_size']),\n",
    "                                       #idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                       idx_to_sample_freq=24,\n",
    "                                       batch_size=int(mc['batch_size']),\n",
    "                                       complete_inputs=False,\n",
    "                                       complete_sample=True,\n",
    "                                       shuffle=False)\n",
    "\n",
    "    print(\"val_ts_loader.ts_windows.shape\", val_ts_loader.ts_windows.shape)\n",
    "    print(f\"len(val_sampling_windows) * 24 = {len(val_ts_loader.windows_sampling_idx)} * 24 = {len(val_ts_loader.windows_sampling_idx) * 24}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    mc['t_cols'] = train_ts_dataset.t_cols\n",
    "    return mc, train_ts_loader, val_ts_loader, scaler_y\n",
    "\n",
    "\n",
    "def model_fit_predict_roll(mc, Y_df, X_df, S_df, n_timestamps_pred, offsets):\n",
    "\n",
    "    X_raw_df = X_df.copy()\n",
    "    Y_raw_df = Y_df.copy()\n",
    "\n",
    "    #-------------------------------------- Rolling prediction on test --------------------------------------#\n",
    "\n",
    "    y_true_list = []\n",
    "    y_hat_list = []\n",
    "    mask_list = []\n",
    "    #offsets = [365*1*24, 0]\n",
    "    n_splits = len(offsets)\n",
    "    for split, offset in enumerate(offsets):\n",
    "        print(10*'-', f'Split {split+1}/{n_splits}', 10*'-')\n",
    "\n",
    "        #----------------------------------------------- Data -----------------------------------------------#\n",
    "        mc, train_ts_loader, val_ts_loader, scaler_y = prepare_data(mc=mc, Y_df=Y_raw_df, X_df=X_raw_df,\n",
    "                                                                    S_df=S_df, n_timestamps_pred=n_timestamps_pred,\n",
    "                                                                    offset=offset)\n",
    "\n",
    "        #--------------------------------------- Finetune and predict ---------------------------------------#\n",
    "        # Instantiate and train model\n",
    "        model = Nbeats(input_size_multiplier=mc['input_size_multiplier'],\n",
    "                       output_size=int(mc['output_size']),\n",
    "                       shared_weights=mc['shared_weights'],\n",
    "                       initialization=mc['initialization'],\n",
    "                       activation=mc['activation'],\n",
    "                       stack_types=mc['stack_types'],\n",
    "                       n_blocks=mc['n_blocks'],\n",
    "                       n_layers=mc['n_layers'],\n",
    "                       #n_hidden=2*[2*[int(mc['n_hidden'])]], # TODO; Revisar n_hidden1, n_hidden2 <------\n",
    "                       n_hidden=mc['n_hidden_list'],\n",
    "                       #n_hidden=2*[[256,256]],\n",
    "                       n_harmonics=int(mc['n_harmonics']),\n",
    "                       n_polynomials=int(mc['n_polynomials']),\n",
    "                       x_s_n_hidden = 0,#int(mc['x_s_n_hidden']),\n",
    "                       exogenous_n_channels=int(mc['exogenous_n_channels']),\n",
    "                       include_var_dict=mc['include_var_dict'],\n",
    "                       t_cols=mc['t_cols'],\n",
    "                       batch_normalization = mc['batch_normalization'],\n",
    "                       dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                       dropout_prob_exogenous=mc['dropout_prob_exogenous'],\n",
    "                       learning_rate=float(mc['learning_rate']),\n",
    "                       lr_decay=float(mc['lr_decay']),\n",
    "                       n_lr_decay_steps=float(mc['n_lr_decay_steps']),\n",
    "                       weight_decay=mc['weight_decay'],\n",
    "                       l1_theta=mc['l1_theta'],\n",
    "                       n_iterations=int(mc['n_iterations']),\n",
    "                       early_stopping=int(mc['early_stopping']),\n",
    "                       loss=mc['loss'],\n",
    "                       loss_hypar=float(mc['loss_hypar']),\n",
    "                       val_loss=mc['val_loss'],\n",
    "                       frequency=mc['frequency'],\n",
    "                       seasonality=int(mc['seasonality']),\n",
    "                       random_seed=int(mc['random_seed']))\n",
    "\n",
    "        if mc['early_stopping'] < mc['n_iterations']:\n",
    "            model.fit(train_ts_loader=train_ts_loader, val_ts_loader=val_ts_loader, eval_steps=mc['eval_steps'])\n",
    "        else:\n",
    "            model.fit(train_ts_loader=train_ts_loader, eval_steps=mc['eval_steps'])\n",
    "        \n",
    "        y_true, y_hat, mask = model.predict(ts_loader=val_ts_loader, eval_mode=True)\n",
    "        y_true_list.append(y_true)\n",
    "        y_hat_list.append(y_hat)\n",
    "        mask_list.append(mask)\n",
    "    y_total = np.vstack(y_true_list)\n",
    "    y_hat_total = np.vstack(y_hat_list)\n",
    "    mask_total = np.vstack(mask_list)\n",
    "\n",
    "    print(f'y_total.shape (#n_windows, #lt) {y_total.shape}')\n",
    "    print(f'y_hat_total.shape (#n_windows, #lt) {y_hat_total.shape}')\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Reshape for univariate and panel model compatibility\n",
    "    n_series = train_ts_loader.ts_dataset.n_series\n",
    "    n_fcds = len(y_total) // n_series\n",
    "    y_total = y_total.reshape(n_series, n_fcds, mc['output_size'])\n",
    "    y_hat_total = y_hat_total.reshape(n_series, n_fcds, mc['output_size'])\n",
    "\n",
    "    print(\"y_total.shape (#n_series, #n_fcds, #lt) \", y_total.shape)\n",
    "    print(\"y_hat_total.shape (#n_series, #n_fcds, #lt) \", y_hat_total.shape)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    meta_data = val_ts_loader.ts_dataset.meta_data\n",
    "\n",
    "    return y_total, y_hat_total, mask_total, meta_data, model\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "#### CRISTIAN\n",
    "############################################################################################\n",
    "\n",
    "def train_val_split(len_series, offset, window_sampling_limit, n_val_weeks, ds_per_day):\n",
    "    last_ds = len_series - offset\n",
    "    first_ds = max(last_ds - window_sampling_limit, 0)\n",
    "\n",
    "    last_day = int(last_ds/ds_per_day)\n",
    "    first_day = int(first_ds/ds_per_day)\n",
    "\n",
    "    days = set(range(first_day, last_day)) # All days, to later get train days\n",
    "    # Sample weeks from here, -7 to avoid sampling from last week\n",
    "    # To not sample first week and have inputs\n",
    "    sampling_days = set(range(first_day + 7, last_day - 7))\n",
    "    validation_days = set({}) # Val days set\n",
    "    \n",
    "    # For loop for n of weeks in validation\n",
    "    for i in range(n_val_weeks):\n",
    "        # Sample random day, init of week\n",
    "        init_day = random.sample(sampling_days, 1)[0]\n",
    "        # Select days of sampled init of week\n",
    "        sampled_days = list(range(init_day, min(init_day+7, last_day)))\n",
    "        # Add days to validation days\n",
    "        validation_days.update(sampled_days)\n",
    "        # Remove days from sampling_days, including overlapping resulting previous week\n",
    "        days_to_remove = set(range(init_day-6, min(init_day+7, last_day)))\n",
    "        sampling_days = sampling_days.difference(days_to_remove)\n",
    "\n",
    "    train_days = days.difference(validation_days)\n",
    "\n",
    "    train_days = sorted(list(train_days))\n",
    "    validation_days = sorted(list(validation_days))\n",
    "\n",
    "    train_idx = []\n",
    "    for day in train_days:\n",
    "        hours_idx = range(day*ds_per_day,(day+1)*ds_per_day)\n",
    "        train_idx += hours_idx\n",
    "\n",
    "    val_idx = []\n",
    "    for day in validation_days:\n",
    "        hours_idx = range(day*ds_per_day,(day+1)*ds_per_day)\n",
    "        val_idx += hours_idx\n",
    "\n",
    "    assert all([idx < last_ds for idx in val_idx]), 'Leakage!!!!'\n",
    "    \n",
    "    return train_idx, val_idx"
   ]
  },
  {
   "source": [
    "# RUN VALIDATION CODE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# run_val_nbeatsx(hyperparameters, Y_df, X_df, data_augmentation, random_validation, trials, trials_file_name)\n",
    "def run_val_nbeatsx(mc, Y_df, X_df, S_df, trials, trials_file_name, final_evaluation=False, return_predictions=False):\n",
    "\n",
    "    # Save trials, can analyze progress\n",
    "    if trials is not None:\n",
    "        save_every_n_step = 5\n",
    "        current_step = len(trials.trials)\n",
    "        if (current_step % save_every_n_step==0):\n",
    "            with open(trials_file_name, \"wb\") as f:\n",
    "                pickle.dump(trials, f)\n",
    "\n",
    "    start_time = time.time()    \n",
    "    \n",
    "    #---------------------------------------- Parse  Hyperparameters ----------------------------------------#\n",
    "    mc['include_var_dict'] = {'y': [-2, -3, -8],\n",
    "                              'Exogenous1': [-1, -2, -8],\n",
    "                              'Exogenous2': [-1, -2, -8],\n",
    "                              'week_day': [-1]}\n",
    "\n",
    "    n_hidden = int(mc['n_hidden'])\n",
    "    mc['n_hidden_list'] =  2*[[n_hidden, n_hidden]]\n",
    "    \n",
    "    #---------------------------------- Instantiate model, fit and predict ----------------------------------#\n",
    "\n",
    "    # y_total, y_hat_total, mask_total, meta_data, model = model_fit_predict_roll(mc=mc, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "    #                                                                             offsets=[0], n_timestamps_pred=728*24)\n",
    "\n",
    "    # y_total, y_hat_total, mask_total, meta_data, model = model_fit_predict_roll(mc=mc, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "    #                                                                 offsets=[365*1*24, 0], n_timestamps_pred=365*1*24)\n",
    "\n",
    "    y_total, y_hat_total, mask_total, meta_data, model = model_fit_predict_roll(mc=mc, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "                                                                                offsets=[7*91*24, 6*91*24, 5*91*24, 4*91*24,\n",
    "                                                                                         3*91*24, 2*91*24, 1*91*24, 0],\n",
    "                                                                                n_timestamps_pred=91*24)\n",
    "\n",
    "    print('Best Model Evaluation')\n",
    "    benchmark_df, y_total_nans_perc, y_hat_total_nans_perc, reported_loss = forecast_evaluation_table(y_total=y_total, \n",
    "                                                                                                      y_hat_total=y_hat_total, \n",
    "                                                                                                      meta_data=meta_data)\n",
    "    print(benchmark_df)\n",
    "    print('\\n')\n",
    "    \n",
    "    # CONDITIONAL ON CORRECT PREDICTION \n",
    "    # Average percentage difference of MAE, SMAPE, MAPE, RMSE\n",
    "    #reported_loss = protect_nan_reported_loss(model)\n",
    "    run_time = time.time() - start_time\n",
    "\n",
    "    results =  {'loss': reported_loss,\n",
    "                'loss_name': mc['val_loss'],\n",
    "                'mc': mc,\n",
    "                'final_insample_loss': model.final_insample_loss,\n",
    "                'final_outsample_loss': model.final_outsample_loss,\n",
    "                'trajectories': model.trajectories,\n",
    "                'run_time': run_time,\n",
    "                'status': STATUS_OK}\n",
    "    \n",
    "    if final_evaluation:\n",
    "        print('Best Model Hyperpars')\n",
    "        print(75*'=')\n",
    "        print(pd.Series(mc))\n",
    "        print(75*'='+'\\n')\n",
    "    \n",
    "    if return_predictions:\n",
    "        return y_total, y_hat_total, mask_total, meta_data\n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "source": [
    "# EXPERIMENT SPACES"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# TODO: eliminate n_harmonics, n_polynomials think on kwargs maybe?\n",
    "# TODO: think on n_consistency for exogenous_n_channels -> n_xt_channels\n",
    "# TODO: x_s_n_hidden -> n_xs_hidden\n",
    "# TODO: input_size_multiplier -> Change for n_xt?\n",
    "# TODO: n_hidden -> n_theta_list\n",
    "def get_experiment_space(args):\n",
    "    if args.space=='nbeats_cristian':\n",
    "        space = {# Architecture parameters\n",
    "                 'input_size_multiplier': hp.choice('input_size_multiplier', [7]),\n",
    "                 'output_size': hp.choice('output_size', [24]),\n",
    "                 'shared_weights': hp.choice('shared_weights', [False]),\n",
    "                 'activation': hp.choice('activation', ['relu','softplus','tanh','selu','lrelu','prelu','sigmoid']),\n",
    "                 'initialization':  hp.choice('initialization', ['orthogonal','he_uniform','he_normal',\n",
    "                                                                 'glorot_uniform','glorot_normal','lecun_normal']),\n",
    "                 'stack_types': hp.choice('stack_types', [ ['identity'],\n",
    "                                                            1*['identity']+['exogenous_wavenet'],\n",
    "                                                                ['exogenous_wavenet']+1*['identity'],\n",
    "                                                            1*['identity']+['exogenous_tcn'],\n",
    "                                                                ['exogenous_tcn']+1*['identity'] ]),\n",
    "                 'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),\n",
    "                 'n_layers': hp.choice('n_layers', [ [2, 2] ]),\n",
    "                 'n_hidden': hp.quniform('n_hidden', 50, 500, 1),\n",
    "                 'n_harmonics': hp.choice('n_harmonics', [1]),\n",
    "                 'n_polynomials': hp.choice('n_polynomials', [2]),\n",
    "                 'exogenous_n_channels': hp.quniform('exogenous_n_channels', 1, 10, 1),\n",
    "                 'x_s_n_hidden': hp.choice('x_s_n_hidden', [0]),\n",
    "                 # Regularization and optimization parameters\n",
    "                 'batch_normalization': hp.choice('batch_normalization', [True, False]),\n",
    "                 'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 1),\n",
    "                 'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),\n",
    "                 'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.1)),\n",
    "                 'lr_decay': hp.choice('lr_decay', [0.5]),\n",
    "                 'n_lr_decay_steps': hp.choice('n_lr_decay_steps', [3]),\n",
    "                 'weight_decay': hp.loguniform('weight_decay', np.log(5e-4), np.log(0.01)),\n",
    "                 'n_iterations': hp.choice('n_iterations', [10_000]), #[args.max_epochs]),\n",
    "                 'early_stopping': hp.choice('early_stopping', [8]),\n",
    "                 'eval_steps': hp.choice('eval_steps', [50]),\n",
    "                 #'n_val_weeks': hp.choice('n_val_weeks', [52]),\n",
    "                 'n_val_weeks': hp.choice('n_val_weeks', [52*2]),\n",
    "                 'loss': hp.choice('loss', ['MAE']),\n",
    "                 'loss_hypar': hp.choice('loss_hypar', [0.5]),\n",
    "                 'val_loss': hp.choice('val_loss', ['MAE']), #[args.val_loss]),\n",
    "                 'l1_theta': hp.choice('l1_theta', [0, hp.loguniform('lambdal1', np.log(1e-5), np.log(1))]),\n",
    "                 # Data parameters\n",
    "                 'normalizer_y': hp.choice('normalizer_y', [None, 'norm', 'norm1', \n",
    "                                                            'std', 'median', 'invariant']),\n",
    "                 'normalizer_x': hp.choice('normalizer_x', [None, 'norm', 'norm1',\n",
    "                                                            'std', 'median', 'invariant']),\n",
    "                 'window_sampling_limit': hp.choice('window_sampling_limit', [50_000]),\n",
    "                 'complete_inputs': hp.choice('complete_inputs', [True]),\n",
    "                 'frequency': hp.choice('frequency', ['H']),\n",
    "                 'seasonality': hp.choice('seasonality', [24]),\n",
    "                 'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [24]),\n",
    "                 'batch_size': hp.choice('batch_size', [128, 256, 512]),\n",
    "                 'random_seed': hp.quniform('random_seed', 1, 1000, 1)}\n",
    "                 # CONSIDERO ESTO INNECESARIO\n",
    "                 # 'n_hidden_1': hp.quniform('n_hidden_1', 50, 500, 1),\n",
    "                 # 'n_hidden_2': hp.quniform('n_hidden_2', 50, 500, 1),\n",
    "                 # 'include_var_dict': hp.choice('include_var_dict', [{'y': [-2, -3, -8],\n",
    "                 #                                                     'Exogenous1': [-1, -2, -8],\n",
    "                 #                                                     'Exogenous2': [-1, -2, -8],\n",
    "                 #                                                     'week_day': [-1]}]),                 \n",
    "                 # 'incl_pr1': hp.choice('incl_pr1', [True]),\n",
    "                 # 'incl_pr2': hp.choice('incl_pr2', [True, False]),\n",
    "                 # 'incl_pr3': hp.choice('incl_pr3', [True, False]),\n",
    "                 # 'incl_pr7': hp.choice('incl_pr7', [True, False]),\n",
    "                 # 'incl_ex1_0': hp.choice('incl_ex1_0', [True, False]),\n",
    "                 # 'incl_ex1_1': hp.choice('incl_ex1_1', [True, False]),\n",
    "                 # 'incl_ex1_7': hp.choice('incl_ex1_7', [True, False]),\n",
    "                 # 'incl_ex2_0': hp.choice('incl_ex2_0', [True, False]),\n",
    "                 # 'incl_ex2_1': hp.choice('incl_ex2_1', [True, False]),\n",
    "                 # 'incl_ex2_7': hp.choice('incl_ex2_7', [True, False]),\n",
    "                 # 'incl_day': hp.choice('incl_day', [True, False]),\n",
    "                 # 'args.data_augmentation'\n",
    "                 # 'n_val_weeks': hp.choice('n_val_weeks', [args.n_val_weeks]}\n",
    "    \n",
    "    elif args.space=='nbeats_collapsed2':\n",
    "        space= {# Architecture parameters\n",
    "                'input_size_multiplier': hp.choice('input_size_multiplier', [7]),\n",
    "                'output_size': hp.choice('output_size', [24]),\n",
    "                'shared_weights': hp.choice('shared_weights', [False]),\n",
    "                'activation': hp.choice('activation', ['selu']),\n",
    "                'initialization':  hp.choice('initialization', ['glorot_normal','he_normal']),\n",
    "                'stack_types': hp.choice('stack_types', [2*['identity'],\n",
    "                                                         1*['identity']+1*['exogenous_tcn'],\n",
    "                                                         1*['exogenous_tcn']+1*['identity'] ]),\n",
    "                'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),\n",
    "                'n_layers': hp.choice('n_layers', [ [2, 2] ]),\n",
    "                #'n_hidden': hp.choice('n_hidden', [400]),\n",
    "                'n_hidden': hp.choice('n_hidden', [400]),\n",
    "                'n_harmonics': hp.choice('n_harmonics', [1]),\n",
    "                'n_polynomials': hp.choice('n_polynomials', [2]),\n",
    "                'exogenous_n_channels': hp.quniform('exogenous_n_channels', 1, 10, 1),\n",
    "                'x_s_n_hidden': hp.choice('x_s_n_hidden', [0]),\n",
    "                # Regularization and optimization parameters\n",
    "                'batch_normalization': hp.choice('batch_normalization', [False]),\n",
    "                'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 0.5),\n",
    "                'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),\n",
    "                'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.001)),\n",
    "                'lr_decay': hp.uniform('lr_decay', 0.3, 0.5),\n",
    "                'n_lr_decay_steps': hp.choice('n_lr_decay_steps', [3]),\n",
    "                'weight_decay': hp.loguniform('weight_decay', np.log(5e-5), np.log(5e-3)),\n",
    "                'n_iterations': hp.choice('n_iterations', [10_000]), #[args.max_epochs]),\n",
    "                'early_stopping': hp.choice('early_stopping', [16]),\n",
    "                'eval_steps': hp.choice('eval_steps', [50]),\n",
    "                'n_val_weeks': hp.choice('n_val_weeks', [52*2]),\n",
    "                #'loss': hp.choice('loss', ['PINBALL']),\n",
    "                #'loss_hypar': hp.uniform('loss_hypar', 0.48, 0.51),\n",
    "                'loss': hp.choice('loss', ['MAE']),\n",
    "                'loss_hypar': hp.choice('loss_hypar', [0.5]),                \n",
    "                'val_loss': hp.choice('val_loss', ['MAE']), #[args.val_loss]),\n",
    "                'l1_theta': hp.choice('l1_theta', [0]),\n",
    "                # Data parameters\n",
    "                'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "                'normalizer_x': hp.choice('normalizer_x', ['median']),\n",
    "                'window_sampling_limit': hp.choice('window_sampling_limit', [50_000]),\n",
    "                'complete_inputs': hp.choice('complete_inputs', [True]),                \n",
    "                'frequency': hp.choice('frequency', ['H']),\n",
    "                'seasonality': hp.choice('seasonality', [24]),\n",
    "                'include_var_dict': hp.choice('include_var_dict', [{'y': [-2, -3, -8],\n",
    "                                                                    'Exogenous1': [-1, -2, -8],\n",
    "                                                                    'Exogenous2': [-1, -2, -8],\n",
    "                                                                    'week_day': [-1]}]),\n",
    "                'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [24]),\n",
    "                'batch_size': hp.choice('batch_size', [256]),\n",
    "                'random_seed': hp.quniform('random_seed', 10, 20, 1)}\n",
    "\n",
    "    elif args.space=='nbeats_speculation':\n",
    "        space= {# Architecture parameters\n",
    "                'input_size_multiplier': hp.choice('input_size_multiplier', [7]),\n",
    "                'output_size': hp.choice('output_size', [24]),\n",
    "                'shared_weights': hp.choice('shared_weights', [False]),\n",
    "                'activation': hp.choice('activation', ['selu']),\n",
    "                'initialization':  hp.choice('initialization', ['glorot_normal','he_normal']),\n",
    "                'stack_types': hp.choice('stack_types', [2*['identity'],\n",
    "                                                         1*['identity']+1*['exogenous_tcn'],\n",
    "                                                         1*['exogenous_tcn']+1*['identity'] ]),\n",
    "                'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),\n",
    "                'n_layers': hp.choice('n_layers', [ [2, 2] ]),\n",
    "                #'n_hidden': hp.choice('n_hidden', [400]),\n",
    "                'n_hidden': hp.choice('n_hidden', [400]),\n",
    "                'n_harmonics': hp.choice('n_harmonics', [1]),\n",
    "                'n_polynomials': hp.choice('n_polynomials', [2]),\n",
    "                'exogenous_n_channels': hp.quniform('exogenous_n_channels', 1, 10, 1),\n",
    "                'x_s_n_hidden': hp.choice('x_s_n_hidden', [0]),\n",
    "                # Regularization and optimization parameters\n",
    "                'batch_normalization': hp.choice('batch_normalization', [False]),\n",
    "                'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 0.5),\n",
    "                'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),\n",
    "                'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.001)),\n",
    "                'lr_decay': hp.uniform('lr_decay', 0.3, 0.7),\n",
    "                'n_lr_decay_steps': hp.choice('n_lr_decay_steps', [3]),\n",
    "                'weight_decay': hp.loguniform('weight_decay', np.log(5e-6), np.log(5e-3)),\n",
    "                # 'n_iterations': hp.choice('n_iterations', [100]),\n",
    "                'n_iterations': hp.choice('n_iterations', [1_000, 2_000, 5_000, 7_000, 10_000]),\n",
    "                'early_stopping': hp.choice('early_stopping', [100_000]),\n",
    "                'eval_steps': hp.choice('eval_steps', [100]),\n",
    "                'n_val_weeks': hp.choice('n_val_weeks', [52*2]),\n",
    "                'loss': hp.choice('loss', ['PINBALL']),\n",
    "                'loss_hypar': hp.uniform('loss_hypar', 0.48, 0.51),\n",
    "                # 'loss': hp.choice('loss', ['MAE']),\n",
    "                # 'loss_hypar': hp.choice('loss_hypar', [0.5]),                \n",
    "                'val_loss': hp.choice('val_loss', ['MAE']), #[args.val_loss]),\n",
    "                'l1_theta': hp.choice('l1_theta', [0]),\n",
    "                # Data parameters\n",
    "                'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "                'normalizer_x': hp.choice('normalizer_x', ['median']),\n",
    "                'window_sampling_limit': hp.choice('window_sampling_limit', [50_000]),\n",
    "                'complete_inputs': hp.choice('complete_inputs', [True]),                \n",
    "                'frequency': hp.choice('frequency', ['H']),\n",
    "                'seasonality': hp.choice('seasonality', [24]),\n",
    "                'include_var_dict': hp.choice('include_var_dict', [{'y': [-2, -3, -8],\n",
    "                                                                    'Exogenous1': [-1, -2, -8],\n",
    "                                                                    'Exogenous2': [-1, -2, -8],\n",
    "                                                                    'week_day': [-1]}]),\n",
    "                'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [24]),\n",
    "                'batch_size': hp.choice('batch_size', [256]),\n",
    "                'random_seed': hp.quniform('random_seed', 10, 20, 1)}\n",
    "    \n",
    "    else:\n",
    "        print(f'Experiment space {args.space} not available')\n",
    "\n",
    "    return space\n",
    "\n",
    "def parse_trials(trials):\n",
    "    # Initialize\n",
    "    trials_dict = {'tid': [], 'loss': [], 'trajectories': [], 'mc': []}\n",
    "    for tidx in range(len(trials)):\n",
    "        # Main\n",
    "        trials_dict['tid']  += [trials.trials[tidx]['tid']]\n",
    "        trials_dict['loss'] += [trials.trials[tidx]['result']['loss']]\n",
    "        trials_dict['trajectories'] += [trials.trials[tidx]['result']['trajectories']]\n",
    "\n",
    "        # Model Configs\n",
    "        mc = trials.trials[tidx]['result']['mc']\n",
    "        trials_dict['mc'] += [mc]\n",
    "    \n",
    "    trials_df = pd.DataFrame(trials_dict)\n",
    "    return trials_df\n",
    "\n",
    "def main(args):\n",
    "    #---------------------------------------------- Directories ----------------------------------------------#\n",
    "    \n",
    "    dataset = eval(args.dataset)\n",
    "    dataset_str = dataset[0]\n",
    "    for market in dataset[1:]:\n",
    "        dataset_str += f'{market}_'\n",
    "    output_dir = f'./results/{dataset_str}/{args.space}/'\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    assert os.path.exists(output_dir), f'Output dir {output_dir} does not exist'\n",
    "\n",
    "    hyperopt_file = output_dir + f'hyperopt_{args.experiment_id}.p'\n",
    "    result_test_file = output_dir + f'result_test_{args.experiment_id}.p'\n",
    "\n",
    "    #---------------------------------------------- Read  Data ----------------------------------------------#\n",
    "    print('\\n'+75*'-')\n",
    "    print(28*'-', 'Preparing Dataset', 28*'-')\n",
    "    print(75*'-'+'\\n')\n",
    "\n",
    "    #TEST_DATE = {'NP': '2016-12-27',\n",
    "    #             'PJM':'2016-12-27',\n",
    "    #             'BE':'2015-01-04',\n",
    "    #             'FR': '2015-01-04',\n",
    "    #             'DE':'2016-01-04'}\n",
    "    #test_date = TEST_DATE[args.dataset]\n",
    "    #Y_insample_df, X_insample_df, Y_outsample_df, X_outsample_df, _ = load_epf(directory='../data/',\n",
    "    #                                                                           market=args.dataset,\n",
    "    #                                                                           first_date_test=test_date,\n",
    "    #                                                                           days_in_test=728)\n",
    "    Y_df, X_df, S_df = EPF.load_groups(directory='data', groups=dataset)\n",
    "\n",
    "    #-------------------------------------- Hyperparameter Optimization --------------------------------------#\n",
    "\n",
    "    if not os.path.isfile(hyperopt_file):\n",
    "        print('\\n'+75*'-')\n",
    "        print(22*'-', 'Start Hyperparameter  tunning', 22*'-')\n",
    "        print(75*'-'+'\\n')\n",
    "\n",
    "        space = get_experiment_space(args)\n",
    "\n",
    "        trials = Trials()\n",
    "        fmin_objective = partial(run_val_nbeatsx, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "                                 trials=trials, trials_file_name=hyperopt_file)\n",
    "        fmin(fmin_objective, space=space, algo=tpe.suggest, max_evals=args.hyperopt_iters, trials=trials, verbose=True)\n",
    "\n",
    "        # Save output\n",
    "        with open(hyperopt_file, \"wb\") as f:\n",
    "            pickle.dump(trials, f)\n",
    "\n",
    "    print('\\n'+75*'-')\n",
    "    print(20*'-', 'Hyperparameter  tunning  finished', 20*'-')\n",
    "    print(75*'-'+'\\n')\n",
    "\n",
    "    #----------------------------------------- Selected 'Best' Model -----------------------------------------#\n",
    "\n",
    "    # Read and parse trials pickle\n",
    "    trials = pickle.load(open(hyperopt_file, 'rb'))\n",
    "    trials_df = parse_trials(trials)\n",
    "\n",
    "    # Get best mc\n",
    "    idx = trials_df.loss.idxmin()\n",
    "    best_mc = trials_df.loc[idx]['mc']\n",
    "    \n",
    "    run_val_nbeatsx(best_mc, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "                    trials=trials, trials_file_name=hyperopt_file, final_evaluation=True)\n",
    "\n",
    "def parse_args():\n",
    "    desc = \"NBEATSx overfit\"\n",
    "    parser = argparse.ArgumentParser(description=desc)\n",
    "\n",
    "    parser.add_argument('--dataset', type=str, required=True, help='NP')\n",
    "    parser.add_argument('--space', type=str, required=True, help='Experiment hyperparameter space')\n",
    "    parser.add_argument('--hyperopt_iters', type=int, help='hyperopt_iters')\n",
    "    #parser.add_argument('--max_epochs', type=int, required=False, default=2000, help='max train epochs')\n",
    "    #parser.add_argument('--val_loss', type=str, required=False, default=None, help='validation loss')\n",
    "    parser.add_argument('--experiment_id', default=None, required=False, type=str, help='string to identify experiment')\n",
    "    return parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # parse arguments\n",
    "    args = parse_args()\n",
    "    if args is None:\n",
    "        exit()\n",
    "    \n",
    "    main(args)\n",
    "\n",
    "# CUDA_VISIBLE_DEVICES=0 PYTHONPATH=. python src/overfit_nbeatsx.py --dataset \"['NP']\" --space 'nbeats_collapsed' --hyperopt_iters 200 --val_loss 'MAE'  --experiment_id 'AvgDiff_2021_01_16'\n",
    "# CUDA_VISIBLE_DEVICES=0 PYTHONPATH=. python src/overfit_nbeatsx.py --dataset \"['NP']\" --space 'nbeats_collapsed' --hyperopt_iters 200 --val_loss 'MAE'  --experiment_id 'AvgDiff_2021_01_16'\n",
    "# CUDA_VISIBLE_DEVICES=1 PYTHONPATH=. python src/overfit_nbeatsx.py --dataset \"['NP']\" --space 'nbeats_collapsed' --hyperopt_iters 200 --val_loss 'MAE'  --experiment_id 'AvgDiff_2021_01_16'\n",
    "# CUDA_VISIBLE_DEVICES=1 PYTHONPATH=. python src/overfit_nbeatsx.py --dataset \"['NP']\" --space 'nbeats_collapsed' --hyperopt_iters 200 --val_loss 'MAE'  --experiment_id 'AvgDiff_2021_01_16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "args = pd.Series({'dataset': \"['NP']\",\n",
    "                  #'dataset': \"['NP', 'PJM', 'BE', 'FR']\",\n",
    "                  'space': 'nbeats_speculation',\n",
    "                  'hyperopt_iters': 2,\n",
    "                  'experiment_id': 'debug8', 'gpu_id': 1})\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu_id)\n",
    "print('cuda devices,', os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "main(args)"
   ]
  },
  {
   "source": [
    "# TEST SINGLE MODEL CONFIG"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mc0 = {# Architecture parameters\n",
    "      'input_size_multiplier': 7,\n",
    "      'output_size': 24,\n",
    "      'shared_weights': False,\n",
    "      'activation': 'selu',\n",
    "      'initialization': 'he_normal',\n",
    "      'stack_types': ['exogenous_tcn']+1*['identity'],\n",
    "      'n_blocks': [1, 1],\n",
    "      'n_layers': [2, 2],\n",
    "      'n_hidden': 364,\n",
    "      #'n_hidden_list': 2*[[462,462]],\n",
    "      'n_polynomials': 2,\n",
    "      'n_harmonics': 1,\n",
    "      'exogenous_n_channels': 3,\n",
    "      'x_s_n_hidden': 0, # TODO: referencia dinámica vs datasets\n",
    "      # Regularization and optimization parameters\n",
    "      'batch_normalization': False,\n",
    "      'dropout_prob_theta': 0.2,\n",
    "      'dropout_prob_exogenous': 0.2,\n",
    "      'learning_rate': 0.0005, #0.002,\n",
    "      'lr_decay': 0.64,\n",
    "      'n_lr_decay_steps': 3,\n",
    "      'weight_decay': 0.00015,\n",
    "      'n_iterations': 1000,\n",
    "      'early_stopping': 8,\n",
    "      'eval_steps': 50,\n",
    "      'n_val_weeks': 52*2,\n",
    "      'loss': 'PINBALL',\n",
    "      'loss_hypar': 0.5, #0.49,\n",
    "      'val_loss': 'MAE',\n",
    "      'l1_theta': 0,\n",
    "      # Data parameters\n",
    "      'normalizer_y': None,\n",
    "      'normalizer_x': 'median',\n",
    "      'window_sampling_limit': 50_000,\n",
    "      'complete_inputs': True,\n",
    "      'frequency':'H',\n",
    "      'seasonality': 24,\n",
    "      'idx_to_sample_freq': 24,\n",
    "      'include_var_dict': {'y': [-2, -3, -8],\n",
    "                           'Exogenous1': [-1, -2, -8],\n",
    "                           'Exogenous2': [-1, -2, -8],\n",
    "                           'week_day': [-1]},\n",
    "      'batch_size': 256,\n",
    "      'random_seed': 10}\n",
    "\n",
    "mc = mc0\n",
    "dataset = ['NP']\n",
    "# dataset = ['BE']\n",
    "# dataset = ['BE', 'FR']\n",
    "# dataset = ['NP', 'PJM']\n",
    "# dataset = ['NP', 'PJM', 'BE', 'FR']\n",
    "if len(dataset)>1: mc['x_s_n_hidden'] = 2\n",
    "\n",
    "Y_df, X_df, S_df = EPF.load_groups(directory='data', groups=dataset)\n",
    "\n",
    "plt.plot(Y_df.y.values)\n",
    "plt.show()\n",
    "\n",
    "y_total, y_hat_total, mask_total, meta_data = run_val_nbeatsx(mc=mc, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "                                                              trials=None, trials_file_name=None, final_evaluation=False, return_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(y_total.flatten()-y_hat_total.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, market_data in enumerate(meta_data):\n",
    "    market = market_data['unique_id']\n",
    "    y_hat_plot = y_hat_total[i,:,:].reshape(-1)\n",
    "    y_true_plot = y_total[i,:,:].reshape(-1)\n",
    "    \n",
    "    performance = np.round(mae(y=y_true_plot, y_hat=y_hat_plot), 5)\n",
    "    plt.plot(range(len(y_true_plot)), y_true_plot, label='true', alpha=0.7)\n",
    "    plt.plot(range(len(y_hat_plot)), y_hat_plot, label='pred', alpha=0.7)\n",
    "    plt.title(f\"{market} predictions \\n all lead time  MAE={performance}\")\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Electricity Price')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}