{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "nbeatsx",
   "display_name": "nbeatsx",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiments.nbeats.hyperopt_epf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import time\n",
    "import os\n",
    "# Limit number of threads in numpy and others to avoid throttling\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\" # export OMP_NUM_THREADS=4\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"2\" # export OPENBLAS_NUM_THREADS=4 \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"3\" # export MKL_NUM_THREADS=6\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"2\" # export VECLIB_MAXIMUM_THREADS=4\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"3\" # export NUMEXPR_NUM_THREADS=6\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import pickle\n",
    "import glob\n",
    "import itertools\n",
    "import random\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "from nixtla.data.scalers import Scaler\n",
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset\n",
    "from nixtla.data.tsloader_fast import TimeSeriesLoader\n",
    "from nixtla.losses.numpy import mae, mape, smape, rmse, pinball_loss\n",
    "\n",
    "# Models\n",
    "from nixtla.models.nbeats.nbeats import Nbeats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def forecast_evaluation_table(y_true, y_hat):\n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_hat = y_hat.reshape(-1)\n",
    "    \n",
    "    _pinball50 = np.round(pinball_loss(y_true, y_hat, tau=0.5),5)\n",
    "    _mae   = np.round(mae(y_true, y_hat),5)\n",
    "    _mape  = np.round(mape(y_true, y_hat),5)\n",
    "    _smape = np.round(smape(y_true, y_hat),5)\n",
    "    _rmse  = np.round(rmse(y_true, y_hat),5)\n",
    "\n",
    "    evaluations = pd.DataFrame({'metric': ['pinball50', 'mae', 'mape', 'smape', 'rmse'],\n",
    "                                'nbeatsx': [_pinball50, _mae, _mape, _smape, _rmse]})                          \n",
    "\n",
    "    return evaluations\n",
    "\n",
    "def run_val_nbeatsx(mc, train_loader, val_loader, trials, trials_file_name, final_evaluation=False):\n",
    "    # Save trials, can analyze progress\n",
    "    save_every_n_step = 5\n",
    "    current_step = len(trials.trials)\n",
    "    if (current_step % save_every_n_step==0):\n",
    "        with open(trials_file_name, \"wb\") as f:\n",
    "            pickle.dump(trials, f)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = Nbeats(input_size_multiplier=int(mc['input_size_multiplier']),\n",
    "                   output_size=int(mc['output_size']),\n",
    "                   shared_weights=int(mc['shared_weights']),\n",
    "                   activation=mc['activation'],\n",
    "                   initialization=mc['initialization'],\n",
    "                   stack_types=mc['stack_types'], #2*['identity'],\n",
    "                   n_blocks=mc['n_blocks'], #2*[1],\n",
    "                   n_layers=mc['n_layers'], #2*[2],\n",
    "                   n_hidden=2*[2*[int(mc['n_hidden'])]], #2*[[256,256]]\n",
    "                   n_polynomials=mc['n_polynomials'], #2,\n",
    "                   n_harmonics=int(mc['n_harmonics']), #1,\n",
    "                   exogenous_n_channels=int(mc['exogenous_n_channels']), #9,\n",
    "                   include_var_dict={'y': [-2, -3, -8],\n",
    "                                     'Exogenous1': [-1, -2, -8],\n",
    "                                     'Exogenous2': [-1, -2, -8],\n",
    "                                     'week_day': [-1]}, #TODO: mc['include_var_dict] \n",
    "                   t_cols=train_loader.ts_dataset.t_cols,\n",
    "                   batch_normalization=mc['batch_normalization'], #False,\n",
    "                   dropout_prob_theta=float(mc['dropout_prob_theta']), #0.01,\n",
    "                   dropout_prob_exogenous=float(mc['dropout_prob_exogenous']), #0.01,\n",
    "                   x_s_n_hidden=int(mc['x_s_n_hidden']), #0,\n",
    "                   learning_rate=float(mc['learning_rate']), #0.007,\n",
    "                   lr_decay=float(mc['lr_decay']), #0.5,\n",
    "                   n_lr_decay_steps=int(mc['n_lr_decay_steps']), #3,\n",
    "                   weight_decay=float(mc['weight_decay']), #0.0000001,\n",
    "                   l1_theta=float(mc['l1_theta']), #0.0001,\n",
    "                   n_iterations=int(mc['n_iterations']), #200,\n",
    "                   early_stopping=int(mc['early_stopping']), #40,\n",
    "                   loss=mc['loss'], #'PINBALL',\n",
    "                   loss_hypar=float(mc['loss_hypar']), #0.5,\n",
    "                   val_loss=mc['val_loss'], #'MAE',\n",
    "                   frequency=mc['frequency'], #'H',\n",
    "                   random_seed=int(mc['random_seed']), #1,\n",
    "                   seasonality=int(mc['seasonality'])) #24)\n",
    "\n",
    "    model.fit(train_ts_loader=train_loader, val_ts_loader=val_loader, verbose=True, eval_steps=10) # aqui val_loader==Test\n",
    "\n",
    "    # TODO: Pytorch numerical error hacky protection\n",
    "    hyperopt_reported_loss = model.final_outsample_loss\n",
    "    if np.isnan(model.final_insample_loss):\n",
    "        hyperopt_reported_loss = 100\n",
    "    if model.final_insample_loss<=0:\n",
    "        hyperopt_reported_loss = 100\n",
    "\n",
    "    if np.isnan(model.final_outsample_loss):\n",
    "        hyperopt_reported_loss = 100    \n",
    "    if model.final_outsample_loss<=0:\n",
    "        hyperopt_reported_loss = 100    \n",
    "\n",
    "    results =  {'loss': model.final_outsample_loss,\n",
    "                'loss_name': mc['val_loss'], #val_mae <--------\n",
    "                'mc': mc,\n",
    "                'final_insample_loss': model.final_insample_loss,\n",
    "                'final_outsample_loss': model.final_outsample_loss,\n",
    "                'trajectories': model.trajectories,\n",
    "                'run_time': time.time() - start_time,\n",
    "                'status': STATUS_OK}\n",
    "\n",
    "    if final_evaluation:\n",
    "        print('Best Model Hyperpars')\n",
    "        print(75*'=')\n",
    "        print(pd.Series(mc))\n",
    "        print(75*'='+'\\n')\n",
    "\n",
    "        print('Best Model Evaluation')\n",
    "        y_true, y_hat, _ = model.predict(ts_loader=val_loader, eval_mode=True)\n",
    "        print(forecast_evaluation_table(y_true, y_hat))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_experiment_space(args):\n",
    "    if args.space=='nbeats_extended1':\n",
    "        space = {# Architecture parameters\n",
    "                 'input_size_multiplier': hp.choice('input_size_multiplier', [7]),  #<------- TODO: Change for n_xt\n",
    "                 'output_size': hp.choice('output_size', [24]),\n",
    "                 'shared_weights': hp.choice('shared_weights', [False]),\n",
    "                 'activation': hp.choice('activation', ['relu','softplus','tanh','selu','lrelu','prelu','sigmoid']),\n",
    "                 'initialization':  hp.choice('initialization', ['orthogonal','he_uniform','he_normal',\n",
    "                                                                 'glorot_uniform','glorot_normal','lecun_normal']),\n",
    "                 'stack_types': hp.choice('stack_types', [ ['identity'],\n",
    "                                                            1*['identity']+['exogenous_wavenet'],\n",
    "                                                                ['exogenous_wavenet']+1*['identity'],\n",
    "                                                            1*['identity']+['exogenous_tcn'],\n",
    "                                                                ['exogenous_tcn']+1*['identity'] ]),\n",
    "                 'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),\n",
    "                 'n_layers': hp.choice('n_layers', [ [2, 2] ]),\n",
    "                 'n_hidden': hp.quniform('n_hidden_1', 50, 500, 1), #<------- TODO: Change for n_theta_list\n",
    "                 'n_harmonics': hp.choice('n_harmonics', [1]), #<--------- TODO: Eliminate unnecesary hypar\n",
    "                 'n_polynomials': hp.choice('n_polynomials', [2]), #<----- TODO: Eliminate unnecesary hypar\n",
    "                 'exogenous_n_channels': hp.quniform('exogenous_n_channels', 1, 10, 1), #<------- TODO: Change for n_xt_channels\n",
    "                 'x_s_n_hidden': hp.choice('x_s_n_hidden', [0]), #<------- TODO: Change for n_xs_hidden\n",
    "                 # Regularization and optimization parameters\n",
    "                 'batch_normalization': hp.choice('batch_normalization', [True, False]),\n",
    "                 'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 1),\n",
    "                 'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),\n",
    "                 'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.1)),\n",
    "                 'lr_decay': hp.choice('lr_decay', [0.5]),\n",
    "                 'n_lr_decay_steps': hp.choice('n_lr_decay_steps', [3]),\n",
    "                 'weight_decay': hp.loguniform('weight_decay', np.log(5e-4), np.log(0.01)),\n",
    "                 'n_iterations': hp.choice('n_iterations', [args.max_epochs]), #<------- TODO: Change for max_epochs\n",
    "                 'early_stopping': hp.choice('early_stopping', [40]),\n",
    "                 'loss': hp.choice('loss', ['PINBALL']),\n",
    "                 'loss_hypar': hp.uniform('loss_hypar', 0.45, 0.55),\n",
    "                 'val_loss': hp.choice('val_loss', [args.val_loss]),\n",
    "                 'l1_theta': hp.choice('l1_theta', [0, hp.loguniform('lambdal1', np.log(1e-5), np.log(1))]),\n",
    "                 # Data parameters\n",
    "                 'frequency': hp.choice('frequency', ['H']),\n",
    "                 'seasonality': hp.choice('seasonality', [24]),             \n",
    "                 'include_var_dict': hp.choice('include_var_dict', [{'y': [-2, -3, -8],\n",
    "                                                                     'Exogenous1': [-1, -2, -8],\n",
    "                                                                     'Exogenous2': [-1, -2, -8],\n",
    "                                                                     'week_day': [-1]}]),\n",
    "                 'random_seed': hp.quniform('random_seed', 1, 20, 1)}\n",
    "\n",
    "\n",
    "    elif args.space=='nbeats_collapsed':\n",
    "        space= {# Architecture parameters\n",
    "                'input_size_multiplier': hp.choice('input_size_multiplier', [7]),  #<------- TODO: Change for n_xt\n",
    "                'output_size': hp.choice('output_size', [24]),\n",
    "                'shared_weights': hp.choice('shared_weights', [False]),\n",
    "                'activation': hp.choice('activation', ['relu','softplus','tanh','selu','lrelu','prelu','sigmoid']),\n",
    "                'initialization':  hp.choice('initialization', ['orthogonal','he_uniform','he_normal',\n",
    "                                                                'glorot_uniform','glorot_normal','lecun_normal']),\n",
    "                'stack_types': hp.choice('stack_types', [ #['identity'],\n",
    "                                                          #  1*['identity']+['exogenous_wavenet'],\n",
    "                                                            ['exogenous_wavenet']+1*['identity'],\n",
    "                                                          #  1*['identity']+['exogenous_tcn'],\n",
    "                                                            ['exogenous_tcn']+1*['identity'] ]),\n",
    "                'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),\n",
    "                'n_layers': hp.choice('n_layers', [ [2, 2] ]),\n",
    "                'n_hidden': hp.quniform('n_hidden_1', 50, 500, 1), #<------- TODO: Change for n_theta_list\n",
    "                'n_harmonics': hp.choice('n_harmonics', [1]), #<--------- TODO: Eliminate unnecesary hypar\n",
    "                'n_polynomials': hp.choice('n_polynomials', [2]), #<----- TODO: Eliminate unnecesary hypar\n",
    "                'exogenous_n_channels': hp.quniform('exogenous_n_channels', 1, 10, 1), #<------- TODO: Change for n_xt_channels\n",
    "                'x_s_n_hidden': hp.choice('x_s_n_hidden', [0]), #<------- TODO: Change for n_xs_hidden\n",
    "                # Regularization and optimization parameters\n",
    "                'batch_normalization': hp.choice('batch_normalization', [False]),\n",
    "                'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 1),\n",
    "                'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),\n",
    "                'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.1)),\n",
    "                'lr_decay': hp.uniform('lr_decay', 0.3, 1.0),\n",
    "                'n_lr_decay_steps': hp.choice('n_lr_decay_steps', [3]),\n",
    "                'weight_decay': hp.loguniform('weight_decay', np.log(5e-5), np.log(5e-3)),\n",
    "                'n_iterations': hp.choice('n_iterations', [args.max_epochs]), #<------- TODO: Change for max_epochs\n",
    "                'early_stopping': hp.choice('early_stopping', [40]),\n",
    "                'loss': hp.choice('loss', ['PINBALL']),\n",
    "                'loss_hypar': hp.uniform('loss_hypar', 0.48, 0.51),\n",
    "                'val_loss': hp.choice('val_loss', [args.val_loss]),\n",
    "                'l1_theta': hp.choice('l1_theta', [0, hp.loguniform('lambdal1', np.log(1e-5), np.log(1))]),\n",
    "                # Data parameters\n",
    "                'frequency': hp.choice('frequency', ['H']),\n",
    "                'seasonality': hp.choice('seasonality', [24]),             \n",
    "                'include_var_dict': hp.choice('include_var_dict', [{'y': [-2, -3, -8],\n",
    "                                                                    'Exogenous1': [-1, -2, -8],\n",
    "                                                                    'Exogenous2': [-1, -2, -8],\n",
    "                                                                    'week_day': [-1]}]),\n",
    "                'random_seed': hp.quniform('random_seed', 10, 20, 1)}\n",
    "    \n",
    "    else:\n",
    "        print(f'Experiment space {args.space} not available')\n",
    "\n",
    "    return space\n",
    "\n",
    "def parse_trials(trials):\n",
    "    # Initialize\n",
    "    trials_dict = {'tid': [], 'loss': [], 'trajectories': [], 'mc': []}\n",
    "    for tidx in range(len(trials)):\n",
    "        # Main\n",
    "        trials_dict['tid']  += [trials.trials[tidx]['tid']]\n",
    "        trials_dict['loss'] += [trials.trials[tidx]['result']['loss']]\n",
    "        trials_dict['trajectories'] += [trials.trials[tidx]['result']['trajectories']]\n",
    "\n",
    "        # Model Configs\n",
    "        mc = trials.trials[tidx]['result']['mc']\n",
    "        trials_dict['mc'] += [mc]\n",
    "    \n",
    "    trials_df = pd.DataFrame(trials_dict)\n",
    "    return trials_df\n",
    "\n",
    "def main(args):\n",
    "    #---------------------------------------------- Directories ----------------------------------------------#\n",
    "    output_dir = f'./results/{args.dataset}/{args.space}/'\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    assert os.path.exists(output_dir), f'Output dir {output_dir} does not exist'\n",
    "\n",
    "    hyperopt_file = output_dir + f'hyperopt_{args.experiment_id}.p'\n",
    "    result_test_file = output_dir + f'result_test_{args.experiment_id}.p'\n",
    "\n",
    "    #---------------------------------------------- Read  Data ----------------------------------------------#\n",
    "    print('\\n'+75*'-')\n",
    "    print(28*'-', 'Preparing Dataset', 28*'-')\n",
    "    print(75*'-'+'\\n')\n",
    "\n",
    "    #TEST_DATE = {'NP': '2016-12-27',\n",
    "    #             'PJM':'2016-12-27',\n",
    "    #             'BE':'2015-01-04',\n",
    "    #             'FR': '2015-01-04',\n",
    "    #             'DE':'2016-01-04'}\n",
    "    #test_date = TEST_DATE[args.dataset]\n",
    "    #Y_insample_df, Xt_insample_df, Y_outsample_df, Xt_outsample_df, _ = load_epf(directory='../data/',\n",
    "    #                                                                             market=args.dataset,\n",
    "    #                                                                             first_date_test=test_date,\n",
    "    #                                                                             days_in_test=728)\n",
    "    Y_df, Xt_df = EPF.load(directory='../data/', group=args.dataset)\n",
    "\n",
    "    # To not modify original data\n",
    "    Xt_scaled_df = Xt_df.copy()\n",
    "\n",
    "    # Transform data with scale transformation\n",
    "    offset = 365 * 24 * 2\n",
    "    scaler = Scaler(normalizer='norm')\n",
    "    Xt_scaled_df['Exogenous1'] = scaler.scale(x=Xt_scaled_df['Exogenous1'].values, offset=offset)\n",
    "\n",
    "    scaler = Scaler(normalizer='norm')\n",
    "    Xt_scaled_df['Exogenous2'] = scaler.scale(x=Xt_scaled_df['Exogenous2'].values, offset=offset)\n",
    "\n",
    "    # train_mask: 1 to keep, 0 to mask\n",
    "    train_outsample_mask = np.ones(len(Y_df))\n",
    "    train_outsample_mask[-offset:] = 0\n",
    "\n",
    "    ts_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=None, X_df=Xt_scaled_df, \n",
    "                                   ts_train_mask=train_outsample_mask)\n",
    "\n",
    "    train_loader = TimeSeriesLoader(ts_dataset=ts_dataset,\n",
    "                                    model='nbeats',\n",
    "                                    offset=0, #offset,\n",
    "                                    window_sampling_limit=365*4*24, \n",
    "                                    input_size=7*24,\n",
    "                                    output_size=24,\n",
    "                                    idx_to_sample_freq=24,\n",
    "                                    batch_size=256,\n",
    "                                    is_train_loader=True,\n",
    "                                    shuffle=True)\n",
    "\n",
    "    val_loader = TimeSeriesLoader(ts_dataset=ts_dataset,\n",
    "                                  model='nbeats',\n",
    "                                  offset=0, #offset,\n",
    "                                  window_sampling_limit=365*4*24, \n",
    "                                  input_size=7*24,\n",
    "                                  output_size=24,\n",
    "                                  idx_to_sample_freq=24,\n",
    "                                  batch_size=1024,\n",
    "                                  is_train_loader=False, # Samples the opposite of train_outsample_mask\n",
    "                                  shuffle=False)                                       \n",
    "\n",
    "    print(f'Dataset: {args.dataset}')\n",
    "    #print(\"Xt_df.columns\", Xt_df.columns)\n",
    "    print(f'Train mask percentage: {np.round(np.sum(train_outsample_mask)/len(train_outsample_mask),2)}')\n",
    "    print('X: time series features, of shape (#hours, #times,#features): \\t' + str(Xt_df.shape))\n",
    "    print('Y: target series (in X), of shape (#hours, #times): \\t \\t' + str(Y_df.shape))\n",
    "    print(f'Train {sum(1-train_outsample_mask)} hours = {np.round(sum(1-train_outsample_mask)/(24*365),2)} years')\n",
    "    print(f'Validation {sum(train_outsample_mask)} hours = {np.round(sum(train_outsample_mask)/(24*365),2)} years')\n",
    "    # print('S: static features, of shape (#series,#features): \\t \\t' + str(S.shape))\n",
    "    #Y_df.head()\n",
    "    print('\\n')\n",
    "\n",
    "    #-------------------------------------- Hyperparameter Optimization --------------------------------------#\n",
    "\n",
    "    if not os.path.isfile(hyperopt_file):\n",
    "        print('\\n'+75*'-')\n",
    "        print(22*'-', 'Start Hyperparameter  tunning', 22*'-')\n",
    "        print(75*'-'+'\\n')\n",
    "\n",
    "        space = get_experiment_space(args)\n",
    "\n",
    "        trials = Trials()\n",
    "        #fmin_objective = partial(run_val_nbeatsx, y_df=y_insample_df, X_t_df=X_t_insample_df, val_ds=365,\n",
    "        #                         trials=trials, trials_file_name=hyperopt_file)\n",
    "        fmin_objective = partial(run_val_nbeatsx, train_loader=train_loader, val_loader=val_loader, \n",
    "                                 trials=trials, trials_file_name=hyperopt_file)\n",
    "        fmin(fmin_objective, space=space, algo=tpe.suggest, max_evals=args.hyperopt_iters, trials=trials, verbose=True)\n",
    "\n",
    "        # Save output\n",
    "        with open(hyperopt_file, \"wb\") as f:\n",
    "            pickle.dump(trials, f)\n",
    "\n",
    "    print('\\n'+75*'-')\n",
    "    print(20*'-', 'Hyperparameter  tunning  finished', 20*'-')\n",
    "    print(75*'-'+'\\n')\n",
    "\n",
    "    # Read and parse trials pickle\n",
    "    trials = pickle.load(open(hyperopt_file, 'rb'))\n",
    "    trials_df = parse_trials(trials)\n",
    "\n",
    "    # Get best mc\n",
    "    idx = trials_df.loss.idxmin()\n",
    "    best_mc = trials_df.loc[idx]['mc']\n",
    "    \n",
    "    run_val_nbeatsx(best_mc, train_loader=train_loader, val_loader=val_loader, \n",
    "                    trials=trials, trials_file_name=hyperopt_file, final_evaluation=True)\n",
    "\n",
    "def parse_args():\n",
    "    desc = \"NBEATSx overfit\"\n",
    "    parser = argparse.ArgumentParser(description=desc)\n",
    "\n",
    "    parser.add_argument('--dataset', type=str, required=True, help='NP')\n",
    "    parser.add_argument('--space', type=str, required=True, help='Experiment hyperparameter space')\n",
    "    parser.add_argument('--hyperopt_iters', type=int, help='hyperopt_iters')\n",
    "    parser.add_argument('--max_epochs', type=int, required=True, default=2000, help='max train epochs')\n",
    "    parser.add_argument('--val_loss', type=str, required=False, default=None, help='validation loss')\n",
    "    parser.add_argument('--experiment_id', default=None, required=False, type=str, help='string to identify experiment')\n",
    "    return parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # parse arguments\n",
    "    args = parse_args()\n",
    "    if args is None:\n",
    "        exit()\n",
    "    \n",
    "    main(args)\n",
    "\n",
    "# CUDA_VISIBLE_DEVICES=2 PYTHONPATH=. python nixtla/experiments/nbeats/hyperopt_epf.py --dataset 'NP' --space \"nbeats_collapsed\" --hyperopt_iters 200 --val_loss \"SMAPE\" --experiment_id \"SMAPEval_20210110\"\n",
    "# CUDA_VISIBLE_DEVICES=2 PYTHONPATH=. python src/overfit_nbeatsx.py --dataset 'NP' --space \"nbeats_collapsed\" --hyperopt_iters 200 --val_loss \"SMAPE\" --experiment_id \"SMAPEval_20210110\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda devices, 1\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------- Preparing Dataset ----------------------------\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Processing dataframes ...\n",
      "Creating ts tensor ...\n",
      "Dataset: NP\n",
      "Train mask percentage: 0.5\n",
      "X: time series features, of shape (#hours, #times,#features): \t(34944, 12)\n",
      "Y: target series (in X), of shape (#hours, #times): \t \t(34944, 3)\n",
      "Train 17520.0 hours = 2.0 years\n",
      "Validation 17424.0 hours = 1.99 years\n",
      "\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "---------------------- Start Hyperparameter  tunning ----------------------\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]INFO:hyperopt.tpe:build_posterior_wrapper took 0.010191 seconds\n",
      "INFO:hyperopt.tpe:TPE using 0 trials\n",
      "============================== Start fitting ==============================\n",
      "Step: 10, Time: 1.382, Insample PINBALL: 242.43962, Outsample MAE: 2.51282\n",
      "Step: 20, Time: 2.948, Insample PINBALL: 145.15848, Outsample MAE: 2.42260\n",
      "Step: 30, Time: 4.453, Insample PINBALL: 99.49187, Outsample MAE: 2.42638\n",
      "Step: 40, Time: 5.884, Insample PINBALL: 77.11043, Outsample MAE: 2.42465\n",
      "Step: 50, Time: 7.333, Insample PINBALL: 62.85504, Outsample MAE: 2.43786\n",
      "Step: 51, Time: 7.500, Insample PINBALL: 62.85504, Outsample MAE: 2.43786\n",
      "==============================  End fitting  ==============================\n",
      " 50%|█████     | 1/2 [00:07<00:07,  7.79s/trial, best loss: 2.437859296798706]INFO:hyperopt.tpe:build_posterior_wrapper took 0.094330 seconds\n",
      "INFO:hyperopt.tpe:TPE using 1/1 trials with best loss 2.437859\n",
      "============================== Start fitting ==============================\n",
      "Step: 10, Time: 1.339, Insample PINBALL: 55.63596, Outsample MAE: 43.25292\n",
      "Step: 20, Time: 2.734, Insample PINBALL: 27.05824, Outsample MAE: 26.67350\n",
      "Step: 30, Time: 3.944, Insample PINBALL: 22.77096, Outsample MAE: 5.16837\n",
      "Step: 40, Time: 5.158, Insample PINBALL: 1.38287, Outsample MAE: 2.38197\n",
      "Step: 50, Time: 6.370, Insample PINBALL: 1.11962, Outsample MAE: 2.41322\n",
      "Step: 51, Time: 6.506, Insample PINBALL: 1.11962, Outsample MAE: 2.41322\n",
      "==============================  End fitting  ==============================\n",
      "100%|██████████| 2/2 [00:14<00:00,  7.31s/trial, best loss: 2.4132168292999268]\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "-------------------- Hyperparameter  tunning  finished --------------------\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "============================== Start fitting ==============================\n",
      "Step: 10, Time: 1.454, Insample PINBALL: 55.63596, Outsample MAE: 43.25292\n",
      "Step: 20, Time: 2.761, Insample PINBALL: 27.05824, Outsample MAE: 26.67350\n",
      "Step: 30, Time: 4.019, Insample PINBALL: 22.77096, Outsample MAE: 5.16837\n",
      "Step: 40, Time: 5.219, Insample PINBALL: 1.38287, Outsample MAE: 2.38197\n",
      "Step: 50, Time: 6.427, Insample PINBALL: 1.11962, Outsample MAE: 2.41322\n",
      "Step: 51, Time: 6.547, Insample PINBALL: 1.11962, Outsample MAE: 2.41322\n",
      "==============================  End fitting  ==============================\n",
      "\n",
      "\n",
      "Best Model Hyperpars\n",
      "===========================================================================\n",
      "activation                                                            prelu\n",
      "batch_normalization                                                    True\n",
      "dropout_prob_exogenous                                              0.41477\n",
      "dropout_prob_theta                                                 0.730104\n",
      "early_stopping                                                           40\n",
      "exogenous_n_channels                                                      2\n",
      "frequency                                                                 H\n",
      "include_var_dict          {'Exogenous1': (-1, -2, -8), 'Exogenous2': (-1...\n",
      "initialization                                                   he_uniform\n",
      "input_size_multiplier                                                     7\n",
      "l1_theta                                                                  0\n",
      "learning_rate                                                     0.0968392\n",
      "loss                                                                PINBALL\n",
      "loss_hypar                                                         0.485222\n",
      "lr_decay                                                                0.5\n",
      "n_blocks                                                             (1, 1)\n",
      "n_harmonics                                                               1\n",
      "n_hidden                                                                419\n",
      "n_iterations                                                             50\n",
      "n_layers                                                             (2, 2)\n",
      "n_lr_decay_steps                                                          3\n",
      "n_polynomials                                                             2\n",
      "output_size                                                              24\n",
      "random_seed                                                               8\n",
      "seasonality                                                              24\n",
      "shared_weights                                                        False\n",
      "stack_types                                   (exogenous_wavenet, identity)\n",
      "val_loss                                                                MAE\n",
      "weight_decay                                                     0.00714941\n",
      "x_s_n_hidden                                                              0\n",
      "dtype: object\n",
      "===========================================================================\n",
      "\n",
      "Best Model Evaluation\n",
      "      metric   nbeatsx\n",
      "0  pinball50   1.20661\n",
      "1        mae   2.41322\n",
      "2       mape  10.54047\n",
      "3      smape  10.70859\n",
      "4       rmse   5.41072\n"
     ]
    }
   ],
   "source": [
    "args = pd.Series({'dataset': 'NP', \n",
    "                  'val_loss': 'MAE',\n",
    "                  'space': 'nbeats_extended1',\n",
    "                  'hyperopt_iters': 2, 'max_epochs': 50,\n",
    "                  'experiment_id': 'debug3', 'gpu_id': 1})\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu_id)\n",
    "print('cuda devices,', os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluations = pd.DataFrame({'metric': ['mae', 'mae', 'mae', 'mae'],\n",
    "#                             'data': ['NP', 'PJM', 'BE', 'FR'],\n",
    "#                             'nbeatsx': [2*0.805, 2*1.42, 2*3.15, 2*1.9],\n",
    "#                             'nbeatsx_overfit': [2*0.71, 2*1.19, 2*2.53, 2*2.06],})\n",
    "# evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}