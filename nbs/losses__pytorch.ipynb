{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp losses.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch as t\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def divide_no_nan(a, b):\n",
    "    \"\"\"\n",
    "    Auxiliary funtion to handle divide by 0\n",
    "    \"\"\"\n",
    "    div = a / b\n",
    "    div[div != div] = 0.0\n",
    "    div[div == float('inf')] = 0.0\n",
    "    return div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERAL TRAIN LOSSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MAPELoss(y, y_hat, mask=None):\n",
    "    \"\"\"MAPE Loss\n",
    "\n",
    "    Calculates Mean Absolute Percentage Error between\n",
    "    y and y_hat. MAPE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the\n",
    "    percentual deviation of the prediction and the true\n",
    "    value at a given time and averages these devations\n",
    "    over the length of the series.\n",
    "    As defined in: https://en.wikipedia.org/wiki/Mean_absolute_percentage_error\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values in torch tensor.\n",
    "    mask: tensor (batch_size, output_size)\n",
    "        specifies date stamps per serie\n",
    "        to consider in loss\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mape:\n",
    "    Mean absolute percentage error.\n",
    "    \"\"\"\n",
    "    mask = divide_no_nan(mask, t.abs(y))\n",
    "    mape = t.abs(y - y_hat) * mask\n",
    "    mape = t.mean(mape)\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def MSELoss(y, y_hat, mask=None):\n",
    "    \"\"\"MSE Loss\n",
    "\n",
    "    Calculates Mean Squared Error between\n",
    "    y and y_hat. MAPE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the\n",
    "    percentual deviation of the prediction and the true\n",
    "    value at a given time and averages these devations\n",
    "    over the length of the series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values in torch tensor.\n",
    "    mask: tensor (batch_size, output_size)\n",
    "        specifies date stamps per serie\n",
    "        to consider in loss\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mse:\n",
    "    Mean Squared Error.\n",
    "    \"\"\"\n",
    "    mse = (y - y_hat)**2\n",
    "    mse = mask * mse\n",
    "    mse = t.mean(mse)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def RMSELoss(y, y_hat, mask=None):\n",
    "    \"\"\"RMSE Loss\n",
    "\n",
    "    Calculates Mean Squared Error between\n",
    "    y and y_hat. MAPE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the\n",
    "    percentual deviation of the prediction and the true\n",
    "    value at a given time and averages these devations\n",
    "    over the length of the series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values in torch tensor.\n",
    "    mask: tensor (batch_size, output_size)\n",
    "        specifies date stamps per serie\n",
    "        to consider in loss\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rmse:\n",
    "    Root Mean Squared Error.\n",
    "    \"\"\"\n",
    "    rmse = (y - y_hat)**2\n",
    "    rmse = mask * rmse\n",
    "    rmse = t.sqrt(t.mean(rmse))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def SMAPELoss(y, y_hat, mask=None):\n",
    "    \"\"\"SMAPE2 Loss\n",
    "\n",
    "    Calculates Symmetric Mean Absolute Percentage Error.\n",
    "    SMAPE measures the relative prediction accuracy of a\n",
    "    forecasting method by calculating the relative deviation\n",
    "    of the prediction and the true value scaled by the sum of the\n",
    "    absolute values for the prediction and true value at a\n",
    "    given time, then averages these devations over the length\n",
    "    of the series. This allows the SMAPE to have bounds between\n",
    "    0% and 200% which is desireble compared to normal MAPE that\n",
    "    may be undetermined.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values in torch tensor.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    smape:\n",
    "        symmetric mean absolute percentage error\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [1] https://robjhyndman.com/hyndsight/smape/ (Makridakis 1993)\n",
    "    \"\"\"\n",
    "    if mask is None:\n",
    "        mask = t.ones(y_hat.size())\n",
    "    delta_y = t.abs((y - y_hat))\n",
    "    scale = t.abs(y) + t.abs(y_hat)\n",
    "    smape = divide_no_nan(delta_y, scale)\n",
    "    smape = smape * mask\n",
    "    smape = 2 * t.mean(smape)\n",
    "    return smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def MASELoss(y, y_hat, y_insample, seasonality, mask=None) :\n",
    "    \"\"\" Calculates the M4 Mean Absolute Scaled Error.\n",
    "\n",
    "    MASE measures the relative prediction accuracy of a\n",
    "    forecasting method by comparinng the mean absolute errors\n",
    "    of the prediction and the true value against the mean\n",
    "    absolute errors of the seasonal naive model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seasonality: int\n",
    "        main frequency of the time series\n",
    "        Hourly 24,  Daily 7, Weekly 52,\n",
    "        Monthly 12, Quarterly 4, Yearly 1\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual test values\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values\n",
    "    y_train: tensor (batch_size, input_size)\n",
    "        actual insample values for Seasonal Naive predictions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mase:\n",
    "        mean absolute scaled error\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [1] https://robjhyndman.com/papers/mase.pdf\n",
    "    \"\"\"\n",
    "    if mask is None:\n",
    "        mask = t.ones(y_hat.size())\n",
    "    delta_y = t.abs(y - y_hat)\n",
    "    scale = t.mean(t.abs(y_insample[:, seasonality:] - \\\n",
    "                            y_insample[:, :-seasonality]), axis=1)\n",
    "    mase = divide_no_nan(delta_y, scale[:, None])\n",
    "    mase = mase * mask\n",
    "    mase = t.mean(mase)\n",
    "    return mase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def MAELoss(y, y_hat, mask=None):\n",
    "    \"\"\"MAE Loss\n",
    "\n",
    "    Calculates Mean Absolute Error between\n",
    "    y and y_hat. MAE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the\n",
    "    deviation of the prediction and the true\n",
    "    value at a given time and averages these devations\n",
    "    over the length of the series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values in torch tensor.\n",
    "    mask: tensor (batch_size, output_size)\n",
    "        specifies date stamps per serie\n",
    "        to consider in loss\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mae:\n",
    "    Mean absolute error.\n",
    "    \"\"\"\n",
    "    mae = t.abs(y - y_hat) * mask\n",
    "    mae = t.mean(mae)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def PinballLoss(y, y_hat, mask=None, tau=0.5):\n",
    "    \"\"\"Pinball Loss\n",
    "    Computes the pinball loss between y and y_hat.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values in torch tensor.\n",
    "    tau: float, between 0 and 1\n",
    "        the slope of the pinball loss, in the context of\n",
    "        quantile regression, the value of tau determines the\n",
    "        conditional quantile level.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pinball:\n",
    "        average accuracy for the predicted quantile\n",
    "    \"\"\"\n",
    "    if mask is None:\n",
    "        mask = t.ones(y_hat.size())\n",
    "    delta_y = t.sub(y, y_hat)\n",
    "    pinball = t.max(t.mul(tau, delta_y), t.mul((tau - 1), delta_y))\n",
    "    pinball = pinball * mask\n",
    "    pinball = t.mean(pinball)\n",
    "    return pinball"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESRNN TRAIN LOSSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def LevelVariabilityLoss(levels, level_variability_penalty):\n",
    "    \"\"\" Level Variability Loss\n",
    "    Computes the variability penalty for the level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    levels: tensor with shape (batch, n_time)\n",
    "        levels obtained from exponential smoothing component of ESRNN\n",
    "    level_variability_penalty: float\n",
    "        this parameter controls the strength of the penalization \n",
    "        to the wigglines of the level vector, induces smoothness\n",
    "        in the output\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    level_var_loss:\n",
    "        wiggliness loss for the level vector\n",
    "    \"\"\"\n",
    "    assert levels.shape[1] > 2\n",
    "    level_prev = t.log(levels[:, :-1])\n",
    "    level_next = t.log(levels[:, 1:])\n",
    "    log_diff_of_levels = t.sub(level_prev, level_next)\n",
    "\n",
    "    log_diff_prev = log_diff_of_levels[:, :-1]\n",
    "    log_diff_next = log_diff_of_levels[:, 1:]\n",
    "    diff = t.sub(log_diff_prev, log_diff_next)\n",
    "    level_var_loss = diff**2\n",
    "    level_var_loss = level_var_loss.mean() * level_variability_penalty\n",
    "    \n",
    "    return level_var_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SmylLoss(y, y_hat, levels, mask, tau, level_variability_penalty=0.0):\n",
    "    \"\"\"Computes the Smyl Loss that combines level variability with\n",
    "    with Pinball loss.\n",
    "    windows_y: tensor of actual values,\n",
    "                            shape (n_windows, batch_size, window_size).\n",
    "    windows_y_hat: tensor of predicted values,\n",
    "                                    shape (n_windows, batch_size, window_size).\n",
    "    levels: levels obtained from exponential smoothing component of ESRNN.\n",
    "                    tensor with shape (batch, n_time).\n",
    "    return: smyl_loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    if mask is None: mask = t.ones(y_hat.size())\n",
    "    smyl_loss = PinballLoss(y, y_hat, mask, tau)\n",
    "    \n",
    "    if level_variability_penalty > 0:\n",
    "        log_diff_of_levels = LevelVariabilityLoss(levels, level_variability_penalty) \n",
    "        smyl_loss += log_diff_of_levels\n",
    "    \n",
    "    return smyl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI-QUANTILE LOSS\n",
    "\n",
    "MQLoss definition and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MQLoss(y, y_hat, quantiles, mask=None): \n",
    "    \"\"\"MQLoss\n",
    "\n",
    "    Calculates Average Multi-quantile Loss function, for\n",
    "    a given set of quantiles, based on the absolute \n",
    "    difference between predicted and true values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size) actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size, n_quantiles) predicted values in torch tensor.\n",
    "    mask: tensor (batch_size, output_size, n_quantiles) specifies date stamps per serie\n",
    "          to consider in loss\n",
    "    quantiles: tensor(n_quantiles) quantiles to estimate from the distribution of y.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lq: tensor(n_quantiles) average multi-quantile loss.\n",
    "    \"\"\"    \n",
    "    assert len(quantiles) > 1, f'your quantiles are of len: {len(quantiles)}'\n",
    "    \n",
    "    if mask is None: mask = t.ones_like(y_hat)\n",
    "        \n",
    "    n_q = len(quantiles)\n",
    "    \n",
    "    y_rep = t.stack([y.T for _ in range(n_q)]).T\n",
    "    error = y_hat - y_rep\n",
    "    sq = t.maximum(-error, t.zeros_like(error))\n",
    "    s1_q = t.maximum(error, t.zeros_like(error))\n",
    "    loss = (quantiles * sq + (1 - quantiles) * s1_q)\n",
    "   \n",
    "    return t.mean(t.mean(loss, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def wMQLoss(y, y_hat, quantiles, mask=None): \n",
    "    \"\"\"wMQLoss\n",
    "\n",
    "    Calculates Average Multi-quantile Loss function, for\n",
    "    a given set of quantiles, based on the absolute \n",
    "    difference between predicted and true values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size) actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size, n_quantiles) predicted values in torch tensor.\n",
    "    mask: tensor (batch_size, output_size, n_quantiles) specifies date stamps per serie\n",
    "          to consider in loss\n",
    "    quantiles: tensor(n_quantiles) quantiles to estimate from the distribution of y.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lq: tensor(n_quantiles) average multi-quantile loss.\n",
    "    \"\"\"    \n",
    "    assert len(quantiles) > 1, f'your quantiles are of len: {len(quantiles)}'\n",
    "    \n",
    "    if mask is None: mask = t.ones_like(y_hat)\n",
    "        \n",
    "    n_q = len(quantiles)\n",
    "    \n",
    "    y_rep = t.stack([y.T for _ in range(n_q)]).T\n",
    "    error = y_hat - y_rep\n",
    "    sq = t.maximum(-error, t.zeros_like(error))\n",
    "    s1_q = t.maximum(error, t.zeros_like(error))\n",
    "    loss = (quantiles * sq + (1 - quantiles) * s1_q)\n",
    "    \n",
    "    loss = divide_no_nan(t.sum(loss * mask, axis=-2), \n",
    "                         t.sum(t.abs(y_rep) * mask, axis=-2))\n",
    "    \n",
    "    return t.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST/DEBUG PYTORCH TRAIN LOSSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats import hmean\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):  \n",
    "\n",
    "    def __init__(self, horizon, n_quantiles):\n",
    "        super(Model, self).__init__()\n",
    "        self.horizon = horizon\n",
    "        self.n_quantiles = n_quantiles\n",
    "        self.linear_layer = nn.Linear(in_features=n_obs, \n",
    "                                      out_features=horizon * n_quantiles, \n",
    "                                      bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_hat = self.linear_layer(x)\n",
    "        y_hat = y_hat.view(-1, self.horizon, self.n_quantiles)\n",
    "        return y_hat\n",
    "    \n",
    "class Data(Dataset):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, Y, X):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.len = Y.shape[0]\n",
    "\n",
    "    # Getter\n",
    "    def __getitem__(self, index):          \n",
    "        return self.X[index], self.Y[index]\n",
    "    \n",
    "    # Get Length\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantiles:\n",
      "tensor([0.0500, 0.3500, 0.6500, 0.9500])\n",
      "Y.shape: torch.Size([1000, 10]), X.shape: torch.Size([1000, 10])\n",
      "Y_test.shape: torch.Size([1000, 10]), X_test.shape: torch.Size([1000, 10])\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters and sample data parameters\n",
    "t.cuda.manual_seed(7)\n",
    "\n",
    "# Sample data\n",
    "n_ts = 1000\n",
    "n_obs = horizon = 10\n",
    "mean = 0.0 # to generate random numbers from N(mean, std)\n",
    "std = 7.0 # to generate random numbers from N(mean, std)\n",
    "start = 0.05 # First quantile\n",
    "end = 0.95 # Last quantiles\n",
    "steps = 4 # Number of quantiles\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 500\n",
    "lr = 0.08\n",
    "epochs = 100\n",
    "\n",
    "# Sample data\n",
    "quantiles = t.Tensor([0.0500, 0.3500, 0.6500, 0.9500])\n",
    "print(f'quantiles:\\n{quantiles}')\n",
    "Y = t.normal(mean=mean, std=std, size=(n_ts, n_obs))\n",
    "X = t.ones(size=(n_ts, n_obs))\n",
    "\n",
    "Y_test = t.normal(mean=mean, std=std, size=(n_ts, horizon))\n",
    "X_test = t.ones(size=(n_ts, horizon))\n",
    "print(f'Y.shape: {Y.shape}, X.shape: {X.shape}')\n",
    "print(f'Y_test.shape: {Y_test.shape}, X_test.shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training \n",
    "model = Model(horizon=horizon, n_quantiles=len(quantiles))\n",
    "dataset = Data(X=X, Y=Y)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def train_model(model, epochs, print_progress=False):\n",
    "\n",
    "    start = time.time()\n",
    "    i = 0 \n",
    "    training_trajectory = {'epoch': [],\n",
    "                           'train_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for x, y in dataloader:\n",
    "            \n",
    "            i += 1\n",
    "            y_hat = model(x)\n",
    "            training_loss = wMQLoss(y=y, y_hat=y_hat, quantiles=quantiles)\n",
    "            if i % (epoch + 1) == 0: \n",
    "                training_trajectory['epoch'].append(i)\n",
    "                training_trajectory['train_loss'].append(training_loss.detach().numpy())\n",
    "            optimizer.zero_grad()\n",
    "            training_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            display_string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(i, \n",
    "                                                                                    time.time()-start, \n",
    "                                                                                    \"MQLoss\", \n",
    "                                                                                    training_loss.cpu().data.numpy())\n",
    "            if print_progress: print(display_string)\n",
    "\n",
    "    return model, training_trajectory\n",
    "\n",
    "model, training_trajectory = train_model(model=model, epochs=epochs)\n",
    "Y_hat = model(X_test).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_trajectory['epoch'], training_trajectory['train_loss'])\n",
    "plt.xlabel('Gradient Steps')\n",
    "plt.ylabel('MQLoss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_pytorch: 1.6508954763412476\n",
      "loss_numpy: 1.650895595550537\n",
      "Difference: 1.1920928955078125e-07\n"
     ]
    }
   ],
   "source": [
    "from nixtla.losses.numpy import mqloss\n",
    "\n",
    "loss_pytorch = MQLoss(Y_test, Y_hat, quantiles)\n",
    "loss_numpy = mqloss(Y_test.numpy(), Y_hat.numpy(), quantiles.numpy())\n",
    "\n",
    "print(f'loss_pytorch: {loss_pytorch}')\n",
    "print(f'loss_numpy: {loss_numpy}')\n",
    "print(f'Difference: {abs(loss_pytorch - loss_numpy)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nixtla",
   "language": "python",
   "name": "nixtla"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
