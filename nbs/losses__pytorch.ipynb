{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp losses.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch as t\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def divide_no_nan(a, b):\n",
    "    \"\"\"\n",
    "    Auxiliary funtion to handle divide by 0\n",
    "    \"\"\"\n",
    "    div = a / b\n",
    "    div[div != div] = 0.0\n",
    "    div[div == float('inf')] = 0.0\n",
    "    return div\n",
    "\n",
    "#############################################################################\n",
    "# FORECASTING LOSSES\n",
    "#############################################################################\n",
    "\n",
    "def MAPELoss(y, y_hat, mask=None):\n",
    "    \"\"\"MAPE Loss\n",
    "\n",
    "    Calculates Mean Absolute Percentage Error between\n",
    "    y and y_hat. MAPE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the\n",
    "    percentual deviation of the prediction and the true\n",
    "    value at a given time and averages these devations\n",
    "    over the length of the series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values in torch tensor.\n",
    "    mask: tensor (batch_size, output_size)\n",
    "        specifies date stamps per serie\n",
    "        to consider in loss\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mape:\n",
    "    Mean absolute percentage error.\n",
    "    \"\"\"\n",
    "    mask = divide_no_nan(mask, t.abs(y))\n",
    "    mape = t.abs(y - y_hat) * mask\n",
    "    mape = t.mean(mape)\n",
    "    return mape\n",
    "\n",
    "# def MAPELoss(forecast: t.Tensor, target: t.Tensor, mask: t.Tensor):\n",
    "#     \"\"\"\n",
    "#     MAPE loss as defined in: https://en.wikipedia.org/wiki/Mean_absolute_percentage_error\n",
    "\n",
    "#     :param forecast: Forecast values. Shape: batch, time\n",
    "#     :param target: Target values. Shape: batch, time\n",
    "#     :param mask: 0/1 mask. Shape: batch, time\n",
    "#     :return: Loss value\n",
    "#     \"\"\"\n",
    "#     weights = divide_no_nan(mask, target)\n",
    "#     return t.mean(t.abs((forecast - target) * weights))\n",
    "\n",
    "def MSELoss(y, y_hat, mask=None):\n",
    "    \"\"\"MSE Loss\n",
    "\n",
    "    Calculates Mean Squared Error between\n",
    "    y and y_hat. MAPE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the\n",
    "    percentual deviation of the prediction and the true\n",
    "    value at a given time and averages these devations\n",
    "    over the length of the series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values in torch tensor.\n",
    "    mask: tensor (batch_size, output_size)\n",
    "        specifies date stamps per serie\n",
    "        to consider in loss\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mse:\n",
    "    Mean Squared Error.\n",
    "    \"\"\"\n",
    "    mse = (y - y_hat)**2\n",
    "    mse = mask * mse\n",
    "    mse = t.mean(mse)\n",
    "    return mse\n",
    "\n",
    "def RMSELoss(y, y_hat, mask=None):\n",
    "    \"\"\"RMSE Loss\n",
    "\n",
    "    Calculates Mean Squared Error between\n",
    "    y and y_hat. MAPE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the\n",
    "    percentual deviation of the prediction and the true\n",
    "    value at a given time and averages these devations\n",
    "    over the length of the series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values in torch tensor.\n",
    "    mask: tensor (batch_size, output_size)\n",
    "        specifies date stamps per serie\n",
    "        to consider in loss\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rmse:\n",
    "    Root Mean Squared Error.\n",
    "    \"\"\"\n",
    "    rmse = (y - y_hat)**2\n",
    "    rmse = mask * rmse\n",
    "    rmse = t.sqrt(t.mean(rmse))\n",
    "    return rmse\n",
    "\n",
    "def SMAPELoss(y, y_hat, mask=None):\n",
    "    \"\"\"SMAPE2 Loss\n",
    "\n",
    "    Calculates Symmetric Mean Absolute Percentage Error.\n",
    "    SMAPE measures the relative prediction accuracy of a\n",
    "    forecasting method by calculating the relative deviation\n",
    "    of the prediction and the true value scaled by the sum of the\n",
    "    absolute values for the prediction and true value at a\n",
    "    given time, then averages these devations over the length\n",
    "    of the series. This allows the SMAPE to have bounds between\n",
    "    0% and 200% which is desireble compared to normal MAPE that\n",
    "    may be undetermined.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values in torch tensor.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    smape:\n",
    "        symmetric mean absolute percentage error\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [1] https://robjhyndman.com/hyndsight/smape/ (Makridakis 1993)\n",
    "    \"\"\"\n",
    "    if mask is None:\n",
    "        mask = t.ones(y_hat.size())\n",
    "    delta_y = t.abs((y - y_hat))\n",
    "    scale = t.abs(y) + t.abs(y_hat)\n",
    "    smape = divide_no_nan(delta_y, scale)\n",
    "    smape = smape * mask\n",
    "    smape = 2 * t.mean(smape)\n",
    "    return smape\n",
    "\n",
    "\n",
    "def MASELoss(y, y_hat, y_insample, seasonality, mask=None) :\n",
    "    \"\"\" Calculates the M4 Mean Absolute Scaled Error.\n",
    "\n",
    "    MASE measures the relative prediction accuracy of a\n",
    "    forecasting method by comparinng the mean absolute errors\n",
    "    of the prediction and the true value against the mean\n",
    "    absolute errors of the seasonal naive model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seasonality: int\n",
    "        main frequency of the time series\n",
    "        Hourly 24,  Daily 7, Weekly 52,\n",
    "        Monthly 12, Quarterly 4, Yearly 1\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual test values\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values\n",
    "    y_train: tensor (batch_size, input_size)\n",
    "        actual insample values for Seasonal Naive predictions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mase:\n",
    "        mean absolute scaled error\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [1] https://robjhyndman.com/papers/mase.pdf\n",
    "    \"\"\"\n",
    "    if mask is None:\n",
    "        mask = t.ones(y_hat.size())\n",
    "    delta_y = t.abs(y - y_hat)\n",
    "    scale = t.mean(t.abs(y_insample[:, seasonality:] - \\\n",
    "                            y_insample[:, :-seasonality]), axis=1)\n",
    "    mase = divide_no_nan(delta_y, scale[:, None])\n",
    "    mase = mase * mask\n",
    "    mase = t.mean(mase)\n",
    "    return mase\n",
    "\n",
    "def MAELoss(y, y_hat, mask=None):\n",
    "    \"\"\"MAE Loss\n",
    "\n",
    "    Calculates Mean Absolute Error between\n",
    "    y and y_hat. MAE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the\n",
    "    deviation of the prediction and the true\n",
    "    value at a given time and averages these devations\n",
    "    over the length of the series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values in torch tensor.\n",
    "    mask: tensor (batch_size, output_size)\n",
    "        specifies date stamps per serie\n",
    "        to consider in loss\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mae:\n",
    "    Mean absolute error.\n",
    "    \"\"\"\n",
    "    mae = t.abs(y - y_hat) * mask\n",
    "    mae = t.mean(mae)\n",
    "    return mae\n",
    "\n",
    "def PinballLoss(y, y_hat, mask=None, tau=0.5):\n",
    "    \"\"\"Pinball Loss\n",
    "    Computes the pinball loss between y and y_hat.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values in torch tensor.\n",
    "    tau: float, between 0 and 1\n",
    "        the slope of the pinball loss, in the context of\n",
    "        quantile regression, the value of tau determines the\n",
    "        conditional quantile level.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pinball:\n",
    "        average accuracy for the predicted quantile\n",
    "    \"\"\"\n",
    "    if mask is None:\n",
    "        mask = t.ones(y_hat.size())\n",
    "    delta_y = t.sub(y, y_hat)\n",
    "    pinball = t.max(t.mul(tau, delta_y), t.mul((tau - 1), delta_y))\n",
    "    pinball = pinball * mask\n",
    "    pinball = t.mean(pinball)\n",
    "    return pinball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def QuadraticBarrierLoss(z, tau):\n",
    "    \"\"\"\n",
    "    Quadratic penalty as substitition to inequality constraints\n",
    "    Learning to play in a day: Faster deep reinforcement learning by optimality tightening.\n",
    "    \"\"\"\n",
    "    barrier = tau * t.max(t.zeros_like(z), z)**2\n",
    "    loss = barrier.mean()\n",
    "    return loss\n",
    "\n",
    "# def LogbarrierLoss(z, t):\n",
    "#     \"\"\"\n",
    "#     https://www.groundai.com/project/log-barrier-constrained-cnns/1\n",
    "#     https://github.com/AnonymousICCVSubmission/extended_log_barrier/blob/master/losses.py\n",
    "#     \"\"\"\n",
    "#     assert z.shape == ()\n",
    "\n",
    "#     if z <= - 1 / t**2:\n",
    "#         return - torch.log(-z) / t\n",
    "#     else:\n",
    "#         return t * z + -np.log(1 / (t**2)) / t + 1 / t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def LevelVariabilityLoss(levels, level_variability_penalty):\n",
    "    \"\"\" Level Variability Loss\n",
    "    Computes the variability penalty for the level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    levels: tensor with shape (batch, n_time)\n",
    "        levels obtained from exponential smoothing component of ESRNN\n",
    "    level_variability_penalty: float\n",
    "        this parameter controls the strength of the penalization \n",
    "        to the wigglines of the level vector, induces smoothness\n",
    "        in the output\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    level_var_loss:\n",
    "        wiggliness loss for the level vector\n",
    "    \"\"\"\n",
    "    assert levels.shape[1] > 2\n",
    "    level_prev = t.log(levels[:, :-1])\n",
    "    level_next = t.log(levels[:, 1:])\n",
    "    log_diff_of_levels = t.sub(level_prev, level_next)\n",
    "\n",
    "    log_diff_prev = log_diff_of_levels[:, :-1]\n",
    "    log_diff_next = log_diff_of_levels[:, 1:]\n",
    "    diff = t.sub(log_diff_prev, log_diff_next)\n",
    "    level_var_loss = diff**2\n",
    "    level_var_loss = level_var_loss.mean() * level_variability_penalty\n",
    "    \n",
    "    return level_var_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SmylLoss(y, y_hat, levels, mask, tau, level_variability_penalty=0.0):\n",
    "    \"\"\"Computes the Smyl Loss that combines level variability with\n",
    "    with Pinball loss.\n",
    "    windows_y: tensor of actual values,\n",
    "                            shape (n_windows, batch_size, window_size).\n",
    "    windows_y_hat: tensor of predicted values,\n",
    "                                    shape (n_windows, batch_size, window_size).\n",
    "    levels: levels obtained from exponential smoothing component of ESRNN.\n",
    "                    tensor with shape (batch, n_time).\n",
    "    return: smyl_loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    if mask is None: mask = t.ones(y_hat.size())\n",
    "    smyl_loss = PinballLoss(y, y_hat, mask, tau)\n",
    "    \n",
    "    if level_variability_penalty > 0:\n",
    "        log_diff_of_levels = LevelVariabilityLoss(levels, level_variability_penalty) \n",
    "        smyl_loss += log_diff_of_levels\n",
    "    \n",
    "    return smyl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-quantile Loss\n",
    "\n",
    "MQLoss definition and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MQLoss(y, y_hat, quantiles, mask=None): \n",
    "    \"\"\"MQLoss\n",
    "\n",
    "    Calculates Average Multi-quantile Loss function, for\n",
    "    a given set of quantiles, based on the absolute \n",
    "    difference between predicted and true values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size) actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size, n_quantiles) predicted values in torch tensor.\n",
    "    mask: tensor (batch_size, output_size, n_quantiles) specifies date stamps per serie\n",
    "          to consider in loss\n",
    "    quantiles: tensor(n_quantiles) quantiles to estimate from the distribution of y.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lq: tensor(n_quantiles) average multi-quantile loss.\n",
    "    \"\"\"    \n",
    "    assert len(quantiles) > 1, f'your quantiles are of len: {len(quantiles)}'\n",
    "    \n",
    "    if mask is None: mask = t.ones_like(y_hat)\n",
    "        \n",
    "    n_q = len(quantiles)\n",
    "    \n",
    "    y_rep = t.stack([y.T for _ in range(n_q)]).T\n",
    "    error = y_hat - y_rep\n",
    "    sq = t.maximum(-error, t.zeros_like(error))\n",
    "    s1_q = t.maximum(error, t.zeros_like(error))\n",
    "    loss = (quantiles * sq + (1 - quantiles) * s1_q)\n",
    "    \n",
    "   \n",
    "    return t.mean(t.mean(loss, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def wMQLoss(y, y_hat, quantiles, mask=None): \n",
    "    \"\"\"wMQLoss\n",
    "\n",
    "    Calculates Average Multi-quantile Loss function, for\n",
    "    a given set of quantiles, based on the absolute \n",
    "    difference between predicted and true values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size) actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size, n_quantiles) predicted values in torch tensor.\n",
    "    mask: tensor (batch_size, output_size, n_quantiles) specifies date stamps per serie\n",
    "          to consider in loss\n",
    "    quantiles: tensor(n_quantiles) quantiles to estimate from the distribution of y.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lq: tensor(n_quantiles) average multi-quantile loss.\n",
    "    \"\"\"    \n",
    "    assert len(quantiles) > 1, f'your quantiles are of len: {len(quantiles)}'\n",
    "    \n",
    "    if mask is None: mask = t.ones_like(y_hat)\n",
    "        \n",
    "    n_q = len(quantiles)\n",
    "    \n",
    "    y_rep = t.stack([y.T for _ in range(n_q)]).T\n",
    "    error = y_hat - y_rep\n",
    "    sq = t.maximum(-error, t.zeros_like(error))\n",
    "    s1_q = t.maximum(error, t.zeros_like(error))\n",
    "    loss = (quantiles * sq + (1 - quantiles) * s1_q)\n",
    "    \n",
    "    loss = divide_no_nan(t.sum(loss * mask, axis=-2), \n",
    "                         t.sum(t.abs(y_rep) * mask, axis=-2))\n",
    "    \n",
    "    return t.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MQTestModel(nn.Module):  \n",
    "\n",
    "    def __init__(self, horizon, n_quantiles):\n",
    "        super(MQTestModel, self).__init__()\n",
    "        self.horizon = horizon\n",
    "        self.n_quantiles = n_quantiles\n",
    "        self.linear_layer = nn.Linear(in_features=n_obs, out_features=horizon * n_quantiles, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_hat = self.linear_layer(x)\n",
    "        y_hat = y_hat.view(-1, self.horizon, self.n_quantiles)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTraining(Dataset):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, y, x):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.len = y.shape[0]\n",
    "\n",
    "    # Getter\n",
    "    def __getitem__(self, index):          \n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    # Get Length\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats import hmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and sample data parameters\n",
    "\n",
    "t.cuda.manual_seed(7)\n",
    "\n",
    "# Sample data\n",
    "n_ts = 1000\n",
    "n_obs = horizon = 10\n",
    "mean = 0.0 # to generate random numbers from N(mean, std)\n",
    "std = 7.0 # to generate random numbers from N(mean, std)\n",
    "start = 0.05 # First quantile\n",
    "end = 0.95 # Last quantiles\n",
    "steps = 4 # Number of quantiles\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 500\n",
    "lr = 0.08\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantiles:\n",
      "tensor([0.0500, 0.3500, 0.6500, 0.9500])\n",
      "\n",
      "y.shape: torch.Size([1000, 10]), x.shape: torch.Size([1000, 10])\n",
      "\n",
      "y_test.shape: torch.Size([1000, 10]), x_test.shape: torch.Size([1000, 10])\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "quantiles = t.linspace(start=start, end=end, steps=steps) \n",
    "print(f'quantiles:\\n{quantiles}')\n",
    "y = t.normal(mean=mean, std=std, size=(n_ts, n_obs))\n",
    "x = t.ones(size=(n_ts, n_obs))\n",
    "\n",
    "y_test = t.normal(mean=mean, std=std, size=(n_ts, horizon))\n",
    "x_test = t.ones(size=(n_ts, horizon))\n",
    "print(f'\\ny.shape: {y.shape}, x.shape: {x.shape}')\n",
    "print(f'\\ny_test.shape: {y_test.shape}, x_test.shape: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training \n",
    "\n",
    "model = MQTestModel(horizon=horizon, n_quantiles=len(quantiles))\n",
    "dataset = DataTraining(x=x, y=y)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def train_model(model, epochs, print_progress=False):\n",
    "\n",
    "    start = time.time()\n",
    "    i = 0 \n",
    "    training_trajectory = [list(), list()]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for x, y in dataloader:\n",
    "            \n",
    "            i += 1\n",
    "            y_hat = model(x)\n",
    "            training_loss = wMQLoss(y=y, y_hat=y_hat, quantiles=quantiles)\n",
    "            if i % (epoch + 1) == 0: \n",
    "                training_trajectory[0].append(i)\n",
    "                training_trajectory[1].append(training_loss.detach().numpy())\n",
    "            optimizer.zero_grad()\n",
    "            training_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            display_string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(i, \n",
    "                                                                                    time.time()-start, \n",
    "                                                                                    \"MQLoss\", \n",
    "                                                                                    training_loss.cpu().data.numpy())\n",
    "            if print_progress: print(display_string)\n",
    "\n",
    "    return model, training_trajectory\n",
    "\n",
    "trained_model = train_model(model=model, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trained_model[1][0], trained_model[1][1])\n",
    "plt.xlabel('# iteration')\n",
    "plt.ylabel('MQLoss')\n",
    "plt.title('MQLoss v. Iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantiles:\n",
      "tensor([0.0500, 0.3500, 0.6500, 0.9500])\n",
      "\n",
      "q_hat:\n",
      "[0.0569 0.365  0.653  0.948 ]\n",
      "\n",
      "absolute errors:\n",
      "tensor([0.0069, 0.0150, 0.0030, 0.0020], dtype=torch.float64)\n",
      "\n",
      "average absolute error: 0.0038280155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "\n",
    "def absolute_error(quantiles, y, y_hat):\n",
    "    \"\"\"accuracy\n",
    "\n",
    "    Calculates the accuracy of the quantiles\n",
    "    estimated by the MQTestModel\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    quantiles: tensor(n_quantiles) quantiles to estimate from the distribution of y.\n",
    "    y: tensor (n_ts, n_obs) actual values in torch tensor.\n",
    "    y_hat: tensor(n_ts, n_obs, n_quantiles) predicted values in torch tensor.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy: np.array(n_quantiles) absolute error (in pp) between the true and estimated quantiles\n",
    "    \"\"\"\n",
    "    \n",
    "    y = np.stack([y.numpy().T for _ in range(len(quantiles))]).T\n",
    "    y_hat = y_hat.numpy()\n",
    "    \n",
    "    q_hat = (y <= y_hat).mean(axis=(0, 1))\n",
    "    abs_error = np.absolute(quantiles - q_hat)\n",
    "    av_abs_error = hmean(abs_error, axis=None)\n",
    "\n",
    "    return q_hat, abs_error, av_abs_error\n",
    "    \n",
    "y_hat = trained_model[0](x_test).detach()\n",
    "mq_error = absolute_error(quantiles=quantiles, y=y_test, y_hat=y_hat)\n",
    "print(f'quantiles:\\n{quantiles}\\n')\n",
    "print(f'q_hat:\\n{mq_error[0]}\\n')\n",
    "print(f'absolute errors:\\n{mq_error[1]}\\n')\n",
    "print(f'average absolute error: {mq_error[2]:.10f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mqloss(y, y_hat, quantiles, mask=None): \n",
    "    \"\"\"mqloss\n",
    "\n",
    "    Calculates Average Multi-quantile Loss function, for\n",
    "    a given set of quantiles, based on the absolute \n",
    "    difference between predicted and true values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: np.array (batch_size, output_size) actual values in torch tensor.\n",
    "    y_hat: np.array (batch_size, output_size, n_quantiles) predicted values in torch tensor.\n",
    "    mask: np.array (batch_size, output_size, n_quantiles) specifies date stamps per serie\n",
    "          to consider in loss\n",
    "    quantiles: np.array(n_quantiles) quantiles to estimate from the distribution of y.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lq: np.array(n_quantiles) average multi-quantile loss.\n",
    "    \"\"\"    \n",
    "    \n",
    "    if mask is None: mask = np.ones_like(y_hat)\n",
    "\n",
    "        \n",
    "    n_q = len(quantiles)\n",
    "    \n",
    "    error = y_hat - np.stack([y.T for _ in range(n_q)]).T\n",
    "    sq = np.maximum(-error, np.zeros_like(error))\n",
    "    s1_q = np.maximum(error, np.zeros_like(error)) \n",
    "    loss = (quantiles * sq + (1 - quantiles) * s1_q) * mask\n",
    "    return np.mean(np.sum(loss, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with the PyTorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_pytorch: 16.55120277404785\n",
      "loss_numpy: 16.55120277404785\n",
      "Difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "loss_pytorch = MQLoss(y_test, y_hat, quantiles)\n",
    "loss_numpy = mqloss(y_test.numpy(), y_hat.numpy(), quantiles.numpy())\n",
    "\n",
    "print(f'loss_pytorch: {loss_pytorch}')\n",
    "print(f'loss_numpy: {loss_numpy}')\n",
    "print(f'Difference: {abs(loss_pytorch - loss_numpy)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nixtla",
   "language": "python",
   "name": "nixtla"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
