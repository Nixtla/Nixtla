{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp losses.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch as t\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def divide_no_nan(a, b):\n",
    "    \"\"\"\n",
    "    Auxiliary funtion to handle divide by 0\n",
    "    \"\"\"\n",
    "    div = a / b\n",
    "    div[div != div] = 0.0\n",
    "    div[div == float('inf')] = 0.0\n",
    "    return div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERAL TRAIN LOSSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MAPELoss(y, y_hat, mask=None):\n",
    "    \"\"\"MAPE Loss\n",
    "\n",
    "    Calculates Mean Absolute Percentage Error between\n",
    "    y and y_hat. MAPE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the\n",
    "    percentual deviation of the prediction and the true\n",
    "    value at a given time and averages these devations\n",
    "    over the length of the series.\n",
    "    As defined in: https://en.wikipedia.org/wiki/Mean_absolute_percentage_error\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values in torch tensor.\n",
    "    mask: tensor (batch_size, output_size)\n",
    "        specifies date stamps per serie\n",
    "        to consider in loss\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mape:\n",
    "    Mean absolute percentage error.\n",
    "    \"\"\"\n",
    "    mask = divide_no_nan(mask, t.abs(y))\n",
    "    mape = t.abs(y - y_hat) * mask\n",
    "    mape = t.mean(mape)\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def MSELoss(y, y_hat, mask=None):\n",
    "    \"\"\"MSE Loss\n",
    "\n",
    "    Calculates Mean Squared Error between\n",
    "    y and y_hat. MAPE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the\n",
    "    percentual deviation of the prediction and the true\n",
    "    value at a given time and averages these devations\n",
    "    over the length of the series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values in torch tensor.\n",
    "    mask: tensor (batch_size, output_size)\n",
    "        specifies date stamps per serie\n",
    "        to consider in loss\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mse:\n",
    "    Mean Squared Error.\n",
    "    \"\"\"\n",
    "    mse = (y - y_hat)**2\n",
    "    mse = mask * mse\n",
    "    mse = t.mean(mse)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def RMSELoss(y, y_hat, mask=None):\n",
    "    \"\"\"RMSE Loss\n",
    "\n",
    "    Calculates Mean Squared Error between\n",
    "    y and y_hat. MAPE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the\n",
    "    percentual deviation of the prediction and the true\n",
    "    value at a given time and averages these devations\n",
    "    over the length of the series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values in torch tensor.\n",
    "    mask: tensor (batch_size, output_size)\n",
    "        specifies date stamps per serie\n",
    "        to consider in loss\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rmse:\n",
    "    Root Mean Squared Error.\n",
    "    \"\"\"\n",
    "    rmse = (y - y_hat)**2\n",
    "    rmse = mask * rmse\n",
    "    rmse = t.sqrt(t.mean(rmse))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def SMAPELoss(y, y_hat, mask=None):\n",
    "    \"\"\"SMAPE2 Loss\n",
    "\n",
    "    Calculates Symmetric Mean Absolute Percentage Error.\n",
    "    SMAPE measures the relative prediction accuracy of a\n",
    "    forecasting method by calculating the relative deviation\n",
    "    of the prediction and the true value scaled by the sum of the\n",
    "    absolute values for the prediction and true value at a\n",
    "    given time, then averages these devations over the length\n",
    "    of the series. This allows the SMAPE to have bounds between\n",
    "    0% and 200% which is desireble compared to normal MAPE that\n",
    "    may be undetermined.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values in torch tensor.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    smape:\n",
    "        symmetric mean absolute percentage error\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [1] https://robjhyndman.com/hyndsight/smape/ (Makridakis 1993)\n",
    "    \"\"\"\n",
    "    if mask is None:\n",
    "        mask = t.ones(y_hat.size())\n",
    "    delta_y = t.abs((y - y_hat))\n",
    "    scale = t.abs(y) + t.abs(y_hat)\n",
    "    smape = divide_no_nan(delta_y, scale)\n",
    "    smape = smape * mask\n",
    "    smape = 2 * t.mean(smape)\n",
    "    return smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def MASELoss(y, y_hat, y_insample, seasonality, mask=None) :\n",
    "    \"\"\" Calculates the M4 Mean Absolute Scaled Error.\n",
    "\n",
    "    MASE measures the relative prediction accuracy of a\n",
    "    forecasting method by comparinng the mean absolute errors\n",
    "    of the prediction and the true value against the mean\n",
    "    absolute errors of the seasonal naive model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seasonality: int\n",
    "        main frequency of the time series\n",
    "        Hourly 24,  Daily 7, Weekly 52,\n",
    "        Monthly 12, Quarterly 4, Yearly 1\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual test values\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values\n",
    "    y_train: tensor (batch_size, input_size)\n",
    "        actual insample values for Seasonal Naive predictions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mase:\n",
    "        mean absolute scaled error\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [1] https://robjhyndman.com/papers/mase.pdf\n",
    "    \"\"\"\n",
    "    if mask is None:\n",
    "        mask = t.ones(y_hat.size())\n",
    "    delta_y = t.abs(y - y_hat)\n",
    "    scale = t.mean(t.abs(y_insample[:, seasonality:] - \\\n",
    "                            y_insample[:, :-seasonality]), axis=1)\n",
    "    mase = divide_no_nan(delta_y, scale[:, None])\n",
    "    mase = mase * mask\n",
    "    mase = t.mean(mase)\n",
    "    return mase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def MAELoss(y, y_hat, mask=None):\n",
    "    \"\"\"MAE Loss\n",
    "\n",
    "    Calculates Mean Absolute Error between\n",
    "    y and y_hat. MAE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the\n",
    "    deviation of the prediction and the true\n",
    "    value at a given time and averages these devations\n",
    "    over the length of the series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values in torch tensor.\n",
    "    mask: tensor (batch_size, output_size)\n",
    "        specifies date stamps per serie\n",
    "        to consider in loss\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mae:\n",
    "    Mean absolute error.\n",
    "    \"\"\"\n",
    "    mae = t.abs(y - y_hat) * mask\n",
    "    mae = t.mean(mae)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def PinballLoss(y, y_hat, mask=None, tau=0.5):\n",
    "    \"\"\"Pinball Loss\n",
    "    Computes the pinball loss between y and y_hat.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size)\n",
    "        actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size)\n",
    "        predicted values in torch tensor.\n",
    "    tau: float, between 0 and 1\n",
    "        the slope of the pinball loss, in the context of\n",
    "        quantile regression, the value of tau determines the\n",
    "        conditional quantile level.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pinball:\n",
    "        average accuracy for the predicted quantile\n",
    "    \"\"\"\n",
    "    if mask is None:\n",
    "        mask = t.ones(y_hat.size())\n",
    "    delta_y = t.sub(y, y_hat)\n",
    "    pinball = t.max(t.mul(tau, delta_y), t.mul((tau - 1), delta_y))\n",
    "    pinball = pinball * mask\n",
    "    pinball = t.mean(pinball)\n",
    "    return pinball"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESRNN TRAIN LOSSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def LevelVariabilityLoss(levels, level_variability_penalty):\n",
    "    \"\"\" Level Variability Loss\n",
    "    Computes the variability penalty for the level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    levels: tensor with shape (batch, n_time)\n",
    "        levels obtained from exponential smoothing component of ESRNN\n",
    "    level_variability_penalty: float\n",
    "        this parameter controls the strength of the penalization \n",
    "        to the wigglines of the level vector, induces smoothness\n",
    "        in the output\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    level_var_loss:\n",
    "        wiggliness loss for the level vector\n",
    "    \"\"\"\n",
    "    assert levels.shape[1] > 2\n",
    "    level_prev = t.log(levels[:, :-1])\n",
    "    level_next = t.log(levels[:, 1:])\n",
    "    log_diff_of_levels = t.sub(level_prev, level_next)\n",
    "\n",
    "    log_diff_prev = log_diff_of_levels[:, :-1]\n",
    "    log_diff_next = log_diff_of_levels[:, 1:]\n",
    "    diff = t.sub(log_diff_prev, log_diff_next)\n",
    "    level_var_loss = diff**2\n",
    "    level_var_loss = level_var_loss.mean() * level_variability_penalty\n",
    "    \n",
    "    return level_var_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SmylLoss(y, y_hat, levels, mask, tau, level_variability_penalty=0.0):\n",
    "    \"\"\"Computes the Smyl Loss that combines level variability with\n",
    "    with Pinball loss.\n",
    "    windows_y: tensor of actual values,\n",
    "                            shape (n_windows, batch_size, window_size).\n",
    "    windows_y_hat: tensor of predicted values,\n",
    "                                    shape (n_windows, batch_size, window_size).\n",
    "    levels: levels obtained from exponential smoothing component of ESRNN.\n",
    "                    tensor with shape (batch, n_time).\n",
    "    return: smyl_loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    if mask is None: mask = t.ones(y_hat.size())\n",
    "    smyl_loss = PinballLoss(y, y_hat, mask, tau)\n",
    "    \n",
    "    if level_variability_penalty > 0:\n",
    "        log_diff_of_levels = LevelVariabilityLoss(levels, level_variability_penalty) \n",
    "        smyl_loss += log_diff_of_levels\n",
    "    \n",
    "    return smyl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI-QUANTILE LOSS\n",
    "\n",
    "MQLoss definition and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MQLoss(y, y_hat, quantiles, mask=None): \n",
    "    \"\"\"MQLoss\n",
    "\n",
    "    Calculates Average Multi-quantile Loss function, for\n",
    "    a given set of quantiles, based on the absolute \n",
    "    difference between predicted and true values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size) actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size, n_quantiles) predicted values in torch tensor.\n",
    "    mask: tensor (batch_size, output_size, n_quantiles) specifies date stamps per serie\n",
    "          to consider in loss\n",
    "    quantiles: tensor(n_quantiles) quantiles to estimate from the distribution of y.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lq: tensor(n_quantiles) average multi-quantile loss.\n",
    "    \"\"\"    \n",
    "    assert len(quantiles) > 1, f'your quantiles are of len: {len(quantiles)}'\n",
    "    \n",
    "    if mask is None: mask = t.ones_like(y_hat)\n",
    "        \n",
    "    n_q = len(quantiles)\n",
    "    \n",
    "    error = y_hat - y.unsqueeze(-1)\n",
    "    sq = t.maximum(-error, t.zeros_like(error))\n",
    "    s1_q = t.maximum(error, t.zeros_like(error))\n",
    "    loss = (quantiles * sq + (1 - quantiles) * s1_q)\n",
    "   \n",
    "    return t.mean(t.mean(loss, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def wMQLoss(y, y_hat, quantiles, mask=None): \n",
    "    \"\"\"wMQLoss\n",
    "\n",
    "    Calculates Average Multi-quantile Loss function, for\n",
    "    a given set of quantiles, based on the absolute \n",
    "    difference between predicted and true values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: tensor (batch_size, output_size) actual values in torch tensor.\n",
    "    y_hat: tensor (batch_size, output_size, n_quantiles) predicted values in torch tensor.\n",
    "    mask: tensor (batch_size, output_size, n_quantiles) specifies date stamps per serie\n",
    "          to consider in loss\n",
    "    quantiles: tensor(n_quantiles) quantiles to estimate from the distribution of y.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lq: tensor(n_quantiles) average multi-quantile loss.\n",
    "    \"\"\"    \n",
    "    assert len(quantiles) > 1, f'your quantiles are of len: {len(quantiles)}'\n",
    "    \n",
    "    if mask is None: mask = t.ones_like(y_hat)\n",
    "    \n",
    "    n_q = len(quantiles)\n",
    "    \n",
    "    error = y_hat - y.unsqueeze(-1)\n",
    "    \n",
    "    sq = t.maximum(-error, t.zeros_like(error))\n",
    "    s1_q = t.maximum(error, t.zeros_like(error))\n",
    "    loss = (quantiles * sq + (1 - quantiles) * s1_q)\n",
    "    \n",
    "    loss = divide_no_nan(t.sum(loss * mask, axis=-2), \n",
    "                         t.sum(t.abs(y.unsqueeze(-1)) * mask, axis=-2))\n",
    "    \n",
    "    return t.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST/DEBUG PYTORCH TRAIN LOSSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats import hmean\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):  \n",
    "\n",
    "    def __init__(self, horizon, n_quantiles):\n",
    "        super(Model, self).__init__()\n",
    "        self.horizon = horizon\n",
    "        self.n_quantiles = n_quantiles\n",
    "        self.linear_layer = nn.Linear(in_features=n_obs, \n",
    "                                      out_features=horizon * n_quantiles, \n",
    "                                      bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_hat = self.linear_layer(x)\n",
    "        y_hat = y_hat.view(-1, self.horizon, self.n_quantiles)\n",
    "        return y_hat\n",
    "    \n",
    "class Data(Dataset):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, Y, X):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.len = Y.shape[0]\n",
    "\n",
    "    # Getter\n",
    "    def __getitem__(self, index):          \n",
    "        return self.X[index], self.Y[index]\n",
    "    \n",
    "    # Get Length\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantiles:\n",
      "tensor([0.0500, 0.3500, 0.6500, 0.9500])\n",
      "Y.shape: torch.Size([1000, 10]), X.shape: torch.Size([1000, 10])\n",
      "Y_test.shape: torch.Size([1000, 10]), X_test.shape: torch.Size([1000, 10])\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters and sample data parameters\n",
    "t.cuda.manual_seed(7)\n",
    "\n",
    "# Sample data\n",
    "n_ts = 1000\n",
    "n_obs = horizon = 10\n",
    "mean = 0.0 # to generate random numbers from N(mean, std)\n",
    "std = 7.0 # to generate random numbers from N(mean, std)\n",
    "start = 0.05 # First quantile\n",
    "end = 0.95 # Last quantiles\n",
    "steps = 4 # Number of quantiles\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 500\n",
    "lr = 0.08\n",
    "epochs = 100\n",
    "\n",
    "# Sample data\n",
    "quantiles = t.Tensor([0.0500, 0.3500, 0.6500, 0.9500])\n",
    "print(f'quantiles:\\n{quantiles}')\n",
    "Y = t.normal(mean=mean, std=std, size=(n_ts, n_obs))\n",
    "X = t.ones(size=(n_ts, n_obs))\n",
    "\n",
    "Y_test = t.normal(mean=mean, std=std, size=(n_ts, horizon))\n",
    "X_test = t.ones(size=(n_ts, horizon))\n",
    "print(f'Y.shape: {Y.shape}, X.shape: {X.shape}')\n",
    "print(f'Y_test.shape: {Y_test.shape}, X_test.shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training \n",
    "model = Model(horizon=horizon, n_quantiles=len(quantiles))\n",
    "dataset = Data(X=X, Y=Y)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def train_model(model, epochs, print_progress=False):\n",
    "\n",
    "    start = time.time()\n",
    "    i = 0 \n",
    "    training_trajectory = {'epoch': [],\n",
    "                           'train_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for x, y in dataloader:\n",
    "            \n",
    "            i += 1\n",
    "            y_hat = model(x)\n",
    "            #training_loss = wMQLoss(y=y, y_hat=y_hat, quantiles=quantiles)\n",
    "            training_loss = MQLoss(y=y, y_hat=y_hat, quantiles=quantiles)\n",
    "            if i % (epoch + 1) == 0: \n",
    "                training_trajectory['epoch'].append(i)\n",
    "                training_trajectory['train_loss'].append(training_loss.detach().numpy())\n",
    "            optimizer.zero_grad()\n",
    "            training_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            display_string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(i, \n",
    "                                                                                    time.time()-start, \n",
    "                                                                                    \"MQLoss\", \n",
    "                                                                                    training_loss.cpu().data.numpy())\n",
    "            if print_progress: print(display_string)\n",
    "\n",
    "    return model, training_trajectory\n",
    "\n",
    "model, training_trajectory = train_model(model=model, epochs=epochs)\n",
    "Y_hat = model(X_test).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_trajectory['epoch'], training_trajectory['train_loss'])\n",
    "plt.xlabel('Gradient Steps')\n",
    "plt.ylabel('MQLoss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_pytorch: 1.6652147769927979\n",
      "loss_numpy: 1.6652147769927979\n",
      "Difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "from nixtla.losses.numpy import mqloss\n",
    "\n",
    "loss_pytorch = MQLoss(Y_test, Y_hat, quantiles)\n",
    "loss_numpy = mqloss(Y_test.numpy(), Y_hat.numpy(), quantiles.numpy())\n",
    "\n",
    "print(f'loss_pytorch: {loss_pytorch}')\n",
    "print(f'loss_numpy: {loss_numpy}')\n",
    "print(f'Difference: {abs(loss_pytorch - loss_numpy)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nixtla",
   "language": "python",
   "name": "nixtla"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
