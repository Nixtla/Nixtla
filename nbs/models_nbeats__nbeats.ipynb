{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.nbeats.nbeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "import torch as t\n",
    "from torch import optim\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "from nixtla.models.nbeats.nbeats_model import NBeats, NBeatsBlock, IdentityBasis, TrendBasis, SeasonalityBasis\n",
    "from nixtla.models.nbeats.nbeats_model import ExogenousBasisInterpretable, ExogenousBasisWavenet, ExogenousBasisTCN\n",
    "from nixtla.losses.pytorch import MAPELoss, MASELoss, SMAPELoss, MSELoss, MAELoss, PinballLoss\n",
    "from nixtla.losses.numpy import mae, mse, mape, smape, rmse, pinball_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_weights(module, initialization):\n",
    "    if type(module) == t.nn.Linear:\n",
    "        if initialization == 'orthogonal':\n",
    "            t.nn.init.orthogonal_(module.weight)\n",
    "        elif initialization == 'he_uniform':\n",
    "            t.nn.init.kaiming_uniform_(module.weight)\n",
    "        elif initialization == 'he_normal':\n",
    "            t.nn.init.kaiming_normal_(module.weight)\n",
    "        elif initialization == 'glorot_uniform':\n",
    "            t.nn.init.xavier_uniform_(module.weight)\n",
    "        elif initialization == 'glorot_normal':\n",
    "            t.nn.init.xavier_normal_(module.weight)\n",
    "        elif initialization == 'lecun_normal':\n",
    "            pass #t.nn.init.normal_(module.weight, 0.0, std=1/np.sqrt(module.weight.numel()))\n",
    "        else:\n",
    "            assert 1<0, f'Initialization {initialization} not found'\n",
    "\n",
    "class Nbeats(object):\n",
    "    \"\"\"\n",
    "    Future documentation\n",
    "    \"\"\"\n",
    "    SEASONALITY_BLOCK = 'seasonality'\n",
    "    TREND_BLOCK = 'trend'\n",
    "    IDENTITY_BLOCK = 'identity'\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size_multiplier,\n",
    "                 output_size,\n",
    "                 shared_weights,\n",
    "                 activation,\n",
    "                 initialization,\n",
    "                 stack_types,\n",
    "                 n_blocks,\n",
    "                 n_layers,\n",
    "                 n_hidden,\n",
    "                 n_harmonics,\n",
    "                 n_polynomials,\n",
    "                 exogenous_n_channels,\n",
    "                 include_var_dict,\n",
    "                 t_cols,\n",
    "                 batch_normalization,\n",
    "                 dropout_prob_theta,\n",
    "                 dropout_prob_exogenous,\n",
    "                 x_s_n_hidden,\n",
    "                 learning_rate,\n",
    "                 lr_decay,\n",
    "                 n_lr_decay_steps,\n",
    "                 weight_decay,\n",
    "                 l1_theta,\n",
    "                 n_iterations,\n",
    "                 early_stopping,\n",
    "                 loss,\n",
    "                 loss_hypar,\n",
    "                 val_loss,\n",
    "                 frequency,\n",
    "                 random_seed,\n",
    "                 seasonality,\n",
    "                 device=None):\n",
    "        super(Nbeats, self).__init__()\n",
    "\n",
    "        if activation == 'selu': initialization = 'lecun_normal'\n",
    "\n",
    "        #------------------------ Model Attributes ------------------------#\n",
    "        # Architecture parameters\n",
    "        self.input_size = int(input_size_multiplier*output_size)\n",
    "        self.output_size = output_size\n",
    "        self.shared_weights = shared_weights\n",
    "        self.activation = activation\n",
    "        self.initialization = initialization\n",
    "        self.stack_types = stack_types\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_harmonics = n_harmonics\n",
    "        self.n_polynomials = n_polynomials\n",
    "        self.exogenous_n_channels = exogenous_n_channels\n",
    "\n",
    "        # Regularization and optimization parameters\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.dropout_prob_theta = dropout_prob_theta\n",
    "        self.dropout_prob_exogenous = dropout_prob_exogenous\n",
    "        self.x_s_n_hidden = x_s_n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.n_lr_decay_steps = n_lr_decay_steps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.n_iterations = n_iterations\n",
    "        self.early_stopping = early_stopping\n",
    "        self.loss = loss\n",
    "        self.loss_hypar = loss_hypar\n",
    "        self.val_loss = val_loss\n",
    "        self.l1_theta = l1_theta\n",
    "        self.l1_conv = 1e-3 # Not a hyperparameter\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Data parameters\n",
    "        self.frequency = frequency\n",
    "        self.seasonality = seasonality        \n",
    "        self.include_var_dict = include_var_dict\n",
    "        self.t_cols = t_cols\n",
    "        #self.scaler = scaler\n",
    "\n",
    "        if device is None:\n",
    "            device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
    "        self.device = device\n",
    "\n",
    "        self._is_instantiated = False\n",
    "\n",
    "    def create_stack(self):\n",
    "        if self.include_var_dict is not None:\n",
    "            x_t_n_inputs = self.output_size * int(sum([len(x) for x in self.include_var_dict.values()]))\n",
    "            \n",
    "            # Correction because week_day only adds 1 no output_size\n",
    "            if len(self.include_var_dict['week_day'])>0:\n",
    "                x_t_n_inputs = x_t_n_inputs - self.output_size + 1 \n",
    "        else:\n",
    "            x_t_n_inputs = self.input_size\n",
    "        \n",
    "        #------------------------ Model Definition ------------------------#\n",
    "        block_list = []\n",
    "        self.blocks_regularizer = []\n",
    "        for i in range(len(self.stack_types)):\n",
    "            #print(f'| --  Stack {self.stack_types[i]} (#{i})')\n",
    "            for block_id in range(self.n_blocks[i]):\n",
    "                \n",
    "                # Batch norm only on first block\n",
    "                if (len(block_list)==0) and (self.batch_normalization):\n",
    "                    batch_normalization_block = True\n",
    "                else:\n",
    "                    batch_normalization_block = False\n",
    "                \n",
    "                # Dummy of regularizer in block. Override with 1 if exogenous_block\n",
    "                self.blocks_regularizer += [0]\n",
    "\n",
    "                # Shared weights\n",
    "                if self.shared_weights and block_id>0:\n",
    "                    nbeats_block = block_list[-1]\n",
    "                else:\n",
    "                    if self.stack_types[i] == 'seasonality':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=4 * int(\n",
    "                                                        np.ceil(self.n_harmonics / 2 * self.output_size) - (self.n_harmonics - 1)),\n",
    "                                                   basis=SeasonalityBasis(harmonics=self.n_harmonics,\n",
    "                                                                          backcast_size=self.input_size,\n",
    "                                                                          forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'trend':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2 * (self.n_polynomials + 1),\n",
    "                                                   basis=TrendBasis(degree_of_polynomial=self.n_polynomials,\n",
    "                                                                            backcast_size=self.input_size,\n",
    "                                                                            forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'identity':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=self.input_size + self.output_size,\n",
    "                                                   basis=IdentityBasis(backcast_size=self.input_size,\n",
    "                                                                       forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2*self.n_x_t,\n",
    "                                                   basis=ExogenousBasisInterpretable(),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous_tcn':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden = self.x_s_n_hidden,\n",
    "                                                   theta_n_dim = 2*(self.exogenous_n_channels),\n",
    "                                                   basis= ExogenousBasisTCN(self.exogenous_n_channels, self.n_x_t),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous_wavenet':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2*(self.exogenous_n_channels),\n",
    "                                                   basis=ExogenousBasisWavenet(self.exogenous_n_channels, self.n_x_t),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                        self.blocks_regularizer[-1] = 1\n",
    "                    else:\n",
    "                        assert 1<0, f'Block type not found!'\n",
    "                # Select type of evaluation and apply it to all layers of block\n",
    "                init_function = partial(init_weights, initialization=self.initialization)                                             \n",
    "                nbeats_block.layers.apply(init_function)\n",
    "                #print(f'     | -- {nbeats_block}')\n",
    "                block_list.append(nbeats_block)\n",
    "        return block_list\n",
    "\n",
    "    def __loss_fn(self, loss_name: str):\n",
    "        def loss(x, loss_hypar, forecast, target, mask):\n",
    "            if loss_name == 'MAPE':\n",
    "                return MAPELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'MASE':\n",
    "                return MASELoss(y=target, y_hat=forecast, y_insample=x, seasonality=loss_hypar, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return SMAPELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'MSE':\n",
    "                return MSELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'MAE':\n",
    "                return MAELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'PINBALL':\n",
    "                return PinballLoss(y=target, y_hat=forecast, mask=mask, tau=loss_hypar) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def __val_loss_fn(self, loss_name='MAE'):\n",
    "        #TODO: mase not implemented\n",
    "        def loss(forecast, target, weights):\n",
    "            if loss_name == 'MAPE':\n",
    "                return mape(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return smape(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MSE':\n",
    "                return mse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'RMSE':\n",
    "                return rmse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MAE':\n",
    "                return mae(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'PINBALL':\n",
    "                return pinball_loss(y=target, y_hat=forecast, weights=weights, tau=0.5)\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "    \n",
    "    def loss_l1_conv_layers(self):\n",
    "        loss_l1 = 0\n",
    "        for i, indicator in enumerate(self.blocks_regularizer):\n",
    "            if indicator:\n",
    "                loss_l1 += self.l1_conv * t.sum(t.abs(self.model.blocks[i].basis.weight))\n",
    "        return loss_l1\n",
    "    \n",
    "    def loss_l1_theta(self):\n",
    "        loss_l1 = 0\n",
    "        for block in self.model.blocks:\n",
    "            for layer in block.modules():\n",
    "                if isinstance(layer, t.nn.Linear):\n",
    "                    loss_l1 += self.l1_theta * layer.weight.abs().sum()\n",
    "        return loss_l1\n",
    "\n",
    "    def to_tensor(self, x: np.ndarray) -> t.Tensor:\n",
    "        tensor = t.as_tensor(x, dtype=t.float32).to(self.device)\n",
    "        return tensor\n",
    "\n",
    "    def evaluate_performance(self, ts_loader, validation_loss_fn):\n",
    "        self.model.eval()\n",
    "\n",
    "        #losses = []\n",
    "        forecasts = []\n",
    "        outsample_ys = []\n",
    "        outsample_masks = []\n",
    "        with t.no_grad():\n",
    "            for batch in iter(ts_loader):\n",
    "                # Parse batch\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                forecast   = self.model(x_s=s_matrix, insample_y=insample_y, \n",
    "                                        insample_x_t=insample_x, outsample_x_t=outsample_x,\n",
    "                                        insample_mask=insample_mask)\n",
    "                #batch_loss = validation_loss_fn(target=outsample_y.cpu().data.numpy(),\n",
    "                #                                forecast=forecast.cpu().data.numpy(),\n",
    "                #                                weights=outsample_mask.cpu().data.numpy())\n",
    "                #losses.append(batch_loss)\n",
    "                forecasts.append(forecast.cpu().data.numpy())\n",
    "                outsample_ys.append(batch['outsample_y'])\n",
    "                outsample_masks.append(batch['outsample_mask'])\n",
    "\n",
    "        forecasts = np.vstack(forecasts)\n",
    "        outsample_ys = np.vstack(outsample_ys)\n",
    "        outsample_masks = np.vstack(outsample_masks)\n",
    "        #loss = np.mean(losses)\n",
    "\n",
    "        complete_loss = validation_loss_fn(target=outsample_ys,\n",
    "                                           forecast=forecasts,\n",
    "                                           weights=outsample_masks)\n",
    "\n",
    "        self.model.train()\n",
    "        return complete_loss\n",
    "\n",
    "    def fit(self, train_ts_loader, val_ts_loader=None, n_iterations=None, verbose=True, eval_steps=1):\n",
    "        # TODO: Indexes hardcoded, information duplicated in train and val datasets\n",
    "        assert (self.input_size)==train_ts_loader.input_size, \\\n",
    "            f'model input_size {self.input_size} data input_size {train_ts_loader.input_size}'\n",
    "\n",
    "        # Random Seeds (model initialization)\n",
    "        t.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed) #TODO: interaccion rara con window_sampling de validacion\n",
    "\n",
    "        # Attributes of ts_dataset\n",
    "        self.n_x_t, self.n_x_s = train_ts_loader.get_n_variables()\n",
    "\n",
    "        # Instantiate model\n",
    "        if not self._is_instantiated:\n",
    "            block_list = self.create_stack()\n",
    "            self.model = NBeats(t.nn.ModuleList(block_list)).to(self.device)\n",
    "            self._is_instantiated = True\n",
    "\n",
    "        # Overwrite n_iterations and train datasets\n",
    "        if n_iterations is None:\n",
    "            n_iterations = self.n_iterations\n",
    "\n",
    "        lr_decay_steps = n_iterations // self.n_lr_decay_steps\n",
    "        if lr_decay_steps == 0:\n",
    "            lr_decay_steps = 1\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_steps, gamma=self.lr_decay)\n",
    "        training_loss_fn = self.__loss_fn(self.loss)\n",
    "        validation_loss_fn = self.__val_loss_fn(self.val_loss) #Uses numpy losses\n",
    "\n",
    "        print('='*30+' Start fitting '+'='*30)\n",
    "\n",
    "        #self.loss_dict = {} # Restart self.loss_dict\n",
    "        start = time.time()\n",
    "        self.trajectories = {'iteration':[],'train_loss':[], 'val_loss':[]}\n",
    "        self.final_insample_loss = None\n",
    "        self.final_outsample_loss = None\n",
    "        \n",
    "        # Training Loop\n",
    "        early_stopping_counter = 0\n",
    "        best_val_loss = np.inf\n",
    "        best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "        break_flag = False\n",
    "        iteration = 0\n",
    "        epoch = 0\n",
    "        while (iteration < n_iterations) and (not break_flag):\n",
    "            epoch +=1\n",
    "            for batch in iter(train_ts_loader):\n",
    "                iteration += 1\n",
    "                if (iteration > n_iterations) or (break_flag):\n",
    "                    continue\n",
    "\n",
    "                self.model.train()\n",
    "                # Parse batch\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                forecast   = self.model(x_s=s_matrix, insample_y=insample_y, \n",
    "                                        insample_x_t=insample_x, outsample_x_t=outsample_x,\n",
    "                                        insample_mask=insample_mask)\n",
    "\n",
    "                training_loss = training_loss_fn(x=insample_y, loss_hypar=self.loss_hypar, forecast=forecast,\n",
    "                                                 target=outsample_y, mask=outsample_mask)\n",
    "\n",
    "                if np.isnan(float(training_loss)):\n",
    "                    break\n",
    "\n",
    "                training_loss.backward()\n",
    "                t.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "                if (iteration % eval_steps == 0):\n",
    "                    display_string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(iteration,\n",
    "                                                                                            time.time()-start,\n",
    "                                                                                            self.loss,\n",
    "                                                                                            training_loss.cpu().data.numpy())\n",
    "                    self.trajectories['iteration'].append(iteration)\n",
    "                    self.trajectories['train_loss'].append(np.float(training_loss.cpu().data.numpy()))\n",
    "\n",
    "                    if val_ts_loader is not None:\n",
    "                        loss = self.evaluate_performance(ts_loader=val_ts_loader, \n",
    "                                                         validation_loss_fn=validation_loss_fn)\n",
    "                        display_string += \", Outsample {}: {:.5f}\".format(self.val_loss, loss)\n",
    "                        self.trajectories['val_loss'].append(loss)\n",
    "\n",
    "                        if self.early_stopping:\n",
    "                            if loss < best_val_loss:\n",
    "                                # Save current model if improves outsample loss\n",
    "                                best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "                                best_insample_loss = training_loss.cpu().data.numpy()\n",
    "                                early_stopping_counter = 0\n",
    "                                best_val_loss = loss\n",
    "                            else:\n",
    "                                early_stopping_counter += 1\n",
    "                            if early_stopping_counter >= self.early_stopping:\n",
    "                                break_flag = True\n",
    "                    \n",
    "                    print(display_string)\n",
    "\n",
    "                    self.model.train()\n",
    "\n",
    "                if break_flag:\n",
    "                    print('\\n')\n",
    "                    print(19*'-',' Stopped training by early stopping', 19*'-')\n",
    "                    self.model.load_state_dict(best_state_dict)\n",
    "                    break\n",
    "\n",
    "        #End of fitting\n",
    "        if n_iterations >0:\n",
    "            # This is batch loss!\n",
    "            self.final_insample_loss = np.float(training_loss.cpu().data.numpy()) if not break_flag else best_insample_loss \n",
    "            string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(iteration,\n",
    "                                                                            time.time()-start,\n",
    "                                                                            self.loss,\n",
    "                                                                            self.final_insample_loss)\n",
    "            if val_ts_loader is not None:\n",
    "                self.final_outsample_loss = self.evaluate_performance(ts_loader=val_ts_loader, \n",
    "                                                                      validation_loss_fn=validation_loss_fn)\n",
    "                string += \", Outsample {}: {:.5f}\".format(self.val_loss, self.final_outsample_loss)\n",
    "            print(string)\n",
    "            print('='*30+'  End fitting  '+'='*30)\n",
    "            print('\\n')\n",
    "\n",
    "    def predict(self, ts_loader, X_test=None, eval_mode=False):\n",
    "        self.model.eval()\n",
    "        assert not ts_loader.shuffle, 'ts_loader must have shuffle as False.'\n",
    "\n",
    "        forecasts = []\n",
    "        outsample_ys = []\n",
    "        outsample_masks = []\n",
    "        with t.no_grad():\n",
    "            for batch in iter(ts_loader):\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                forecast = self.model(insample_y=insample_y, insample_x_t=insample_x,\n",
    "                                      insample_mask=insample_mask, outsample_x_t=outsample_x, x_s=s_matrix)\n",
    "                forecasts.append(forecast.cpu().data.numpy())\n",
    "                outsample_ys.append(batch['outsample_y'])\n",
    "                outsample_masks.append(batch['outsample_mask'])\n",
    "        forecasts = np.vstack(forecasts)\n",
    "        outsample_ys = np.vstack(outsample_ys)\n",
    "        outsample_masks = np.vstack(outsample_masks)\n",
    "\n",
    "        self.model.train()\n",
    "        if eval_mode:\n",
    "            return outsample_ys, forecasts, outsample_masks\n",
    "\n",
    "        # Pandas wrangling\n",
    "        frequency = ts_loader.get_frequency()\n",
    "        unique_ids = ts_loader.get_meta_data_col('unique_id')\n",
    "        last_ds = ts_loader.get_meta_data_col('last_ds') #TODO: ajustar of offset\n",
    "\n",
    "        # Predictions for panel\n",
    "        Y_hat_panel = pd.DataFrame(columns=['unique_id', 'ds'])\n",
    "        for i, unique_id in enumerate(unique_ids):\n",
    "            Y_hat_id = pd.DataFrame([unique_id]*self.output_size, columns=[\"unique_id\"])\n",
    "            ds = pd.date_range(start=last_ds[i], periods=self.output_size+1, freq=frequency)\n",
    "            Y_hat_id[\"ds\"] = ds[1:]\n",
    "            Y_hat_panel = Y_hat_panel.append(Y_hat_id, sort=False).reset_index(drop=True)\n",
    "\n",
    "        Y_hat_panel['y_hat'] = forecasts.flatten()\n",
    "\n",
    "        if X_test is not None:\n",
    "            Y_hat_panel = X_test.merge(Y_hat_panel, on=['unique_id', 'ds'], how='left')\n",
    "        \n",
    "        return Y_hat_panel\n",
    "\n",
    "\n",
    "    def save(self, model_dir, model_id, state_dict = None):\n",
    "    \n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        if state_dict is None:\n",
    "            state_dict = self.model.state_dict()\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        print('Saving model to:\\n {}'.format(model_file)+'\\n')\n",
    "        t.save({'model_state_dict': state_dict}, model_file)\n",
    "\n",
    "    def load(self, model_dir, model_id):\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        path = Path(model_file)\n",
    "\n",
    "        assert path.is_file(), 'No model_*.model file found in this path!'\n",
    "\n",
    "        print('Loading model from:\\n {}'.format(model_file)+'\\n')\n",
    "\n",
    "        checkpoint = t.load(model_file, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(self.device)"
   ]
  },
  {
   "source": [
    "# SINGLE TIME SERIES TEST"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nixtla.data.scalers import Scaler\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset as TimeSeriesDataset\n",
    "from nixtla.data.tsloader_fast import TimeSeriesLoader\n",
    "#from nixtla.data.tsloader_general import TimeSeriesLoader\n",
    "\n",
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "\n",
    "# Hacked MAE NP hypars\n",
    "hacked_np = {'input_size_multiplier': 7,\n",
    "             'output_size': 24,\n",
    "             'window_sampling_limit': 365*4*24,\n",
    "             'idx_to_sample_freq': 24,\n",
    "             'batch_size': 256,\n",
    "             'shared_weights': False,\n",
    "             'activation': 'relu',\n",
    "             'initialization': 'he_normal',\n",
    "             'stack_types': ['exogenous_tcn']+1*['identity'],\n",
    "             'n_blocks': 2*[1],\n",
    "             'n_layers': 2*[2],\n",
    "             'n_hidden': 2*[[462,462]],\n",
    "             'n_polynomials': 2,\n",
    "             'n_harmonics': 1,\n",
    "             'exogenous_n_channels': 8,\n",
    "             'include_var_dict': {'y': [-2, -3, -8],\n",
    "                                  'Exogenous1': [-1, -2, -8],\n",
    "                                  'Exogenous2': [-1, -2, -8],\n",
    "                                  'week_day': [-1]},\n",
    "             'batch_normalization': False,\n",
    "             'dropout_prob_theta': 0.05,\n",
    "             'dropout_prob_exogenous': 0.35,\n",
    "             'x_s_n_hidden': 0,\n",
    "             'learning_rate': 0.0016,\n",
    "             'lr_decay': 0.5,\n",
    "             'n_lr_decay_steps': 3,\n",
    "             'weight_decay': 6e-4,\n",
    "             'l1_theta': 1.0e-05,\n",
    "             'n_iterations': 200, #2000 <-------------\n",
    "             'early_stopping': 40,\n",
    "             'loss': 'PINBALL',\n",
    "             'loss_hypar': 0.49,\n",
    "             'val_loss': 'MAE', #'SMAPE',\n",
    "             'frequency': 'H',\n",
    "             'random_seed': 17,\n",
    "             'seasonality': 24}\n",
    "\n",
    "# Hacked MAE BE hypars\n",
    "hacked_be = {'input_size_multiplier': 7,\n",
    "             'output_size': 24,\n",
    "             'window_sampling_limit': 365*4*24,\n",
    "             'idx_to_sample_freq': 24,\n",
    "             'batch_size': 256,\n",
    "             'shared_weights': False,\n",
    "             'activation': 'relu',\n",
    "             'initialization': 'lecun_normal',\n",
    "             'stack_types': ['exogenous_wavenet']+1*['identity'],\n",
    "             'n_blocks': 2*[1],\n",
    "             'n_layers': 2*[2],\n",
    "             'n_hidden': 2*[[462,462]],\n",
    "             'n_polynomials': 2,\n",
    "             'n_harmonics': 1,\n",
    "             'exogenous_n_channels': 8,\n",
    "             'include_var_dict': {'y': [-2, -3, -8],\n",
    "                                  'Exogenous1': [-1, -2, -8],\n",
    "                                  'Exogenous2': [-1, -2, -8],\n",
    "                                  'week_day': [-1]},\n",
    "             'batch_normalization': False,\n",
    "             'dropout_prob_theta': 0.14,\n",
    "             'dropout_prob_exogenous': 0.08, #<-------- Interesante\n",
    "             'x_s_n_hidden': 0,\n",
    "             'learning_rate': 0.0013,\n",
    "             'lr_decay': 0.5,\n",
    "             'n_lr_decay_steps': 3,\n",
    "             'weight_decay': 0.0017,\n",
    "             'l1_theta': 1.5e-05,\n",
    "             'n_iterations': 200, #2000 <-------------\n",
    "             'early_stopping': 40,\n",
    "             'loss': 'PINBALL',\n",
    "             'loss_hypar': 0.502,\n",
    "             'val_loss': 'MAE', #'MAE',\n",
    "             'frequency': 'H',\n",
    "             'random_seed': 16,\n",
    "             'seasonality': 24}\n",
    "\n",
    "mc = hacked_np\n",
    "#mc = hacked_be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_evaluation_table(y_true, y_hat):\n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_hat = y_hat.reshape(-1)\n",
    "    \n",
    "    #_pinball50 = np.round(pinball_loss(y_true, y_hat, tau=0.5),5)\n",
    "    _mae   = np.round(mae(y_true, y_hat),5)\n",
    "    _mape  = np.round(mape(y_true, y_hat),5)\n",
    "    _smape = np.round(smape(y_true, y_hat),5)\n",
    "    _rmse  = np.round(rmse(y_true, y_hat),5)\n",
    "\n",
    "    performance = pd.DataFrame({'metric': ['mae', 'mape', 'smape', 'rmse'],\n",
    "                                'measure': [_mae, _mape, _smape, _rmse]})                          \n",
    "\n",
    "    return performance\n",
    "\n",
    "def get_last_n_hours_mask_df(Y_df, n_hours):\n",
    "    # Creates outsample_mask\n",
    "    # train 1 validation 0\n",
    "\n",
    "    last_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    last_df.sort_values(by=['unique_id', 'ds'], inplace=True, ascending=False)\n",
    "    last_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    last_df = last_df.groupby('unique_id').head(n_hours)\n",
    "    last_df['mask'] = 1\n",
    "\n",
    "    last_df = last_df[['unique_id', 'ds', 'mask']]\n",
    "\n",
    "    mask_df = Y_df.merge(last_df, on=['unique_id', 'ds'], how='left')\n",
    "    mask_df['mask'] = mask_df['mask'].fillna(0)    # The first len(Y)-n_hours used as train\n",
    "\n",
    "    mask_df = mask_df[['unique_id', 'ds', 'mask']]\n",
    "    mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "\n",
    "    assert len(mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "\n",
    "    return mask_df\n",
    "\n",
    "def scale_data(Xt_df, mask_df):\n",
    "    # Conditional on sample_mask, scale all the Exogenous variables.\n",
    "\n",
    "    # To not modify original data\n",
    "    Xt_scaled_df = Xt_df.copy()\n",
    "\n",
    "    # Transform data with scale transformation\n",
    "    scaler = Scaler(normalizer='norm')\n",
    "    Xt_scaled_df['Exogenous1'] = scaler.scale(x=Xt_scaled_df['Exogenous1'].values, offset=val_ds*24)\n",
    "\n",
    "    scaler = Scaler(normalizer='norm')\n",
    "    Xt_scaled_df['Exogenous2'] = scaler.scale(x=Xt_scaled_df['Exogenous2'].values, offset=val_ds*24)\n",
    "\n",
    "    return Xt_scaled_df\n",
    "\n",
    "def run_val_nbeatsx(mc, Y_df, Xt_df, S_df, mask_df): #, trials, trials_file_name):\n",
    "\n",
    "    #---------------------------------------------- Read  Data ----------------------------------------------#\n",
    "\n",
    "    mask_df['sample_mask'] = mask_df['sample_mask1']\n",
    "    Xt_scaled_df = scale_data(Xt_df=Xt_df, mask_df=mask_df)\n",
    "    train_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=S_df, X_df=Xt_scaled_df, mask_df=mask_df)\n",
    "\n",
    "    mask_df['sample_mask'] = (1-mask_df['sample_mask1'])\n",
    "    val_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=S_df, X_df=Xt_scaled_df, mask_df=mask_df)\n",
    "\n",
    "    # # To not modify original data\n",
    "    # Xt_scaled_df = Xt_df.copy()\n",
    "\n",
    "    # # Transform data with scale transformation\n",
    "    # scaler = Scaler(normalizer='norm')\n",
    "    # Xt_scaled_df['Exogenous1'] = scaler.scale(x=Xt_scaled_df['Exogenous1'].values, offset=val_ds*24)\n",
    "\n",
    "    # scaler = Scaler(normalizer='norm')\n",
    "    # Xt_scaled_df['Exogenous2'] = scaler.scale(x=Xt_scaled_df['Exogenous2'].values, offset=val_ds*24)\n",
    "\n",
    "    # #print(f'Dataset: {args.dataset}')\n",
    "    # #print(\"Xt_df.columns\", Xt_df.columns)\n",
    "    # print('X: time series features, of shape (#hours, #times,#features): \\t' + str(Xt_df.shape))\n",
    "    # if S_df is not None:\n",
    "    #     print('S: static features, of shape (#series,#features+unique_id): \\t' + str(S_df.shape))\n",
    "    # print('Y: target series (in X), of shape (#hours, #times): \\t \\t' + str(Y_df.shape))\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    # last_n_hours_mask_df = get_last_n_hours_mask_df(Y_df, n_hours=val_ds*24)\n",
    "    # last_n_hours_mask_df['available_mask'] = np.ones(len(last_n_hours_mask_df))\n",
    "    # last_n_hours_mask_df['sample_mask'] = (1-last_n_hours_mask_df['mask'])\n",
    "\n",
    "    print(\"Train Validation splits\")\n",
    "    #mask = last_n_hours_mask_df['sample_mask'].values\n",
    "    sample_mask = mask_df['sample_mask'].values\n",
    "    available_mask = mask_df['available_mask'].values\n",
    "    print(mask_df.groupby(['unique_id', 'sample_mask']).agg({'ds': ['min', 'max']}))\n",
    "    print(f'Data available percentage {np.round(sum(available_mask)/len(Y_df),2)}, \\\n",
    "            {sum(available_mask)} hours = {np.round(sum(available_mask)/(24*365),2)} years')    \n",
    "    print(f'Train insample percentage {np.round(sum(sample_mask)/len(Y_df),2)}, \\\n",
    "            {sum(sample_mask)} hours = {np.round(sum(sample_mask)/(24*365),2)} years')\n",
    "    print(f'Train outsample percentage {np.round(sum(1-sample_mask)/len(Y_df),2)}, \\\n",
    "            {sum(1-sample_mask)} hours = {np.round(sum(1-sample_mask)/(24*365),2)} years')\n",
    "    #Y_df.head()\n",
    "    print('\\n')\n",
    "\n",
    "    train_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=S_df, X_df=Xt_scaled_df, mask_df=mask_df)\n",
    "\n",
    "    mask_df['sample_mask'] = (1-mask_df['sample_mask'])\n",
    "    val_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=S_df, X_df=Xt_scaled_df, mask_df=mask_df)\n",
    "\n",
    "    train_loader = TimeSeriesLoader(ts_dataset=train_dataset,\n",
    "                                    model='nbeats',\n",
    "                                    offset=0, #offset,\n",
    "                                    window_sampling_limit=int(mc['window_sampling_limit']), \n",
    "                                    input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                                    output_size=int(mc['output_size']),\n",
    "                                    idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                    batch_size=int(mc['batch_size']),\n",
    "                                    is_train_loader=True,\n",
    "                                    shuffle=True)\n",
    "\n",
    "    val_loader = TimeSeriesLoader(ts_dataset=val_dataset,\n",
    "                                    model='nbeats',\n",
    "                                    offset=0, #offset,\n",
    "                                    window_sampling_limit=int(mc['window_sampling_limit']), \n",
    "                                    input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                                    output_size=int(mc['output_size']),\n",
    "                                    idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                    batch_size=1024,\n",
    "                                    is_train_loader=False, # Samples the opposite of train_outsample_mask\n",
    "                                    shuffle=False)                                  \n",
    "\n",
    "    #-------------------------------------- Hyperparameter Optimization --------------------------------------#\n",
    "\n",
    "    model = Nbeats(input_size_multiplier=int(mc['input_size_multiplier']),\n",
    "                    output_size=int(mc['output_size']),\n",
    "                    shared_weights=int(mc['shared_weights']),\n",
    "                    activation=mc['activation'],\n",
    "                    initialization=mc['initialization'],\n",
    "                    stack_types=mc['stack_types'], #2*['identity'],\n",
    "                    n_blocks=mc['n_blocks'], #2*[1],\n",
    "                    n_layers=mc['n_layers'], #2*[2],\n",
    "                    n_hidden=mc['n_hidden'], #2*[[256,256]],\n",
    "                    n_polynomials=mc['n_polynomials'], #2,\n",
    "                    n_harmonics=int(mc['n_harmonics']), #1,\n",
    "                    exogenous_n_channels=int(mc['exogenous_n_channels']), #9,\n",
    "                    include_var_dict={'y': [-2, -3, -8],\n",
    "                                        'Exogenous1': [-1, -2, -8],\n",
    "                                        'Exogenous2': [-1, -2, -8],\n",
    "                                        'week_day': [-1]},\n",
    "                    t_cols=train_dataset.t_cols,\n",
    "                    batch_normalization=mc['batch_normalization'], #False,\n",
    "                    dropout_prob_theta=float(mc['dropout_prob_theta']), #0.01,\n",
    "                    dropout_prob_exogenous=float(mc['dropout_prob_exogenous']), #0.01,\n",
    "                    x_s_n_hidden=int(mc['x_s_n_hidden']), #0,\n",
    "                    learning_rate=float(mc['learning_rate']), #0.007,\n",
    "                    lr_decay=float(mc['lr_decay']), #0.5,\n",
    "                    n_lr_decay_steps=int(mc['n_lr_decay_steps']), #3,\n",
    "                    weight_decay=float(mc['weight_decay']), #0.0000001,\n",
    "                    l1_theta=float(mc['l1_theta']), #0.0001,\n",
    "                    n_iterations=int(mc['n_iterations']), #200,\n",
    "                    early_stopping=int(mc['early_stopping']), #40,\n",
    "                    loss=mc['loss'], #'PINBALL',\n",
    "                    loss_hypar=float(mc['loss_hypar']), #0.5,\n",
    "                    val_loss=mc['val_loss'], #MAE\n",
    "                    frequency=mc['frequency'], #'H',\n",
    "                    random_seed=int(mc['random_seed']), #1,\n",
    "                    seasonality=int(mc['seasonality'])) #24)\n",
    "\n",
    "    model.fit(train_ts_loader=train_loader, val_ts_loader=val_loader, verbose=True, eval_steps=10)\n",
    "\n",
    "    print('Best Model Evaluation')\n",
    "    y_true, y_hat, _ = model.predict(ts_loader=val_loader, eval_mode=True)\n",
    "    print(forecast_evaluation_table(y_true, y_hat))\n",
    "\n",
    "    return model, train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "type(mask_df.ds) <class 'pandas.core.series.Series'>\n",
      "mask_df.dtypes unique_id                 object\n",
      "ds                datetime64[ns]\n",
      "available_mask             int64\n",
      "dtype: object\n",
      "\n",
      "\n",
      "Processing dataframes ...\n",
      "Creating ts tensor ...\n",
      "Processing dataframes ...\n",
      "Creating ts tensor ...\n",
      "Train Validation splits\n",
      "                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2014-12-27 23:00:00\n",
      "          1           2014-12-28 2016-12-26 23:00:00\n",
      "Data available percentage 1.0,             34944 hours = 3.99 years\n",
      "Train insample percentage 0.5,             17520 hours = 2.0 years\n",
      "Train outsample percentage 0.5,             17424 hours = 1.99 years\n",
      "\n",
      "\n",
      "Processing dataframes ...\n",
      "Creating ts tensor ...\n",
      "Processing dataframes ...\n",
      "Creating ts tensor ...\n",
      "============================== Start fitting ==============================\n",
      "Step: 10, Time: 1.968, Insample PINBALL: 17.97612, Outsample MAE: 26.55954\n",
      "Step: 20, Time: 4.136, Insample PINBALL: 3.06614, Outsample MAE: 5.69465\n",
      "Step: 30, Time: 5.902, Insample PINBALL: 1.74728, Outsample MAE: 4.94517\n",
      "Step: 40, Time: 7.681, Insample PINBALL: 1.29453, Outsample MAE: 2.49358\n",
      "Step: 50, Time: 9.541, Insample PINBALL: 1.20151, Outsample MAE: 1.84924\n",
      "Step: 60, Time: 11.486, Insample PINBALL: 1.15894, Outsample MAE: 1.71719\n",
      "Step: 70, Time: 13.638, Insample PINBALL: 1.03244, Outsample MAE: 1.71396\n",
      "Step: 80, Time: 15.449, Insample PINBALL: 0.95118, Outsample MAE: 1.73945\n",
      "Step: 90, Time: 17.346, Insample PINBALL: 1.08692, Outsample MAE: 1.68165\n",
      "Step: 100, Time: 19.191, Insample PINBALL: 0.97931, Outsample MAE: 1.53703\n",
      "Step: 110, Time: 20.978, Insample PINBALL: 0.90937, Outsample MAE: 1.54653\n",
      "Step: 120, Time: 22.797, Insample PINBALL: 0.83911, Outsample MAE: 1.53625\n",
      "Step: 130, Time: 24.676, Insample PINBALL: 0.86256, Outsample MAE: 1.50435\n",
      "Step: 140, Time: 26.495, Insample PINBALL: 0.90035, Outsample MAE: 1.48315\n",
      "Step: 150, Time: 28.306, Insample PINBALL: 0.84681, Outsample MAE: 1.47244\n",
      "Step: 160, Time: 30.110, Insample PINBALL: 0.83934, Outsample MAE: 1.47184\n",
      "Step: 170, Time: 31.883, Insample PINBALL: 0.76048, Outsample MAE: 1.48093\n",
      "Step: 180, Time: 33.652, Insample PINBALL: 0.74709, Outsample MAE: 1.45019\n",
      "Step: 190, Time: 35.467, Insample PINBALL: 0.81702, Outsample MAE: 1.45617\n",
      "Step: 200, Time: 37.252, Insample PINBALL: 0.75132, Outsample MAE: 1.44234\n",
      "Step: 201, Time: 37.484, Insample PINBALL: 0.75132, Outsample MAE: 1.44234\n",
      "==============================  End fitting  ==============================\n",
      "\n",
      "\n",
      "Best Model Evaluation\n",
      "  metric  measure\n",
      "0    mae  1.44234\n",
      "1   mape  4.79260\n",
      "2  smape  4.60144\n",
      "3   rmse  2.66514\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "\n",
    "dataset = 'NP'\n",
    "Y_df, Xt_df = EPF.load(directory='../data/', group=dataset)\n",
    "val_ds = 365*2\n",
    "\n",
    "Y_balanced_df = Y_df\n",
    "\n",
    "# Create insample_mask\n",
    "# TODO: Include not null exogenous condition to available mask\n",
    "mask_df = Y_balanced_df[['unique_id', 'ds', 'y']].copy()\n",
    "mask_df['available_mask'] = (1-Y_balanced_df.y.isnull().values)\n",
    "del mask_df['y']\n",
    "\n",
    "print('type(mask_df.ds)', type(mask_df.ds))\n",
    "print('mask_df.dtypes', mask_df.dtypes)\n",
    "#mask_df['sample_mask1'] = (mask_df['ds'] <= pd.to_datetime('2013-01-03 23:00:00')) * 1\n",
    "mask_df['sample_mask1'] = (mask_df['ds'] <= pd.to_datetime('2014-12-27 23:00:00')) * 1\n",
    "print('\\n')\n",
    "\n",
    "model, train_loader, val_loader = run_val_nbeatsx(mc=mc, Y_df=Y_df, Xt_df=Xt_df, \n",
    "                                                  S_df=None, mask_df=mask_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_hat, _ = model.predict(ts_loader=val_loader, eval_mode=True)\n",
    "y_plot = Y_df['y'][-len(y_hat)*24:] #[-2*168:]\n",
    "y_hat = y_hat.reshape(-1)#[-2*168:]\n",
    "#lead_time = 23\n",
    "#y_hat_plot1 = y_hat[-2*168:, lead_time] # Forecast lead_time=?, for the last week\n",
    "#y_hat_plot2 = y_hat[-1, :]\n",
    "#y_hat_plot = np.concatenate([y_hat_plot1, y_hat_plot2])\n",
    "\n",
    "print(\"y_hat.shape  \\t\\t(#fcds, #lt) \\t\", y_hat.shape)\n",
    "print(\"y_plot.shape \\t\\t(#fcds,) \\t\", y_plot.shape)\n",
    "#print(\"y_true.shape \\t\\t(#fcds,) \\t\", y_true.shape)\n",
    "# print(\"y_hat_plot1.shape \\t(#fcds, lt=0) \\t\", y_hat_plot1.shape)\n",
    "# print(\"y_hat_plot2.shape \\t(#fcds, lt=0) \\t\", y_hat_plot2.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "performance = np.round(mae(y_plot, y_hat), 5)\n",
    "plt.plot(range(len(y_plot)), y_plot, label='true', alpha=0.7)\n",
    "plt.plot(range(len(y_hat)), y_hat, label='pred', alpha=0.7)\n",
    "plt.title(f\"Testing predictionsx {dataset} all lead time  MAE={performance}\")\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Electricity Price')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# MULTIVARIATE TIME SERIES TEST"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nixtla.data.scalers import Scaler\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset\n",
    "# from nixtla.data.tsloader_fast import TimeSeriesLoader\n",
    "from nixtla.data.tsloader_general import TimeSeriesLoader\n",
    "\n",
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "\n",
    "val_ds = 2 * 365\n",
    "args = pd.Series({'dataset': ['NP', 'PJM', 'BE', 'FR']})\n",
    "\n",
    "Y_df, Xt_df, S_df = EPF.load_groups(directory='data', groups=args.dataset)\n",
    "val_ds = 365*2\n",
    "\n",
    "model, train_loader, val_loader = run_val_nbeatsx(mc, Y_df, Xt_df, S_df=S_df, val_ds=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_hat, _ = model.predict(ts_loader=val_loader, eval_mode=True)\n",
    "y_true = y_true.reshape(4, 2920//4, 24)\n",
    "y_hat  = y_hat.reshape(4, 2920//4, 24)\n",
    "\n",
    "print(\"y_hat.shape  \\t\\t(#fcds, #lt) \\t\", y_hat.shape)\n",
    "print(\"y_true.shape \\t\\t(#fcds,) \\t\", y_true.shape)\n",
    "\n",
    "y_hat_plot = y_hat[2,:,:].reshape(-1)\n",
    "y_true_plot = y_true[2,:,:].reshape(-1)\n",
    "\n",
    "print(\"y_hat_plot.shape  \\t(#fcds,) \\t\", y_hat_plot.shape)\n",
    "print(\"y_true_plot.shape \\t(#fcds,) \\t\", y_true_plot.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "performance_df = {'unique_id': [], 'metric': [], 'nbeatsx': []}\n",
    "for i, meta_data in enumerate(val_loader.ts_dataset.meta_data):\n",
    "    dataset = meta_data['unique_id']\n",
    "    y_hat_plot = y_hat[i,:,:].reshape(-1)\n",
    "    y_true_plot = y_true[i,:,:].reshape(-1)\n",
    "\n",
    "    # Create performance table\n",
    "    performance_df['unique_id'] += [dataset]*4\n",
    "    performance_df['metric'] += ['MAE', 'MAPE', 'SMAPE', 'RMSE']\n",
    "    performance = forecast_evaluation_table(y_true_plot, y_hat_plot)\n",
    "    performance_df['nbeatsx'] += performance.measure.to_list()\n",
    "    \n",
    "    # performance = np.round(mae(y_true_plot, y_hat_plot), 5)\n",
    "    # plt.plot(range(len(y_true_plot)), y_true_plot, label='true', alpha=0.7)\n",
    "    # plt.plot(range(len(y_hat_plot)), y_hat_plot, label='pred', alpha=0.7)\n",
    "    # plt.title(f\"Testing predictions {dataset} \\n all lead time  MAE={performance}\")\n",
    "    # plt.xlabel('Hour')\n",
    "    # plt.ylabel('Electricity Price')\n",
    "    # plt.legend()\n",
    "    # plt.grid()\n",
    "    # plt.show()\n",
    "    \n",
    "performance_df = pd.DataFrame(performance_df)\n",
    "benchmark_df = pd.DataFrame({'id': [1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4],\n",
    "                             'unique_id': ['NP', 'NP', 'NP', 'NP', 'PJM', 'PJM', 'PJM', 'PJM',\n",
    "                                           'BE', 'BE', 'BE', 'BE', 'FR', 'FR', 'FR', 'FR'],\n",
    "                             'metric': ['MAE', 'MAPE', 'SMAPE', 'RMSE', 'MAE', 'MAPE', 'SMAPE', 'RMSE',\n",
    "                                        'MAE', 'MAPE', 'SMAPE', 'RMSE', 'MAE', 'MAPE', 'SMAPE', 'RMSE'],    \n",
    "                             'dnn' : [1.67, 5.38, 4.85, 3.33, 2.78, 28.66, 11.22, 4.64, 5.82,\n",
    "                                      26.11, 13.33, 16.13, 3.91, 14.77, 10.98, 11.74]})\n",
    "\n",
    "benchmark_df.sort_values(['id'], inplace=True)\n",
    "benchmark_df.reset_index(drop=True, inplace=True)\n",
    "benchmark_df = benchmark_df.merge(performance_df, on=['unique_id', 'metric'], how='left')\n",
    "benchmark_df['perc_diff'] = 100 * (benchmark_df['nbeatsx']-benchmark_df['dnn'])/benchmark_df['dnn']\n",
    "benchmark_df['improvement'] = benchmark_df['perc_diff'] < 0\n",
    "benchmark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nixtla.data.scalers import Scaler\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset\n",
    "# from nixtla.data.tsloader_fast import TimeSeriesLoader\n",
    "from nixtla.data.tsloader_general import TimeSeriesLoader\n",
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "\n",
    "def get_last_n_hours_weights_df(Y_df, n_hours):\n",
    "    # Creates outsample_mask\n",
    "    # train 1 validation 0\n",
    "\n",
    "    last_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    last_df.sort_values(by=['unique_id', 'ds'], inplace=True, ascending=False)\n",
    "    last_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    last_df = last_df.groupby('unique_id').head(n_hours)\n",
    "    last_df['weights'] = 1\n",
    "\n",
    "    last_df = last_df[['unique_id', 'ds', 'weights']]\n",
    "\n",
    "    weights_df = Y_df.merge(last_df, on=['unique_id', 'ds'], how='left')\n",
    "    weights_df['weights'] = weights_df['weights'].fillna(0)    # The first len(Y)-n_hours used as train\n",
    "\n",
    "    weights_df = weights_df[['unique_id', 'ds', 'weights']]\n",
    "    weights_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "\n",
    "    assert len(weights_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(weights_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "\n",
    "    return weights_df\n",
    "\n",
    "def balance_data(Y_df, Xt_df):\n",
    "    # Train Validation splits\n",
    "    #                        ds                    \n",
    "    #                       min                 max\n",
    "    # unique_id mask                               \n",
    "    # BE        0.0  2013-01-04 2015-01-03 23:00:00\n",
    "    #           1.0  2011-01-09 2013-01-03 23:00:00\n",
    "    # FR        0.0  2013-01-04 2015-01-03 23:00:00\n",
    "    #           1.0  2011-01-09 2013-01-03 23:00:00\n",
    "    # NP        0.0  2014-12-28 2016-12-26 23:00:00\n",
    "    #           1.0  2013-01-01 2014-12-27 23:00:00\n",
    "    # PJM       0.0  2014-12-28 2016-12-26 23:00:00\n",
    "    #           1.0  2013-01-01 2014-12-27 23:00:00\n",
    "\n",
    "    # Create balanced placeholder dataframe\n",
    "    balance_ids = {'unique_id': Y_df.unique_id.unique(),\n",
    "                   'ds': Y_df.ds.unique()}\n",
    "\n",
    "    product_list = list(itertools.product(*list(balance_ids.values())))\n",
    "    balance_df = pd.DataFrame(product_list, columns=list(balance_ids.keys()))\n",
    "\n",
    "    # Create mask for weighted losses for the las 2 years of each unique_id\n",
    "    weights_df = get_last_n_hours_weights_df(Y_df=Y_df, n_hours=2*365*24)\n",
    "    \n",
    "    # Balance with merge\n",
    "    Y_balanced_df = balance_df.merge(Y_df, on=['unique_id', 'ds'], how='left')\n",
    "    Xt_balanced_df = balance_df.merge(Xt_df, on=['unique_id', 'ds'], how='left')\n",
    "    weights_balanced_df = balance_df.merge(weights_df, on=['unique_id', 'ds'], how='left')\n",
    "    print(weights_balanced_df.groupby(['unique_id', 'weights']).agg({'ds': ['min', 'max']}))\n",
    "\n",
    "    # Create insample_mask\n",
    "    # TODO: Include not null exogenous condition to available mask\n",
    "    mask_df = Y_balanced_df[['unique_id', 'ds', 'y']].copy()\n",
    "    mask_df['available_mask'] = (1-Y_balanced_df.y.isnull().values)\n",
    "    del mask_df['y']\n",
    "\n",
    "    mask_df['sample_mask1'] = (mask_df['ds'] <= '2013-01-03 23:00:00') * 1\n",
    "    mask_df['sample_mask2'] = (mask_df['ds'] <= '2014-12-27 23:00:00') * 1\n",
    "    print(mask_df.groupby(['unique_id', 'sample_mask1']).agg({'ds': ['min', 'max']}))\n",
    "    print(mask_df.groupby(['unique_id', 'sample_mask2']).agg({'ds': ['min', 'max']}))\n",
    "\n",
    "    print('\\n')\n",
    "    print('Y_df.shape \\t\\t', Y_df.shape)\n",
    "    print('Xt_df.shape \\t\\t', Xt_df.shape)\n",
    "    print('Y_balanced_df.shape \\t', Y_balanced_df.shape)\n",
    "    print('Xt_balanced_df.shape \\t', Xt_balanced_df.shape)\n",
    "    print('mask_df.shape \\t\\t', mask_df.shape)\n",
    "\n",
    "    return Y_balanced_df, Xt_balanced_df, mask_df, weights_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "run_val_nbeatsx() got an unexpected keyword argument 'val_ds'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-94267e042850>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m# model.fit(train_ts_loader=train_loader, val_ts_loader=val_loader, verbose=True, eval_steps=10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_val_nbeatsx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mS_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: run_val_nbeatsx() got an unexpected keyword argument 'val_ds'"
     ]
    }
   ],
   "source": [
    "args = pd.Series({'dataset': ['NP', 'PJM', 'BE', 'FR']})\n",
    "\n",
    "Y_df, Xt_df, S_df = EPF.load_groups(directory='data', groups=args.dataset)\n",
    "# Y_df, Xt_df, mask_df, weights_df = balance_data(Y_df=Y_df, Xt_df=Xt_df)\n",
    "\n",
    "# mask_df['sample_mask'] = mask_df['sample_mask1']\n",
    "# Xt_scaled_df = scale_data(Xt_df=Xt_df, mask_df=mask_df)\n",
    "# train_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=S_df, X_df=Xt_scaled_df, mask_df=mask_df)\n",
    "\n",
    "# mask_df['sample_mask'] = (1-mask_df['sample_mask1'])\n",
    "# val_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=S_df, X_df=Xt_scaled_df, mask_df=mask_df)\n",
    "\n",
    "# train_loader = TimeSeriesLoader(ts_dataset=train_dataset,\n",
    "#                                 model='nbeats',\n",
    "#                                 offset=0, #offset,\n",
    "#                                 window_sampling_limit=int(mc['window_sampling_limit']), \n",
    "#                                 input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "#                                 output_size=int(mc['output_size']),\n",
    "#                                 idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "#                                 batch_size=int(mc['batch_size']),\n",
    "#                                 is_train_loader=True, # Samples all available_mask + sample_mask\n",
    "#                                 shuffle=True)\n",
    "\n",
    "# val_loader = TimeSeriesLoader(ts_dataset=val_dataset,\n",
    "#                                 model='nbeats',\n",
    "#                                 offset=0, #offset,\n",
    "#                                 window_sampling_limit=int(mc['window_sampling_limit']), \n",
    "#                                 input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "#                                 output_size=int(mc['output_size']),\n",
    "#                                 idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "#                                 batch_size=1024,\n",
    "#                                 is_train_loader=False, # Samples all sample_mask\n",
    "#                                 shuffle=False)\n",
    "\n",
    "# #-------------------------------------- Hyperparameter Optimization --------------------------------------#\n",
    "\n",
    "# model = Nbeats(input_size_multiplier=int(mc['input_size_multiplier']),\n",
    "#                 output_size=int(mc['output_size']),\n",
    "#                 shared_weights=int(mc['shared_weights']),\n",
    "#                 activation=mc['activation'],\n",
    "#                 initialization=mc['initialization'],\n",
    "#                 stack_types=mc['stack_types'], #2*['identity'],\n",
    "#                 n_blocks=mc['n_blocks'], #2*[1],\n",
    "#                 n_layers=mc['n_layers'], #2*[2],\n",
    "#                 n_hidden=mc['n_hidden'], #2*[[256,256]],\n",
    "#                 n_polynomials=mc['n_polynomials'], #2,\n",
    "#                 n_harmonics=int(mc['n_harmonics']), #1,\n",
    "#                 exogenous_n_channels=int(mc['exogenous_n_channels']), #9,\n",
    "#                 include_var_dict={'y': [-2, -3, -8],\n",
    "#                                     'Exogenous1': [-1, -2, -8],\n",
    "#                                     'Exogenous2': [-1, -2, -8],\n",
    "#                                     'week_day': [-1]},\n",
    "#                 t_cols=train_dataset.t_cols,\n",
    "#                 batch_normalization=mc['batch_normalization'], #False,\n",
    "#                 dropout_prob_theta=float(mc['dropout_prob_theta']), #0.01,\n",
    "#                 dropout_prob_exogenous=float(mc['dropout_prob_exogenous']), #0.01,\n",
    "#                 x_s_n_hidden=int(mc['x_s_n_hidden']), #0,\n",
    "#                 learning_rate=float(mc['learning_rate']), #0.007,\n",
    "#                 lr_decay=float(mc['lr_decay']), #0.5,\n",
    "#                 n_lr_decay_steps=int(mc['n_lr_decay_steps']), #3,\n",
    "#                 weight_decay=float(mc['weight_decay']), #0.0000001,\n",
    "#                 l1_theta=float(mc['l1_theta']), #0.0001,\n",
    "#                 n_iterations=int(mc['n_iterations']), #200,\n",
    "#                 early_stopping=int(mc['early_stopping']), #40,\n",
    "#                 loss=mc['loss'], #'PINBALL',\n",
    "#                 loss_hypar=float(mc['loss_hypar']), #0.5,\n",
    "#                 val_loss=mc['val_loss'], #MAE\n",
    "#                 frequency=mc['frequency'], #'H',\n",
    "#                 random_seed=int(mc['random_seed']), #1,\n",
    "#                 seasonality=int(mc['seasonality'])) #24)\n",
    "\n",
    "# model.fit(train_ts_loader=train_loader, val_ts_loader=val_loader, verbose=True, eval_steps=10)\n",
    "\n",
    "model, train_loader, val_loader = run_val_nbeatsx(mc, Y_df=Y_df, Xt_df=Xt_df, S_df=S_df, val_ds=365*24*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c68c0f618c4f787b1e02bfee278e48d25b62407cb335aab5258cedc4db4ced"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}