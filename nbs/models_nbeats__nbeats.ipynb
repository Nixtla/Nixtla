{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.nbeats.nbeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "import torch as t\n",
    "from torch import optim\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "from nixtla.models.nbeats.nbeats_model import NBeats, NBeatsBlock, IdentityBasis, TrendBasis, SeasonalityBasis\n",
    "from nixtla.models.nbeats.nbeats_model import ExogenousBasisInterpretable, ExogenousBasisWavenet, ExogenousBasisTCN\n",
    "from nixtla.losses.pytorch import MAPELoss, MASELoss, SMAPELoss, MSELoss, MAELoss, PinballLoss\n",
    "from nixtla.losses.numpy import mae, mse, mape, smape, rmse, pinball_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_weights(module, initialization):\n",
    "    if type(module) == t.nn.Linear:\n",
    "        if initialization == 'orthogonal':\n",
    "            t.nn.init.orthogonal_(module.weight)\n",
    "        elif initialization == 'he_uniform':\n",
    "            t.nn.init.kaiming_uniform_(module.weight)\n",
    "        elif initialization == 'he_normal':\n",
    "            t.nn.init.kaiming_normal_(module.weight)\n",
    "        elif initialization == 'glorot_uniform':\n",
    "            t.nn.init.xavier_uniform_(module.weight)\n",
    "        elif initialization == 'glorot_normal':\n",
    "            t.nn.init.xavier_normal_(module.weight)\n",
    "        elif initialization == 'lecun_normal':\n",
    "            pass #t.nn.init.normal_(module.weight, 0.0, std=1/np.sqrt(module.weight.numel()))\n",
    "        else:\n",
    "            assert 1<0, f'Initialization {initialization} not found'\n",
    "\n",
    "class Nbeats(object):\n",
    "    \"\"\"\n",
    "    Future documentation\n",
    "    \"\"\"\n",
    "    SEASONALITY_BLOCK = 'seasonality'\n",
    "    TREND_BLOCK = 'trend'\n",
    "    IDENTITY_BLOCK = 'identity'\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size_multiplier,\n",
    "                 output_size,\n",
    "                 shared_weights,\n",
    "                 activation,\n",
    "                 initialization,\n",
    "                 stack_types,\n",
    "                 n_blocks,\n",
    "                 n_layers,\n",
    "                 n_hidden,\n",
    "                 n_harmonics,\n",
    "                 n_polynomials,\n",
    "                 exogenous_n_channels,\n",
    "                 include_var_dict,\n",
    "                 t_cols,\n",
    "                 batch_normalization,\n",
    "                 dropout_prob_theta,\n",
    "                 dropout_prob_exogenous,\n",
    "                 x_s_n_hidden,\n",
    "                 learning_rate,\n",
    "                 lr_decay,\n",
    "                 n_lr_decay_steps,\n",
    "                 weight_decay,\n",
    "                 l1_theta,\n",
    "                 n_iterations,\n",
    "                 early_stopping,\n",
    "                 loss,\n",
    "                 loss_hypar,\n",
    "                 val_loss,\n",
    "                 frequency,\n",
    "                 random_seed,\n",
    "                 seasonality,\n",
    "                 device=None):\n",
    "        super(Nbeats, self).__init__()\n",
    "\n",
    "        if activation == 'selu': initialization = 'lecun_normal'\n",
    "\n",
    "        #------------------------ Model Attributes ------------------------#\n",
    "        # Architecture parameters\n",
    "        self.input_size = int(input_size_multiplier*output_size)\n",
    "        self.output_size = output_size\n",
    "        self.shared_weights = shared_weights\n",
    "        self.activation = activation\n",
    "        self.initialization = initialization\n",
    "        self.stack_types = stack_types\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_harmonics = n_harmonics\n",
    "        self.n_polynomials = n_polynomials\n",
    "        self.exogenous_n_channels = exogenous_n_channels\n",
    "\n",
    "        # Regularization and optimization parameters\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.dropout_prob_theta = dropout_prob_theta\n",
    "        self.dropout_prob_exogenous = dropout_prob_exogenous\n",
    "        self.x_s_n_hidden = x_s_n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.n_lr_decay_steps = n_lr_decay_steps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.n_iterations = n_iterations\n",
    "        self.early_stopping = early_stopping\n",
    "        self.loss = loss\n",
    "        self.loss_hypar = loss_hypar\n",
    "        self.val_loss = val_loss\n",
    "        self.l1_theta = l1_theta\n",
    "        self.l1_conv = 1e-3 # Not a hyperparameter\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Data parameters\n",
    "        self.frequency = frequency\n",
    "        self.seasonality = seasonality        \n",
    "        self.include_var_dict = include_var_dict\n",
    "        self.t_cols = t_cols\n",
    "        #self.scaler = scaler\n",
    "\n",
    "        if device is None:\n",
    "            device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
    "        self.device = device\n",
    "\n",
    "        self._is_instantiated = False\n",
    "\n",
    "    def create_stack(self):\n",
    "        if self.include_var_dict is not None:\n",
    "            x_t_n_inputs = self.output_size * int(sum([len(x) for x in self.include_var_dict.values()]))\n",
    "            \n",
    "            # Correction because week_day only adds 1 no output_size\n",
    "            if len(self.include_var_dict['week_day'])>0:\n",
    "                x_t_n_inputs = x_t_n_inputs - self.output_size + 1 \n",
    "        else:\n",
    "            x_t_n_inputs = self.input_size\n",
    "        \n",
    "        #------------------------ Model Definition ------------------------#\n",
    "        block_list = []\n",
    "        self.blocks_regularizer = []\n",
    "        for i in range(len(self.stack_types)):\n",
    "            #print(f'| --  Stack {self.stack_types[i]} (#{i})')\n",
    "            for block_id in range(self.n_blocks[i]):\n",
    "                \n",
    "                # Batch norm only on first block\n",
    "                if (len(block_list)==0) and (self.batch_normalization):\n",
    "                    batch_normalization_block = True\n",
    "                else:\n",
    "                    batch_normalization_block = False\n",
    "                \n",
    "                # Dummy of regularizer in block. Override with 1 if exogenous_block\n",
    "                self.blocks_regularizer += [0]\n",
    "\n",
    "                # Shared weights\n",
    "                if self.shared_weights and block_id>0:\n",
    "                    nbeats_block = block_list[-1]\n",
    "                else:\n",
    "                    if self.stack_types[i] == 'seasonality':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=4 * int(\n",
    "                                                        np.ceil(self.n_harmonics / 2 * self.output_size) - (self.n_harmonics - 1)),\n",
    "                                                   basis=SeasonalityBasis(harmonics=self.n_harmonics,\n",
    "                                                                          backcast_size=self.input_size,\n",
    "                                                                          forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'trend':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2 * (self.n_polynomials + 1),\n",
    "                                                   basis=TrendBasis(degree_of_polynomial=self.n_polynomials,\n",
    "                                                                            backcast_size=self.input_size,\n",
    "                                                                            forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'identity':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=self.input_size + self.output_size,\n",
    "                                                   basis=IdentityBasis(backcast_size=self.input_size,\n",
    "                                                                       forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2*self.n_x_t,\n",
    "                                                   basis=ExogenousBasisInterpretable(),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous_tcn':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden = self.x_s_n_hidden,\n",
    "                                                   theta_n_dim = 2*(self.exogenous_n_channels),\n",
    "                                                   basis= ExogenousBasisTCN(self.exogenous_n_channels, self.n_x_t),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous_wavenet':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2*(self.exogenous_n_channels),\n",
    "                                                   basis=ExogenousBasisWavenet(self.exogenous_n_channels, self.n_x_t),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                        self.blocks_regularizer[-1] = 1\n",
    "                    else:\n",
    "                        assert 1<0, f'Block type not found!'\n",
    "                # Select type of evaluation and apply it to all layers of block\n",
    "                init_function = partial(init_weights, initialization=self.initialization)                                             \n",
    "                nbeats_block.layers.apply(init_function)\n",
    "                #print(f'     | -- {nbeats_block}')\n",
    "                block_list.append(nbeats_block)\n",
    "        return block_list\n",
    "\n",
    "    def __loss_fn(self, loss_name: str):\n",
    "        def loss(x, loss_hypar, forecast, target, mask):\n",
    "            if loss_name == 'MAPE':\n",
    "                return MAPELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'MASE':\n",
    "                return MASELoss(y=target, y_hat=forecast, y_insample=x, seasonality=loss_hypar, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return SMAPELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'MSE':\n",
    "                return MSELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'MAE':\n",
    "                return MAELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'PINBALL':\n",
    "                return PinballLoss(y=target, y_hat=forecast, mask=mask, tau=loss_hypar) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def __val_loss_fn(self, loss_name='MAE'):\n",
    "        #TODO: mase not implemented\n",
    "        def loss(forecast, target, weights):\n",
    "            if loss_name == 'MAPE':\n",
    "                return mape(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return smape(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MSE':\n",
    "                return mse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'RMSE':\n",
    "                return rmse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MAE':\n",
    "                return mae(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'PINBALL':\n",
    "                return pinball_loss(y=target, y_hat=forecast, weights=weights, tau=0.5)\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "    \n",
    "    def loss_l1_conv_layers(self):\n",
    "        loss_l1 = 0\n",
    "        for i, indicator in enumerate(self.blocks_regularizer):\n",
    "            if indicator:\n",
    "                loss_l1 += self.l1_conv * t.sum(t.abs(self.model.blocks[i].basis.weight))\n",
    "        return loss_l1\n",
    "    \n",
    "    def loss_l1_theta(self):\n",
    "        loss_l1 = 0\n",
    "        for block in self.model.blocks:\n",
    "            for layer in block.modules():\n",
    "                if isinstance(layer, t.nn.Linear):\n",
    "                    loss_l1 += self.l1_theta * layer.weight.abs().sum()\n",
    "        return loss_l1\n",
    "\n",
    "    def to_tensor(self, x: np.ndarray) -> t.Tensor:\n",
    "        tensor = t.as_tensor(x, dtype=t.float32).to(self.device)\n",
    "        return tensor\n",
    "\n",
    "    def evaluate_performance(self, ts_loader, validation_loss_fn):\n",
    "        self.model.eval()\n",
    "\n",
    "        #losses = []\n",
    "        forecasts = []\n",
    "        outsample_ys = []\n",
    "        outsample_masks = []\n",
    "        with t.no_grad():\n",
    "            for batch in iter(ts_loader):\n",
    "                # Parse batch\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                forecast   = self.model(x_s=s_matrix, insample_y=insample_y, \n",
    "                                        insample_x_t=insample_x, outsample_x_t=outsample_x,\n",
    "                                        insample_mask=insample_mask)\n",
    "                #batch_loss = validation_loss_fn(target=outsample_y.cpu().data.numpy(),\n",
    "                #                                forecast=forecast.cpu().data.numpy(),\n",
    "                #                                weights=outsample_mask.cpu().data.numpy())\n",
    "                #losses.append(batch_loss)\n",
    "                forecasts.append(forecast.cpu().data.numpy())\n",
    "                outsample_ys.append(batch['outsample_y'])\n",
    "                outsample_masks.append(batch['outsample_mask'])\n",
    "\n",
    "        forecasts = np.vstack(forecasts)\n",
    "        outsample_ys = np.vstack(outsample_ys)\n",
    "        outsample_masks = np.vstack(outsample_masks)\n",
    "        #loss = np.mean(losses)\n",
    "\n",
    "        complete_loss = validation_loss_fn(target=outsample_ys,\n",
    "                                           forecast=forecasts,\n",
    "                                           weights=outsample_masks)\n",
    "\n",
    "        self.model.train()\n",
    "        return complete_loss\n",
    "\n",
    "    def fit(self, train_ts_loader, val_ts_loader=None, n_iterations=None, verbose=True, eval_steps=1):\n",
    "        # TODO: Indexes hardcoded, information duplicated in train and val datasets\n",
    "        assert (self.input_size)==train_ts_loader.input_size, \\\n",
    "            f'model input_size {self.input_size} data input_size {train_ts_loader.input_size}'\n",
    "\n",
    "        # Random Seeds (model initialization)\n",
    "        t.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed) #TODO: interaccion rara con window_sampling de validacion\n",
    "\n",
    "        # Attributes of ts_dataset\n",
    "        self.n_x_t, self.n_x_s = train_ts_loader.get_n_variables()\n",
    "\n",
    "        # Instantiate model\n",
    "        if not self._is_instantiated:\n",
    "            block_list = self.create_stack()\n",
    "            self.model = NBeats(t.nn.ModuleList(block_list)).to(self.device)\n",
    "            self._is_instantiated = True\n",
    "\n",
    "        # Overwrite n_iterations and train datasets\n",
    "        if n_iterations is None:\n",
    "            n_iterations = self.n_iterations\n",
    "\n",
    "        lr_decay_steps = n_iterations // self.n_lr_decay_steps\n",
    "        if lr_decay_steps == 0:\n",
    "            lr_decay_steps = 1\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_steps, gamma=self.lr_decay)\n",
    "        training_loss_fn = self.__loss_fn(self.loss)\n",
    "        validation_loss_fn = self.__val_loss_fn(self.val_loss) #Uses numpy losses\n",
    "\n",
    "        print('='*30+' Start fitting '+'='*30)\n",
    "\n",
    "        #self.loss_dict = {} # Restart self.loss_dict\n",
    "        start = time.time()\n",
    "        self.trajectories = {'iteration':[],'train_loss':[], 'val_loss':[]}\n",
    "        self.final_insample_loss = None\n",
    "        self.final_outsample_loss = None\n",
    "        \n",
    "        # Training Loop\n",
    "        early_stopping_counter = 0\n",
    "        best_val_loss = np.inf\n",
    "        best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "        break_flag = False\n",
    "        iteration = 0\n",
    "        epoch = 0\n",
    "        while (iteration < n_iterations) and (not break_flag):\n",
    "            epoch +=1\n",
    "            for batch in iter(train_ts_loader):\n",
    "                iteration += 1\n",
    "                if (iteration > n_iterations) or (break_flag):\n",
    "                    continue\n",
    "\n",
    "                self.model.train()\n",
    "                # Parse batch\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                forecast   = self.model(x_s=s_matrix, insample_y=insample_y, \n",
    "                                        insample_x_t=insample_x, outsample_x_t=outsample_x,\n",
    "                                        insample_mask=insample_mask)\n",
    "\n",
    "                training_loss = training_loss_fn(x=insample_y, loss_hypar=self.loss_hypar, forecast=forecast,\n",
    "                                                 target=outsample_y, mask=outsample_mask)\n",
    "\n",
    "                if np.isnan(float(training_loss)):\n",
    "                    break\n",
    "\n",
    "                training_loss.backward()\n",
    "                t.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "                if (iteration % eval_steps == 0):\n",
    "                    display_string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(iteration,\n",
    "                                                                                            time.time()-start,\n",
    "                                                                                            self.loss,\n",
    "                                                                                            training_loss.cpu().data.numpy())\n",
    "                    self.trajectories['iteration'].append(iteration)\n",
    "                    self.trajectories['train_loss'].append(np.float(training_loss.cpu().data.numpy()))\n",
    "\n",
    "                    if val_ts_loader is not None:\n",
    "                        loss = self.evaluate_performance(ts_loader=val_ts_loader, \n",
    "                                                         validation_loss_fn=validation_loss_fn)\n",
    "                        display_string += \", Outsample {}: {:.5f}\".format(self.val_loss, loss)\n",
    "                        self.trajectories['val_loss'].append(loss)\n",
    "\n",
    "                        if self.early_stopping:\n",
    "                            if loss < best_val_loss:\n",
    "                                # Save current model if improves outsample loss\n",
    "                                best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "                                best_insample_loss = training_loss.cpu().data.numpy()\n",
    "                                early_stopping_counter = 0\n",
    "                                best_val_loss = loss\n",
    "                            else:\n",
    "                                early_stopping_counter += 1\n",
    "                            if early_stopping_counter >= self.early_stopping:\n",
    "                                break_flag = True\n",
    "                    \n",
    "                    print(display_string)\n",
    "\n",
    "                    self.model.train()\n",
    "\n",
    "                if break_flag:\n",
    "                    print('\\n')\n",
    "                    print(19*'-',' Stopped training by early stopping', 19*'-')\n",
    "                    self.model.load_state_dict(best_state_dict)\n",
    "                    break\n",
    "\n",
    "        #End of fitting\n",
    "        if n_iterations >0:\n",
    "            # This is batch loss!\n",
    "            self.final_insample_loss = np.float(training_loss.cpu().data.numpy()) if not break_flag else best_insample_loss \n",
    "            string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(iteration,\n",
    "                                                                            time.time()-start,\n",
    "                                                                            self.loss,\n",
    "                                                                            self.final_insample_loss)\n",
    "            if val_ts_loader is not None:\n",
    "                self.final_outsample_loss = self.evaluate_performance(ts_loader=val_ts_loader, \n",
    "                                                                      validation_loss_fn=validation_loss_fn)\n",
    "                string += \", Outsample {}: {:.5f}\".format(self.val_loss, self.final_outsample_loss)\n",
    "            print(string)\n",
    "            print('='*30+'  End fitting  '+'='*30)\n",
    "            print('\\n')\n",
    "\n",
    "    def predict(self, ts_loader, X_test=None, eval_mode=False):\n",
    "        self.model.eval()\n",
    "        assert not ts_loader.shuffle, 'ts_loader must have shuffle as False.'\n",
    "\n",
    "        forecasts = []\n",
    "        outsample_ys = []\n",
    "        outsample_masks = []\n",
    "        with t.no_grad():\n",
    "            for batch in iter(ts_loader):\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                forecast = self.model(insample_y=insample_y, insample_x_t=insample_x,\n",
    "                                      insample_mask=insample_mask, outsample_x_t=outsample_x, x_s=s_matrix)\n",
    "                forecasts.append(forecast.cpu().data.numpy())\n",
    "                outsample_ys.append(batch['outsample_y'])\n",
    "                outsample_masks.append(batch['outsample_mask'])\n",
    "        forecasts = np.vstack(forecasts)\n",
    "        outsample_ys = np.vstack(outsample_ys)\n",
    "        outsample_masks = np.vstack(outsample_masks)\n",
    "\n",
    "        self.model.train()\n",
    "        if eval_mode:\n",
    "            return outsample_ys, forecasts, outsample_masks\n",
    "\n",
    "        # Pandas wrangling\n",
    "        frequency = ts_loader.get_frequency()\n",
    "        unique_ids = ts_loader.get_meta_data_col('unique_id')\n",
    "        last_ds = ts_loader.get_meta_data_col('last_ds') #TODO: ajustar of offset\n",
    "\n",
    "        # Predictions for panel\n",
    "        Y_hat_panel = pd.DataFrame(columns=['unique_id', 'ds'])\n",
    "        for i, unique_id in enumerate(unique_ids):\n",
    "            Y_hat_id = pd.DataFrame([unique_id]*self.output_size, columns=[\"unique_id\"])\n",
    "            ds = pd.date_range(start=last_ds[i], periods=self.output_size+1, freq=frequency)\n",
    "            Y_hat_id[\"ds\"] = ds[1:]\n",
    "            Y_hat_panel = Y_hat_panel.append(Y_hat_id, sort=False).reset_index(drop=True)\n",
    "\n",
    "        Y_hat_panel['y_hat'] = forecasts.flatten()\n",
    "\n",
    "        if X_test is not None:\n",
    "            Y_hat_panel = X_test.merge(Y_hat_panel, on=['unique_id', 'ds'], how='left')\n",
    "        \n",
    "        return Y_hat_panel\n",
    "\n",
    "\n",
    "    def save(self, model_dir, model_id, state_dict = None):\n",
    "    \n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        if state_dict is None:\n",
    "            state_dict = self.model.state_dict()\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        print('Saving model to:\\n {}'.format(model_file)+'\\n')\n",
    "        t.save({'model_state_dict': state_dict}, model_file)\n",
    "\n",
    "    def load(self, model_dir, model_id):\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        path = Path(model_file)\n",
    "\n",
    "        assert path.is_file(), 'No model_*.model file found in this path!'\n",
    "\n",
    "        print('Loading model from:\\n {}'.format(model_file)+'\\n')\n",
    "\n",
    "        checkpoint = t.load(model_file, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(self.device)"
   ]
  },
  {
   "source": [
    "# SINGLE TIME SERIES TEST"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nixtla.data.scalers import Scaler\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset as TimeSeriesDataset\n",
    "from nixtla.data.tsloader_fast import TimeSeriesLoader\n",
    "#from nixtla.data.tsloader_general import TimeSeriesLoader\n",
    "\n",
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "\n",
    "# Hacked MAE NP hypars\n",
    "hacked_np = {'input_size_multiplier': 7,\n",
    "             'output_size': 24,\n",
    "             'window_sampling_limit': 365*4*24,\n",
    "             'idx_to_sample_freq': 24,\n",
    "             'batch_size': 256,\n",
    "             'shared_weights': False,\n",
    "             'activation': 'relu',\n",
    "             'initialization': 'he_normal',\n",
    "             'stack_types': ['exogenous_tcn']+1*['identity'],\n",
    "             'n_blocks': 2*[1],\n",
    "             'n_layers': 2*[2],\n",
    "             'n_hidden': 2*[[462,462]],\n",
    "             'n_polynomials': 2,\n",
    "             'n_harmonics': 1,\n",
    "             'exogenous_n_channels': 8,\n",
    "             'include_var_dict': {'y': [-2, -3, -8],\n",
    "                                  'Exogenous1': [-1, -2, -8],\n",
    "                                  'Exogenous2': [-1, -2, -8],\n",
    "                                  'week_day': [-1]},\n",
    "             'batch_normalization': False,\n",
    "             'dropout_prob_theta': 0.05,\n",
    "             'dropout_prob_exogenous': 0.35,\n",
    "             'x_s_n_hidden': 0,\n",
    "             'learning_rate': 0.0016,\n",
    "             'lr_decay': 0.5,\n",
    "             'n_lr_decay_steps': 3,\n",
    "             'weight_decay': 6e-4,\n",
    "             'l1_theta': 1.0e-05,\n",
    "             'n_iterations': 2000, #2000 <-------------\n",
    "             'early_stopping': 40,\n",
    "             'loss': 'PINBALL',\n",
    "             'loss_hypar': 0.49,\n",
    "             'val_loss': 'MAE', #'SMAPE',\n",
    "             'frequency': 'H',\n",
    "             'random_seed': 17,\n",
    "             'seasonality': 24}\n",
    "\n",
    "# Hacked MAE BE hypars\n",
    "hacked_be = {'input_size_multiplier': 7,\n",
    "             'output_size': 24,\n",
    "             'window_sampling_limit': 365*4*24,\n",
    "             'idx_to_sample_freq': 24,\n",
    "             'batch_size': 256,\n",
    "             'shared_weights': False,\n",
    "             'activation': 'relu',\n",
    "             'initialization': 'lecun_normal',\n",
    "             'stack_types': ['exogenous_wavenet']+1*['identity'],\n",
    "             'n_blocks': 2*[1],\n",
    "             'n_layers': 2*[2],\n",
    "             'n_hidden': 2*[[462,462]],\n",
    "             'n_polynomials': 2,\n",
    "             'n_harmonics': 1,\n",
    "             'exogenous_n_channels': 8,\n",
    "             'include_var_dict': {'y': [-2, -3, -8],\n",
    "                                  'Exogenous1': [-1, -2, -8],\n",
    "                                  'Exogenous2': [-1, -2, -8],\n",
    "                                  'week_day': [-1]},\n",
    "             'batch_normalization': False,\n",
    "             'dropout_prob_theta': 0.14,\n",
    "             'dropout_prob_exogenous': 0.08, #<-------- Interesante\n",
    "             'x_s_n_hidden': 0,\n",
    "             'learning_rate': 0.0013,\n",
    "             'lr_decay': 0.5,\n",
    "             'n_lr_decay_steps': 3,\n",
    "             'weight_decay': 0.0017,\n",
    "             'l1_theta': 1.5e-05,\n",
    "             'n_iterations': 200, #2000 <-------------\n",
    "             'early_stopping': 40,\n",
    "             'loss': 'PINBALL',\n",
    "             'loss_hypar': 0.502,\n",
    "             'val_loss': 'MAE', #'MAE',\n",
    "             'frequency': 'H',\n",
    "             'random_seed': 16,\n",
    "             'seasonality': 24}\n",
    "\n",
    "mc = hacked_np\n",
    "#mc = hacked_be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_evaluation_table(y_true, y_hat):\n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_hat = y_hat.reshape(-1)\n",
    "    \n",
    "    #_pinball50 = np.round(pinball_loss(y_true, y_hat, tau=0.5),5)\n",
    "    _mae   = np.round(mae(y_true, y_hat),5)\n",
    "    _mape  = np.round(mape(y_true, y_hat),5)\n",
    "    _smape = np.round(smape(y_true, y_hat),5)\n",
    "    _rmse  = np.round(rmse(y_true, y_hat),5)\n",
    "\n",
    "    performance = pd.DataFrame({'metric': ['mae', 'mape', 'smape', 'rmse'],\n",
    "                                'measure': [_mae, _mape, _smape, _rmse]})                          \n",
    "\n",
    "    return performance\n",
    "\n",
    "def get_last_n_hours_mask_df(Y_df, n_hours):\n",
    "    # Creates outsample_mask\n",
    "    # train 1 validation 0\n",
    "\n",
    "    last_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    last_df.sort_values(by=['unique_id', 'ds'], inplace=True, ascending=False)\n",
    "    last_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    last_df = last_df.groupby('unique_id').head(n_hours)\n",
    "    last_df['mask'] = 1\n",
    "\n",
    "    last_df = last_df[['unique_id', 'ds', 'mask']]\n",
    "\n",
    "    mask_df = Y_df.merge(last_df, on=['unique_id', 'ds'], how='left')\n",
    "    mask_df['mask'] = mask_df['mask'].fillna(0)    # The first len(Y)-n_hours used as train\n",
    "\n",
    "    mask_df = mask_df[['unique_id', 'ds', 'mask']]\n",
    "    mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "\n",
    "    assert len(mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "\n",
    "    return mask_df\n",
    "\n",
    "def scale_data(Xt_df, mask_df):\n",
    "    # TODO: write sample_mask conditional/groupby(['unique_id]) scaling\n",
    "    # Conditional on sample_mask, scale all the Exogenous variables.\n",
    "\n",
    "    # To not modify original data\n",
    "    Xt_scaled_df = Xt_df.copy()\n",
    "\n",
    "    # Transform data with scale transformation\n",
    "    scaler = Scaler(normalizer='norm')\n",
    "    Xt_scaled_df['Exogenous1'] = scaler.scale(x=Xt_scaled_df['Exogenous1'].values, offset=0)\n",
    "\n",
    "    scaler = Scaler(normalizer='norm')\n",
    "    Xt_scaled_df['Exogenous2'] = scaler.scale(x=Xt_scaled_df['Exogenous2'].values, offset=0)\n",
    "\n",
    "    return Xt_scaled_df\n",
    "\n",
    "def run_val_nbeatsx(mc, Y_df, Xt_df, S_df, mask_df): #, trials, trials_file_name):\n",
    "\n",
    "    #---------------------------------------------- Read  Data ----------------------------------------------#\n",
    "\n",
    "    # Scale data # TODO: write sample_mask conditional/groupby(['unique_id]) scaling\n",
    "    Xt_scaled_df = scale_data(Xt_df=Xt_df, mask_df=mask_df)\n",
    "\n",
    "    # # To not modify original data\n",
    "    # Xt_scaled_df = Xt_df.copy()\n",
    "\n",
    "    # # Transform data with scale transformation\n",
    "    # scaler = Scaler(normalizer='norm')\n",
    "    # Xt_scaled_df['Exogenous1'] = scaler.scale(x=Xt_scaled_df['Exogenous1'].values, offset=val_ds*24)\n",
    "\n",
    "    # scaler = Scaler(normalizer='norm')\n",
    "    # Xt_scaled_df['Exogenous2'] = scaler.scale(x=Xt_scaled_df['Exogenous2'].values, offset=val_ds*24)\n",
    "\n",
    "    # #print(f'Dataset: {args.dataset}')\n",
    "    # #print(\"Xt_df.columns\", Xt_df.columns)\n",
    "    # print('X: time series features, of shape (#hours, #times,#features): \\t' + str(Xt_df.shape))\n",
    "    # if S_df is not None:\n",
    "    #     print('S: static features, of shape (#series,#features+unique_id): \\t' + str(S_df.shape))\n",
    "    # print('Y: target series (in X), of shape (#hours, #times): \\t \\t' + str(Y_df.shape))\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    print(\"Train Validation splits\")\n",
    "    n_hours = len(mask_df)\n",
    "    n_avl = mask_df.available_mask.sum()\n",
    "    n_trn = mask_df.sample_mask.sum()\n",
    "    n_prd = len(mask_df)-n_trn\n",
    "    avl_prc = np.round(n_avl/n_hours,5)\n",
    "    trn_prc = np.round(n_trn/n_hours,5)\n",
    "    prd_prc = np.round(n_prd/n_hours,5)\n",
    "    print(mask_df.groupby(['unique_id', 'sample_mask']).agg({'ds': ['min', 'max']}))\n",
    "    print(f'Data available prc = {avl_prc}, \\t{n_avl} hours = {np.round(n_avl/(24*365),3)} years')\n",
    "    print(f'Train prc = {trn_prc}, \\t\\t{n_trn} hours = {np.round(n_trn/(24*365),3)} years')\n",
    "    print(f'Predict prc = {prd_prc}, \\t\\t{n_prd} hours = {np.round(n_prd/(24*365),3)} years')\n",
    "    #Y_df.head()\n",
    "    print('\\n')\n",
    "\n",
    "    train_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=S_df, X_df=Xt_scaled_df, mask_df=mask_df)\n",
    "\n",
    "    mask_df['sample_mask'] = (1-mask_df['sample_mask'])\n",
    "    val_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=S_df, X_df=Xt_scaled_df, mask_df=mask_df)\n",
    "\n",
    "    train_loader = TimeSeriesLoader(ts_dataset=train_dataset,\n",
    "                                    model='nbeats',\n",
    "                                    offset=0, #offset,\n",
    "                                    window_sampling_limit=int(mc['window_sampling_limit']), \n",
    "                                    input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                                    output_size=int(mc['output_size']),\n",
    "                                    idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                    batch_size=int(mc['batch_size']),\n",
    "                                    is_train_loader=True,\n",
    "                                    shuffle=True)\n",
    "\n",
    "    val_loader = TimeSeriesLoader(ts_dataset=val_dataset,\n",
    "                                    model='nbeats',\n",
    "                                    offset=0, #offset,\n",
    "                                    window_sampling_limit=int(mc['window_sampling_limit']), \n",
    "                                    input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                                    output_size=int(mc['output_size']),\n",
    "                                    idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                    batch_size=1024,\n",
    "                                    is_train_loader=False, # Samples the opposite of train_outsample_mask\n",
    "                                    shuffle=False)                                  \n",
    "\n",
    "    #-------------------------------------- Hyperparameter Optimization --------------------------------------#\n",
    "\n",
    "    model = Nbeats(input_size_multiplier=int(mc['input_size_multiplier']),\n",
    "                    output_size=int(mc['output_size']),\n",
    "                    shared_weights=int(mc['shared_weights']),\n",
    "                    activation=mc['activation'],\n",
    "                    initialization=mc['initialization'],\n",
    "                    stack_types=mc['stack_types'], #2*['identity'],\n",
    "                    n_blocks=mc['n_blocks'], #2*[1],\n",
    "                    n_layers=mc['n_layers'], #2*[2],\n",
    "                    n_hidden=mc['n_hidden'], #2*[[256,256]],\n",
    "                    n_polynomials=mc['n_polynomials'], #2,\n",
    "                    n_harmonics=int(mc['n_harmonics']), #1,\n",
    "                    exogenous_n_channels=int(mc['exogenous_n_channels']), #9,\n",
    "                    include_var_dict={'y': [-2, -3, -8],\n",
    "                                        'Exogenous1': [-1, -2, -8],\n",
    "                                        'Exogenous2': [-1, -2, -8],\n",
    "                                        'week_day': [-1]},\n",
    "                    t_cols=train_dataset.t_cols,\n",
    "                    batch_normalization=mc['batch_normalization'], #False,\n",
    "                    dropout_prob_theta=float(mc['dropout_prob_theta']), #0.01,\n",
    "                    dropout_prob_exogenous=float(mc['dropout_prob_exogenous']), #0.01,\n",
    "                    x_s_n_hidden=int(mc['x_s_n_hidden']), #0,\n",
    "                    learning_rate=float(mc['learning_rate']), #0.007,\n",
    "                    lr_decay=float(mc['lr_decay']), #0.5,\n",
    "                    n_lr_decay_steps=int(mc['n_lr_decay_steps']), #3,\n",
    "                    weight_decay=float(mc['weight_decay']), #0.0000001,\n",
    "                    l1_theta=float(mc['l1_theta']), #0.0001,\n",
    "                    n_iterations=int(mc['n_iterations']), #200,\n",
    "                    early_stopping=int(mc['early_stopping']), #40,\n",
    "                    loss=mc['loss'], #'PINBALL',\n",
    "                    loss_hypar=float(mc['loss_hypar']), #0.5,\n",
    "                    val_loss=mc['val_loss'], #MAE\n",
    "                    frequency=mc['frequency'], #'H',\n",
    "                    random_seed=int(mc['random_seed']), #1,\n",
    "                    seasonality=int(mc['seasonality'])) #24)\n",
    "\n",
    "    model.fit(train_ts_loader=train_loader, val_ts_loader=val_loader, verbose=True, eval_steps=10)\n",
    "\n",
    "    print('Best Model Evaluation')\n",
    "    y_true, y_hat, _ = model.predict(ts_loader=val_loader, eval_mode=True)\n",
    "    print(forecast_evaluation_table(y_true, y_hat))\n",
    "\n",
    "    return model, train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Validation splits\n",
      "                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2014-12-28 2016-12-26 23:00:00\n",
      "          1           2013-01-01 2014-12-27 23:00:00\n",
      "Data available prc = 1.0, \t34944 hours = 3.989 years\n",
      "Train prc = 0.49863, \t\t17424 hours = 1.989 years\n",
      "Predict prc = 0.50137, \t\t17520 hours = 2.0 years\n",
      "\n",
      "\n",
      "============================== Start fitting ==============================\n",
      "Step: 10, Time: 1.653, Insample PINBALL: 28.95926, Outsample MAE: 40.45193\n",
      "Step: 20, Time: 3.498, Insample PINBALL: 5.73772, Outsample MAE: 5.67054\n",
      "Step: 30, Time: 5.387, Insample PINBALL: 2.65269, Outsample MAE: 3.21683\n",
      "Step: 40, Time: 7.812, Insample PINBALL: 1.46208, Outsample MAE: 2.32824\n",
      "Step: 50, Time: 10.116, Insample PINBALL: 2.22205, Outsample MAE: 2.85270\n",
      "Step: 60, Time: 11.887, Insample PINBALL: 1.08443, Outsample MAE: 2.23185\n",
      "Step: 70, Time: 13.625, Insample PINBALL: 0.98054, Outsample MAE: 1.83693\n",
      "Step: 80, Time: 15.402, Insample PINBALL: 0.95775, Outsample MAE: 1.77320\n",
      "Step: 90, Time: 17.120, Insample PINBALL: 0.83861, Outsample MAE: 1.77475\n",
      "Step: 100, Time: 18.838, Insample PINBALL: 0.85270, Outsample MAE: 1.72932\n",
      "Step: 110, Time: 20.563, Insample PINBALL: 0.82413, Outsample MAE: 1.67629\n",
      "Step: 120, Time: 22.328, Insample PINBALL: 0.79760, Outsample MAE: 1.67306\n",
      "Step: 130, Time: 24.409, Insample PINBALL: 0.76226, Outsample MAE: 1.67440\n",
      "Step: 140, Time: 26.243, Insample PINBALL: 0.85135, Outsample MAE: 1.75250\n",
      "Step: 150, Time: 28.010, Insample PINBALL: 0.72604, Outsample MAE: 1.65929\n",
      "Step: 160, Time: 29.841, Insample PINBALL: 0.75529, Outsample MAE: 1.63771\n",
      "Step: 170, Time: 31.741, Insample PINBALL: 0.71124, Outsample MAE: 1.64496\n",
      "Step: 180, Time: 33.464, Insample PINBALL: 0.74722, Outsample MAE: 1.85313\n",
      "Step: 190, Time: 35.204, Insample PINBALL: 0.66402, Outsample MAE: 1.59904\n",
      "Step: 200, Time: 36.956, Insample PINBALL: 0.68818, Outsample MAE: 1.60531\n",
      "Step: 210, Time: 38.661, Insample PINBALL: 0.60616, Outsample MAE: 1.68948\n",
      "Step: 220, Time: 40.396, Insample PINBALL: 0.64805, Outsample MAE: 1.59791\n",
      "Step: 230, Time: 42.139, Insample PINBALL: 0.60853, Outsample MAE: 1.63689\n",
      "Step: 240, Time: 43.837, Insample PINBALL: 0.68129, Outsample MAE: 1.62762\n",
      "Step: 250, Time: 45.594, Insample PINBALL: 0.64678, Outsample MAE: 1.61348\n",
      "Step: 260, Time: 47.317, Insample PINBALL: 0.61265, Outsample MAE: 1.56438\n",
      "Step: 270, Time: 49.128, Insample PINBALL: 0.62006, Outsample MAE: 1.60306\n",
      "Step: 280, Time: 51.872, Insample PINBALL: 0.55190, Outsample MAE: 1.60432\n",
      "Step: 290, Time: 54.871, Insample PINBALL: 0.61115, Outsample MAE: 1.57563\n",
      "Step: 300, Time: 57.328, Insample PINBALL: 0.58346, Outsample MAE: 1.65878\n",
      "Step: 310, Time: 59.443, Insample PINBALL: 0.60242, Outsample MAE: 1.57425\n",
      "Step: 320, Time: 61.619, Insample PINBALL: 0.66475, Outsample MAE: 1.70207\n",
      "Step: 330, Time: 64.105, Insample PINBALL: 0.59122, Outsample MAE: 1.58897\n",
      "Step: 340, Time: 66.582, Insample PINBALL: 0.55293, Outsample MAE: 1.59143\n",
      "Step: 350, Time: 69.028, Insample PINBALL: 0.57710, Outsample MAE: 1.74288\n",
      "Step: 360, Time: 71.449, Insample PINBALL: 0.66416, Outsample MAE: 1.73929\n",
      "Step: 370, Time: 73.727, Insample PINBALL: 0.58142, Outsample MAE: 1.57021\n",
      "Step: 380, Time: 75.535, Insample PINBALL: 0.58356, Outsample MAE: 1.61920\n",
      "Step: 390, Time: 77.257, Insample PINBALL: 0.55586, Outsample MAE: 1.60531\n",
      "Step: 400, Time: 78.991, Insample PINBALL: 0.62962, Outsample MAE: 1.62803\n",
      "Step: 410, Time: 81.056, Insample PINBALL: 0.61188, Outsample MAE: 1.55022\n",
      "Step: 420, Time: 83.371, Insample PINBALL: 0.59442, Outsample MAE: 1.71855\n",
      "Step: 430, Time: 85.791, Insample PINBALL: 0.67707, Outsample MAE: 1.57618\n",
      "Step: 440, Time: 87.622, Insample PINBALL: 0.53901, Outsample MAE: 1.55098\n",
      "Step: 450, Time: 89.453, Insample PINBALL: 0.69497, Outsample MAE: 1.57754\n",
      "Step: 460, Time: 91.308, Insample PINBALL: 0.52364, Outsample MAE: 1.58524\n",
      "Step: 470, Time: 93.122, Insample PINBALL: 0.50210, Outsample MAE: 1.54557\n",
      "Step: 480, Time: 95.364, Insample PINBALL: 0.55848, Outsample MAE: 1.65708\n",
      "Step: 490, Time: 97.107, Insample PINBALL: 0.50631, Outsample MAE: 1.56332\n",
      "Step: 500, Time: 98.837, Insample PINBALL: 0.47858, Outsample MAE: 1.57113\n",
      "Step: 510, Time: 100.553, Insample PINBALL: 0.58045, Outsample MAE: 1.63208\n",
      "Step: 520, Time: 102.289, Insample PINBALL: 0.51589, Outsample MAE: 1.56861\n",
      "Step: 530, Time: 104.021, Insample PINBALL: 0.51287, Outsample MAE: 1.56513\n",
      "Step: 540, Time: 105.761, Insample PINBALL: 0.57813, Outsample MAE: 1.58209\n",
      "Step: 550, Time: 107.498, Insample PINBALL: 0.60108, Outsample MAE: 1.61730\n",
      "Step: 560, Time: 109.227, Insample PINBALL: 0.62325, Outsample MAE: 1.63812\n",
      "Step: 570, Time: 110.930, Insample PINBALL: 0.57839, Outsample MAE: 1.59407\n",
      "Step: 580, Time: 112.664, Insample PINBALL: 0.54500, Outsample MAE: 1.59105\n",
      "Step: 590, Time: 114.391, Insample PINBALL: 0.50960, Outsample MAE: 1.56820\n",
      "Step: 600, Time: 116.105, Insample PINBALL: 0.54552, Outsample MAE: 1.57787\n",
      "Step: 610, Time: 117.849, Insample PINBALL: 0.57452, Outsample MAE: 1.60218\n",
      "Step: 620, Time: 119.578, Insample PINBALL: 0.59106, Outsample MAE: 1.64614\n",
      "Step: 630, Time: 121.287, Insample PINBALL: 0.53743, Outsample MAE: 1.64412\n",
      "Step: 640, Time: 123.027, Insample PINBALL: 0.54754, Outsample MAE: 1.55374\n",
      "Step: 650, Time: 124.749, Insample PINBALL: 0.55698, Outsample MAE: 1.57793\n",
      "Step: 660, Time: 126.461, Insample PINBALL: 0.58430, Outsample MAE: 1.55907\n",
      "Step: 670, Time: 128.190, Insample PINBALL: 0.58280, Outsample MAE: 1.53144\n",
      "Step: 680, Time: 129.929, Insample PINBALL: 0.48885, Outsample MAE: 1.54589\n",
      "Step: 690, Time: 131.630, Insample PINBALL: 0.47529, Outsample MAE: 1.58624\n",
      "Step: 700, Time: 133.359, Insample PINBALL: 0.50503, Outsample MAE: 1.57606\n",
      "Step: 710, Time: 135.116, Insample PINBALL: 0.47074, Outsample MAE: 1.54583\n",
      "Step: 720, Time: 136.826, Insample PINBALL: 0.46325, Outsample MAE: 1.57338\n",
      "Step: 730, Time: 138.552, Insample PINBALL: 0.46536, Outsample MAE: 1.55701\n",
      "Step: 740, Time: 140.279, Insample PINBALL: 0.44485, Outsample MAE: 1.57273\n",
      "Step: 750, Time: 141.978, Insample PINBALL: 0.43536, Outsample MAE: 1.56686\n",
      "Step: 760, Time: 143.719, Insample PINBALL: 0.45099, Outsample MAE: 1.55787\n",
      "Step: 770, Time: 145.447, Insample PINBALL: 0.46247, Outsample MAE: 1.55836\n",
      "Step: 780, Time: 147.158, Insample PINBALL: 0.43814, Outsample MAE: 1.55073\n",
      "Step: 790, Time: 148.893, Insample PINBALL: 0.51230, Outsample MAE: 1.54991\n",
      "Step: 800, Time: 150.618, Insample PINBALL: 0.52542, Outsample MAE: 1.56069\n",
      "Step: 810, Time: 152.680, Insample PINBALL: 0.51403, Outsample MAE: 1.53673\n",
      "Step: 820, Time: 154.969, Insample PINBALL: 0.42239, Outsample MAE: 1.56770\n",
      "Step: 830, Time: 156.869, Insample PINBALL: 0.42716, Outsample MAE: 1.57781\n",
      "Step: 840, Time: 158.674, Insample PINBALL: 0.44245, Outsample MAE: 1.56079\n",
      "Step: 850, Time: 160.578, Insample PINBALL: 0.42257, Outsample MAE: 1.56920\n",
      "Step: 860, Time: 162.361, Insample PINBALL: 0.45465, Outsample MAE: 1.56628\n",
      "Step: 870, Time: 164.090, Insample PINBALL: 0.45630, Outsample MAE: 1.56899\n",
      "Step: 880, Time: 165.995, Insample PINBALL: 0.42689, Outsample MAE: 1.55457\n",
      "Step: 890, Time: 167.755, Insample PINBALL: 0.44842, Outsample MAE: 1.55985\n",
      "Step: 900, Time: 169.505, Insample PINBALL: 0.44756, Outsample MAE: 1.55474\n",
      "Step: 910, Time: 171.250, Insample PINBALL: 0.55418, Outsample MAE: 1.61355\n",
      "Step: 920, Time: 173.070, Insample PINBALL: 0.44124, Outsample MAE: 1.55325\n",
      "Step: 930, Time: 174.826, Insample PINBALL: 0.45821, Outsample MAE: 1.59914\n",
      "Step: 940, Time: 176.705, Insample PINBALL: 0.39549, Outsample MAE: 1.57484\n",
      "Step: 950, Time: 178.523, Insample PINBALL: 0.41965, Outsample MAE: 1.53997\n",
      "Step: 960, Time: 180.299, Insample PINBALL: 0.46645, Outsample MAE: 1.53761\n",
      "Step: 970, Time: 182.699, Insample PINBALL: 0.42892, Outsample MAE: 1.53305\n",
      "Step: 980, Time: 185.030, Insample PINBALL: 0.49479, Outsample MAE: 1.54365\n",
      "Step: 990, Time: 187.273, Insample PINBALL: 0.46817, Outsample MAE: 1.61996\n",
      "Step: 1000, Time: 189.269, Insample PINBALL: 0.46129, Outsample MAE: 1.52856\n",
      "Step: 1010, Time: 191.111, Insample PINBALL: 0.49100, Outsample MAE: 1.54200\n",
      "Step: 1020, Time: 192.932, Insample PINBALL: 0.40876, Outsample MAE: 1.55503\n",
      "Step: 1030, Time: 194.831, Insample PINBALL: 0.40343, Outsample MAE: 1.57870\n",
      "Step: 1040, Time: 196.662, Insample PINBALL: 0.46066, Outsample MAE: 1.57663\n",
      "Step: 1050, Time: 198.636, Insample PINBALL: 0.41919, Outsample MAE: 1.55711\n",
      "Step: 1060, Time: 200.509, Insample PINBALL: 0.43613, Outsample MAE: 1.65045\n",
      "Step: 1070, Time: 202.286, Insample PINBALL: 0.46493, Outsample MAE: 1.52633\n",
      "Step: 1080, Time: 204.043, Insample PINBALL: 0.50086, Outsample MAE: 1.57370\n",
      "Step: 1090, Time: 205.852, Insample PINBALL: 0.42755, Outsample MAE: 1.53624\n",
      "Step: 1100, Time: 207.691, Insample PINBALL: 0.42089, Outsample MAE: 1.54429\n",
      "Step: 1110, Time: 209.425, Insample PINBALL: 0.49662, Outsample MAE: 1.54545\n",
      "Step: 1120, Time: 211.237, Insample PINBALL: 0.47863, Outsample MAE: 1.52492\n",
      "Step: 1130, Time: 212.991, Insample PINBALL: 0.42245, Outsample MAE: 1.54527\n",
      "Step: 1140, Time: 214.733, Insample PINBALL: 0.50738, Outsample MAE: 1.54571\n",
      "Step: 1150, Time: 216.561, Insample PINBALL: 0.48931, Outsample MAE: 1.54921\n",
      "Step: 1160, Time: 218.358, Insample PINBALL: 0.39398, Outsample MAE: 1.55136\n",
      "Step: 1170, Time: 220.150, Insample PINBALL: 0.44672, Outsample MAE: 1.57133\n",
      "Step: 1180, Time: 221.975, Insample PINBALL: 0.45167, Outsample MAE: 1.56913\n",
      "Step: 1190, Time: 223.760, Insample PINBALL: 0.42011, Outsample MAE: 1.54964\n",
      "Step: 1200, Time: 225.642, Insample PINBALL: 0.38902, Outsample MAE: 1.56202\n",
      "Step: 1210, Time: 227.490, Insample PINBALL: 0.45911, Outsample MAE: 1.55006\n",
      "Step: 1220, Time: 229.405, Insample PINBALL: 0.47589, Outsample MAE: 1.55086\n",
      "Step: 1230, Time: 231.222, Insample PINBALL: 0.40634, Outsample MAE: 1.57168\n",
      "Step: 1240, Time: 233.044, Insample PINBALL: 0.44020, Outsample MAE: 1.54622\n",
      "Step: 1250, Time: 234.859, Insample PINBALL: 0.39307, Outsample MAE: 1.57092\n",
      "Step: 1260, Time: 236.704, Insample PINBALL: 0.41683, Outsample MAE: 1.52864\n",
      "Step: 1270, Time: 238.620, Insample PINBALL: 0.41844, Outsample MAE: 1.57098\n",
      "Step: 1280, Time: 240.413, Insample PINBALL: 0.46337, Outsample MAE: 1.55518\n",
      "Step: 1290, Time: 242.148, Insample PINBALL: 0.47489, Outsample MAE: 1.55950\n",
      "Step: 1300, Time: 243.898, Insample PINBALL: 0.38230, Outsample MAE: 1.56982\n",
      "Step: 1310, Time: 245.642, Insample PINBALL: 0.43855, Outsample MAE: 1.58679\n",
      "Step: 1320, Time: 247.375, Insample PINBALL: 0.41107, Outsample MAE: 1.54316\n",
      "Step: 1330, Time: 249.131, Insample PINBALL: 0.44816, Outsample MAE: 1.56822\n",
      "Step: 1340, Time: 250.882, Insample PINBALL: 0.45460, Outsample MAE: 1.55128\n",
      "Step: 1350, Time: 252.607, Insample PINBALL: 0.40742, Outsample MAE: 1.54494\n",
      "Step: 1360, Time: 254.356, Insample PINBALL: 0.39287, Outsample MAE: 1.52926\n",
      "Step: 1370, Time: 256.188, Insample PINBALL: 0.44540, Outsample MAE: 1.56511\n",
      "Step: 1380, Time: 257.998, Insample PINBALL: 0.45947, Outsample MAE: 1.53850\n",
      "Step: 1390, Time: 259.804, Insample PINBALL: 0.41923, Outsample MAE: 1.55072\n",
      "Step: 1400, Time: 261.583, Insample PINBALL: 0.39386, Outsample MAE: 1.57030\n",
      "Step: 1410, Time: 263.338, Insample PINBALL: 0.41055, Outsample MAE: 1.57272\n",
      "Step: 1420, Time: 265.111, Insample PINBALL: 0.41430, Outsample MAE: 1.53549\n",
      "Step: 1430, Time: 266.928, Insample PINBALL: 0.37185, Outsample MAE: 1.54924\n",
      "Step: 1440, Time: 268.722, Insample PINBALL: 0.44089, Outsample MAE: 1.56019\n",
      "Step: 1450, Time: 270.523, Insample PINBALL: 0.42020, Outsample MAE: 1.54560\n",
      "Step: 1460, Time: 272.446, Insample PINBALL: 0.37820, Outsample MAE: 1.55903\n",
      "Step: 1470, Time: 274.178, Insample PINBALL: 0.37093, Outsample MAE: 1.55489\n",
      "Step: 1480, Time: 275.926, Insample PINBALL: 0.42966, Outsample MAE: 1.53606\n",
      "Step: 1490, Time: 277.687, Insample PINBALL: 0.34551, Outsample MAE: 1.54629\n",
      "Step: 1500, Time: 279.414, Insample PINBALL: 0.42210, Outsample MAE: 1.54069\n",
      "Step: 1510, Time: 281.172, Insample PINBALL: 0.42340, Outsample MAE: 1.54115\n",
      "Step: 1520, Time: 282.934, Insample PINBALL: 0.47211, Outsample MAE: 1.54751\n",
      "\n",
      "\n",
      "-------------------  Stopped training by early stopping -------------------\n",
      "Step: 1520, Time: 283.158, Insample PINBALL: 0.47863, Outsample MAE: 1.52492\n",
      "==============================  End fitting  ==============================\n",
      "\n",
      "\n",
      "Best Model Evaluation\n",
      "  metric  measure\n",
      "0    mae  1.52492\n",
      "1   mape  6.98445\n",
      "2  smape  6.92473\n",
      "3   rmse  3.32264\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "\n",
    "dataset = 'NP'\n",
    "Y_df, Xt_df = EPF.load(directory='../data/', group=dataset)\n",
    "val_ds = 365*2\n",
    "\n",
    "Y_balanced_df = Y_df\n",
    "\n",
    "# Create available_mask and sample_mask\n",
    "# TODO: Include not null exogenous condition to available mask\n",
    "mask_df = Y_balanced_df[['unique_id', 'ds', 'y']].copy()\n",
    "mask_df['available_mask'] = (1-Y_balanced_df.y.isnull().values)\n",
    "del mask_df['y']\n",
    "\n",
    "# Train Validation splits\n",
    "#                        ds                    \n",
    "#                       min                 max\n",
    "# unique_id mask                               \n",
    "# BE        0.0  2013-01-04 2015-01-03 23:00:00\n",
    "#           1.0  2011-01-09 2013-01-03 23:00:00\n",
    "# FR        0.0  2013-01-04 2015-01-03 23:00:00\n",
    "#           1.0  2011-01-09 2013-01-03 23:00:00\n",
    "# NP        0.0  2014-12-28 2016-12-26 23:00:00\n",
    "#           1.0  2013-01-01 2014-12-27 23:00:00\n",
    "# PJM       0.0  2014-12-28 2016-12-26 23:00:00\n",
    "#           1.0  2013-01-01 2014-12-27 23:00:00\n",
    "mask_df['sample_mask1'] = (mask_df['ds'] <= pd.to_datetime('2013-01-03 23:00:00')) * 1\n",
    "mask_df['sample_mask2'] = (mask_df['ds'] <= pd.to_datetime('2014-12-27 23:00:00')) * 1\n",
    "mask_df['sample_mask'] = mask_df['sample_mask2']\n",
    "# print(\"aqui np.sum(mask_df.sample_mask)\", np.sum(mask_df.sample_mask))\n",
    "# print(\"aqui np.sum(mask_df['sample_mask'].values)/len(mask_df)\", \\\n",
    "#     np.sum(mask_df['sample_mask'].values)/len(mask_df))\n",
    "\n",
    "model, train_loader, val_loader = run_val_nbeatsx(mc=mc, Y_df=Y_df, Xt_df=Xt_df, S_df=None, mask_df=mask_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_hat, _ = model.predict(ts_loader=val_loader, eval_mode=True)\n",
    "y_plot = Y_df['y'][-len(y_hat)*24:] #[-2*168:]\n",
    "y_hat = y_hat.reshape(-1)#[-2*168:]\n",
    "#lead_time = 23\n",
    "#y_hat_plot1 = y_hat[-2*168:, lead_time] # Forecast lead_time=?, for the last week\n",
    "#y_hat_plot2 = y_hat[-1, :]\n",
    "#y_hat_plot = np.concatenate([y_hat_plot1, y_hat_plot2])\n",
    "\n",
    "print(\"y_true.shape \\t\\t(#fcds,) \\t\", y_true.shape)\n",
    "print(\"y_hat.shape  \\t\\t(#fcds, #lt) \\t\", y_hat.shape)\n",
    "print(\"y_plot.shape \\t\\t(#fcds,) \\t\", y_plot.shape)\n",
    "# print(\"y_hat_plot1.shape \\t(#fcds, lt=0) \\t\", y_hat_plot1.shape)\n",
    "# print(\"y_hat_plot2.shape \\t(#fcds, lt=0) \\t\", y_hat_plot2.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "performance = np.round(mae(y_plot, y_hat), 5)\n",
    "plt.plot(range(len(y_plot)), y_plot, label='true', alpha=0.7)\n",
    "plt.plot(range(len(y_hat)), y_hat, label='pred', alpha=0.7)\n",
    "plt.title(f\"{dataset} predictions \\n all lead time  MAE={performance}\")\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Electricity Price')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# MULTIVARIATE TIME SERIES TEST"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nixtla.data.scalers import Scaler\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset\n",
    "# from nixtla.data.tsloader_fast import TimeSeriesLoader\n",
    "from nixtla.data.tsloader_general import TimeSeriesLoader\n",
    "\n",
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "\n",
    "args = pd.Series({'dataset': ['NP', 'PJM', 'BE', 'FR']})\n",
    "\n",
    "Y_df, Xt_df, S_df = EPF.load_groups(directory='data', groups=args.dataset)\n",
    "\n",
    "# Train Validation splits\n",
    "#                        ds                    \n",
    "#                       min                 max\n",
    "# unique_id mask                               \n",
    "# BE        0.0  2013-01-04 2015-01-03 23:00:00\n",
    "#           1.0  2011-01-09 2013-01-03 23:00:00\n",
    "# FR        0.0  2013-01-04 2015-01-03 23:00:00\n",
    "#           1.0  2011-01-09 2013-01-03 23:00:00\n",
    "# NP        0.0  2014-12-28 2016-12-26 23:00:00\n",
    "#           1.0  2013-01-01 2014-12-27 23:00:00\n",
    "# PJM       0.0  2014-12-28 2016-12-26 23:00:00\n",
    "#           1.0  2013-01-01 2014-12-27 23:00:00\n",
    "mask_df['sample_mask1'] = (mask_df['ds'] <= pd.to_datetime('2013-01-03 23:00:00')) * 1\n",
    "mask_df['sample_mask2'] = (mask_df['ds'] <= pd.to_datetime('2014-12-27 23:00:00')) * 1\n",
    "mask_df['sample_mask'] = mask_df['sample_mask2']\n",
    "print(\"aqui np.sum(mask_df.sample_mask)\", np.sum(mask_df.sample_mask))\n",
    "print(\"aqui np.sum(mask_df['sample_mask'].values)/len(mask_df)\", \\\n",
    "    np.sum(mask_df['sample_mask'].values)/len(mask_df))\n",
    "\n",
    "model, train_loader, val_loader = run_val_nbeatsx(mc=mc, Y_df=Y_df, Xt_df=Xt_df, S_df=S_df, mask_df=mask_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_hat, _ = model.predict(ts_loader=val_loader, eval_mode=True)\n",
    "y_true = y_true.reshape(4, 2920//4, 24)\n",
    "y_hat  = y_hat.reshape(4, 2920//4, 24)\n",
    "\n",
    "print(\"y_hat.shape  \\t\\t(#fcds, #lt) \\t\", y_hat.shape)\n",
    "print(\"y_true.shape \\t\\t(#fcds,) \\t\", y_true.shape)\n",
    "\n",
    "y_hat_plot = y_hat[2,:,:].reshape(-1)\n",
    "y_true_plot = y_true[2,:,:].reshape(-1)\n",
    "\n",
    "print(\"y_hat_plot.shape  \\t(#fcds,) \\t\", y_hat_plot.shape)\n",
    "print(\"y_true_plot.shape \\t(#fcds,) \\t\", y_true_plot.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "performance_df = {'unique_id': [], 'metric': [], 'nbeatsx': []}\n",
    "for i, meta_data in enumerate(val_loader.ts_dataset.meta_data):\n",
    "    dataset = meta_data['unique_id']\n",
    "    y_hat_plot = y_hat[i,:,:].reshape(-1)\n",
    "    y_true_plot = y_true[i,:,:].reshape(-1)\n",
    "\n",
    "    # Create performance table\n",
    "    performance_df['unique_id'] += [dataset]*4\n",
    "    performance_df['metric'] += ['MAE', 'MAPE', 'SMAPE', 'RMSE']\n",
    "    performance = forecast_evaluation_table(y_true_plot, y_hat_plot)\n",
    "    performance_df['nbeatsx'] += performance.measure.to_list()\n",
    "    \n",
    "    # performance = np.round(mae(y_true_plot, y_hat_plot), 5)\n",
    "    # plt.plot(range(len(y_true_plot)), y_true_plot, label='true', alpha=0.7)\n",
    "    # plt.plot(range(len(y_hat_plot)), y_hat_plot, label='pred', alpha=0.7)\n",
    "    # plt.title(f\"Testing predictions {dataset} \\n all lead time  MAE={performance}\")\n",
    "    # plt.xlabel('Hour')\n",
    "    # plt.ylabel('Electricity Price')\n",
    "    # plt.legend()\n",
    "    # plt.grid()\n",
    "    # plt.show()\n",
    "    \n",
    "performance_df = pd.DataFrame(performance_df)\n",
    "benchmark_df = pd.DataFrame({'id': [1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4],\n",
    "                             'unique_id': ['NP', 'NP', 'NP', 'NP', 'PJM', 'PJM', 'PJM', 'PJM',\n",
    "                                           'BE', 'BE', 'BE', 'BE', 'FR', 'FR', 'FR', 'FR'],\n",
    "                             'metric': ['MAE', 'MAPE', 'SMAPE', 'RMSE', 'MAE', 'MAPE', 'SMAPE', 'RMSE',\n",
    "                                        'MAE', 'MAPE', 'SMAPE', 'RMSE', 'MAE', 'MAPE', 'SMAPE', 'RMSE'],    \n",
    "                             'dnn' : [1.67, 5.38, 4.85, 3.33, 2.78, 28.66, 11.22, 4.64, 5.82,\n",
    "                                      26.11, 13.33, 16.13, 3.91, 14.77, 10.98, 11.74]})\n",
    "\n",
    "benchmark_df.sort_values(['id'], inplace=True)\n",
    "benchmark_df.reset_index(drop=True, inplace=True)\n",
    "benchmark_df = benchmark_df.merge(performance_df, on=['unique_id', 'metric'], how='left')\n",
    "benchmark_df['perc_diff'] = 100 * (benchmark_df['nbeatsx']-benchmark_df['dnn'])/benchmark_df['dnn']\n",
    "benchmark_df['improvement'] = benchmark_df['perc_diff'] < 0\n",
    "benchmark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nixtla.data.scalers import Scaler\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset\n",
    "# from nixtla.data.tsloader_fast import TimeSeriesLoader\n",
    "from nixtla.data.tsloader_general import TimeSeriesLoader\n",
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "\n",
    "def get_last_n_hours_weights_df(Y_df, n_hours):\n",
    "    # Creates outsample_mask\n",
    "    # train 1 validation 0\n",
    "\n",
    "    last_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    last_df.sort_values(by=['unique_id', 'ds'], inplace=True, ascending=False)\n",
    "    last_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    last_df = last_df.groupby('unique_id').head(n_hours)\n",
    "    last_df['weights'] = 1\n",
    "\n",
    "    last_df = last_df[['unique_id', 'ds', 'weights']]\n",
    "\n",
    "    weights_df = Y_df.merge(last_df, on=['unique_id', 'ds'], how='left')\n",
    "    weights_df['weights'] = weights_df['weights'].fillna(0)    # The first len(Y)-n_hours used as train\n",
    "\n",
    "    weights_df = weights_df[['unique_id', 'ds', 'weights']]\n",
    "    weights_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "\n",
    "    assert len(weights_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(weights_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "\n",
    "    return weights_df\n",
    "\n",
    "def balance_data(Y_df, Xt_df):\n",
    "    # Train Validation splits\n",
    "    #                        ds                    \n",
    "    #                       min                 max\n",
    "    # unique_id mask                               \n",
    "    # BE        0.0  2013-01-04 2015-01-03 23:00:00\n",
    "    #           1.0  2011-01-09 2013-01-03 23:00:00\n",
    "    # FR        0.0  2013-01-04 2015-01-03 23:00:00\n",
    "    #           1.0  2011-01-09 2013-01-03 23:00:00\n",
    "    # NP        0.0  2014-12-28 2016-12-26 23:00:00\n",
    "    #           1.0  2013-01-01 2014-12-27 23:00:00\n",
    "    # PJM       0.0  2014-12-28 2016-12-26 23:00:00\n",
    "    #           1.0  2013-01-01 2014-12-27 23:00:00\n",
    "\n",
    "    # Create balanced placeholder dataframe\n",
    "    balance_ids = {'unique_id': Y_df.unique_id.unique(),\n",
    "                   'ds': Y_df.ds.unique()}\n",
    "\n",
    "    product_list = list(itertools.product(*list(balance_ids.values())))\n",
    "    balance_df = pd.DataFrame(product_list, columns=list(balance_ids.keys()))\n",
    "\n",
    "    # Create mask for weighted losses for the las 2 years of each unique_id\n",
    "    weights_df = get_last_n_hours_weights_df(Y_df=Y_df, n_hours=2*365*24)\n",
    "    \n",
    "    # Balance with merge\n",
    "    Y_balanced_df = balance_df.merge(Y_df, on=['unique_id', 'ds'], how='left')\n",
    "    Xt_balanced_df = balance_df.merge(Xt_df, on=['unique_id', 'ds'], how='left')\n",
    "    weights_balanced_df = balance_df.merge(weights_df, on=['unique_id', 'ds'], how='left')\n",
    "    print(weights_balanced_df.groupby(['unique_id', 'weights']).agg({'ds': ['min', 'max']}))\n",
    "\n",
    "    # Create insample_mask\n",
    "    # TODO: Include not null exogenous condition to available mask\n",
    "    mask_df = Y_balanced_df[['unique_id', 'ds', 'y']].copy()\n",
    "    mask_df['available_mask'] = (1-Y_balanced_df.y.isnull().values)\n",
    "    del mask_df['y']\n",
    "\n",
    "    mask_df['sample_mask1'] = (mask_df['ds'] <= '2013-01-03 23:00:00') * 1\n",
    "    mask_df['sample_mask2'] = (mask_df['ds'] <= '2014-12-27 23:00:00') * 1\n",
    "    print(mask_df.groupby(['unique_id', 'sample_mask1']).agg({'ds': ['min', 'max']}))\n",
    "    print(mask_df.groupby(['unique_id', 'sample_mask2']).agg({'ds': ['min', 'max']}))\n",
    "\n",
    "    print('\\n')\n",
    "    print('Y_df.shape \\t\\t', Y_df.shape)\n",
    "    print('Xt_df.shape \\t\\t', Xt_df.shape)\n",
    "    print('Y_balanced_df.shape \\t', Y_balanced_df.shape)\n",
    "    print('Xt_balanced_df.shape \\t', Xt_balanced_df.shape)\n",
    "    print('mask_df.shape \\t\\t', mask_df.shape)\n",
    "\n",
    "    return Y_balanced_df, Xt_balanced_df, mask_df, weights_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = pd.Series({'dataset': ['NP', 'PJM', 'BE', 'FR']})\n",
    "\n",
    "Y_df, Xt_df, S_df = EPF.load_groups(directory='data', groups=args.dataset)\n",
    "# Y_df, Xt_df, mask_df, weights_df = balance_data(Y_df=Y_df, Xt_df=Xt_df)\n",
    "\n",
    "# mask_df['sample_mask'] = mask_df['sample_mask1']\n",
    "# Xt_scaled_df = scale_data(Xt_df=Xt_df, mask_df=mask_df)\n",
    "# train_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=S_df, X_df=Xt_scaled_df, mask_df=mask_df)\n",
    "\n",
    "# mask_df['sample_mask'] = (1-mask_df['sample_mask1'])\n",
    "# val_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=S_df, X_df=Xt_scaled_df, mask_df=mask_df)\n",
    "\n",
    "# train_loader = TimeSeriesLoader(ts_dataset=train_dataset,\n",
    "#                                 model='nbeats',\n",
    "#                                 offset=0, #offset,\n",
    "#                                 window_sampling_limit=int(mc['window_sampling_limit']), \n",
    "#                                 input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "#                                 output_size=int(mc['output_size']),\n",
    "#                                 idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "#                                 batch_size=int(mc['batch_size']),\n",
    "#                                 is_train_loader=True, # Samples all available_mask + sample_mask\n",
    "#                                 shuffle=True)\n",
    "\n",
    "# val_loader = TimeSeriesLoader(ts_dataset=val_dataset,\n",
    "#                                 model='nbeats',\n",
    "#                                 offset=0, #offset,\n",
    "#                                 window_sampling_limit=int(mc['window_sampling_limit']), \n",
    "#                                 input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "#                                 output_size=int(mc['output_size']),\n",
    "#                                 idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "#                                 batch_size=1024,\n",
    "#                                 is_train_loader=False, # Samples all sample_mask\n",
    "#                                 shuffle=False)\n",
    "\n",
    "# #-------------------------------------- Hyperparameter Optimization --------------------------------------#\n",
    "\n",
    "# model = Nbeats(input_size_multiplier=int(mc['input_size_multiplier']),\n",
    "#                 output_size=int(mc['output_size']),\n",
    "#                 shared_weights=int(mc['shared_weights']),\n",
    "#                 activation=mc['activation'],\n",
    "#                 initialization=mc['initialization'],\n",
    "#                 stack_types=mc['stack_types'], #2*['identity'],\n",
    "#                 n_blocks=mc['n_blocks'], #2*[1],\n",
    "#                 n_layers=mc['n_layers'], #2*[2],\n",
    "#                 n_hidden=mc['n_hidden'], #2*[[256,256]],\n",
    "#                 n_polynomials=mc['n_polynomials'], #2,\n",
    "#                 n_harmonics=int(mc['n_harmonics']), #1,\n",
    "#                 exogenous_n_channels=int(mc['exogenous_n_channels']), #9,\n",
    "#                 include_var_dict={'y': [-2, -3, -8],\n",
    "#                                     'Exogenous1': [-1, -2, -8],\n",
    "#                                     'Exogenous2': [-1, -2, -8],\n",
    "#                                     'week_day': [-1]},\n",
    "#                 t_cols=train_dataset.t_cols,\n",
    "#                 batch_normalization=mc['batch_normalization'], #False,\n",
    "#                 dropout_prob_theta=float(mc['dropout_prob_theta']), #0.01,\n",
    "#                 dropout_prob_exogenous=float(mc['dropout_prob_exogenous']), #0.01,\n",
    "#                 x_s_n_hidden=int(mc['x_s_n_hidden']), #0,\n",
    "#                 learning_rate=float(mc['learning_rate']), #0.007,\n",
    "#                 lr_decay=float(mc['lr_decay']), #0.5,\n",
    "#                 n_lr_decay_steps=int(mc['n_lr_decay_steps']), #3,\n",
    "#                 weight_decay=float(mc['weight_decay']), #0.0000001,\n",
    "#                 l1_theta=float(mc['l1_theta']), #0.0001,\n",
    "#                 n_iterations=int(mc['n_iterations']), #200,\n",
    "#                 early_stopping=int(mc['early_stopping']), #40,\n",
    "#                 loss=mc['loss'], #'PINBALL',\n",
    "#                 loss_hypar=float(mc['loss_hypar']), #0.5,\n",
    "#                 val_loss=mc['val_loss'], #MAE\n",
    "#                 frequency=mc['frequency'], #'H',\n",
    "#                 random_seed=int(mc['random_seed']), #1,\n",
    "#                 seasonality=int(mc['seasonality'])) #24)\n",
    "\n",
    "# model.fit(train_ts_loader=train_loader, val_ts_loader=val_loader, verbose=True, eval_steps=10)\n",
    "\n",
    "model, train_loader, val_loader = run_val_nbeatsx(mc, Y_df=Y_df, Xt_df=Xt_df, S_df=S_df, val_ds=365*24*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c68c0f618c4f787b1e02bfee278e48d25b62407cb335aab5258cedc4db4ced"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}