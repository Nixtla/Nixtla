{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.nbeats.nbeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "import torch as t\n",
    "from torch import optim\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "from nixtla.models.nbeats.nbeats_model import NBeats, NBeatsBlock, IdentityBasis, TrendBasis, SeasonalityBasis\n",
    "from nixtla.models.nbeats.nbeats_model import ExogenousBasisInterpretable, ExogenousBasisWavenet, ExogenousBasisTCN\n",
    "from nixtla.losses.pytorch import MAPELoss, MASELoss, SMAPELoss, MSELoss, MAELoss, PinballLoss\n",
    "from nixtla.losses.numpy import mae, mse, mape, smape, rmse, pinball_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_weights(module, initialization):\n",
    "    if type(module) == t.nn.Linear:\n",
    "        if initialization == 'orthogonal':\n",
    "            t.nn.init.orthogonal_(module.weight)\n",
    "        elif initialization == 'he_uniform':\n",
    "            t.nn.init.kaiming_uniform_(module.weight)\n",
    "        elif initialization == 'he_normal':\n",
    "            t.nn.init.kaiming_normal_(module.weight)\n",
    "        elif initialization == 'glorot_uniform':\n",
    "            t.nn.init.xavier_uniform_(module.weight)\n",
    "        elif initialization == 'glorot_normal':\n",
    "            t.nn.init.xavier_normal_(module.weight)\n",
    "        elif initialization == 'lecun_normal':\n",
    "            pass #t.nn.init.normal_(module.weight, 0.0, std=1/np.sqrt(module.weight.numel()))\n",
    "        else:\n",
    "            assert 1<0, f'Initialization {initialization} not found'\n",
    "\n",
    "class Nbeats(object):\n",
    "    \"\"\"\n",
    "    Future documentation\n",
    "    \"\"\"\n",
    "    SEASONALITY_BLOCK = 'seasonality'\n",
    "    TREND_BLOCK = 'trend'\n",
    "    IDENTITY_BLOCK = 'identity'\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size_multiplier,\n",
    "                 output_size,\n",
    "                 shared_weights,\n",
    "                 activation,\n",
    "                 initialization,\n",
    "                 stack_types,\n",
    "                 n_blocks,\n",
    "                 n_layers,\n",
    "                 n_hidden,\n",
    "                 n_harmonics,\n",
    "                 n_polynomials,\n",
    "                 exogenous_n_channels,\n",
    "                 include_var_dict,\n",
    "                 t_cols,\n",
    "                 batch_normalization,\n",
    "                 dropout_prob_theta,\n",
    "                 dropout_prob_exogenous,\n",
    "                 x_s_n_hidden,\n",
    "                 learning_rate,\n",
    "                 lr_decay,\n",
    "                 n_lr_decay_steps,\n",
    "                 weight_decay,\n",
    "                 l1_theta,\n",
    "                 n_iterations,\n",
    "                 early_stopping,\n",
    "                 loss,\n",
    "                 loss_hypar,\n",
    "                 val_loss,\n",
    "                 frequency,\n",
    "                 random_seed,\n",
    "                 seasonality,\n",
    "                 device=None):\n",
    "        super(Nbeats, self).__init__()\n",
    "\n",
    "        if activation == 'selu': initialization = 'lecun_normal'\n",
    "\n",
    "        #------------------------ Model Attributes ------------------------#\n",
    "        # Architecture parameters\n",
    "        self.input_size = int(input_size_multiplier*output_size)\n",
    "        self.output_size = output_size\n",
    "        self.shared_weights = shared_weights\n",
    "        self.activation = activation\n",
    "        self.initialization = initialization\n",
    "        self.stack_types = stack_types\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_harmonics = n_harmonics\n",
    "        self.n_polynomials = n_polynomials\n",
    "        self.exogenous_n_channels = exogenous_n_channels\n",
    "\n",
    "        # Regularization and optimization parameters\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.dropout_prob_theta = dropout_prob_theta\n",
    "        self.dropout_prob_exogenous = dropout_prob_exogenous\n",
    "        self.x_s_n_hidden = x_s_n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.n_lr_decay_steps = n_lr_decay_steps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.n_iterations = n_iterations\n",
    "        self.early_stopping = early_stopping\n",
    "        self.loss = loss\n",
    "        self.loss_hypar = loss_hypar\n",
    "        self.val_loss = val_loss\n",
    "        self.l1_theta = l1_theta\n",
    "        self.l1_conv = 1e-3 # Not a hyperparameter\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Data parameters\n",
    "        self.frequency = frequency\n",
    "        self.seasonality = seasonality        \n",
    "        self.include_var_dict = include_var_dict\n",
    "        self.t_cols = t_cols\n",
    "        #self.scaler = scaler\n",
    "\n",
    "        if device is None:\n",
    "            device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
    "        self.device = device\n",
    "\n",
    "        self._is_instantiated = False\n",
    "\n",
    "    def create_stack(self):\n",
    "        if self.include_var_dict is not None:\n",
    "            x_t_n_inputs = self.output_size * int(sum([len(x) for x in self.include_var_dict.values()]))\n",
    "            \n",
    "            # Correction because week_day only adds 1 no output_size\n",
    "            if len(self.include_var_dict['week_day'])>0:\n",
    "                x_t_n_inputs = x_t_n_inputs - self.output_size + 1 \n",
    "        else:\n",
    "            x_t_n_inputs = self.input_size\n",
    "        \n",
    "        #------------------------ Model Definition ------------------------#\n",
    "        block_list = []\n",
    "        self.blocks_regularizer = []\n",
    "        for i in range(len(self.stack_types)):\n",
    "            #print(f'| --  Stack {self.stack_types[i]} (#{i})')\n",
    "            for block_id in range(self.n_blocks[i]):\n",
    "                \n",
    "                # Batch norm only on first block\n",
    "                if (len(block_list)==0) and (self.batch_normalization):\n",
    "                    batch_normalization_block = True\n",
    "                else:\n",
    "                    batch_normalization_block = False\n",
    "                \n",
    "                # Dummy of regularizer in block. Override with 1 if exogenous_block\n",
    "                self.blocks_regularizer += [0]\n",
    "\n",
    "                # Shared weights\n",
    "                if self.shared_weights and block_id>0:\n",
    "                    nbeats_block = block_list[-1]\n",
    "                else:\n",
    "                    if self.stack_types[i] == 'seasonality':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=4 * int(\n",
    "                                                        np.ceil(self.n_harmonics / 2 * self.output_size) - (self.n_harmonics - 1)),\n",
    "                                                   basis=SeasonalityBasis(harmonics=self.n_harmonics,\n",
    "                                                                          backcast_size=self.input_size,\n",
    "                                                                          forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'trend':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2 * (self.n_polynomials + 1),\n",
    "                                                   basis=TrendBasis(degree_of_polynomial=self.n_polynomials,\n",
    "                                                                            backcast_size=self.input_size,\n",
    "                                                                            forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'identity':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=self.input_size + self.output_size,\n",
    "                                                   basis=IdentityBasis(backcast_size=self.input_size,\n",
    "                                                                       forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2*self.n_x_t,\n",
    "                                                   basis=ExogenousBasisInterpretable(),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous_tcn':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden = self.x_s_n_hidden,\n",
    "                                                   theta_n_dim = 2*(self.exogenous_n_channels),\n",
    "                                                   basis= ExogenousBasisTCN(self.exogenous_n_channels, self.n_x_t),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous_wavenet':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2*(self.exogenous_n_channels),\n",
    "                                                   basis=ExogenousBasisWavenet(self.exogenous_n_channels, self.n_x_t),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                        self.blocks_regularizer[-1] = 1\n",
    "                    else:\n",
    "                        assert 1<0, f'Block type not found!'\n",
    "                # Select type of evaluation and apply it to all layers of block\n",
    "                init_function = partial(init_weights, initialization=self.initialization)                                             \n",
    "                nbeats_block.layers.apply(init_function)\n",
    "                #print(f'     | -- {nbeats_block}')\n",
    "                block_list.append(nbeats_block)\n",
    "        return block_list\n",
    "\n",
    "    def __loss_fn(self, loss_name: str):\n",
    "        def loss(x, loss_hypar, forecast, target, mask):\n",
    "            if loss_name == 'MAPE':\n",
    "                return MAPELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'MASE':\n",
    "                return MASELoss(y=target, y_hat=forecast, y_insample=x, seasonality=loss_hypar, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return SMAPELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'MSE':\n",
    "                return MSELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'MAE':\n",
    "                return MAELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'PINBALL':\n",
    "                return PinballLoss(y=target, y_hat=forecast, mask=mask, tau=loss_hypar) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def __val_loss_fn(self, loss_name='MAE'):\n",
    "        #TODO: mase not implemented\n",
    "        def loss(forecast, target, weights):\n",
    "            if loss_name == 'MAPE':\n",
    "                return mape(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return smape(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MSE':\n",
    "                return mse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'RMSE':\n",
    "                return rmse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MAE':\n",
    "                return mae(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'PINBALL':\n",
    "                return pinball_loss(y=target, y_hat=forecast, weights=weights, tau=0.5)\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "    \n",
    "    def loss_l1_conv_layers(self):\n",
    "        loss_l1 = 0\n",
    "        for i, indicator in enumerate(self.blocks_regularizer):\n",
    "            if indicator:\n",
    "                loss_l1 += self.l1_conv * t.sum(t.abs(self.model.blocks[i].basis.weight))\n",
    "        return loss_l1\n",
    "    \n",
    "    def loss_l1_theta(self):\n",
    "        loss_l1 = 0\n",
    "        for block in self.model.blocks:\n",
    "            for layer in block.modules():\n",
    "                if isinstance(layer, t.nn.Linear):\n",
    "                    loss_l1 += self.l1_theta * layer.weight.abs().sum()\n",
    "        return loss_l1\n",
    "\n",
    "    def to_tensor(self, x: np.ndarray) -> t.Tensor:\n",
    "        tensor = t.as_tensor(x, dtype=t.float32).to(self.device)\n",
    "        return tensor\n",
    "\n",
    "    def evaluate_performance(self, ts_loader, validation_loss_fn):\n",
    "        self.model.eval()\n",
    "\n",
    "        #losses = []\n",
    "        forecasts = []\n",
    "        outsample_ys = []\n",
    "        outsample_masks = []\n",
    "        with t.no_grad():\n",
    "            for batch in iter(ts_loader):\n",
    "                # Parse batch\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                forecast   = self.model(x_s=s_matrix, insample_y=insample_y, \n",
    "                                        insample_x_t=insample_x, outsample_x_t=outsample_x,\n",
    "                                        insample_mask=insample_mask)\n",
    "                #batch_loss = validation_loss_fn(target=outsample_y.cpu().data.numpy(),\n",
    "                #                                forecast=forecast.cpu().data.numpy(),\n",
    "                #                                weights=outsample_mask.cpu().data.numpy())\n",
    "                #losses.append(batch_loss)\n",
    "                forecasts.append(forecast.cpu().data.numpy())\n",
    "                outsample_ys.append(batch['outsample_y'])\n",
    "                outsample_masks.append(batch['outsample_mask'])\n",
    "\n",
    "        forecasts = np.vstack(forecasts)\n",
    "        outsample_ys = np.vstack(outsample_ys)\n",
    "        outsample_masks = np.vstack(outsample_masks)\n",
    "        #loss = np.mean(losses)\n",
    "\n",
    "        # TODO: think how to do this with weights or mask\n",
    "        # np.nan protection\n",
    "        # not_nan = ~np.isnan(outsample_ys)\n",
    "        # outsample_ys = outsample_ys[non_nan]\n",
    "        # forecasts = outsample_ys[non_nan]\n",
    "        # outsample_masks = outsample_masks[non_nan]\n",
    "\n",
    "        if np.sum(np.isnan(forecasts))>0:\n",
    "            print(f'y_hat has {np.sum(np.isnan(forecasts))} nan values')\n",
    "            print('y_hat.shape', forecasts.shape)\n",
    "\n",
    "        if np.sum(np.isnan(outsample_ys))>0:\n",
    "            print(f'y_true_insample has {np.sum(np.isnan(outsample_ys))} nan values')\n",
    "            print('y_true_insample.shape', outsample_ys.shape)\n",
    "\n",
    "\n",
    "\n",
    "        complete_loss = validation_loss_fn(target=outsample_ys,\n",
    "                                           forecast=forecasts,\n",
    "                                           weights=outsample_masks)\n",
    "\n",
    "        self.model.train()\n",
    "        return complete_loss\n",
    "\n",
    "    def fit(self, train_ts_loader, val_ts_loader=None, n_iterations=None, verbose=True, eval_steps=1):\n",
    "        # TODO: Indexes hardcoded, information duplicated in train and val datasets\n",
    "        assert (self.input_size)==train_ts_loader.input_size, \\\n",
    "            f'model input_size {self.input_size} data input_size {train_ts_loader.input_size}'\n",
    "\n",
    "        # Random Seeds (model initialization)\n",
    "        t.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed) #TODO: interaccion rara con window_sampling de validacion\n",
    "\n",
    "        # Attributes of ts_dataset\n",
    "        self.n_x_t, self.n_x_s = train_ts_loader.get_n_variables()\n",
    "\n",
    "        # Instantiate model\n",
    "        if not self._is_instantiated:\n",
    "            block_list = self.create_stack()\n",
    "            self.model = NBeats(t.nn.ModuleList(block_list)).to(self.device)\n",
    "            self._is_instantiated = True\n",
    "\n",
    "        # Overwrite n_iterations and train datasets\n",
    "        if n_iterations is None:\n",
    "            n_iterations = self.n_iterations\n",
    "\n",
    "        lr_decay_steps = n_iterations // self.n_lr_decay_steps\n",
    "        if lr_decay_steps == 0:\n",
    "            lr_decay_steps = 1\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_steps, gamma=self.lr_decay)\n",
    "        training_loss_fn = self.__loss_fn(self.loss)\n",
    "        validation_loss_fn = self.__val_loss_fn(self.val_loss) #Uses numpy losses\n",
    "\n",
    "        print('='*30+' Start fitting '+'='*30)\n",
    "\n",
    "        #self.loss_dict = {} # Restart self.loss_dict\n",
    "        start = time.time()\n",
    "        self.trajectories = {'iteration':[],'train_loss':[], 'val_loss':[]}\n",
    "        self.final_insample_loss = None\n",
    "        self.final_outsample_loss = None\n",
    "        \n",
    "        # Training Loop\n",
    "        early_stopping_counter = 0\n",
    "        best_val_loss = np.inf\n",
    "        best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "        break_flag = False\n",
    "        iteration = 0\n",
    "        epoch = 0\n",
    "        while (iteration < n_iterations) and (not break_flag):\n",
    "            epoch +=1\n",
    "            for batch in iter(train_ts_loader):\n",
    "                iteration += 1\n",
    "                if (iteration > n_iterations) or (break_flag):\n",
    "                    continue\n",
    "\n",
    "                self.model.train()\n",
    "                # Parse batch\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                forecast   = self.model(x_s=s_matrix, insample_y=insample_y, \n",
    "                                        insample_x_t=insample_x, outsample_x_t=outsample_x,\n",
    "                                        insample_mask=insample_mask)\n",
    "                \n",
    "\n",
    "                ###########\n",
    "                ###########\n",
    "                ###########\n",
    "\n",
    "                print(\"\\n\")\n",
    "                print(\"\\n\")\n",
    "                print(\"\\n\")\n",
    "                print(\"INTENTO RARO DE LIMPIEZA8\")\n",
    "                insample_y = batch['insample_y'].cpu().numpy()\n",
    "                outsample_y = batch['outsample_y'].cpu().numpy()\n",
    "                insample_x = batch['insample_x'].cpu().numpy()\n",
    "                outsample_x = batch['outsample_x'].cpu().numpy()\n",
    "                available_mask = batch['insample_mask'].cpu().numpy()\n",
    "                sample_mask = batch['outsample_mask'].cpu().numpy()\n",
    "\n",
    "                print(\"insample_y_nans\", (np.sum(np.isnan(insample_y))) * 1)\n",
    "                print(\"outsample_y_nans\", (np.sum(np.isnan(outsample_y))) * 1)\n",
    "                print(\"insample_x_nans\", (np.sum(np.isnan(insample_x))) * 1)\n",
    "                print(\"outsample_x_nans\", (np.sum(np.isnan(outsample_x))) * 1)\n",
    "                print(\"available_mask_nans\", np.sum(available_mask))\n",
    "                print(\"sample_mask_nans\", np.sum(sample_mask))\n",
    "\n",
    "                data = outsample_x\n",
    "                print(\"data.shape\", data.shape)\n",
    "                data_nans = (np.sum(np.isnan(data), axis=0)) * 1\n",
    "                for channel in range(len(data_nans)):\n",
    "                    print(\"channel\", channel)\n",
    "                    print(data_nans[channel,:])\n",
    "                \n",
    "                print(\"sum(data_nans)/len(data_nans)\", sum(data_nans)/len(data_nans))\n",
    "\n",
    "                insample_availability = (np.sum(available_mask, axis=1) > 0 ) * 1\n",
    "                outsample_availability = (np.sum(sample_mask, axis=1) > 0 ) * 1\n",
    "                print(\"available_mask.shape\", available_mask.shape)\n",
    "                print(\"sum(available_mask)/len(availability)\", sum(insample_availability)/len(insample_availability))\n",
    "                print(\"sum(sample_mask)/len(availability)\", sum(outsample_availability)/len(outsample_availability))\n",
    "\n",
    "                print(\"available_mask[np.isnan(available_mask)]\")\n",
    "                print(available_mask[np.isnan(available_mask)])\n",
    "\n",
    "                #print(y_true_insample[y_true_nans])\n",
    "                assert 1<0\n",
    "\n",
    "                print(\"\\n\")\n",
    "                print(\"\\n\")\n",
    "                print(\"\\n\")\n",
    "\n",
    "                ###########\n",
    "                ###########\n",
    "                ###########\n",
    "                \n",
    "                #     print(f'y_true has {np.sum(np.isnan(batch['insample_y'])) } nan values')\n",
    "                #     print('y_true.shape', batch['insample_y'].shape)\n",
    "\n",
    "                # if np.sum(np.isnan(batch['outsample_y']))>1.0:\n",
    "                #     print(f'y_true_outsample has {np.sum(np.isnan(batch['outsample_y']))} nan values')\n",
    "                #     print('y_true_outsample.shape', batch['outsample_y'].shape)\n",
    "\n",
    "\n",
    "\n",
    "                training_loss = training_loss_fn(x=insample_y, loss_hypar=self.loss_hypar, forecast=forecast,\n",
    "                                                 target=outsample_y, mask=outsample_mask)\n",
    "\n",
    "                if np.isnan(float(training_loss)):\n",
    "                    break\n",
    "\n",
    "                training_loss.backward()\n",
    "                t.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "                if (iteration % eval_steps == 0):\n",
    "                    display_string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(iteration,\n",
    "                                                                                            time.time()-start,\n",
    "                                                                                            self.loss,\n",
    "                                                                                            training_loss.cpu().data.numpy())\n",
    "                    self.trajectories['iteration'].append(iteration)\n",
    "                    self.trajectories['train_loss'].append(np.float(training_loss.cpu().data.numpy()))\n",
    "\n",
    "                    if val_ts_loader is not None:\n",
    "                        loss = self.evaluate_performance(ts_loader=val_ts_loader, \n",
    "                                                         validation_loss_fn=validation_loss_fn)\n",
    "                        display_string += \", Outsample {}: {:.5f}\".format(self.val_loss, loss)\n",
    "                        self.trajectories['val_loss'].append(loss)\n",
    "\n",
    "                        if self.early_stopping:\n",
    "                            if loss < best_val_loss:\n",
    "                                # Save current model if improves outsample loss\n",
    "                                best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "                                best_insample_loss = training_loss.cpu().data.numpy()\n",
    "                                early_stopping_counter = 0\n",
    "                                best_val_loss = loss\n",
    "                            else:\n",
    "                                early_stopping_counter += 1\n",
    "                            if early_stopping_counter >= self.early_stopping:\n",
    "                                break_flag = True\n",
    "                    \n",
    "                    print(display_string)\n",
    "\n",
    "                    self.model.train()\n",
    "\n",
    "                if break_flag:\n",
    "                    print('\\n')\n",
    "                    print(19*'-',' Stopped training by early stopping', 19*'-')\n",
    "                    self.model.load_state_dict(best_state_dict)\n",
    "                    break\n",
    "\n",
    "        #End of fitting\n",
    "        if n_iterations >0:\n",
    "            # This is batch loss!\n",
    "            self.final_insample_loss = np.float(training_loss.cpu().data.numpy()) if not break_flag else best_insample_loss \n",
    "            string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(iteration,\n",
    "                                                                            time.time()-start,\n",
    "                                                                            self.loss,\n",
    "                                                                            self.final_insample_loss)\n",
    "            if val_ts_loader is not None:\n",
    "                self.final_outsample_loss = self.evaluate_performance(ts_loader=val_ts_loader, \n",
    "                                                                      validation_loss_fn=validation_loss_fn)\n",
    "                string += \", Outsample {}: {:.5f}\".format(self.val_loss, self.final_outsample_loss)\n",
    "            print(string)\n",
    "            print('='*30+'  End fitting  '+'='*30)\n",
    "            print('\\n')\n",
    "\n",
    "    def predict(self, ts_loader, X_test=None, eval_mode=False):\n",
    "        self.model.eval()\n",
    "        assert not ts_loader.shuffle, 'ts_loader must have shuffle as False.'\n",
    "\n",
    "        forecasts = []\n",
    "        outsample_ys = []\n",
    "        outsample_masks = []\n",
    "        with t.no_grad():\n",
    "            for batch in iter(ts_loader):\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                forecast = self.model(insample_y=insample_y, insample_x_t=insample_x,\n",
    "                                      insample_mask=insample_mask, outsample_x_t=outsample_x, x_s=s_matrix)\n",
    "                forecasts.append(forecast.cpu().data.numpy())\n",
    "                outsample_ys.append(batch['outsample_y'])\n",
    "                outsample_masks.append(batch['outsample_mask'])\n",
    "        forecasts = np.vstack(forecasts)\n",
    "        outsample_ys = np.vstack(outsample_ys)\n",
    "        outsample_masks = np.vstack(outsample_masks)\n",
    "        \n",
    "        # Reshape for univariate and panel model compatibility\n",
    "        n_series = ts_loader.ts_dataset.n_series\n",
    "        n_fcds = len(outsample_ys) // n_series\n",
    "        outsample_ys = outsample_ys.reshape(n_series, n_fcds, self.output_size)\n",
    "        forecasts = forecasts.reshape(n_series, n_fcds, self.output_size)\n",
    "        outsample_masks = outsample_masks.reshape(n_series, n_fcds, self.output_size)\n",
    "\n",
    "        self.model.train()\n",
    "        if eval_mode:\n",
    "            return outsample_ys, forecasts, outsample_masks\n",
    "\n",
    "        # Pandas wrangling\n",
    "        frequency = ts_loader.get_frequency()\n",
    "        unique_ids = ts_loader.get_meta_data_col('unique_id')\n",
    "        last_ds = ts_loader.get_meta_data_col('last_ds') #TODO: ajustar of offset\n",
    "\n",
    "        # Predictions for panel\n",
    "        Y_hat_panel = pd.DataFrame(columns=['unique_id', 'ds'])\n",
    "        for i, unique_id in enumerate(unique_ids):\n",
    "            Y_hat_id = pd.DataFrame([unique_id]*self.output_size, columns=[\"unique_id\"])\n",
    "            ds = pd.date_range(start=last_ds[i], periods=self.output_size+1, freq=frequency)\n",
    "            Y_hat_id[\"ds\"] = ds[1:]\n",
    "            Y_hat_panel = Y_hat_panel.append(Y_hat_id, sort=False).reset_index(drop=True)\n",
    "\n",
    "        Y_hat_panel['y_hat'] = forecasts.flatten()\n",
    "\n",
    "        if X_test is not None:\n",
    "            Y_hat_panel = X_test.merge(Y_hat_panel, on=['unique_id', 'ds'], how='left')\n",
    "        \n",
    "        return Y_hat_panel\n",
    "\n",
    "\n",
    "    def save(self, model_dir, model_id, state_dict = None):\n",
    "    \n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        if state_dict is None:\n",
    "            state_dict = self.model.state_dict()\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        print('Saving model to:\\n {}'.format(model_file)+'\\n')\n",
    "        t.save({'model_state_dict': state_dict}, model_file)\n",
    "\n",
    "    def load(self, model_dir, model_id):\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        path = Path(model_file)\n",
    "\n",
    "        assert path.is_file(), 'No model_*.model file found in this path!'\n",
    "\n",
    "        print('Loading model from:\\n {}'.format(model_file)+'\\n')\n",
    "\n",
    "        checkpoint = t.load(model_file, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(self.device)"
   ]
  },
  {
   "source": [
    "# SINGLE/MULTIPLE TIME SERIES TEST"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nixtla.data.scalers import Scaler\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset as TimeSeriesDataset\n",
    "from nixtla.data.tsloader_fast import TimeSeriesLoader\n",
    "# from nixtla.data.tsloader_general import TimeSeriesLoader\n",
    "\n",
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "\n",
    "# Hacked MAE NP hypars\n",
    "hacked_np = {'input_size_multiplier': 7,\n",
    "             'output_size': 24,\n",
    "             'idx_to_sample_freq': 24,\n",
    "             'batch_size': 2, #256 <-------------\n",
    "             'shared_weights': False,\n",
    "             'activation': 'relu',\n",
    "             'initialization': 'he_normal',\n",
    "             'stack_types': ['exogenous_tcn']+1*['identity'],\n",
    "             'n_blocks': 2*[1],\n",
    "             'n_layers': 2*[2],\n",
    "             'n_hidden': 2*[[462,462]],\n",
    "             'n_polynomials': 2,\n",
    "             'n_harmonics': 1,\n",
    "             'exogenous_n_channels': 8,\n",
    "             'include_var_dict': {'y': [-2, -3, -8],\n",
    "                                  'Exogenous1': [-1, -2, -8],\n",
    "                                  'Exogenous2': [-1, -2, -8],\n",
    "                                  'week_day': [-1]},\n",
    "             'batch_normalization': False,\n",
    "             'dropout_prob_theta': 0.05,\n",
    "             'dropout_prob_exogenous': 0.35,\n",
    "             'x_s_n_hidden': 0,\n",
    "             'learning_rate': 0.0016,\n",
    "             'lr_decay': 0.5,\n",
    "             'n_lr_decay_steps': 3,\n",
    "             'weight_decay': 6e-4,\n",
    "             'l1_theta': 1.0e-05,\n",
    "             'n_iterations': 100, #2000 <-------------\n",
    "             'early_stopping': 40,\n",
    "             'loss': 'PINBALL',\n",
    "             'loss_hypar': 0.49,\n",
    "             'val_loss': 'MAE',\n",
    "             'frequency': 'H',\n",
    "             'random_seed': 17,\n",
    "             'seasonality': 24}\n",
    "\n",
    "# Hacked MAE BE hypars\n",
    "hacked_be = {'input_size_multiplier': 7,\n",
    "             'output_size': 24,\n",
    "             'idx_to_sample_freq': 24,\n",
    "             'batch_size': 2,\n",
    "             'shared_weights': False,\n",
    "             'activation': 'relu',\n",
    "             'initialization': 'lecun_normal',\n",
    "             'stack_types': ['exogenous_wavenet']+1*['identity'],\n",
    "             'n_blocks': 2*[1],\n",
    "             'n_layers': 2*[2],\n",
    "             'n_hidden': 2*[[462,462]],\n",
    "             'n_polynomials': 2,\n",
    "             'n_harmonics': 1,\n",
    "             'exogenous_n_channels': 8,\n",
    "             'include_var_dict': {'y': [-2, -3, -8],\n",
    "                                  'Exogenous1': [-1, -2, -8],\n",
    "                                  'Exogenous2': [-1, -2, -8],\n",
    "                                  'week_day': [-1]},\n",
    "             'batch_normalization': False,\n",
    "             'dropout_prob_theta': 0.14,\n",
    "             'dropout_prob_exogenous': 0.08, #<-------- Interesante\n",
    "             'x_s_n_hidden': 0,\n",
    "             'learning_rate': 0.0013,\n",
    "             'lr_decay': 0.5,\n",
    "             'n_lr_decay_steps': 3,\n",
    "             'weight_decay': 0.0017,\n",
    "             'l1_theta': 1.5e-05,\n",
    "             'n_iterations': 2000, #2000 <-------------\n",
    "             'early_stopping': 40,\n",
    "             'loss': 'PINBALL',\n",
    "             'loss_hypar': 0.502,\n",
    "             'val_loss': 'MAE', #'MAE',\n",
    "             'frequency': 'H',\n",
    "             'random_seed': 16,\n",
    "             'seasonality': 24}\n",
    "\n",
    "mc = hacked_np\n",
    "#mc = hacked_be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_evaluation_table(y_true, y_hat):\n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_hat = y_hat.reshape(-1)\n",
    "    \n",
    "    #_pinball50 = np.round(pinball_loss(y_true, y_hat, tau=0.5),5)\n",
    "    _mae   = np.round(mae(y_true, y_hat),5)\n",
    "    _mape  = np.round(mape(y_true, y_hat),5)\n",
    "    _smape = np.round(smape(y_true, y_hat),5)\n",
    "    _rmse  = np.round(rmse(y_true, y_hat),5)\n",
    "\n",
    "    performance = pd.DataFrame({'metric': ['mae', 'mape', 'smape', 'rmse'],\n",
    "                                'measure': [_mae, _mape, _smape, _rmse]})                          \n",
    "\n",
    "    return performance\n",
    "\n",
    "def get_last_n_timestamps_mask_df(Y_df, n_timestamps):\n",
    "    # Creates outsample_mask\n",
    "    # train 1 validation 0\n",
    "\n",
    "    last_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    last_df.sort_values(by=['unique_id', 'ds'], inplace=True, ascending=False)\n",
    "    last_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    last_df = last_df.groupby('unique_id').head(n_timestamps)\n",
    "    last_df['mask'] = 1\n",
    "\n",
    "    last_df = last_df[['unique_id', 'ds', 'mask']]\n",
    "\n",
    "    mask_df = Y_df.merge(last_df, on=['unique_id', 'ds'], how='left')\n",
    "    mask_df['mask'] = mask_df['mask'].fillna(0)    # The first len(Y)-n_hours used as train\n",
    "\n",
    "    mask_df = mask_df[['unique_id', 'ds', 'mask']]\n",
    "    mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "\n",
    "    assert len(mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "\n",
    "    return mask_df\n",
    "\n",
    "def scale_data(Xt_df, mask_df):\n",
    "    # TODO: write sample_mask conditional/groupby(['unique_id]) scaling\n",
    "    # Conditional on sample_mask, scale all the Exogenous variables.\n",
    "\n",
    "    # To not modify original data\n",
    "    Xt_scaled_df = Xt_df.copy()\n",
    "\n",
    "    # Transform data with scale transformation\n",
    "    scaler = Scaler(normalizer='norm')\n",
    "    Xt_scaled_df['Exogenous1'] = scaler.scale(x=Xt_scaled_df['Exogenous1'].values, \n",
    "                                              mask=mask_df.sample_mask.values)\n",
    "\n",
    "    Xt_scaled_df['Exogenous1'] = scaler.scale(x=Xt_scaled_df['Exogenous1'].values, \n",
    "                                              mask=np.ones(len(Xt_scaled_df)))                                              \n",
    "\n",
    "    scaler = Scaler(normalizer='norm')\n",
    "    Xt_scaled_df['Exogenous2'] = scaler.scale(x=Xt_scaled_df['Exogenous2'].values,\n",
    "                                              mask=np.ones(len(Xt_scaled_df)))\n",
    "\n",
    "    return Xt_scaled_df\n",
    "\n",
    "def balance_data(Y_df, Xt_df):\n",
    "    # Create balanced placeholder dataframe\n",
    "    balance_ids = {'unique_id': Y_df.unique_id.unique(),\n",
    "                   'ds': Y_df.ds.unique()}\n",
    "\n",
    "    product_list = list(itertools.product(*list(balance_ids.values())))\n",
    "    balance_df = pd.DataFrame(product_list, columns=list(balance_ids.keys()))\n",
    "\n",
    "    # Create mask for weighted losses for the las 2 years of each unique_id\n",
    "    weights_df = get_last_n_timestamps_mask_df(Y_df=Y_df, n_timestamps=365*2*24)\n",
    "    weights_df['weights'] = weights_df['mask']\n",
    "    del weights_df['mask']\n",
    "    \n",
    "    # Balance with merge\n",
    "    Y_balanced_df = balance_df.merge(Y_df, on=['unique_id', 'ds'], how='left')\n",
    "    Xt_balanced_df = balance_df.merge(Xt_df, on=['unique_id', 'ds'], how='left')\n",
    "    weights_balanced_df = balance_df.merge(weights_df, on=['unique_id', 'ds'], how='left')\n",
    "    #print(weights_balanced_df.groupby(['unique_id', 'weights']).agg({'ds': ['min', 'max']}))\n",
    "\n",
    "    print('\\n')\n",
    "    print('Y_df.shape \\t\\t\\t', Y_df.shape)\n",
    "    print('Xt_df.shape \\t\\t\\t', Xt_df.shape)\n",
    "    print('Y_balanced_df.shape \\t\\t', Y_balanced_df.shape)\n",
    "    print('Xt_balanced_df.shape \\t\\t', Xt_balanced_df.shape)\n",
    "    print('weights_balanced_df.shape \\t', weights_balanced_df.shape)\n",
    "\n",
    "    \n",
    "\n",
    "    return Y_balanced_df, Xt_balanced_df, weights_balanced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_val_nbeatsx(mc, Y_df, Xt_df, S_df, mask_df): #, trials, trials_file_name):\n",
    "\n",
    "    #------------------------------------------------- Data -------------------------------------------------#\n",
    "\n",
    "    ts_dataset = TimeSeriesDataset(Y_df=Y_df, X_df=Xt_df, S_df=S_df, mask_df=mask_df)\n",
    "\n",
    "    train_loader = TimeSeriesLoader(ts_dataset=ts_dataset,\n",
    "                                    model='nbeats',\n",
    "                                    offset=0, #offset,\n",
    "                                    window_sampling_limit=ts_dataset.max_len, \n",
    "                                    input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                                    output_size=int(mc['output_size']),\n",
    "                                    idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                    batch_size=int(mc['batch_size']),\n",
    "                                    is_train_loader=True,\n",
    "                                    shuffle=True, random_seed=int(mc['random_seed']))\n",
    "\n",
    "    print(\"train_loader.ts_windows.shape\", train_loader.ts_windows.shape)\n",
    "    print(f\"len(train_loader.windows_sampling_idx) * 24 = \\\n",
    "       {len(train_loader.windows_sampling_idx)} * 24 = {len(train_loader.windows_sampling_idx) * 24}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    val_loader = TimeSeriesLoader(ts_dataset=ts_dataset,\n",
    "                                model='nbeats',\n",
    "                                offset=0, #offset,\n",
    "                                window_sampling_limit=ts_dataset.max_len,\n",
    "                                input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                                output_size=int(mc['output_size']),\n",
    "                                idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                batch_size=128,\n",
    "                                is_train_loader=False, # Samples the opposite of train_outsample_mask\n",
    "                                shuffle=False, random_seed=int(mc['random_seed']))\n",
    "\n",
    "    print(\"val_loader.ts_windows.shape\", val_loader.ts_windows.shape)\n",
    "    print(f\"len(val_loader.windows_sampling_idx) * 24 = \\\n",
    "       {len(val_loader.windows_sampling_idx)} * 24 = {len(val_loader.windows_sampling_idx) * 24}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    #-------------------------------------- Hyperparameter Optimization --------------------------------------#\n",
    "\n",
    "    model = Nbeats(input_size_multiplier=int(mc['input_size_multiplier']),\n",
    "                    output_size=int(mc['output_size']),\n",
    "                    shared_weights=int(mc['shared_weights']),\n",
    "                    activation=mc['activation'],\n",
    "                    initialization=mc['initialization'],\n",
    "                    stack_types=mc['stack_types'], #2*['identity'],\n",
    "                    n_blocks=mc['n_blocks'], #2*[1],\n",
    "                    n_layers=mc['n_layers'], #2*[2],\n",
    "                    n_hidden=mc['n_hidden'], #2*[[256,256]],\n",
    "                    n_polynomials=mc['n_polynomials'], #2,\n",
    "                    n_harmonics=int(mc['n_harmonics']), #1,\n",
    "                    exogenous_n_channels=int(mc['exogenous_n_channels']), #9,\n",
    "                    include_var_dict={'y': [-2, -3, -8],\n",
    "                                        'Exogenous1': [-1, -2, -8],\n",
    "                                        'Exogenous2': [-1, -2, -8],\n",
    "                                        'week_day': [-1]},\n",
    "                    t_cols=ts_dataset.t_cols,\n",
    "                    batch_normalization=mc['batch_normalization'], #False,\n",
    "                    dropout_prob_theta=float(mc['dropout_prob_theta']), #0.01,\n",
    "                    dropout_prob_exogenous=float(mc['dropout_prob_exogenous']), #0.01,\n",
    "                    x_s_n_hidden=int(mc['x_s_n_hidden']), #0,\n",
    "                    learning_rate=float(mc['learning_rate']), #0.007,\n",
    "                    lr_decay=float(mc['lr_decay']), #0.5,\n",
    "                    n_lr_decay_steps=int(mc['n_lr_decay_steps']), #3,\n",
    "                    weight_decay=float(mc['weight_decay']), #0.0000001,\n",
    "                    l1_theta=float(mc['l1_theta']), #0.0001,\n",
    "                    n_iterations=int(mc['n_iterations']), #200,\n",
    "                    early_stopping=int(mc['early_stopping']), #40,\n",
    "                    loss=mc['loss'], #'PINBALL',\n",
    "                    loss_hypar=float(mc['loss_hypar']), #0.5,\n",
    "                    val_loss=mc['val_loss'], #MAE\n",
    "                    frequency=mc['frequency'], #'H',\n",
    "                    random_seed=int(mc['random_seed']), #1,\n",
    "                    seasonality=int(mc['seasonality'])) #24)\n",
    "\n",
    "    model.fit(train_ts_loader=train_loader, val_ts_loader=val_loader, verbose=True, eval_steps=10)\n",
    "\n",
    "    print('Best Model Evaluation')\n",
    "    y_true, y_hat, _ = model.predict(ts_loader=val_loader, eval_mode=True)\n",
    "    print(forecast_evaluation_table(y_true, y_hat))\n",
    "\n",
    "    return model, train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "Y_df.shape \t\t\t (139776, 3)\n",
      "Xt_df.shape \t\t\t (139776, 12)\n",
      "Y_balanced_df.shape \t\t (209184, 3)\n",
      "Xt_balanced_df.shape \t\t (209184, 12)\n",
      "weights_balanced_df.shape \t (209184, 3)\n",
      "\n",
      "\n",
      "Xt_balanced_df.columns Index(['unique_id', 'ds', 'Exogenous1', 'Exogenous2', 'week_day', 'day_0',\n",
      "       'day_1', 'day_2', 'day_3', 'day_4', 'day_5', 'day_6'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "porcentaje1 0.6681964203763194\n",
      "porcentaje2 0.6681964203763194\n",
      "porcentaje3 0.6681964203763194\n",
      "np.sum(np.isnan(mask_df['available_mask'].values)) 0\n",
      "Train Validation splits\n",
      "                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "BE        0           2014-12-28 2016-12-26 23:00:00\n",
      "          1           2011-01-09 2014-12-27 23:00:00\n",
      "FR        0           2014-12-28 2016-12-26 23:00:00\n",
      "          1           2011-01-09 2014-12-27 23:00:00\n",
      "NP        0           2014-12-28 2016-12-26 23:00:00\n",
      "          1           2011-01-09 2014-12-27 23:00:00\n",
      "PJM       0           2014-12-28 2016-12-26 23:00:00\n",
      "          1           2011-01-09 2014-12-27 23:00:00\n",
      "Total data \t\t\t209184 time stamps\n",
      "Available prc = 0.6682, \t\t139776 time stamps\n",
      "Train prc = 0.49908, \t\t104400 time stamps\n",
      "Predict prc = 0.33502, \t\t70080 time stamps\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TSDATASET _create_tensor\n",
      "y_nans prc 0.33180357962368057\n",
      "x_nans prc 0.33180357962368057\n",
      "xy_nans prc 0.33180357962368057\n",
      "xya_nans prc 0.0\n",
      "y_nans 17352\n",
      "x_nans 52296\n",
      "available_mask_nans 0\n",
      "sample_mask_nans 0\n",
      "DATASET BE Available Mask 0.6682\n",
      "DATASET BE Sample Mask 0.66498\n",
      "DATASET BE Train Mask 0.66498\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TSDATASET _create_tensor\n",
      "y_nans prc 0.33180357962368057\n",
      "x_nans prc 0.33180357962368057\n",
      "xy_nans prc 0.33180357962368057\n",
      "xya_nans prc 0.0\n",
      "y_nans 17352\n",
      "x_nans 52296\n",
      "available_mask_nans 0\n",
      "sample_mask_nans 0\n",
      "DATASET FR Available Mask 0.6682\n",
      "DATASET FR Sample Mask 0.66498\n",
      "DATASET FR Train Mask 0.66498\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TSDATASET _create_tensor\n",
      "y_nans prc 0.33180357962368057\n",
      "x_nans prc 0.33180357962368057\n",
      "xy_nans prc 0.33180357962368057\n",
      "xya_nans prc 0.0\n",
      "y_nans 17352\n",
      "x_nans 52296\n",
      "available_mask_nans 0\n",
      "sample_mask_nans 0\n",
      "DATASET NP Available Mask 0.6682\n",
      "DATASET NP Sample Mask 0.66498\n",
      "DATASET NP Train Mask 0.33318\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TSDATASET _create_tensor\n",
      "y_nans prc 0.33180357962368057\n",
      "x_nans prc 0.33180357962368057\n",
      "xy_nans prc 0.33180357962368057\n",
      "xya_nans prc 0.0\n",
      "y_nans 17352\n",
      "x_nans 52296\n",
      "available_mask_nans 0\n",
      "sample_mask_nans 0\n",
      "DATASET PJM Available Mask 0.6682\n",
      "DATASET PJM Sample Mask 0.66498\n",
      "DATASET PJM Train Mask 0.33318\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "available_mask.shape (52296,)\n",
      "y_nans prc 0.33180357962368057\n",
      "x_nans prc 0.33180357962368057\n",
      "xy_nans prc 0.33180357962368057\n",
      "xya_nans prc 0.0\n",
      "y_nans 17352\n",
      "x_nans 52296\n",
      "available_mask_nans 0\n",
      "sample_mask_nans 0\n",
      "LOADER BE Available Mask 0.6682000160217285\n",
      "LOADER BE Sample Mask 0.6649799942970276\n",
      "LOADER BE Train Mask 0.6649799942970276\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "available_mask.shape (52296,)\n",
      "y_nans prc 0.33180357962368057\n",
      "x_nans prc 0.33180357962368057\n",
      "xy_nans prc 0.33180357962368057\n",
      "xya_nans prc 0.0\n",
      "y_nans 17352\n",
      "x_nans 52296\n",
      "available_mask_nans 0\n",
      "sample_mask_nans 0\n",
      "LOADER FR Available Mask 0.6682000160217285\n",
      "LOADER FR Sample Mask 0.6649799942970276\n",
      "LOADER FR Train Mask 0.6649799942970276\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "available_mask.shape (52296,)\n",
      "y_nans prc 0.33180357962368057\n",
      "x_nans prc 0.33180357962368057\n",
      "xy_nans prc 0.33180357962368057\n",
      "xya_nans prc 0.0\n",
      "y_nans 17352\n",
      "x_nans 52296\n",
      "available_mask_nans 0\n",
      "sample_mask_nans 0\n",
      "LOADER NP Available Mask 0.6682000160217285\n",
      "LOADER NP Sample Mask 0.6649799942970276\n",
      "LOADER NP Train Mask 0.3331800103187561\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "available_mask.shape (52296,)\n",
      "y_nans prc 0.33180357962368057\n",
      "x_nans prc 0.33180357962368057\n",
      "xy_nans prc 0.33180357962368057\n",
      "xya_nans prc 0.0\n",
      "y_nans 17352\n",
      "x_nans 52296\n",
      "available_mask_nans 0\n",
      "sample_mask_nans 0\n",
      "LOADER PJM Available Mask 0.6682000160217285\n",
      "LOADER PJM Sample Mask 0.6649799942970276\n",
      "LOADER PJM Train Mask 0.3331800103187561\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "INTENTO RARO DE LIMPIEZA8\n",
      "completely_available_condition.shape torch.Size([8720])\n",
      "complete prc 0.6646788990825688\n",
      "available_condition tensor([ 24.,  48.,  72.,  ..., 192., 192., 168.])\n",
      "completely_available_condition tensor([0, 0, 0,  ..., 1, 1, 0])\n",
      "completely_available_condition * sample_condition > 0 tensor([False, False, False,  ..., False, False, False])\n",
      "train_loader.ts_windows.shape torch.Size([8720, 13, 192])\n",
      "len(train_loader.windows_sampling_idx) * 24 =        4350 * 24 = 104400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "available_mask.shape (52296,)\n",
      "y_nans prc 0.33180357962368057\n",
      "x_nans prc 0.33180357962368057\n",
      "xy_nans prc 0.33180357962368057\n",
      "xya_nans prc 0.0\n",
      "y_nans 17352\n",
      "x_nans 52296\n",
      "available_mask_nans 0\n",
      "sample_mask_nans 0\n",
      "LOADER BE Available Mask 0.6682000160217285\n",
      "LOADER BE Sample Mask 0.6649799942970276\n",
      "LOADER BE Train Mask 0.6649799942970276\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "available_mask.shape (52296,)\n",
      "y_nans prc 0.33180357962368057\n",
      "x_nans prc 0.33180357962368057\n",
      "xy_nans prc 0.33180357962368057\n",
      "xya_nans prc 0.0\n",
      "y_nans 17352\n",
      "x_nans 52296\n",
      "available_mask_nans 0\n",
      "sample_mask_nans 0\n",
      "LOADER FR Available Mask 0.6682000160217285\n",
      "LOADER FR Sample Mask 0.6649799942970276\n",
      "LOADER FR Train Mask 0.6649799942970276\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "available_mask.shape (52296,)\n",
      "y_nans prc 0.33180357962368057\n",
      "x_nans prc 0.33180357962368057\n",
      "xy_nans prc 0.33180357962368057\n",
      "xya_nans prc 0.0\n",
      "y_nans 17352\n",
      "x_nans 52296\n",
      "available_mask_nans 0\n",
      "sample_mask_nans 0\n",
      "LOADER NP Available Mask 0.6682000160217285\n",
      "LOADER NP Sample Mask 0.6649799942970276\n",
      "LOADER NP Train Mask 0.3331800103187561\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "available_mask.shape (52296,)\n",
      "y_nans prc 0.33180357962368057\n",
      "x_nans prc 0.33180357962368057\n",
      "xy_nans prc 0.33180357962368057\n",
      "xya_nans prc 0.0\n",
      "y_nans 17352\n",
      "x_nans 52296\n",
      "available_mask_nans 0\n",
      "sample_mask_nans 0\n",
      "LOADER PJM Available Mask 0.6682000160217285\n",
      "LOADER PJM Sample Mask 0.6649799942970276\n",
      "LOADER PJM Train Mask 0.3331800103187561\n",
      "\n",
      "\n",
      "val_loader.ts_windows.shape torch.Size([8720, 13, 192])\n",
      "len(val_loader.windows_sampling_idx) * 24 =        2920 * 24 = 70080\n",
      "\n",
      "\n",
      "============================== Start fitting ==============================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "INTENTO RARO DE LIMPIEZA8\n",
      "insample_y_nans 0\n",
      "outsample_y_nans 0\n",
      "insample_x_nans 672\n",
      "outsample_x_nans 96\n",
      "available_mask_nans 336.0\n",
      "sample_mask_nans 48.0\n",
      "data.shape (2, 10, 24)\n",
      "channel 0\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "channel 1\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "channel 2\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "channel 3\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "channel 4\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "channel 5\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "channel 6\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "channel 7\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "channel 8\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "channel 9\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "sum(data_nans)/len(data_nans) [0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4\n",
      " 0.4 0.4 0.4 0.4 0.4 0.4]\n",
      "available_mask.shape (2, 168)\n",
      "sum(available_mask)/len(availability) 1.0\n",
      "sum(sample_mask)/len(availability) 1.0\n",
      "available_mask[np.isnan(available_mask)]\n",
      "[]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-db0967493119>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mXt_balanced_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_val_nbeatsx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_balanced_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXt_scaled_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-91f13656c6da>\u001b[0m in \u001b[0;36mrun_val_nbeatsx\u001b[0;34m(mc, Y_df, Xt_df, S_df, mask_df)\u001b[0m\n\u001b[1;32m     74\u001b[0m                     seasonality=int(mc['seasonality'])) #24)\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ts_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ts_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best Model Evaluation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ecbed1217038>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_ts_loader, val_ts_loader, n_iterations, verbose, eval_steps)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;31m#print(y_true_insample[y_true_nans])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = pd.Series({'dataset': ['NP', 'PJM', 'BE', 'FR']})\n",
    "# args = pd.Series({'dataset': ['NP', 'PJM']})\n",
    "# args = pd.Series({'dataset': ['NP']})\n",
    "\n",
    "Y_df, Xt_df, S_df = EPF.load_groups(directory='data', groups=args.dataset)\n",
    "Y_balanced_df, Xt_balanced_df, weights_balanced_df = balance_data(Y_df, Xt_df)\n",
    "del Y_df, Xt_df\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Xt_balanced_df.columns\", Xt_balanced_df.columns)\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Create available_mask and sample_mask\n",
    "# TODO: Include not null exogenous condition to available mask\n",
    "mask_df = Y_balanced_df[['unique_id', 'ds', 'y']].copy()\n",
    "mask_df['available_mask'] = (1-Y_balanced_df.y.isnull().values)\n",
    "mask_df['available_mask2'] = (1-Xt_balanced_df['Exogenous1'].isnull().values)\n",
    "mask_df['available_mask3'] = mask_df['available_mask'] * mask_df['available_mask2']\n",
    "\n",
    "print(\"porcentaje1\", sum(mask_df.available_mask)/len(mask_df.available_mask))\n",
    "print(\"porcentaje2\", sum(mask_df.available_mask2)/len(mask_df.available_mask))\n",
    "print(\"porcentaje3\", sum(mask_df.available_mask2)/len(mask_df.available_mask))\n",
    "print(\"np.sum(np.isnan(mask_df['available_mask'].values))\", np.sum(np.isnan(mask_df['available_mask'].values)))\n",
    "\n",
    "del mask_df['y']\n",
    "\n",
    "# Train Validation splits\n",
    "#                        ds                    \n",
    "#                       min                 max\n",
    "# unique_id mask                               \n",
    "# BE        0.0  2013-01-04 2015-01-03 23:00:00\n",
    "#           1.0  2011-01-09 2013-01-03 23:00:00\n",
    "# FR        0.0  2013-01-04 2015-01-03 23:00:00\n",
    "#           1.0  2011-01-09 2013-01-03 23:00:00\n",
    "# NP        0.0  2014-12-28 2016-12-26 23:00:00\n",
    "#           1.0  2013-01-01 2014-12-27 23:00:00\n",
    "# PJM       0.0  2014-12-28 2016-12-26 23:00:00\n",
    "#           1.0  2013-01-01 2014-12-27 23:00:00\n",
    "mask_df['sample_mask1'] = (mask_df['ds'] <= pd.to_datetime('2013-01-03 23:00:00')) * 1\n",
    "mask_df['sample_mask2'] = (mask_df['ds'] <= pd.to_datetime('2014-12-27 23:00:00')) * 1\n",
    "mask_df['sample_mask'] = mask_df['sample_mask2']\n",
    "\n",
    "# Scale data # TODO: write sample_mask conditional/groupby(['unique_id]) scaling\n",
    "Xt_scaled_df = scale_data(Xt_df=Xt_balanced_df, mask_df=mask_df)\n",
    "del Xt_balanced_df\n",
    "\n",
    "model, train_loader, val_loader = run_val_nbeatsx(mc=mc, Y_df=Y_balanced_df, Xt_df=Xt_scaled_df, S_df=None, mask_df=mask_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_hat, _ = model.predict(ts_loader=val_loader, eval_mode=True)\n",
    "print(\"y_true.shape \\t\\t(#n_series, #n_fcds, #lt) \\t\", y_true.shape)\n",
    "print(\"y_hat.shape  \\t\\t(#n_series, #n_fcds, #lt) \\t\", y_hat.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "performance_df = {'unique_id': [], 'metric': [], 'nbeatsx': []}\n",
    "for i, meta_data in enumerate(val_loader.ts_dataset.meta_data):\n",
    "    #dataset = meta_data['unique_id']\n",
    "    y_hat_plot = y_hat[i,:,:].reshape(-1)\n",
    "    y_true_plot = y_true[i,:,:].reshape(-1)\n",
    "\n",
    "    # Create performance table\n",
    "    market = val_loader.ts_dataset.meta_data[i]['unique_id']\n",
    "    performance_df['unique_id'] += [market]*4\n",
    "    performance_df['metric'] += ['MAE', 'MAPE', 'SMAPE', 'RMSE']\n",
    "    performance = forecast_evaluation_table(y_true_plot, y_hat_plot)\n",
    "    performance_df['nbeatsx'] += performance.measure.to_list()\n",
    "    \n",
    "    performance = np.round(mae(y_true_plot, y_hat_plot), 5)\n",
    "    plt.plot(range(len(y_true_plot)), y_true_plot, label='true', alpha=0.7)\n",
    "    plt.plot(range(len(y_hat_plot)), y_hat_plot, label='pred', alpha=0.7)\n",
    "    plt.title(f\"{market} predictions \\n all lead time  MAE={performance}\")\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Electricity Price')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "performance_df = pd.DataFrame(performance_df)\n",
    "benchmark_df = pd.DataFrame({'id': [1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4],\n",
    "                             'unique_id': ['NP', 'NP', 'NP', 'NP', 'PJM', 'PJM', 'PJM', 'PJM',\n",
    "                                           'BE', 'BE', 'BE', 'BE', 'FR', 'FR', 'FR', 'FR'],\n",
    "                             'metric': ['MAE', 'MAPE', 'SMAPE', 'RMSE', 'MAE', 'MAPE', 'SMAPE', 'RMSE',\n",
    "                                        'MAE', 'MAPE', 'SMAPE', 'RMSE', 'MAE', 'MAPE', 'SMAPE', 'RMSE'],    \n",
    "                             'dnn' : [1.67, 5.38, 4.85, 3.33, 2.78, 28.66, 11.22, 4.64, 5.82,\n",
    "                                      26.11, 13.33, 16.13, 3.91, 14.77, 10.98, 11.74]})\n",
    "\n",
    "benchmark_df.sort_values(['id'], inplace=True)\n",
    "benchmark_df.reset_index(drop=True, inplace=True)\n",
    "benchmark_df = performance_df.merge(benchmark_df, on=['unique_id', 'metric'], how='left')\n",
    "benchmark_df['perc_diff'] = 100 * (benchmark_df['nbeatsx']-benchmark_df['dnn'])/benchmark_df['dnn']\n",
    "benchmark_df['improvement'] = benchmark_df['perc_diff'] < 0\n",
    "benchmark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c68c0f618c4f787b1e02bfee278e48d25b62407cb335aab5258cedc4db4ced"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}