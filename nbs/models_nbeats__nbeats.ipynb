{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.nbeats.nbeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "import torch as t\n",
    "from torch import optim\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "from nixtla.models.nbeats.nbeats_model import NBeats, NBeatsBlock, IdentityBasis, TrendBasis, SeasonalityBasis\n",
    "from nixtla.models.nbeats.nbeats_model import ExogenousBasisInterpretable, ExogenousBasisWavenet, ExogenousBasisTCN\n",
    "from nixtla.losses.pytorch import MAPELoss, MASELoss, SMAPELoss, MSELoss, MAELoss, PinballLoss\n",
    "from nixtla.losses.numpy import mae, mse, mape, smape, rmse, pinball_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_weights(module, initialization):\n",
    "    if type(module) == t.nn.Linear:\n",
    "        if initialization == 'orthogonal':\n",
    "            t.nn.init.orthogonal_(module.weight)\n",
    "        elif initialization == 'he_uniform':\n",
    "            t.nn.init.kaiming_uniform_(module.weight)\n",
    "        elif initialization == 'he_normal':\n",
    "            t.nn.init.kaiming_normal_(module.weight)\n",
    "        elif initialization == 'glorot_uniform':\n",
    "            t.nn.init.xavier_uniform_(module.weight)\n",
    "        elif initialization == 'glorot_normal':\n",
    "            t.nn.init.xavier_normal_(module.weight)\n",
    "        elif initialization == 'lecun_normal':\n",
    "            pass #t.nn.init.normal_(module.weight, 0.0, std=1/np.sqrt(module.weight.numel()))\n",
    "        else:\n",
    "            assert 1<0, f'Initialization {initialization} not found'\n",
    "\n",
    "class Nbeats(object):\n",
    "    \"\"\"\n",
    "    Future documentation\n",
    "    \"\"\"\n",
    "    SEASONALITY_BLOCK = 'seasonality'\n",
    "    TREND_BLOCK = 'trend'\n",
    "    IDENTITY_BLOCK = 'identity'\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 output_size,\n",
    "                 shared_weights,\n",
    "                 activation,\n",
    "                 initialization,\n",
    "                 stack_types,\n",
    "                 n_blocks,\n",
    "                 n_layers,\n",
    "                 n_hidden,\n",
    "                 n_harmonics,\n",
    "                 n_polynomials,\n",
    "                 exogenous_n_channels,\n",
    "                 include_var_dict,\n",
    "                 t_cols,\n",
    "                 batch_normalization,\n",
    "                 dropout_prob_theta,\n",
    "                 dropout_prob_exogenous,\n",
    "                 x_s_n_hidden,\n",
    "                 learning_rate,\n",
    "                 lr_decay,\n",
    "                 n_lr_decay_steps,\n",
    "                 weight_decay,\n",
    "                 l1_theta,\n",
    "                 n_iterations,\n",
    "                 early_stopping,\n",
    "                 loss,\n",
    "                 loss_hypar,\n",
    "                 frequency,\n",
    "                 random_seed,\n",
    "                 seasonality,\n",
    "                 device=None):\n",
    "        super(Nbeats, self).__init__()\n",
    "\n",
    "        if activation == 'selu': initialization = 'lecun_normal'\n",
    "\n",
    "        #------------------------ Model Attributes ------------------------#\n",
    "        # Architecture parameters\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.shared_weights = shared_weights\n",
    "        self.activation = activation\n",
    "        self.initialization = initialization\n",
    "        self.stack_types = stack_types\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_harmonics = n_harmonics\n",
    "        self.n_polynomials = n_polynomials\n",
    "        self.exogenous_n_channels = exogenous_n_channels\n",
    "\n",
    "        # Regularization and optimization parameters\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.dropout_prob_theta = dropout_prob_theta\n",
    "        self.dropout_prob_exogenous = dropout_prob_exogenous\n",
    "        self.x_s_n_hidden = x_s_n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.n_lr_decay_steps = n_lr_decay_steps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.n_iterations = n_iterations\n",
    "        self.early_stopping = early_stopping\n",
    "        self.loss = loss\n",
    "        self.loss_hypar = loss_hypar\n",
    "        self.l1_theta = l1_theta\n",
    "        self.l1_conv = 1e-3 # Not a hyperparameter\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Data parameters\n",
    "        self.frequency = frequency\n",
    "        self.seasonality = seasonality        \n",
    "        self.include_var_dict = include_var_dict\n",
    "        self.t_cols = t_cols\n",
    "        #self.scaler = scaler\n",
    "\n",
    "        if device is None:\n",
    "            device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
    "        self.device = device\n",
    "\n",
    "        self._is_instantiated = False\n",
    "\n",
    "    def create_stack(self):\n",
    "        if self.include_var_dict is not None:\n",
    "            x_t_n_inputs = self.output_size * int(sum([len(x) for x in self.include_var_dict.values()]))\n",
    "            \n",
    "            # Correction because week_day only adds 1 no output_size\n",
    "            if len(self.include_var_dict['week_day'])>0:\n",
    "                x_t_n_inputs = x_t_n_inputs - self.output_size + 1 \n",
    "        else:\n",
    "            x_t_n_inputs = self.input_size\n",
    "        \n",
    "        #------------------------ Model Definition ------------------------#\n",
    "        block_list = []\n",
    "        self.blocks_regularizer = []\n",
    "        for i in range(len(self.stack_types)):\n",
    "            #print(f'| --  Stack {self.stack_types[i]} (#{i})')\n",
    "            for block_id in range(self.n_blocks[i]):\n",
    "                \n",
    "                # Batch norm only on first block\n",
    "                if (len(block_list)==0) and (self.batch_normalization):\n",
    "                    batch_normalization_block = True\n",
    "                else:\n",
    "                    batch_normalization_block = False\n",
    "                \n",
    "                # Dummy of regularizer in block. Override with 1 if exogenous_block\n",
    "                self.blocks_regularizer += [0]\n",
    "\n",
    "                # Shared weights\n",
    "                if self.shared_weights and block_id>0:\n",
    "                    nbeats_block = block_list[-1]\n",
    "                else:\n",
    "                    if self.stack_types[i] == 'seasonality':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=4 * int(\n",
    "                                                        np.ceil(self.n_harmonics / 2 * self.output_size) - (self.n_harmonics - 1)),\n",
    "                                                   basis=SeasonalityBasis(harmonics=self.n_harmonics,\n",
    "                                                                          backcast_size=self.input_size,\n",
    "                                                                          forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'trend':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2 * (self.n_polynomials + 1),\n",
    "                                                   basis=TrendBasis(degree_of_polynomial=self.n_polynomials,\n",
    "                                                                            backcast_size=self.input_size,\n",
    "                                                                            forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'identity':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=self.input_size + self.output_size,\n",
    "                                                   basis=IdentityBasis(backcast_size=self.input_size,\n",
    "                                                                       forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2*self.n_x_t,\n",
    "                                                   basis=ExogenousBasisInterpretable(),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous_tcn':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden = self.x_s_n_hidden,\n",
    "                                                   theta_n_dim = 2*(self.exogenous_n_channels),\n",
    "                                                   basis= ExogenousBasisTCN(self.exogenous_n_channels, self.n_x_t),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous_wavenet':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2*(self.exogenous_n_channels),\n",
    "                                                   basis=ExogenousBasisWavenet(self.exogenous_n_channels, self.n_x_t),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                        self.blocks_regularizer[-1] = 1\n",
    "                    else:\n",
    "                        assert 1<0, f'Block type not found!'\n",
    "                # Select type of evaluation and apply it to all layers of block\n",
    "                init_function = partial(init_weights, initialization=self.initialization)                                             \n",
    "                nbeats_block.layers.apply(init_function)\n",
    "                #print(f'     | -- {nbeats_block}')\n",
    "                block_list.append(nbeats_block)\n",
    "        return block_list\n",
    "\n",
    "    def __loss_fn(self, loss_name: str):\n",
    "        def loss(x, loss_hypar, forecast, target, mask):\n",
    "            if loss_name == 'MAPE':\n",
    "                return MAPELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'MASE':\n",
    "                return MASELoss(y=target, y_hat=forecast, y_insample=x, seasonality=loss_hypar, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return SMAPELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'MSE':\n",
    "                return MSELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'MAE':\n",
    "                return MAELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'PINBALL':\n",
    "                return PinballLoss(y=target, y_hat=forecast, mask=mask, tau=loss_hypar) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def __val_loss_fn(self, loss_name='MAE'):\n",
    "        #TODO: mase not implemented\n",
    "        def loss(forecast, target, weights):\n",
    "            if loss_name == 'MAPE':\n",
    "                return mape(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return smape(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MSE':\n",
    "                return mse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'RMSE':\n",
    "                return rmse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MAE':\n",
    "                return mae(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'PINBALL':\n",
    "                return pinball_loss(y=target, y_hat=forecast, weights=weights, tau=0.5)\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "    \n",
    "    def loss_l1_conv_layers(self):\n",
    "        loss_l1 = 0\n",
    "        for i, indicator in enumerate(self.blocks_regularizer):\n",
    "            if indicator:\n",
    "                loss_l1 += self.l1_conv * t.sum(t.abs(self.model.blocks[i].basis.weight))\n",
    "        return loss_l1\n",
    "    \n",
    "    def loss_l1_theta(self):\n",
    "        loss_l1 = 0\n",
    "        for block in self.model.blocks:\n",
    "            for layer in block.modules():\n",
    "                if isinstance(layer, t.nn.Linear):\n",
    "                    loss_l1 += self.l1_theta * layer.weight.abs().sum()\n",
    "        return loss_l1\n",
    "\n",
    "    def to_tensor(self, x: np.ndarray) -> t.Tensor:\n",
    "        tensor = t.as_tensor(x, dtype=t.float32).to(self.device)\n",
    "        return tensor\n",
    "\n",
    "    def evaluate_performance(self, ts_loader, validation_loss_fn):\n",
    "        #TODO: mas opciones que mae\n",
    "        self.model.eval()\n",
    "\n",
    "        losses = []\n",
    "        with t.no_grad():\n",
    "            for batch in iter(ts_loader):\n",
    "                # Parse batch\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                forecast   = self.model(x_s=s_matrix, insample_y=insample_y, \n",
    "                                        insample_x_t=insample_x, outsample_x_t=outsample_x,\n",
    "                                        insample_mask=insample_mask)\n",
    "                batch_loss = validation_loss_fn(target=forecast.cpu().data.numpy(),\n",
    "                                                forecast=outsample_y.cpu().data.numpy(),\n",
    "                                                weights=outsample_mask.cpu().data.numpy())\n",
    "                losses.append(batch_loss)\n",
    "        loss = np.mean(losses)\n",
    "        self.model.train()\n",
    "        return loss\n",
    "\n",
    "    def predict_all(self, ts_loader):\n",
    "        self.model.eval()\n",
    "        shuffle = ts_loader.shuffle\n",
    "        ts_loader.shuffle = False\n",
    "        \n",
    "        y_true = []\n",
    "        forecasts = []\n",
    "        with t.no_grad():\n",
    "            for batch in iter(ts_loader):\n",
    "                # Parse batch\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                forecast   = self.model(x_s=s_matrix, insample_y=insample_y, \n",
    "                                        insample_x_t=insample_x, outsample_x_t=outsample_x,\n",
    "                                        insample_mask=insample_mask)\n",
    "\n",
    "                y_true.append(batch['outsample_y'])\n",
    "                forecasts.append(forecast.cpu().data.numpy())\n",
    "\n",
    "        y_true = np.vstack(y_true)\n",
    "        forecasts = np.vstack(forecasts)\n",
    "\n",
    "        self.model.train()\n",
    "        ts_loader.shuffle = ts_loader.shuffle\n",
    "        return y_true, forecasts\n",
    "\n",
    "    def fit(self, train_ts_loader, val_ts_loader=None, n_iterations=None, verbose=True, eval_steps=1):\n",
    "        # TODO: Indexes hardcoded, information duplicated in train and val datasets\n",
    "        assert (self.input_size)==train_ts_loader.input_size, \\\n",
    "            f'model input_size {self.input_size} data input_size {train_ts_loader.input_size}'\n",
    "\n",
    "        # Random Seeds (model initialization)\n",
    "        t.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed) #TODO: interaccion rara con window_sampling de validacion\n",
    "\n",
    "        # Attributes of ts_dataset\n",
    "        self.n_x_t, self.n_x_s = train_ts_loader.get_n_variables()\n",
    "\n",
    "        # Instantiate model\n",
    "        if not self._is_instantiated:\n",
    "            block_list = self.create_stack()\n",
    "            self.model = NBeats(t.nn.ModuleList(block_list)).to(self.device)\n",
    "            self._is_instantiated = True\n",
    "\n",
    "        # Overwrite n_iterations and train datasets\n",
    "        if n_iterations is None:\n",
    "            n_iterations = self.n_iterations\n",
    "\n",
    "        lr_decay_steps = n_iterations // self.n_lr_decay_steps\n",
    "        if lr_decay_steps == 0:\n",
    "            lr_decay_steps = 1\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_steps, gamma=self.lr_decay)\n",
    "        training_loss_fn = self.__loss_fn(self.loss)\n",
    "        validation_loss_fn = self.__val_loss_fn(self.loss) #Uses numpy losses\n",
    "\n",
    "        print('='*30+' Start fitting '+'='*30)\n",
    "\n",
    "        #self.loss_dict = {} # Restart self.loss_dict\n",
    "        start = time.time()\n",
    "        self.trajectories = {'iteration':[],'train_loss':[], 'val_loss':[]}\n",
    "        self.final_insample_loss = None\n",
    "        self.final_outsample_loss = None\n",
    "        \n",
    "        # Training Loop\n",
    "        early_stopping_counter = 0\n",
    "        best_val_loss = np.inf\n",
    "        best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "        break_flag = False\n",
    "        iteration = 0\n",
    "        epoch = 0\n",
    "        while (iteration < n_iterations) and (not break_flag):\n",
    "            epoch +=1\n",
    "            for batch in iter(train_ts_loader):\n",
    "                iteration += 1\n",
    "                if (iteration > n_iterations) or (break_flag):\n",
    "                    continue\n",
    "\n",
    "                self.model.train()\n",
    "                train_ts_loader.train()\n",
    "                # Parse batch\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                forecast   = self.model(x_s=s_matrix, insample_y=insample_y, \n",
    "                                        insample_x_t=insample_x, outsample_x_t=outsample_x,\n",
    "                                        insample_mask=insample_mask)\n",
    "\n",
    "                training_loss = training_loss_fn(x=insample_y, loss_hypar=self.loss_hypar, forecast=forecast,\n",
    "                                                 target=outsample_y, mask=outsample_mask)\n",
    "\n",
    "                if np.isnan(float(training_loss)):\n",
    "                    break\n",
    "\n",
    "                training_loss.backward()\n",
    "                t.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "                if (iteration % eval_steps == 0):\n",
    "                    display_string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(iteration,\n",
    "                                                                                    time.time()-start,\n",
    "                                                                                    self.loss,\n",
    "                                                                                    training_loss.cpu().data.numpy())\n",
    "                    self.trajectories['iteration'].append(iteration)\n",
    "                    self.trajectories['train_loss'].append(training_loss.cpu().data.numpy())\n",
    "\n",
    "                    if val_ts_loader is not None:\n",
    "                        loss = self.evaluate_performance(ts_loader=val_ts_loader, \n",
    "                                                         validation_loss_fn=validation_loss_fn)\n",
    "                        display_string += \", Outsample {}: {:.5f}\".format(self.loss, loss)\n",
    "                        self.trajectories['val_loss'].append(loss)\n",
    "\n",
    "                        if self.early_stopping:\n",
    "                            if loss < best_val_loss:\n",
    "                                # Save current model if improves outsample loss\n",
    "                                best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "                                best_insample_loss = training_loss.cpu().data.numpy()\n",
    "                                early_stopping_counter = 0\n",
    "                                best_val_loss = loss\n",
    "                            else:\n",
    "                                early_stopping_counter += 1\n",
    "                            if early_stopping_counter >= self.early_stopping:\n",
    "                                break_flag = True\n",
    "                    \n",
    "                    print(display_string)\n",
    "\n",
    "                    self.model.train()\n",
    "                    train_ts_loader.train()\n",
    "\n",
    "            if break_flag:\n",
    "                print(10*'-',' Stopped training by early stopping', 10*'-')\n",
    "                self.model.load_state_dict(best_state_dict)\n",
    "                break\n",
    "\n",
    "        #End of fitting\n",
    "        if n_iterations >0:\n",
    "            self.final_insample_loss = training_loss.cpu().data.numpy() if not break_flag else best_insample_loss #This is batch!\n",
    "            string = 'Iteration: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(iteration,\n",
    "                                                                            time.time()-start,\n",
    "                                                                            self.loss,\n",
    "                                                                            self.final_insample_loss)\n",
    "            if val_ts_loader is not None:\n",
    "                self.final_outsample_loss = self.evaluate_performance(ts_loader=val_ts_loader, \n",
    "                                                                      validation_loss_fn=validation_loss_fn)\n",
    "                string += \", Outsample {}: {:.5f}\".format(self.loss, self.final_outsample_loss)\n",
    "            print(string)\n",
    "            print('='*30+'End fitting '+'='*30)\n",
    "\n",
    "    def predict(self, ts_loader, X_test=None, eval_mode=False):\n",
    "\n",
    "        ts_loader.eval()\n",
    "        frequency = ts_loader.get_frequency()\n",
    "\n",
    "        # Build forecasts\n",
    "        unique_ids = ts_loader.get_meta_data_col('unique_id')\n",
    "        last_ds = ts_loader.get_meta_data_col('last_ds') #TODO: ajustar of offset\n",
    "\n",
    "        self.model.eval()\n",
    "        with t.no_grad():\n",
    "            forecasts = []\n",
    "            outsample_ys = []\n",
    "            outsample_masks = []\n",
    "            for batch in iter(ts_loader):\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                forecast = self.model(insample_y=insample_y, insample_x_t=insample_x,\n",
    "                                      insample_mask=insample_mask, outsample_x_t=outsample_x, x_s=s_matrix)\n",
    "                forecasts += [forecast.cpu().data.numpy()]\n",
    "                outsample_ys += [outsample_y.cpu().data.numpy()]\n",
    "                outsample_masks += [outsample_mask.cpu().data.numpy()]\n",
    "        forecasts = np.vstack(forecasts)\n",
    "        outsample_ys = np.vstack(outsample_ys)\n",
    "        outsample_masks = np.vstack(outsample_masks)\n",
    "\n",
    "        if eval_mode:\n",
    "            return forecasts, outsample_ys, outsample_masks\n",
    "\n",
    "        # Predictions for panel\n",
    "        Y_hat_panel = pd.DataFrame(columns=['unique_id', 'ds'])\n",
    "        for i, unique_id in enumerate(unique_ids):\n",
    "            Y_hat_id = pd.DataFrame([unique_id]*self.output_size, columns=[\"unique_id\"])\n",
    "            ds = pd.date_range(start=last_ds[i], periods=self.output_size+1, freq=frequency)\n",
    "            Y_hat_id[\"ds\"] = ds[1:]\n",
    "            Y_hat_panel = Y_hat_panel.append(Y_hat_id, sort=False).reset_index(drop=True)\n",
    "\n",
    "        Y_hat_panel['y_hat'] = forecasts.flatten()\n",
    "\n",
    "        if X_test is not None:\n",
    "            Y_hat_panel = X_test.merge(Y_hat_panel, on=['unique_id', 'ds'], how='left')\n",
    "        \n",
    "        return Y_hat_panel\n",
    "\n",
    "\n",
    "    def save(self, model_dir, model_id, state_dict = None):\n",
    "    \n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        if state_dict is None:\n",
    "            state_dict = self.model.state_dict()\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        print('Saving model to:\\n {}'.format(model_file)+'\\n')\n",
    "        t.save({'model_state_dict': state_dict}, model_file)\n",
    "\n",
    "    def load(self, model_dir, model_id):\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        path = Path(model_file)\n",
    "\n",
    "        assert path.is_file(), 'No model_*.model file found in this path!'\n",
    "\n",
    "        print('Loading model from:\\n {}'.format(model_file)+'\\n')\n",
    "\n",
    "        checkpoint = t.load(model_file, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(self.device)"
   ]
  },
  {
   "source": [
    "# MINI TEST"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nixtla.data.scalers import Scaler\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset\n",
    "from nixtla.data.tsloader_fast import TimeSeriesLoader\n",
    "#from nixtla.data.tsloader_general import TimeSeriesLoader\n",
    "\n",
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "\n",
    "mc = {'input_size': 7*24,\n",
    "      'output_size': 24,\n",
    "      'window_sampling_limit': 365*4*24,\n",
    "      'idx_to_sample_freq': 1,\n",
    "      'batch_size': 256,\n",
    "      'shared_weights': False,\n",
    "      'activation': 'relu',\n",
    "      'initialization': 'lecun_normal',\n",
    "      'stack_types': 2*['identity'],\n",
    "      'n_blocks': 2*[1],\n",
    "      'n_layers': 2*[2],\n",
    "      'n_hidden': 2*[[256,256]],\n",
    "      'n_polynomials': 2,\n",
    "      'n_harmonics': 1,\n",
    "      'exogenous_n_channels': 9,\n",
    "      'include_var_dict': {'y': [-2, -3, -8],\n",
    "                           'Exogenous1': [-1, -2, -8],\n",
    "                           'Exogenous2': [-1, -2, -8],\n",
    "                           'week_day': [-1]},\n",
    "      'batch_normalization': False,\n",
    "      'dropout_prob_theta': 0.01,\n",
    "      'dropout_prob_exogenous': 0.01,\n",
    "      'x_s_n_hidden': 0,\n",
    "      'learning_rate': 0.007,\n",
    "      'lr_decay': 0.5,\n",
    "      'n_lr_decay_steps': 3,\n",
    "      'weight_decay': 0.0000001,\n",
    "      'l1_theta': 0.0001,\n",
    "      'n_iterations': 200,\n",
    "      'early_stopping': 40,\n",
    "      'loss': 'MAE',\n",
    "      'loss_hypar': 0.5,\n",
    "      'frequency': 'H',\n",
    "      'random_seed': 1,\n",
    "      'seasonality': 24}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_val_nbeatsx(mc, Y_df, Xt_df, val_ds): #, trials, trials_file_name):\n",
    "\n",
    "    offset = val_ds * mc['output_size']\n",
    "\n",
    "    # To not modify original data\n",
    "    Xt_scaled_df = Xt_df.copy()\n",
    "\n",
    "    # Transform data with scale transformation\n",
    "    scaler = Scaler(normalizer='norm')\n",
    "    Xt_scaled_df['Exogenous1'] = scaler.scale(x=Xt_scaled_df['Exogenous1'].values, offset=offset)\n",
    "\n",
    "    scaler = Scaler(normalizer='norm')\n",
    "    Xt_scaled_df['Exogenous2'] = scaler.scale(x=Xt_scaled_df['Exogenous2'].values, offset=offset)\n",
    "\n",
    "    # train_mask: 1 to keep, 0 to mask\n",
    "    train_outsample_mask = np.ones(len(Y_df))\n",
    "    train_outsample_mask[-val_ds * mc['output_size']:] = 0\n",
    "\n",
    "    ts_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=None, X_df=Xt_scaled_df, \n",
    "                                   ts_train_mask=train_outsample_mask)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print('EPFInfo.groups[0]', EPFInfo.groups[0])\n",
    "    #print(\"X_df.columns\", X_df.columns)\n",
    "    print(f'Train mask percentage: {np.round(np.sum(train_outsample_mask)/len(train_outsample_mask),2)}')\n",
    "    print('X: time series features, of shape (#hours, #times,#features): \\t' + str(Xt_df.shape))\n",
    "    print('Y: target series (in X), of shape (#hours, #times): \\t \\t' + str(Y_df.shape))\n",
    "    print(f'{len(Xt_df)} hours = {np.round(len(Xt_df)/(24*365),2)} years')\n",
    "    # print('S: static features, of shape (#series,#features): \\t \\t' + str(S.shape))\n",
    "    #Y_df.head()\n",
    "    print(\"\\n\")\n",
    "\n",
    "    train_loader = TimeSeriesLoader(ts_dataset=ts_dataset,\n",
    "                                    model='nbeats',\n",
    "                                    offset=0, #offset,\n",
    "                                    window_sampling_limit=int(mc['window_sampling_limit']), \n",
    "                                    input_size=int(mc['input_size']),\n",
    "                                    output_size=int(mc['output_size']),\n",
    "                                    idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                    batch_size=int(mc['batch_size']),\n",
    "                                    is_train_loader=True,\n",
    "                                    shuffle=True)\n",
    "\n",
    "    val_loader = TimeSeriesLoader(ts_dataset=ts_dataset,\n",
    "                                  model='nbeats',\n",
    "                                  offset=0, #offset,\n",
    "                                  window_sampling_limit=int(mc['window_sampling_limit']), \n",
    "                                  input_size=int(mc['input_size']),\n",
    "                                  output_size=int(mc['output_size']),\n",
    "                                  idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                  batch_size=1024,\n",
    "                                  is_train_loader=False, # Samples the opposite of train_outsample_mask\n",
    "                                  shuffle=False)\n",
    "\n",
    "    model = Nbeats(input_size=int(mc['input_size']),\n",
    "                   output_size=int(mc['output_size']),\n",
    "                   shared_weights=int(mc['shared_weights']),\n",
    "                   activation=mc['activation'],\n",
    "                   initialization=mc['initialization'],\n",
    "                   stack_types=mc['stack_types'], #2*['identity'],\n",
    "                   n_blocks=mc['n_blocks'], #2*[1],\n",
    "                   n_layers=mc['n_layers'], #2*[2],\n",
    "                   n_hidden=mc['n_hidden'], #2*[[256,256]],\n",
    "                   n_polynomials=mc['n_polynomials'], #2,\n",
    "                   n_harmonics=int(mc['n_harmonics']), #1,\n",
    "                   exogenous_n_channels=int(mc['exogenous_n_channels']), #9,\n",
    "                   include_var_dict={'y': [-2, -3, -8],\n",
    "                                     'Exogenous1': [-1, -2, -8],\n",
    "                                     'Exogenous2': [-1, -2, -8],\n",
    "                                     'week_day': [-1]},\n",
    "                   t_cols=ts_dataset.t_cols,\n",
    "                   batch_normalization=mc['batch_normalization'], #False,\n",
    "                   dropout_prob_theta=float(mc['dropout_prob_theta']), #0.01,\n",
    "                   dropout_prob_exogenous=float(mc['dropout_prob_exogenous']), #0.01,\n",
    "                   x_s_n_hidden=int(mc['x_s_n_hidden']), #0,\n",
    "                   learning_rate=float(mc['learning_rate']), #0.007,\n",
    "                   lr_decay=float(mc['lr_decay']), #0.5,\n",
    "                   n_lr_decay_steps=int(mc['n_lr_decay_steps']), #3,\n",
    "                   weight_decay=float(mc['weight_decay']), #0.0000001,\n",
    "                   l1_theta=float(mc['l1_theta']), #0.0001,\n",
    "                   n_iterations=int(mc['n_iterations']), #200,\n",
    "                   early_stopping=int(mc['early_stopping']), #40,\n",
    "                   loss=mc['loss'], #'MAE',\n",
    "                   loss_hypar=float(mc['loss_hypar']), #0.5,\n",
    "                   frequency=mc['frequency'], #'H',\n",
    "                   random_seed=int(mc['random_seed']), #1,\n",
    "                   seasonality=int(mc['seasonality'])) #24)\n",
    "\n",
    "    model.fit(train_ts_loader=train_loader, val_ts_loader=val_loader, verbose=True, eval_steps=10)\n",
    "\n",
    "    return model, train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing dataframes ...\n",
      "Creating ts tensor ...\n",
      "\n",
      "\n",
      "EPFInfo.groups[0] NP\n",
      "Train mask percentage: 0.75\n",
      "X: time series features, of shape (#hours, #times,#features): \t(34944, 12)\n",
      "Y: target series (in X), of shape (#hours, #times): \t \t(34944, 3)\n",
      "34944 hours = 3.99 years\n",
      "\n",
      "\n",
      "============================== Start fitting ==============================\n",
      "Step: 10, Time: 0.400, Insample MAE: 3.65162, Outsample MAE: 3.72664\n",
      "Step: 20, Time: 1.577, Insample MAE: 3.22409, Outsample MAE: 4.04275\n",
      "Step: 30, Time: 2.780, Insample MAE: 4.07978, Outsample MAE: 3.09760\n",
      "Step: 40, Time: 4.033, Insample MAE: 3.06313, Outsample MAE: 3.06522\n",
      "Step: 50, Time: 5.132, Insample MAE: 3.28339, Outsample MAE: 3.06456\n",
      "Step: 60, Time: 6.234, Insample MAE: 2.77190, Outsample MAE: 2.98823\n",
      "Step: 70, Time: 7.326, Insample MAE: 2.65850, Outsample MAE: 2.79409\n",
      "Step: 80, Time: 8.420, Insample MAE: 2.86717, Outsample MAE: 2.70383\n",
      "Step: 90, Time: 9.509, Insample MAE: 2.68181, Outsample MAE: 2.63725\n",
      "Step: 100, Time: 10.600, Insample MAE: 2.57018, Outsample MAE: 2.57422\n",
      "Step: 110, Time: 11.676, Insample MAE: 2.41161, Outsample MAE: 2.54167\n",
      "Step: 120, Time: 12.767, Insample MAE: 2.56655, Outsample MAE: 2.49583\n",
      "Step: 130, Time: 13.872, Insample MAE: 2.31770, Outsample MAE: 2.44622\n",
      "Step: 140, Time: 14.961, Insample MAE: 2.38873, Outsample MAE: 2.48053\n",
      "Step: 150, Time: 16.050, Insample MAE: 2.26737, Outsample MAE: 2.42598\n",
      "Step: 160, Time: 17.144, Insample MAE: 2.19757, Outsample MAE: 2.37217\n",
      "Step: 170, Time: 18.230, Insample MAE: 2.05034, Outsample MAE: 2.35414\n",
      "Step: 180, Time: 19.321, Insample MAE: 2.11842, Outsample MAE: 2.33031\n",
      "Step: 190, Time: 20.411, Insample MAE: 2.23406, Outsample MAE: 2.30415\n",
      "Step: 200, Time: 21.506, Insample MAE: 2.11313, Outsample MAE: 2.29173\n",
      "Iteration: 206, Time: 22.227, Insample MAE: 2.11313, Outsample MAE: 2.29173\n",
      "==============================End fitting ==============================\n"
     ]
    }
   ],
   "source": [
    "Y_df, Xt_df = EPF.load(directory='../data/', group='NP')\n",
    "model, train_loader, val_loader = run_val_nbeatsx(mc, Y_df, Xt_df, val_ds=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "y_hat.shape  \t\t(#fcds, #lt) \t (8783, 24)\ny_plot.shape \t\t(#fcds,) \t (336,)\ny_hat_plot1.shape \t(#fcds, lt=0) \t (336,)\ny_hat_plot2.shape \t(#fcds, lt=0) \t (24,)\n\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 382.603125 277.314375\" width=\"382.603125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-01-08T00:31:34.752879</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 382.603125 277.314375 \nL 382.603125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 239.758125 \nL 375.403125 239.758125 \nL 375.403125 22.318125 \nL 40.603125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p86d8dcaa3b)\" d=\"M 55.821307 239.758125 \nL 55.821307 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"me47f1398d1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"55.821307\" xlink:href=\"#me47f1398d1\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(52.640057 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p86d8dcaa3b)\" d=\"M 98.211785 239.758125 \nL 98.211785 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"98.211785\" xlink:href=\"#me47f1398d1\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 50 -->\n      <g transform=\"translate(91.849285 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p86d8dcaa3b)\" d=\"M 140.602264 239.758125 \nL 140.602264 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"140.602264\" xlink:href=\"#me47f1398d1\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 100 -->\n      <g transform=\"translate(131.058514 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p86d8dcaa3b)\" d=\"M 182.992743 239.758125 \nL 182.992743 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"182.992743\" xlink:href=\"#me47f1398d1\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 150 -->\n      <g transform=\"translate(173.448993 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p86d8dcaa3b)\" d=\"M 225.383221 239.758125 \nL 225.383221 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"225.383221\" xlink:href=\"#me47f1398d1\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 200 -->\n      <g transform=\"translate(215.839471 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p86d8dcaa3b)\" d=\"M 267.7737 239.758125 \nL 267.7737 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"267.7737\" xlink:href=\"#me47f1398d1\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 250 -->\n      <g transform=\"translate(258.22995 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p86d8dcaa3b)\" d=\"M 310.164178 239.758125 \nL 310.164178 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"310.164178\" xlink:href=\"#me47f1398d1\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 300 -->\n      <g transform=\"translate(300.620428 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p86d8dcaa3b)\" d=\"M 352.554657 239.758125 \nL 352.554657 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"352.554657\" xlink:href=\"#me47f1398d1\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 350 -->\n      <g transform=\"translate(343.010907 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- Hour -->\n     <g transform=\"translate(195.959375 268.034687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 43.015625 \nL 55.515625 43.015625 \nL 55.515625 72.90625 \nL 65.375 72.90625 \nL 65.375 0 \nL 55.515625 0 \nL 55.515625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-72\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"75.195312\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"136.376953\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"199.755859\" xlink:href=\"#DejaVuSans-114\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p86d8dcaa3b)\" d=\"M 40.603125 235.890053 \nL 375.403125 235.890053 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mbf33e3ec37\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mbf33e3ec37\" y=\"235.890053\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(20.878125 239.689271)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p86d8dcaa3b)\" d=\"M 40.603125 203.548311 \nL 375.403125 203.548311 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mbf33e3ec37\" y=\"203.548311\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(20.878125 207.347529)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p86d8dcaa3b)\" d=\"M 40.603125 171.206569 \nL 375.403125 171.206569 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mbf33e3ec37\" y=\"171.206569\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(20.878125 175.005787)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p86d8dcaa3b)\" d=\"M 40.603125 138.864827 \nL 375.403125 138.864827 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mbf33e3ec37\" y=\"138.864827\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 30 -->\n      <g transform=\"translate(20.878125 142.664045)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p86d8dcaa3b)\" d=\"M 40.603125 106.523085 \nL 375.403125 106.523085 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mbf33e3ec37\" y=\"106.523085\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 35 -->\n      <g transform=\"translate(20.878125 110.322303)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p86d8dcaa3b)\" d=\"M 40.603125 74.181343 \nL 375.403125 74.181343 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mbf33e3ec37\" y=\"74.181343\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 40 -->\n      <g transform=\"translate(20.878125 77.980561)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#p86d8dcaa3b)\" d=\"M 40.603125 41.8396 \nL 375.403125 41.8396 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mbf33e3ec37\" y=\"41.8396\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 45 -->\n      <g transform=\"translate(20.878125 45.638819)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- Electricity Price -->\n     <g transform=\"translate(14.798438 169.661562)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n       <path id=\"DejaVuSans-32\"/>\n       <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"90.966797\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"152.490234\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"207.470703\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"246.679688\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"287.792969\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"315.576172\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"370.556641\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"398.339844\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"437.548828\" xlink:href=\"#DejaVuSans-121\"/>\n      <use x=\"496.728516\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"528.515625\" xlink:href=\"#DejaVuSans-80\"/>\n      <use x=\"587.068359\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"628.181641\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"655.964844\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"710.945312\" xlink:href=\"#DejaVuSans-101\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_31\">\n    <path clip-path=\"url(#p86d8dcaa3b)\" d=\"M 55.821307 131.749643 \nL 56.669116 135.048501 \nL 57.516926 135.501285 \nL 58.364736 135.113184 \nL 60.060355 129.09762 \nL 60.908164 113.832318 \nL 61.755974 95.85031 \nL 62.603783 82.978296 \nL 63.451593 100.054736 \nL 64.299403 104.841314 \nL 65.147212 107.493337 \nL 65.995022 106.393718 \nL 66.842831 105.035364 \nL 67.690641 104.711947 \nL 68.53845 101.736507 \nL 69.38626 98.243599 \nL 70.23407 98.243599 \nL 71.081879 101.542456 \nL 71.929689 110.986245 \nL 72.777498 115.125988 \nL 73.625308 120.235983 \nL 74.473117 126.963066 \nL 75.320927 131.943694 \nL 76.168737 132.849263 \nL 77.016546 135.695336 \nL 77.864356 137.506473 \nL 79.559975 131.620276 \nL 80.407784 123.987625 \nL 81.255594 109.563208 \nL 82.951213 69.200714 \nL 83.799023 83.948549 \nL 84.646832 85.048168 \nL 85.494642 85.82437 \nL 86.342451 85.242218 \nL 87.190261 85.371585 \nL 88.038071 81.231842 \nL 89.73369 32.201761 \nL 90.581499 40.675298 \nL 91.429309 56.587435 \nL 92.277118 88.79981 \nL 93.124928 103.28891 \nL 93.972738 108.722323 \nL 95.668357 123.987625 \nL 96.516166 127.868634 \nL 97.363976 130.003189 \nL 98.211785 129.744455 \nL 99.059595 129.291671 \nL 99.907405 125.345978 \nL 100.755214 119.653832 \nL 101.603024 107.752071 \nL 102.450833 65.901857 \nL 103.298643 49.989719 \nL 104.146452 85.177535 \nL 104.994262 97.079296 \nL 105.842072 94.168539 \nL 106.689881 90.610947 \nL 107.537691 94.297906 \nL 108.3855 93.651071 \nL 109.23331 85.371585 \nL 110.081119 86.988672 \nL 110.928929 82.848929 \nL 111.776739 92.098668 \nL 112.624548 104.065112 \nL 113.472358 113.31485 \nL 114.320167 116.549025 \nL 116.015786 128.838887 \nL 116.863596 129.809139 \nL 117.711406 132.137744 \nL 118.559215 133.36673 \nL 119.407025 132.590529 \nL 120.254834 128.90357 \nL 121.102644 123.534841 \nL 121.950453 102.383342 \nL 123.646073 48.178582 \nL 124.493882 76.251214 \nL 125.341692 76.186531 \nL 126.189501 84.142599 \nL 127.037311 75.216278 \nL 127.88512 80.26159 \nL 128.73293 75.863113 \nL 129.58074 75.151595 \nL 130.428549 74.051976 \nL 131.276359 69.653499 \nL 132.971978 91.387149 \nL 133.819787 105.035364 \nL 134.667597 110.015993 \nL 136.363216 121.529653 \nL 137.211026 125.475345 \nL 138.058835 128.515469 \nL 138.906645 129.744455 \nL 139.754454 130.326607 \nL 140.602264 130.19724 \nL 141.450074 128.450786 \nL 143.145693 126.186864 \nL 143.993502 121.982437 \nL 144.841312 115.125988 \nL 145.689121 114.41447 \nL 146.536931 114.867254 \nL 147.384741 119.847882 \nL 148.23255 122.176488 \nL 149.08036 121.270919 \nL 149.928169 117.131176 \nL 150.775979 110.015993 \nL 151.623788 107.881438 \nL 152.471598 113.573584 \nL 154.167217 126.510281 \nL 155.015027 128.256735 \nL 155.862836 131.814327 \nL 156.710646 136.212804 \nL 157.558455 143.910138 \nL 158.406265 149.602285 \nL 159.254075 149.731652 \nL 160.101884 150.960638 \nL 160.949694 148.437982 \nL 161.797503 143.910138 \nL 162.645313 138.476726 \nL 164.340932 131.296859 \nL 165.188742 125.604712 \nL 166.036551 121.917754 \nL 166.884361 119.783199 \nL 167.73217 119.524465 \nL 168.57998 120.235983 \nL 169.427789 120.753451 \nL 170.275599 116.743075 \nL 171.123409 110.33941 \nL 171.971218 107.493337 \nL 173.666837 114.543837 \nL 174.514647 118.68358 \nL 175.362456 121.141552 \nL 176.210266 125.216611 \nL 177.058076 133.107996 \nL 177.905885 135.630652 \nL 178.753695 137.44179 \nL 179.601504 138.282675 \nL 180.449314 137.571157 \nL 181.297123 134.789767 \nL 182.144933 128.90357 \nL 182.992743 119.912566 \nL 183.840552 99.149167 \nL 184.688362 97.661447 \nL 185.536171 109.369158 \nL 186.383981 115.384722 \nL 187.23179 117.066492 \nL 188.0796 114.091052 \nL 188.92741 113.638268 \nL 189.775219 112.215231 \nL 190.623029 106.0703 \nL 191.470838 102.900809 \nL 192.318648 103.09486 \nL 194.014267 119.847882 \nL 194.862077 123.34079 \nL 195.709886 125.992813 \nL 196.557696 129.421038 \nL 198.253315 141.775583 \nL 199.101124 146.109377 \nL 199.948934 148.632033 \nL 200.796744 147.079629 \nL 201.644553 141.193432 \nL 202.492363 136.406854 \nL 203.340172 127.480533 \nL 204.187982 116.419658 \nL 205.035791 111.956497 \nL 205.883601 116.031557 \nL 206.731411 118.812947 \nL 207.57922 118.166112 \nL 208.42703 116.872442 \nL 209.274839 114.867254 \nL 210.122649 114.155736 \nL 210.970459 111.63308 \nL 211.818268 111.374346 \nL 212.666078 111.244979 \nL 213.513887 116.354974 \nL 214.361697 123.082056 \nL 215.209506 126.316231 \nL 216.057316 130.132556 \nL 216.905126 135.95407 \nL 218.600745 151.090005 \nL 220.296364 156.135317 \nL 221.144173 154.000762 \nL 224.535412 130.973442 \nL 225.383221 129.873822 \nL 226.231031 133.754831 \nL 227.07884 135.307235 \nL 227.92665 135.371918 \nL 229.622269 135.113184 \nL 230.470079 134.336983 \nL 231.317888 133.302047 \nL 232.165698 133.107996 \nL 233.013507 134.013565 \nL 233.861317 134.6604 \nL 235.556936 140.158496 \nL 236.404746 141.452166 \nL 237.252555 145.268491 \nL 238.100365 152.836459 \nL 238.948174 162.280248 \nL 239.795984 164.673537 \nL 240.643794 164.285436 \nL 241.491603 160.792528 \nL 243.187222 146.109377 \nL 244.035032 140.999382 \nL 244.882841 132.461162 \nL 245.730651 128.450786 \nL 246.578461 133.36673 \nL 247.42627 136.083437 \nL 248.27408 136.665588 \nL 249.121889 134.919134 \nL 249.969699 137.377106 \nL 250.817508 135.889386 \nL 251.665318 135.177868 \nL 252.513128 135.177868 \nL 253.360937 136.471538 \nL 255.056556 140.805331 \nL 256.752175 144.362923 \nL 257.599985 148.243932 \nL 258.447795 155.941266 \nL 259.295604 167.131509 \nL 260.143414 172.629605 \nL 260.991223 176.122513 \nL 261.839033 174.182009 \nL 265.230271 143.651404 \nL 266.078081 139.511661 \nL 266.92589 140.870015 \nL 267.7737 141.90495 \nL 268.621509 143.133937 \nL 269.469319 143.069253 \nL 270.317129 144.168872 \nL 271.164938 144.49229 \nL 272.860557 142.357735 \nL 273.708367 142.939886 \nL 274.556176 144.880391 \nL 275.403986 148.502666 \nL 276.251796 151.284056 \nL 277.099605 153.095193 \nL 277.947415 157.170253 \nL 278.795224 174.311376 \nL 279.643034 169.00733 \nL 280.490843 178.063018 \nL 281.338653 183.043646 \nL 282.186463 185.954403 \nL 283.034272 183.819848 \nL 283.882082 178.063018 \nL 284.729891 170.559734 \nL 286.42551 163.962018 \nL 287.27332 154.971014 \nL 288.12113 152.771776 \nL 288.968939 154.388863 \nL 289.816749 157.040886 \nL 290.664558 158.011138 \nL 291.512368 158.528606 \nL 293.207987 154.000762 \nL 294.055797 161.95683 \nL 294.903606 168.81328 \nL 295.751416 171.012518 \nL 296.599225 170.883151 \nL 297.447035 171.724036 \nL 298.294844 169.912899 \nL 299.142654 178.127701 \nL 299.990464 180.844408 \nL 300.838273 193.651738 \nL 301.686083 211.633746 \nL 302.533892 212.474631 \nL 303.381702 211.310329 \nL 304.229511 193.781105 \nL 305.077321 196.368444 \nL 305.925131 178.257068 \nL 307.62075 165.126321 \nL 308.468559 165.126321 \nL 309.316369 160.921895 \nL 310.164178 162.474298 \nL 311.011988 162.927083 \nL 311.859798 160.857211 \nL 313.555417 148.696716 \nL 314.403226 149.084817 \nL 316.098845 152.383675 \nL 316.946655 155.100381 \nL 317.794465 154.194812 \nL 318.642274 159.110757 \nL 319.490084 167.32556 \nL 320.337893 167.972394 \nL 321.185703 187.37744 \nL 322.033512 210.340076 \nL 322.881322 229.874489 \nL 323.729132 226.122847 \nL 324.576941 225.928796 \nL 325.424751 185.760353 \nL 326.27256 172.629605 \nL 327.968179 166.678725 \nL 328.815989 163.05645 \nL 329.663799 160.857211 \nL 330.511608 162.280248 \nL 331.359418 166.419991 \nL 332.207227 166.678725 \nL 333.055037 158.269872 \nL 333.902846 155.682532 \nL 334.750656 152.189624 \nL 335.598466 153.159877 \nL 337.294085 157.558353 \nL 338.141894 159.434175 \nL 338.989704 160.533794 \nL 339.837513 166.808092 \nL 339.837513 166.808092 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_32\">\n    <path clip-path=\"url(#p86d8dcaa3b)\" d=\"M 55.821307 126.687509 \nL 57.516926 85.247567 \nL 58.364736 68.480528 \nL 59.212545 60.955605 \nL 60.060355 60.634932 \nL 60.908164 78.619375 \nL 61.755974 89.423948 \nL 62.603783 87.078264 \nL 63.451593 81.853278 \nL 64.299403 97.112218 \nL 65.147212 101.052955 \nL 65.995022 103.602478 \nL 66.842831 104.110877 \nL 67.690641 105.6456 \nL 68.53845 114.114654 \nL 69.38626 121.458143 \nL 70.23407 126.531453 \nL 71.081879 126.287148 \nL 71.929689 128.799202 \nL 72.777498 138.430291 \nL 73.625308 140.247528 \nL 74.473117 139.633742 \nL 75.320927 136.792662 \nL 76.168737 129.08385 \nL 77.016546 116.176554 \nL 77.864356 106.038274 \nL 79.559975 91.246921 \nL 80.407784 91.08804 \nL 81.255594 97.636927 \nL 82.103404 100.075808 \nL 82.951213 89.217346 \nL 83.799023 76.446266 \nL 84.646832 93.241952 \nL 85.494642 93.095606 \nL 86.342451 92.022697 \nL 87.190261 90.030356 \nL 88.038071 90.253712 \nL 88.88588 90.241325 \nL 89.73369 74.508703 \nL 90.581499 63.546878 \nL 91.429309 80.354901 \nL 92.277118 100.554968 \nL 93.124928 131.6076 \nL 93.972738 139.093685 \nL 94.820547 137.439586 \nL 95.668357 134.80227 \nL 96.516166 127.753879 \nL 98.211785 104.726414 \nL 99.059595 92.81725 \nL 99.907405 87.605737 \nL 100.755214 85.890937 \nL 102.450833 95.548601 \nL 103.298643 66.892187 \nL 104.146452 58.266744 \nL 104.994262 92.845749 \nL 105.842072 101.191134 \nL 106.689881 96.348114 \nL 107.537691 92.215752 \nL 108.3855 97.738242 \nL 109.23331 104.408504 \nL 110.081119 103.235514 \nL 110.928929 110.372229 \nL 111.776739 112.208897 \nL 113.472358 135.405137 \nL 114.320167 141.111688 \nL 115.167977 138.771778 \nL 116.015786 135.016966 \nL 116.863596 129.961372 \nL 118.559215 109.175452 \nL 119.407025 99.961071 \nL 120.254834 95.253639 \nL 121.102644 92.912495 \nL 121.950453 95.134213 \nL 122.798263 90.401489 \nL 124.493882 52.239902 \nL 125.341692 81.433362 \nL 126.189501 82.232233 \nL 127.037311 89.605974 \nL 127.88512 80.808054 \nL 128.73293 86.559057 \nL 129.58074 86.336441 \nL 131.276359 94.96862 \nL 132.124168 95.642982 \nL 132.971978 109.98328 \nL 134.667597 132.40055 \nL 135.515407 132.817948 \nL 136.363216 131.905845 \nL 137.211026 129.269022 \nL 138.058835 123.18222 \nL 138.906645 116.066109 \nL 139.754454 106.785649 \nL 140.602264 102.163592 \nL 141.450074 101.441731 \nL 142.297883 105.248607 \nL 143.145693 113.185796 \nL 143.993502 119.859957 \nL 144.841312 117.812714 \nL 145.689121 113.258069 \nL 146.536931 113.015664 \nL 147.384741 113.533661 \nL 148.23255 117.633427 \nL 149.08036 120.571208 \nL 150.775979 122.887455 \nL 151.623788 121.66667 \nL 152.471598 124.560332 \nL 153.319408 133.660728 \nL 155.015027 147.322074 \nL 155.862836 146.803657 \nL 157.558455 143.68047 \nL 158.406265 142.8796 \nL 159.254075 138.632908 \nL 160.101884 127.76275 \nL 161.797503 115.687623 \nL 162.645313 114.006381 \nL 163.493122 113.294267 \nL 164.340932 116.321567 \nL 165.188742 118.655556 \nL 166.884361 113.283336 \nL 167.73217 112.667749 \nL 168.57998 112.934977 \nL 169.427789 114.514164 \nL 171.123409 122.964515 \nL 171.971218 126.547899 \nL 172.819028 130.632046 \nL 174.514647 145.006209 \nL 175.362456 149.686758 \nL 176.210266 147.838283 \nL 177.905885 139.721326 \nL 179.601504 121.049085 \nL 180.449314 110.954751 \nL 181.297123 104.206319 \nL 182.144933 102.21536 \nL 182.992743 105.444155 \nL 183.840552 106.765539 \nL 184.688362 94.841989 \nL 185.536171 99.499849 \nL 186.383981 111.375407 \nL 187.23179 114.949871 \nL 188.0796 114.23408 \nL 188.92741 110.217444 \nL 189.775219 110.459109 \nL 190.623029 113.828969 \nL 191.470838 116.702522 \nL 192.318648 123.269298 \nL 193.166457 127.66679 \nL 194.014267 138.110617 \nL 194.862077 146.558241 \nL 195.709886 147.635962 \nL 196.557696 144.323063 \nL 197.405505 140.353938 \nL 198.253315 137.582317 \nL 199.101124 132.427359 \nL 199.948934 123.577091 \nL 200.796744 117.545264 \nL 201.644553 112.824804 \nL 202.492363 110.195014 \nL 203.340172 116.380713 \nL 204.187982 119.129904 \nL 205.035791 114.420868 \nL 205.883601 111.694082 \nL 206.731411 115.314515 \nL 207.57922 117.405679 \nL 209.274839 115.065571 \nL 210.122649 114.890676 \nL 210.970459 119.548709 \nL 212.666078 134.294142 \nL 213.513887 139.963903 \nL 215.209506 156.140536 \nL 216.057316 157.377385 \nL 216.905126 155.831126 \nL 218.600745 148.834454 \nL 219.448554 143.587224 \nL 220.296364 131.848377 \nL 221.144173 125.372577 \nL 221.991983 117.966118 \nL 222.839793 113.149253 \nL 223.687602 117.963699 \nL 224.535412 125.286511 \nL 225.383221 126.947112 \nL 226.231031 127.627063 \nL 227.07884 131.185711 \nL 227.92665 131.047729 \nL 228.77446 129.208791 \nL 229.622269 127.872158 \nL 230.470079 128.570849 \nL 231.317888 133.097859 \nL 232.165698 140.813049 \nL 233.861317 153.713721 \nL 234.709127 158.026776 \nL 235.556936 163.811349 \nL 236.404746 165.313093 \nL 237.252555 160.581838 \nL 238.948174 153.860474 \nL 239.795984 148.516914 \nL 240.643794 137.048466 \nL 242.339413 121.285321 \nL 243.187222 116.847708 \nL 244.035032 123.226561 \nL 244.882841 131.515477 \nL 245.730651 128.790233 \nL 246.578461 125.445367 \nL 247.42627 132.293905 \nL 248.27408 135.920754 \nL 249.121889 136.118868 \nL 249.969699 133.177731 \nL 250.817508 135.76434 \nL 251.665318 137.359516 \nL 252.513128 142.111215 \nL 255.056556 162.357387 \nL 255.904366 165.49354 \nL 256.752175 166.381993 \nL 258.447795 162.74541 \nL 259.295604 161.92295 \nL 260.143414 161.357947 \nL 260.991223 153.764896 \nL 262.686842 142.90235 \nL 263.534652 136.177421 \nL 265.230271 140.486923 \nL 266.078081 141.692792 \nL 266.92589 138.83592 \nL 267.7737 142.751637 \nL 268.621509 144.858222 \nL 269.469319 147.637887 \nL 270.317129 147.374249 \nL 272.012748 150.840391 \nL 272.860557 152.844268 \nL 273.708367 156.740565 \nL 276.251796 170.869647 \nL 277.099605 174.08587 \nL 277.947415 174.111149 \nL 278.795224 174.484837 \nL 279.643034 185.354378 \nL 280.490843 173.371411 \nL 281.338653 175.245227 \nL 282.186463 170.727347 \nL 283.034272 167.867712 \nL 283.882082 163.554262 \nL 284.729891 157.848069 \nL 285.577701 153.563205 \nL 286.42551 154.693088 \nL 287.27332 154.381408 \nL 288.12113 147.392878 \nL 288.968939 145.95345 \nL 289.816749 147.821443 \nL 290.664558 150.773584 \nL 291.512368 151.930005 \nL 292.360177 153.804277 \nL 293.207987 155.338038 \nL 294.055797 158.375727 \nL 294.903606 170.72615 \nL 295.751416 180.9966 \nL 296.599225 186.813522 \nL 297.447035 188.337437 \nL 298.294844 188.152944 \nL 299.142654 182.644 \nL 299.990464 186.648114 \nL 300.838273 182.733681 \nL 301.686083 186.974648 \nL 302.533892 193.830206 \nL 303.381702 186.918316 \nL 304.229511 182.950079 \nL 305.077321 167.781535 \nL 305.925131 175.069284 \nL 306.77294 164.037974 \nL 307.62075 162.757365 \nL 308.468559 160.458205 \nL 309.316369 162.987285 \nL 310.164178 160.056487 \nL 311.011988 161.476731 \nL 311.859798 161.583412 \nL 312.707607 161.431872 \nL 314.403226 157.666993 \nL 316.946655 174.987943 \nL 317.794465 179.587596 \nL 318.642274 179.331336 \nL 319.490084 182.745599 \nL 320.337893 183.523694 \nL 321.185703 175.160938 \nL 322.881322 191.033257 \nL 323.729132 197.536693 \nL 324.576941 186.705113 \nL 325.424751 183.98605 \nL 326.27256 152.61668 \nL 327.12037 149.209868 \nL 327.968179 152.117139 \nL 328.815989 152.740288 \nL 330.511608 151.594514 \nL 331.359418 154.592933 \nL 332.207227 158.745084 \nL 333.055037 159.009055 \nL 333.902846 152.415321 \nL 334.750656 154.080179 \nL 335.598466 156.473436 \nL 337.294085 172.330925 \nL 338.141894 178.713553 \nL 338.989704 182.101389 \nL 339.837513 180.055158 \nL 340.685323 166.085279 \nL 341.533133 172.144544 \nL 342.380942 176.599664 \nL 343.228752 179.625435 \nL 344.076561 180.055158 \nL 344.924371 174.8831 \nL 346.61999 161.430688 \nL 347.4678 152.300559 \nL 348.315609 147.38592 \nL 349.163419 144.215284 \nL 350.011228 141.953679 \nL 350.859038 144.368317 \nL 351.706847 145.921274 \nL 352.554657 146.687266 \nL 354.250276 145.752301 \nL 355.098086 142.894886 \nL 355.945895 140.47767 \nL 356.793705 140.215722 \nL 357.641514 141.900048 \nL 358.489324 145.051438 \nL 360.184943 155.158356 \nL 360.184943 155.158356 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 239.758125 \nL 40.603125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 375.403125 239.758125 \nL 375.403125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 239.758125 \nL 375.403125 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 22.318125 \nL 375.403125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_18\">\n    <!-- Testing predictions lead_time=4 MAE=2.24 -->\n    <g transform=\"translate(78.25875 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n      <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n      <path d=\"M 10.59375 45.40625 \nL 73.1875 45.40625 \nL 73.1875 37.203125 \nL 10.59375 37.203125 \nz\nM 10.59375 25.484375 \nL 73.1875 25.484375 \nL 73.1875 17.1875 \nL 10.59375 17.1875 \nz\n\" id=\"DejaVuSans-61\"/>\n      <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n      <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n      <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-84\"/>\n     <use x=\"44.083984\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"105.607422\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"157.707031\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"196.916016\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"224.699219\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"288.078125\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"351.554688\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"383.341797\" xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"446.818359\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"485.681641\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"547.205078\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"610.681641\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"638.464844\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"693.445312\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"732.654297\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"760.4375\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"821.619141\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"884.998047\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"937.097656\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"968.884766\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"996.667969\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1058.191406\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1119.470703\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"1182.947266\" xlink:href=\"#DejaVuSans-95\"/>\n     <use x=\"1232.947266\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1272.15625\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1299.939453\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"1397.351562\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1458.875\" xlink:href=\"#DejaVuSans-61\"/>\n     <use x=\"1542.664062\" xlink:href=\"#DejaVuSans-52\"/>\n     <use x=\"1606.287109\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1638.074219\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"1724.353516\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"1792.761719\" xlink:href=\"#DejaVuSans-69\"/>\n     <use x=\"1855.945312\" xlink:href=\"#DejaVuSans-61\"/>\n     <use x=\"1939.734375\" xlink:href=\"#DejaVuSans-50\"/>\n     <use x=\"2003.357422\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"2035.144531\" xlink:href=\"#DejaVuSans-50\"/>\n     <use x=\"2098.767578\" xlink:href=\"#DejaVuSans-52\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 313.667187 59.674375 \nL 368.403125 59.674375 \nQ 370.403125 59.674375 370.403125 57.674375 \nL 370.403125 29.318125 \nQ 370.403125 27.318125 368.403125 27.318125 \nL 313.667187 27.318125 \nQ 311.667187 27.318125 311.667187 29.318125 \nL 311.667187 57.674375 \nQ 311.667187 59.674375 313.667187 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_33\">\n     <path d=\"M 315.667187 35.416562 \nL 335.667187 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_34\"/>\n    <g id=\"text_19\">\n     <!-- true -->\n     <g transform=\"translate(343.667187 38.916562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"143.701172\" xlink:href=\"#DejaVuSans-101\"/>\n     </g>\n    </g>\n    <g id=\"line2d_35\">\n     <path d=\"M 315.667187 50.094687 \nL 335.667187 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_36\"/>\n    <g id=\"text_20\">\n     <!-- pred -->\n     <g transform=\"translate(343.667187 53.594687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"102.339844\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"163.863281\" xlink:href=\"#DejaVuSans-100\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p86d8dcaa3b\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"40.603125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "y_true, y_hat = model.predict_all(ts_loader=val_loader)\n",
    "y_plot = Y_df['y'][-2*168:]\n",
    "lead_time = 4\n",
    "y_hat_plot1 = y_hat[-2*168:, lead_time] # Forecast lead_time=?, for the last week\n",
    "y_hat_plot2 = y_hat[-1, :]\n",
    "y_hat_plot = np.concatenate([y_hat_plot1, y_hat_plot2])\n",
    "\n",
    "print(\"y_hat.shape  \\t\\t(#fcds, #lt) \\t\", y_hat.shape)\n",
    "print(\"y_plot.shape \\t\\t(#fcds,) \\t\", y_plot.shape)\n",
    "#print(\"y_true.shape \\t\\t(#fcds,) \\t\", y_true.shape)\n",
    "print(\"y_hat_plot1.shape \\t(#fcds, lt=0) \\t\", y_hat_plot1.shape)\n",
    "print(\"y_hat_plot2.shape \\t(#fcds, lt=0) \\t\", y_hat_plot2.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "mae0 = np.round(mae(y_plot, y_hat_plot1),2)\n",
    "plt.plot(range(len(y_plot)), y_plot, label='true')\n",
    "plt.plot(range(len(y_hat_plot)), y_hat_plot, label='pred')\n",
    "plt.title(f\"Testing predictions lead_time={lead_time} MAE={mae0}\")\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Electricity Price')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "y_true.shape (8783, 24)\ny_hat.shape (8783, 24)\ny_true.shape (#fcds, #lt) (8760, 24)\ny_hat.shape  (#fcds, #lt) (8760, 24)\ny_true.shape (#fcd_day, #fcd_hour, #lt) (365, 24, 24)\ny_hat.shape (#fcd_day, #fcd_hour, #lt) (365, 24, 24)\nmae      2.14\nmape     7.31\nsmape    7.24\nrmse     5.67\ndtype: float64\n"
     ]
    }
   ],
   "source": [
    "def forecast_evaluation_table(y_true, y_hat):\n",
    "    print(\"y_true.shape\", y_true.shape)\n",
    "    print(\"y_hat.shape\", y_hat.shape)\n",
    "\n",
    "    n_days = len(y_true) // 24\n",
    "    y_true = y_true[-n_days*24:, :]\n",
    "    y_hat  = y_hat[-n_days*24:, :]\n",
    "\n",
    "    print(\"y_true.shape (#fcds, #lt)\", y_true.shape)\n",
    "    print(\"y_hat.shape  (#fcds, #lt)\", y_hat.shape)\n",
    "\n",
    "    y_true = y_true.reshape(n_days, 24, 24)\n",
    "    y_hat = y_hat.reshape(n_days, 24, 24)\n",
    "\n",
    "    print(\"y_true.shape (#fcd_day, #fcd_hour, #lt)\", y_true.shape)\n",
    "    print(\"y_hat.shape (#fcd_day, #fcd_hour, #lt)\", y_hat.shape)\n",
    "    \n",
    "    _mae   = np.round(mae(y_true[:,0,:], y_hat[:,0,:]),2)\n",
    "    _mape  = np.round(mape(y_true[:,0,:], y_hat[:,0,:]),2)\n",
    "    _smape = np.round(smape(y_true[:,0,:], y_hat[:,0,:]),2)\n",
    "    _rmse  = np.round(rmse(y_true[:,0,:], y_hat[:,0,:]),2)\n",
    "\n",
    "    evaluations = pd.Series({'mae': _mae, 'mape': _mape, 'smape': _smape, 'rmse': _rmse})\n",
    "\n",
    "    return evaluations\n",
    "\n",
    "evaluations = forecast_evaluation_table(y_true, y_hat)\n",
    "print(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c68c0f618c4f787b1e02bfee278e48d25b62407cb335aab5258cedc4db4ced"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}