{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.nbeats.nbeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "import torch as t\n",
    "from torch import optim\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "from nixtla.models.nbeats.nbeats_model import NBeats, NBeatsBlock, IdentityBasis, TrendBasis, SeasonalityBasis\n",
    "from nixtla.models.nbeats.nbeats_model import ExogenousBasisInterpretable, ExogenousBasisWavenet, ExogenousBasisTCN\n",
    "from nixtla.losses.pytorch import MAPELoss, MASELoss, SMAPELoss, MSELoss, MAELoss, PinballLoss, QuadraticBarrierLoss\n",
    "from nixtla.losses.numpy import mae, mse, mape, smape, rmse, pinball_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_weights(module, initialization):\n",
    "    if type(module) == t.nn.Linear:\n",
    "        if initialization == 'orthogonal':\n",
    "            t.nn.init.orthogonal_(module.weight)\n",
    "        elif initialization == 'he_uniform':\n",
    "            t.nn.init.kaiming_uniform_(module.weight)\n",
    "        elif initialization == 'he_normal':\n",
    "            t.nn.init.kaiming_normal_(module.weight)\n",
    "        elif initialization == 'glorot_uniform':\n",
    "            t.nn.init.xavier_uniform_(module.weight)\n",
    "        elif initialization == 'glorot_normal':\n",
    "            t.nn.init.xavier_normal_(module.weight)\n",
    "        elif initialization == 'lecun_normal':\n",
    "            pass #t.nn.init.normal_(module.weight, 0.0, std=1/np.sqrt(module.weight.numel()))\n",
    "        else:\n",
    "            assert 1<0, f'Initialization {initialization} not found'\n",
    "\n",
    "class Nbeats(object):\n",
    "    \"\"\"\n",
    "    Future documentation\n",
    "    \"\"\"\n",
    "    SEASONALITY_BLOCK = 'seasonality'\n",
    "    TREND_BLOCK = 'trend'\n",
    "    IDENTITY_BLOCK = 'identity'\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size_multiplier,\n",
    "                 output_size,\n",
    "                 shared_weights,\n",
    "                 activation,\n",
    "                 initialization,\n",
    "                 stack_types,\n",
    "                 n_blocks,\n",
    "                 n_layers,\n",
    "                 n_hidden,\n",
    "                 n_harmonics,\n",
    "                 n_polynomials,\n",
    "                 exogenous_n_channels,\n",
    "                 include_var_dict,\n",
    "                 t_cols,\n",
    "                 batch_normalization,\n",
    "                 dropout_prob_theta,\n",
    "                 dropout_prob_exogenous,\n",
    "                 x_s_n_hidden,\n",
    "                 learning_rate,\n",
    "                 lr_decay,\n",
    "                 n_lr_decay_steps,\n",
    "                 weight_decay,\n",
    "                 l1_theta,\n",
    "                 n_iterations,\n",
    "                 early_stopping,\n",
    "                 loss,\n",
    "                 loss_hypar,\n",
    "                 val_loss,\n",
    "                 frequency,\n",
    "                 random_seed,\n",
    "                 seasonality,\n",
    "                 device=None):\n",
    "        super(Nbeats, self).__init__()\n",
    "\n",
    "        if activation == 'selu': initialization = 'lecun_normal'\n",
    "\n",
    "        #------------------------ Model Attributes ------------------------#\n",
    "        # Architecture parameters\n",
    "        self.input_size = int(input_size_multiplier*output_size)\n",
    "        self.output_size = output_size\n",
    "        self.shared_weights = shared_weights\n",
    "        self.activation = activation\n",
    "        self.initialization = initialization\n",
    "        self.stack_types = stack_types\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_harmonics = n_harmonics\n",
    "        self.n_polynomials = n_polynomials\n",
    "        self.exogenous_n_channels = exogenous_n_channels\n",
    "\n",
    "        # Regularization and optimization parameters\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.dropout_prob_theta = dropout_prob_theta\n",
    "        self.dropout_prob_exogenous = dropout_prob_exogenous\n",
    "        self.x_s_n_hidden = x_s_n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.n_lr_decay_steps = n_lr_decay_steps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.n_iterations = n_iterations\n",
    "        self.early_stopping = early_stopping\n",
    "        self.loss = loss\n",
    "        self.loss_hypar = loss_hypar\n",
    "        self.val_loss = val_loss\n",
    "        self.l1_theta = l1_theta\n",
    "        self.l1_conv = 1e-3 # Not a hyperparameter\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Data parameters\n",
    "        self.frequency = frequency\n",
    "        self.seasonality = seasonality        \n",
    "        self.include_var_dict = include_var_dict\n",
    "        self.t_cols = t_cols\n",
    "        #self.scaler = scaler\n",
    "\n",
    "        if device is None:\n",
    "            device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
    "        self.device = device\n",
    "\n",
    "        self._is_instantiated = False\n",
    "\n",
    "    def create_stack(self):\n",
    "        if self.include_var_dict is not None:\n",
    "            x_t_n_inputs = self.output_size * int(sum([len(x) for x in self.include_var_dict.values()]))\n",
    "            \n",
    "            # Correction because week_day only adds 1 no output_size\n",
    "            if len(self.include_var_dict['week_day'])>0:\n",
    "                x_t_n_inputs = x_t_n_inputs - self.output_size + 1 \n",
    "        else:\n",
    "            x_t_n_inputs = self.input_size\n",
    "        \n",
    "        #------------------------ Model Definition ------------------------#\n",
    "        block_list = []\n",
    "        self.blocks_regularizer = []\n",
    "        for i in range(len(self.stack_types)):\n",
    "            #print(f'| --  Stack {self.stack_types[i]} (#{i})')\n",
    "            for block_id in range(self.n_blocks[i]):\n",
    "                \n",
    "                # Batch norm only on first block\n",
    "                if (len(block_list)==0) and (self.batch_normalization):\n",
    "                    batch_normalization_block = True\n",
    "                else:\n",
    "                    batch_normalization_block = False\n",
    "                \n",
    "                # Dummy of regularizer in block. Override with 1 if exogenous_block\n",
    "                self.blocks_regularizer += [0]\n",
    "\n",
    "                # Shared weights\n",
    "                if self.shared_weights and block_id>0:\n",
    "                    nbeats_block = block_list[-1]\n",
    "                else:\n",
    "                    if self.stack_types[i] == 'seasonality':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=4 * int(\n",
    "                                                        np.ceil(self.n_harmonics / 2 * self.output_size) - (self.n_harmonics - 1)),\n",
    "                                                   basis=SeasonalityBasis(harmonics=self.n_harmonics,\n",
    "                                                                          backcast_size=self.input_size,\n",
    "                                                                          forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'trend':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2 * (self.n_polynomials + 1),\n",
    "                                                   basis=TrendBasis(degree_of_polynomial=self.n_polynomials,\n",
    "                                                                            backcast_size=self.input_size,\n",
    "                                                                            forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'identity':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=self.input_size + self.output_size,\n",
    "                                                   basis=IdentityBasis(backcast_size=self.input_size,\n",
    "                                                                       forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2*self.n_x_t,\n",
    "                                                   basis=ExogenousBasisInterpretable(),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous_tcn':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden = self.x_s_n_hidden,\n",
    "                                                   theta_n_dim = 2*(self.exogenous_n_channels),\n",
    "                                                   basis= ExogenousBasisTCN(self.exogenous_n_channels, self.n_x_t),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous_wavenet':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2*(self.exogenous_n_channels),\n",
    "                                                   basis=ExogenousBasisWavenet(self.exogenous_n_channels, self.n_x_t),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                        self.blocks_regularizer[-1] = 1\n",
    "                    else:\n",
    "                        assert 1<0, f'Block type not found!'\n",
    "                # Select type of evaluation and apply it to all layers of block\n",
    "                init_function = partial(init_weights, initialization=self.initialization)                                             \n",
    "                nbeats_block.layers.apply(init_function)\n",
    "                #print(f'     | -- {nbeats_block}')\n",
    "                block_list.append(nbeats_block)\n",
    "        return block_list\n",
    "\n",
    "    def __loss_fn(self, loss_name: str):\n",
    "        def loss(x, loss_hypar, forecast, target, mask):\n",
    "            if loss_name == 'MAPE':\n",
    "                return MAPELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'MASE':\n",
    "                return MASELoss(y=target, y_hat=forecast, y_insample=x, seasonality=loss_hypar, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return SMAPELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'MSE':\n",
    "                return MSELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'MAE':\n",
    "                return MAELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'PINBALL':\n",
    "                return PinballLoss(y=target, y_hat=forecast, mask=mask, tau=loss_hypar) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()\n",
    "            elif loss_name == 'PINBALL2':\n",
    "                return PinballLoss(y=target, y_hat=forecast, mask=mask, tau=0.5) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta() + \\\n",
    "                       QuadraticBarrierLoss(z=(-forecast), tau=loss_hypar) # To induce forecast positivity\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def __val_loss_fn(self, loss_name='MAE'):\n",
    "        #TODO: mase not implemented\n",
    "        def loss(forecast, target, weights):\n",
    "            if loss_name == 'MAPE':\n",
    "                return mape(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return smape(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MSE':\n",
    "                return mse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'RMSE':\n",
    "                return rmse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MAE':\n",
    "                return mae(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'PINBALL':\n",
    "                return pinball_loss(y=target, y_hat=forecast, weights=weights, tau=0.5)\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "    \n",
    "    def loss_l1_conv_layers(self):\n",
    "        loss_l1 = 0\n",
    "        for i, indicator in enumerate(self.blocks_regularizer):\n",
    "            if indicator:\n",
    "                loss_l1 += self.l1_conv * t.sum(t.abs(self.model.blocks[i].basis.weight))\n",
    "        return loss_l1\n",
    "    \n",
    "    def loss_l1_theta(self):\n",
    "        loss_l1 = 0\n",
    "        for block in self.model.blocks:\n",
    "            for layer in block.modules():\n",
    "                if isinstance(layer, t.nn.Linear):\n",
    "                    loss_l1 += self.l1_theta * layer.weight.abs().sum()\n",
    "        return loss_l1\n",
    "\n",
    "    def to_tensor(self, x: np.ndarray) -> t.Tensor:\n",
    "        tensor = t.as_tensor(x, dtype=t.float32).to(self.device)\n",
    "        return tensor\n",
    "\n",
    "    def fit(self, train_ts_loader, val_ts_loader=None, n_iterations=None, verbose=True, eval_steps=1):\n",
    "        # TODO: Indexes hardcoded, information duplicated in train and val datasets\n",
    "        assert (self.input_size)==train_ts_loader.input_size, \\\n",
    "            f'model input_size {self.input_size} data input_size {train_ts_loader.input_size}'\n",
    "\n",
    "        # Random Seeds (model initialization)\n",
    "        t.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed) #TODO: interaccion rara con window_sampling de validacion\n",
    "\n",
    "        # Attributes of ts_dataset\n",
    "        self.n_x_t, self.n_x_s = train_ts_loader.get_n_variables()\n",
    "\n",
    "        # Instantiate model\n",
    "        if not self._is_instantiated:\n",
    "            block_list = self.create_stack()\n",
    "            self.model = NBeats(t.nn.ModuleList(block_list)).to(self.device)\n",
    "            self._is_instantiated = True\n",
    "\n",
    "        # Overwrite n_iterations and train datasets\n",
    "        if n_iterations is None:\n",
    "            n_iterations = self.n_iterations\n",
    "\n",
    "        lr_decay_steps = n_iterations // self.n_lr_decay_steps\n",
    "        if lr_decay_steps == 0:\n",
    "            lr_decay_steps = 1\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_steps, gamma=self.lr_decay)\n",
    "        training_loss_fn = self.__loss_fn(self.loss)\n",
    "        validation_loss_fn = self.__val_loss_fn(self.val_loss) #Uses numpy losses\n",
    "\n",
    "        print('\\n')\n",
    "        print('='*30+' Start fitting '+'='*30)\n",
    "\n",
    "        #self.loss_dict = {} # Restart self.loss_dict\n",
    "        start = time.time()\n",
    "        self.trajectories = {'iteration':[],'train_loss':[], 'val_loss':[]}\n",
    "        self.final_insample_loss = None\n",
    "        self.final_outsample_loss = None\n",
    "        \n",
    "        # Training Loop\n",
    "        early_stopping_counter = 0\n",
    "        best_val_loss = np.inf\n",
    "        best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "        break_flag = False\n",
    "        iteration = 0\n",
    "        epoch = 0\n",
    "        while (iteration < n_iterations) and (not break_flag):\n",
    "            epoch +=1\n",
    "            for batch in iter(train_ts_loader):\n",
    "                iteration += 1\n",
    "                if (iteration > n_iterations) or (break_flag):\n",
    "                    continue\n",
    "\n",
    "                self.model.train()\n",
    "                # Parse batch\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                forecast   = self.model(x_s=s_matrix, insample_y=insample_y, \n",
    "                                        insample_x_t=insample_x, outsample_x_t=outsample_x,\n",
    "                                        insample_mask=insample_mask)\n",
    "\n",
    "                training_loss = training_loss_fn(x=insample_y, loss_hypar=self.loss_hypar, forecast=forecast,\n",
    "                                                 target=outsample_y, mask=outsample_mask)\n",
    "\n",
    "                # Protection to exploding gradients\n",
    "                # if np.isnan(float(training_loss)):\n",
    "                #    break\n",
    "                if not np.isnan(float(training_loss)):\n",
    "                    training_loss.backward()\n",
    "                    t.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    early_stopping_counter = self.early_stopping\n",
    "\n",
    "                lr_scheduler.step()\n",
    "                if (iteration % eval_steps == 0):\n",
    "                    display_string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(iteration,\n",
    "                                                                                            time.time()-start,\n",
    "                                                                                            self.loss,\n",
    "                                                                                            training_loss.cpu().data.numpy())\n",
    "                    self.trajectories['iteration'].append(iteration)\n",
    "                    self.trajectories['train_loss'].append(np.float(training_loss.cpu().data.numpy()))\n",
    "\n",
    "                    if val_ts_loader is not None:\n",
    "                        loss = self.evaluate_performance(ts_loader=val_ts_loader, \n",
    "                                                         validation_loss_fn=validation_loss_fn)\n",
    "                        display_string += \", Outsample {}: {:.5f}\".format(self.val_loss, loss)\n",
    "                        self.trajectories['val_loss'].append(loss)\n",
    "\n",
    "                        if self.early_stopping:\n",
    "                            if loss < best_val_loss:\n",
    "                                # Save current model if improves outsample loss\n",
    "                                best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "                                best_insample_loss = training_loss.cpu().data.numpy()\n",
    "                                early_stopping_counter = 0\n",
    "                                best_val_loss = loss\n",
    "                            else:\n",
    "                                early_stopping_counter += 1\n",
    "                            if early_stopping_counter >= self.early_stopping:\n",
    "                                break_flag = True\n",
    "                    \n",
    "                    print(display_string)\n",
    "\n",
    "                    self.model.train()\n",
    "\n",
    "                if break_flag:\n",
    "                    print('\\n')\n",
    "                    print(19*'-',' Stopped training by early stopping', 19*'-')\n",
    "                    self.model.load_state_dict(best_state_dict)\n",
    "                    break\n",
    "\n",
    "        #End of fitting\n",
    "        if n_iterations >0:\n",
    "            # This is batch loss!\n",
    "            self.final_insample_loss = np.float(training_loss.cpu().data.numpy()) if not break_flag else best_insample_loss \n",
    "            string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(iteration,\n",
    "                                                                            time.time()-start,\n",
    "                                                                            self.loss,\n",
    "                                                                            self.final_insample_loss)\n",
    "            if val_ts_loader is not None:\n",
    "                self.final_outsample_loss = self.evaluate_performance(ts_loader=val_ts_loader, \n",
    "                                                                      validation_loss_fn=validation_loss_fn)\n",
    "                string += \", Outsample {}: {:.5f}\".format(self.val_loss, self.final_outsample_loss)\n",
    "            print(string)\n",
    "            print('='*30+'  End fitting  '+'='*30)\n",
    "            print('\\n')\n",
    "\n",
    "    def predict(self, ts_loader, X_test=None, eval_mode=False):\n",
    "        self.model.eval()\n",
    "        assert not ts_loader.shuffle, 'ts_loader must have shuffle as False.'\n",
    "\n",
    "        forecasts = []\n",
    "        outsample_ys = []\n",
    "        outsample_masks = []\n",
    "        with t.no_grad():\n",
    "            for batch in iter(ts_loader):\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                forecast = self.model(insample_y=insample_y, insample_x_t=insample_x,\n",
    "                                      insample_mask=insample_mask, outsample_x_t=outsample_x, x_s=s_matrix)\n",
    "                forecasts.append(forecast.cpu().data.numpy())\n",
    "                outsample_ys.append(batch['outsample_y'])\n",
    "                outsample_masks.append(batch['outsample_mask'])\n",
    "        forecasts = np.vstack(forecasts)\n",
    "        outsample_ys = np.vstack(outsample_ys)\n",
    "        outsample_masks = np.vstack(outsample_masks)\n",
    "\n",
    "        self.model.train()\n",
    "        if eval_mode:\n",
    "            return outsample_ys, forecasts, outsample_masks\n",
    "\n",
    "        # Pandas wrangling\n",
    "        frequency = ts_loader.get_frequency()\n",
    "        unique_ids = ts_loader.get_meta_data_col('unique_id')\n",
    "        last_ds = ts_loader.get_meta_data_col('last_ds') #TODO: ajustar of offset\n",
    "\n",
    "        # Predictions for panel\n",
    "        Y_hat_panel = pd.DataFrame(columns=['unique_id', 'ds'])\n",
    "        for i, unique_id in enumerate(unique_ids):\n",
    "            Y_hat_id = pd.DataFrame([unique_id]*self.output_size, columns=[\"unique_id\"])\n",
    "            ds = pd.date_range(start=last_ds[i], periods=self.output_size+1, freq=frequency)\n",
    "            Y_hat_id[\"ds\"] = ds[1:]\n",
    "            Y_hat_panel = Y_hat_panel.append(Y_hat_id, sort=False).reset_index(drop=True)\n",
    "\n",
    "        Y_hat_panel['y_hat'] = forecasts.flatten()\n",
    "\n",
    "        if X_test is not None:\n",
    "            Y_hat_panel = X_test.merge(Y_hat_panel, on=['unique_id', 'ds'], how='left')\n",
    "        \n",
    "        return Y_hat_panel\n",
    "\n",
    "    def evaluate_performance(self, ts_loader, validation_loss_fn):\n",
    "        self.model.eval()\n",
    "\n",
    "        target, forecast, _ = self.predict(ts_loader=ts_loader, eval_mode=True)\n",
    "\n",
    "        target = target.reshape(-1)\n",
    "        forecast = forecast.reshape(-1)\n",
    "\n",
    "        complete_loss = validation_loss_fn(target=target, forecast=forecast, weights=None)\n",
    "\n",
    "        self.model.train()\n",
    "        return complete_loss\n",
    "\n",
    "    def save(self, model_dir, model_id, state_dict = None):\n",
    "    \n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        if state_dict is None:\n",
    "            state_dict = self.model.state_dict()\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        print('Saving model to:\\n {}'.format(model_file)+'\\n')\n",
    "        t.save({'model_state_dict': state_dict}, model_file)\n",
    "\n",
    "    def load(self, model_dir, model_id):\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        path = Path(model_file)\n",
    "\n",
    "        assert path.is_file(), 'No model_*.model file found in this path!'\n",
    "\n",
    "        print('Loading model from:\\n {}'.format(model_file)+'\\n')\n",
    "\n",
    "        checkpoint = t.load(model_file, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c68c0f618c4f787b1e02bfee278e48d25b62407cb335aab5258cedc4db4ced"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}