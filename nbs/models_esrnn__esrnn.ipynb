{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.esrnn.esrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESRNN model\n",
    "\n",
    "> API details."
   ]
  },
  {
   "source": [
    "#TODO: validation loader\n",
    "\n",
    "#TODO: variables exogenas\n",
    "\n",
    "#TODO: sacar mc"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "from nixtla.models.esrnn.utils.config import ModelConfig\n",
    "from nixtla.models.esrnn.utils.esrnn import _ESRNN\n",
    "from nixtla.models.esrnn.utils.losses import SmylLoss, PinballLoss\n",
    "from nixtla.models.esrnn.utils.data import Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ESRNN(object):\n",
    "    \"\"\" Exponential Smoothing Recurrent Neural Network\n",
    "\n",
    "    Pytorch Implementation of the M4 time series forecasting competition winner.\n",
    "    Proposed by Smyl. The model uses a hybrid approach of Machine Learning and\n",
    "    statistical methods by combining recurrent neural networks to model a common\n",
    "    trend with shared parameters across series, and multiplicative Holt-Winter\n",
    "    exponential smoothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_epochs: int\n",
    "        maximum number of complete passes to train data during fit\n",
    "    freq_of_test: int\n",
    "        period for the diagnostic evaluation of the model.\n",
    "    learning_rate: float\n",
    "        size of the stochastic gradient descent steps\n",
    "    lr_scheduler_step_size: int\n",
    "        this step_size is the period for each learning rate decay\n",
    "    per_series_lr_multip: float\n",
    "        multiplier for per-series parameters smoothing and initial\n",
    "        seasonalities learning rate (default 1.0)\n",
    "    gradient_eps: float\n",
    "        term added to the Adam optimizer denominator to improve\n",
    "        numerical stability (default: 1e-8)\n",
    "    gradient_clipping_threshold: float\n",
    "        max norm of gradient vector, with all parameters treated\n",
    "        as a single vector\n",
    "    rnn_weight_decay: float\n",
    "        parameter to control classic L2/Tikhonov regularization\n",
    "        of the rnn parameters\n",
    "    noise_std: float\n",
    "        standard deviation of white noise added to input during\n",
    "        fit to avoid the model from memorizing the train data\n",
    "    level_variability_penalty: float\n",
    "        this parameter controls the strength of the penalization\n",
    "        to the wigglines of the level vector, induces smoothness\n",
    "        in the output\n",
    "    testing_percentile: float\n",
    "        This value is only for diagnostic evaluation.\n",
    "        In case of percentile predictions this parameter controls\n",
    "        for the value predicted, when forecasting point value,\n",
    "        the forecast is the median, so percentile=50.\n",
    "    training_percentile: float\n",
    "        To reduce the model's tendency to over estimate, the\n",
    "        training_percentile can be set to fit a smaller value\n",
    "        through the Pinball Loss.\n",
    "    batch_size: int\n",
    "        number of training examples for the stochastic gradient steps\n",
    "    seasonality: int list\n",
    "        list of seasonalities of the time series\n",
    "        Hourly [24, 168], Daily [7], Weekly [52], Monthly [12],\n",
    "        Quarterly [4], Yearly [].\n",
    "    input_size: int\n",
    "        input size of the recurrent neural network, usually a\n",
    "        multiple of seasonality\n",
    "    output_size: int\n",
    "        output_size or forecast horizon of the recurrent neural\n",
    "        network, usually multiple of seasonality\n",
    "    random_seed: int\n",
    "        random_seed for pseudo random pytorch initializer and\n",
    "        numpy random generator\n",
    "    exogenous_size: int\n",
    "        size of one hot encoded categorical variable, invariannt\n",
    "        per time series of the panel\n",
    "    min_inp_seq_length: int\n",
    "        description\n",
    "    max_periods: int\n",
    "        Parameter to chop longer series, to last max_periods,\n",
    "        max e.g. 40 years\n",
    "    cell_type: str\n",
    "        Type of RNN cell, available GRU, LSTM, RNN, ResidualLSTM.\n",
    "    state_hsize: int\n",
    "        dimension of hidden state of the recurrent neural network\n",
    "    dilations: int list\n",
    "        each list represents one chunk of Dilated LSTMS, connected in\n",
    "        standard ResNet fashion\n",
    "    add_nl_layer: bool\n",
    "        whether to insert a tanh() layer between the RNN stack and the\n",
    "        linear adaptor (output) layers\n",
    "    device: str\n",
    "        pytorch device either 'cpu' or 'cuda'\n",
    "    Notes\n",
    "    -----\n",
    "    **References:**\n",
    "    `M4 Competition Conclusions\n",
    "    <https://rpubs.com/fotpetr/m4competition>`__\n",
    "    `Original Dynet Implementation of ESRNN\n",
    "    <https://github.com/M4Competition/M4-methods/tree/master/118%20-%20slaweks17>`__\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_size=4,\n",
    "                 output_size=8,\n",
    "                 max_epochs=15,\n",
    "                 freq_of_test=-1,\n",
    "                 learning_rate=1e-3,\n",
    "                 lr_scheduler_step_size=9,\n",
    "                 lr_decay=0.9,\n",
    "                 per_series_lr_multip=1.0,\n",
    "                 gradient_eps=1e-8,\n",
    "                 gradient_clipping_threshold=20,\n",
    "                 rnn_weight_decay=0,\n",
    "                 noise_std=0.001,\n",
    "                 level_variability_penalty=80,\n",
    "                 testing_percentile=50,\n",
    "                 training_percentile=50,\n",
    "                 cell_type='LSTM',\n",
    "                 state_hsize=40,\n",
    "                 dilations=[[1, 2], [4, 8]],\n",
    "                 add_nl_layer=False,\n",
    "                 seasonality=[4],\n",
    "                 frequency=None,\n",
    "                 max_periods=20,\n",
    "                 random_seed=1,\n",
    "                 device='cpu', root_dir='./'):\n",
    "        super(ESRNN, self).__init__()\n",
    "        self.mc = ModelConfig(max_epochs=max_epochs,\n",
    "                            freq_of_test=freq_of_test, learning_rate=learning_rate,\n",
    "                            lr_scheduler_step_size=lr_scheduler_step_size, lr_decay=lr_decay,\n",
    "                            per_series_lr_multip=per_series_lr_multip,\n",
    "                            gradient_eps=gradient_eps, gradient_clipping_threshold=gradient_clipping_threshold,\n",
    "                            rnn_weight_decay=rnn_weight_decay, noise_std=noise_std,\n",
    "                            level_variability_penalty=level_variability_penalty,\n",
    "                            testing_percentile=testing_percentile, training_percentile=training_percentile,\n",
    "                            cell_type=cell_type,\n",
    "                            state_hsize=state_hsize, dilations=dilations, add_nl_layer=add_nl_layer,\n",
    "                            seasonality=seasonality, input_size=input_size, output_size=output_size,\n",
    "                            frequency=frequency, max_periods=max_periods, random_seed=random_seed,\n",
    "                            device=device, root_dir=root_dir)\n",
    "        self._fitted = False\n",
    "\n",
    "    def to_tensor(self, x: np.ndarray, dtype = t.float32) -> t.Tensor:\n",
    "        tensor = t.as_tensor(x, dtype=dtype).to(self.mc.device)\n",
    "        return tensor\n",
    "\n",
    "    def __loss_fn(self, loss_name: str):\n",
    "        def loss(x, freq, forecast, target, mask):\n",
    "            if loss_name == 'SMYL':\n",
    "                return \n",
    "            if loss_name == 'MAPE':\n",
    "                return MAPELoss(y=target, y_hat=forecast, mask=mask)\n",
    "            elif loss_name == 'MASE':\n",
    "                return MASELoss(y=target, y_hat=forecast, y_insample=x, seasonality=freq, mask=mask)\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return SMAPELoss(y=target, y_hat=forecast, mask=mask)\n",
    "            elif loss_name == 'MSE':\n",
    "                return MSELoss(y=target, y_hat=forecast, mask=mask)\n",
    "            elif loss_name == 'MAE':\n",
    "                return MAELoss(y=target, y_hat=forecast, mask=mask)\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def __val_loss_fn(self, loss_name='MAE'):\n",
    "        #TODO: mase not implemented\n",
    "        def loss(forecast, target, weights):\n",
    "            if loss_name == 'MAPE':\n",
    "                return mape(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return smape(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MSE':\n",
    "                return mse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'RMSE':\n",
    "                return rmse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MAE':\n",
    "                return mae(y=target, y_hat=forecast, weights=weights)\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "       \n",
    "    def evaluate_performance(self, ts_loader, validation_loss_fn):\n",
    "        \"\"\"\n",
    "        Auxiliary function, evaluate ESRNN model for training\n",
    "        procedure supervision.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataloader: pytorch dataloader\n",
    "        criterion: pytorch test criterion\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model_loss: float\n",
    "            loss for train supervision purpose.\n",
    "        \"\"\"\n",
    "\n",
    "        with t.no_grad():\n",
    "            # Create fast dataloader\n",
    "            # if self.mc.n_series < self.mc.batch_size_test: new_batch_size = self.mc.n_series\n",
    "            # else: new_batch_size = self.mc.batch_size_test\n",
    "            # dataloader.update_batch_size(new_batch_size)\n",
    "\n",
    "            model_loss = 0.0\n",
    "            for batch in iter(ts_loader):\n",
    "                windows_y, windows_y_hat, _ = self.esrnn(batch)\n",
    "                loss = validation_loss_fn(windows_y, windows_y_hat)\n",
    "                model_loss += loss.data.cpu().numpy()\n",
    "\n",
    "            model_loss /= dataloader.n_batches\n",
    "            dataloader.update_batch_size(self.mc.batch_size)\n",
    "        return model_loss\n",
    "\n",
    "\n",
    "    #def fit(self, X_df, y_df, X_test_df=None, shuffle=True, verbose=True):\n",
    "    def fit(self, train_ts_loader, val_ts_loader=None, max_epochs=None, verbose=True, eval_epochs=1):\n",
    "        \"\"\"\n",
    "        Fit ESRNN model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_df : pandas dataframe\n",
    "            Train dataframe in long format with columns 'unique_id', 'ds'\n",
    "            and 'x'.\n",
    "            - 'unique_id' an identifier of each independent time series.\n",
    "            - 'ds' is a datetime column\n",
    "            - 'x' is a single exogenous variable\n",
    "        y_df : pandas dataframe\n",
    "            Train dataframe in long format with columns 'unique_id', 'ds' and 'y'.\n",
    "            - 'unique_id' an identifier of each independent time series.\n",
    "            - 'ds' is a datetime column\n",
    "            - 'y' is the column with the target values\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "\n",
    "        # Exogenous variables\n",
    "        exogenous_size = 0 #TODO: mementaneo, no lo queremos ahora\n",
    "\n",
    "        # Random Seeds (model initialization)\n",
    "        t.manual_seed(self.mc.random_seed)\n",
    "        np.random.seed(self.mc.random_seed)\n",
    "\n",
    "        # Initialize model\n",
    "        n_series = train_ts_loader.get_n_series()\n",
    "        self.instantiate_esrnn(exogenous_size, n_series)\n",
    "\n",
    "        self.mc.frequency = train_ts_loader.get_frequency()\n",
    "        print(\"Infered frequency: {}\".format(self.mc.frequency))\n",
    "\n",
    "        # Train model\n",
    "        self._fitted = True\n",
    "\n",
    "        # dataloader, max_epochs, shuffle, verbose\n",
    "        if verbose: print(15*'='+' Training ESRNN  ' + 15*'=' + '\\n')\n",
    "\n",
    "        # Optimizers\n",
    "        self.es_optimizer = optim.Adam(params=self.esrnn.es.parameters(),\n",
    "                                        lr=self.mc.learning_rate*self.mc.per_series_lr_multip,\n",
    "                                        betas=(0.9, 0.999), eps=self.mc.gradient_eps)\n",
    "\n",
    "        self.es_scheduler = StepLR(optimizer=self.es_optimizer,\n",
    "                                    step_size=self.mc.lr_scheduler_step_size,\n",
    "                                    gamma=0.9)\n",
    "\n",
    "        self.rnn_optimizer = optim.Adam(params=self.esrnn.rnn.parameters(),\n",
    "                                        lr=self.mc.learning_rate,\n",
    "                                        betas=(0.9, 0.999), eps=self.mc.gradient_eps,\n",
    "                                        weight_decay=self.mc.rnn_weight_decay)\n",
    "\n",
    "        self.rnn_scheduler = StepLR(optimizer=self.rnn_optimizer,\n",
    "                                    step_size=self.mc.lr_scheduler_step_size,\n",
    "                                    gamma=self.mc.lr_decay)\n",
    "\n",
    "        # Loss Functions\n",
    "        #TODO: cambiar por similar a Nbeats\n",
    "        train_tau = self.mc.training_percentile / 100\n",
    "        train_loss = SmylLoss(tau=train_tau,\n",
    "                              level_variability_penalty=self.mc.level_variability_penalty)\n",
    "\n",
    "        eval_tau = self.mc.testing_percentile / 100\n",
    "        eval_loss = PinballLoss(tau=eval_tau)\n",
    "\n",
    "        # Overwrite n_iterations and train datasets\n",
    "        if max_epochs is None:\n",
    "            max_epochs = self.mc.max_epochs\n",
    "\n",
    "        start = time.time()\n",
    "        self.trajectories = {'epoch':[],'train_loss':[], 'val_loss':[]}\n",
    "\n",
    "        # for epoch in range(max_epochs):\n",
    "        #     self.esrnn.train()\n",
    "        #     start = time.time()\n",
    "\n",
    "        #     losses = []\n",
    "        # Training Loop\n",
    "        for epoch in range(max_epochs):\n",
    "            losses = []\n",
    "            for batch in iter(train_ts_loader):\n",
    "                self.esrnn.train()\n",
    "                self.es_optimizer.zero_grad()\n",
    "                self.rnn_optimizer.zero_grad()\n",
    "                \n",
    "                insample_y  = self.to_tensor(x=batch['insample_y'])\n",
    "                idxs        = self.to_tensor(x=batch['idxs'], dtype=t.long)\n",
    "                categories  = self.to_tensor(x=batch['categories']) #TODO: S_matrix\n",
    "\n",
    "                windows_y, windows_y_hat, levels = self.esrnn(y=insample_y, idxs=idxs, categories=categories)\n",
    "                \n",
    "                # Pinball loss on normalized values\n",
    "                training_loss = train_loss(windows_y, windows_y_hat, levels)\n",
    "                training_loss.backward()\n",
    "                losses.append(training_loss.cpu().data.numpy())\n",
    "                \n",
    "                t.nn.utils.clip_grad_norm_(self.esrnn.rnn.parameters(),\n",
    "                                            self.mc.gradient_clipping_threshold)\n",
    "                t.nn.utils.clip_grad_norm_(self.esrnn.es.parameters(),\n",
    "                                            self.mc.gradient_clipping_threshold)\n",
    "                self.rnn_optimizer.step()\n",
    "                self.es_optimizer.step()\n",
    "\n",
    "                # Decay learning rate\n",
    "                self.es_scheduler.step()\n",
    "                self.rnn_scheduler.step()\n",
    "\n",
    "            # Evaluation\n",
    "            if (epoch % eval_epochs == 0):\n",
    "                display_string = 'Epoch: {}, Time: {:03.3f}, Insample loss: {:.5f}'.format(epoch,\n",
    "                                                                                time.time()-start,\n",
    "                                                                                np.mean(losses))\n",
    "                self.trajectories['epoch'].append(epoch)\n",
    "                self.trajectories['train_loss'].append(training_loss.cpu().data.numpy())\n",
    "\n",
    "                if val_ts_loader is not None:\n",
    "                    loss = self.evaluate_performance(ts_loader=val_ts_loader, \n",
    "                                                        validation_loss_fn=eval_loss)\n",
    "                    display_string += \", Outsample loss: {:.5f}\".format(loss)\n",
    "                    self.trajectories['val_loss'].append(loss)\n",
    "                \n",
    "                print(display_string)\n",
    "\n",
    "                self.esrnn.train()\n",
    "                train_ts_loader.train()\n",
    "\n",
    "    def instantiate_esrnn(self, exogenous_size, n_series):\n",
    "        \"\"\"Auxiliary function used at beginning of train to instantiate ESRNN\"\"\"\n",
    "\n",
    "        self.mc.exogenous_size = exogenous_size\n",
    "        self.mc.n_series = n_series\n",
    "        self.esrnn = _ESRNN(self.mc).to(self.mc.device)\n",
    "\n",
    "\n",
    "    def predict(self, ts_loader, X_test=None):\n",
    "        assert self._fitted, \"Model not fitted yet\"\n",
    "        self.esrnn.eval()\n",
    "        ts_loader.eval()\n",
    "        frequency = ts_loader.get_frequency()\n",
    "\n",
    "        # Build forecasts\n",
    "        unique_ids = ts_loader.get_meta_data_col('unique_id')\n",
    "        last_ds = ts_loader.get_meta_data_col('last_ds') #TODO: ajustar of offset\n",
    "\n",
    "        with t.no_grad():\n",
    "            forecasts = []\n",
    "            for batch in iter(ts_loader):\n",
    "                insample_y  = self.to_tensor(x=batch['insample_y'])\n",
    "                idxs        = self.to_tensor(x=batch['idxs'], dtype=t.long)\n",
    "                categories  = self.to_tensor(x=batch['categories']) #TODO: S_matrix\n",
    "\n",
    "                forecast = self.esrnn.predict(y=insample_y, idxs=idxs, categories=categories)\n",
    "                forecasts += [forecast.cpu().data.numpy()]\n",
    "        forecasts = np.vstack(forecasts)\n",
    "\n",
    "        # Predictions for panel\n",
    "        Y_hat_panel = pd.DataFrame(columns=['unique_id', 'ds'])\n",
    "        for i, unique_id in enumerate(unique_ids):\n",
    "            Y_hat_id = pd.DataFrame([unique_id]*self.mc.output_size, columns=[\"unique_id\"])\n",
    "            ds = pd.date_range(start=last_ds[i], periods=self.mc.output_size+1, freq=frequency)\n",
    "            Y_hat_id[\"ds\"] = ds[1:]\n",
    "            Y_hat_panel = Y_hat_panel.append(Y_hat_id, sort=False).reset_index(drop=True)\n",
    "\n",
    "        Y_hat_panel['y_hat'] = forecasts.flatten()\n",
    "\n",
    "        if X_test is not None:\n",
    "            Y_hat_panel = X_test.merge(Y_hat_panel, on=['unique_id', 'ds'], how='left')\n",
    "        \n",
    "        return Y_hat_panel\n",
    "\n",
    "    def save(self, model_dir=None, copy=None):\n",
    "        \"\"\"Auxiliary function to save ESRNN model\"\"\"\n",
    "        if copy is not None:\n",
    "                self.mc.copy = copy\n",
    "\n",
    "        if not model_dir:\n",
    "            assert self.mc.root_dir\n",
    "            model_dir = self.get_dir_name()\n",
    "\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        rnn_filepath = os.path.join(model_dir, \"rnn.model\")\n",
    "        es_filepath = os.path.join(model_dir, \"es.model\")\n",
    "\n",
    "        print('Saving model to:\\n {}'.format(model_dir)+'\\n')\n",
    "        t.save({'model_state_dict': self.es.state_dict()}, es_filepath)\n",
    "        t.save({'model_state_dict': self.rnn.state_dict()}, rnn_filepath)\n",
    "\n",
    "    def load(self, model_dir=None, copy=None):\n",
    "        \"\"\"Auxiliary function to load ESRNN model\"\"\"\n",
    "        if copy is not None:\n",
    "            self.mc.copy = copy\n",
    "\n",
    "        if not model_dir:\n",
    "            assert self.mc.root_dir\n",
    "            model_dir = self.get_dir_name()\n",
    "\n",
    "        rnn_filepath = os.path.join(model_dir, \"rnn.model\")\n",
    "        es_filepath = os.path.join(model_dir, \"es.model\")\n",
    "        path = Path(es_filepath)\n",
    "\n",
    "        if path.is_file():\n",
    "            print('Loading model from:\\n {}'.format(model_dir)+'\\n')\n",
    "\n",
    "            checkpoint = t.load(es_filepath, map_location=self.mc.device)\n",
    "            self.es.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.es.to(self.mc.device)\n",
    "\n",
    "            checkpoint = t.load(rnn_filepath, map_location=self.mc.device)\n",
    "            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.rnn.to(self.mc.device)\n",
    "        else:\n",
    "            print('Model path {} does not exist'.format(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nixtla.data.tsdataset import TimeSeriesDataset\n",
    "from nixtla.data.tsloader_general import TimeSeriesLoader as TimeSeriesLoaderGeneral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TourismInfo.groups[0] Yearly\n",
      "Processing dataframes ...\n",
      "Creating ts tensor ...\n"
     ]
    }
   ],
   "source": [
    "from nixtla.data.datasets.tourism import Tourism, TourismInfo\n",
    "group = TourismInfo.groups[0]\n",
    "print(\"TourismInfo.groups[0]\", group)\n",
    "Y_df, _ = Tourism.load(directory='data', group=group)\n",
    "tourism_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=None, X_df=None, ts_train_mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = TimeSeriesLoaderGeneral(ts_dataset=tourism_dataset,\n",
    "                                            model='esrnn',\n",
    "                                            offset=4,\n",
    "                                            window_sampling_limit=20*4, \n",
    "                                            input_size=2*4,\n",
    "                                            output_size=4,\n",
    "                                            idx_to_sample_freq=1,\n",
    "                                            batch_size= 32,\n",
    "                                            n_series_per_batch=32,\n",
    "                                            is_train_loader=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "esrnn = ESRNN(input_size=1*4,\n",
    "              output_size=4,\n",
    "              max_epochs=5,\n",
    "              freq_of_test=-1,\n",
    "              learning_rate=1e-3,\n",
    "              lr_scheduler_step_size=9,\n",
    "              lr_decay=0.9,\n",
    "              per_series_lr_multip=1.0,\n",
    "              gradient_eps=1e-8,\n",
    "              gradient_clipping_threshold=20,\n",
    "              rnn_weight_decay=0,\n",
    "              noise_std=0.001,\n",
    "              level_variability_penalty=200,\n",
    "              testing_percentile=50,\n",
    "              training_percentile=50,\n",
    "              cell_type='LSTM',\n",
    "              state_hsize=40,\n",
    "              dilations=[[1, 2], [4, 8]],\n",
    "              add_nl_layer=False,\n",
    "              seasonality=[],\n",
    "              frequency=None,\n",
    "              max_periods=20,\n",
    "              random_seed=1,\n",
    "              device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.1234e+04,\n",
      "         1.7522e+04, 2.0252e+04, 2.2472e+04, 2.3885e+04, 2.3228e+04],\n",
      "        [7.0900e+02, 5.0200e+02, 5.7300e+02, 8.0400e+02, 8.6200e+02, 7.5600e+02,\n",
      "         1.1120e+03, 9.7000e+02, 1.1060e+03, 1.5700e+03, 1.5990e+03],\n",
      "        [1.7340e+03, 1.0500e+03, 1.8640e+03, 1.7210e+03, 1.6280e+03, 1.3990e+03,\n",
      "         1.3200e+03, 1.2210e+03, 1.2580e+03, 1.1730e+03, 1.6550e+03],\n",
      "        [5.4000e+03, 5.2000e+03, 5.5000e+03, 6.5000e+03, 6.1000e+03, 5.9000e+03,\n",
      "         4.0000e+03, 5.3000e+03, 4.9000e+03, 4.2000e+03, 5.5000e+03],\n",
      "        [7.0590e+03, 8.3400e+03, 8.6350e+03, 8.5620e+03, 1.3378e+04, 1.1618e+04,\n",
      "         1.0253e+04, 8.3160e+03, 1.0422e+04, 1.2883e+04, 1.6722e+04],\n",
      "        [2.0496e+03, 1.6369e+03, 9.0604e+02, 7.7165e+02, 7.4132e+02, 6.9593e+02,\n",
      "         6.1362e+02, 3.7112e+02, 5.9176e+02, 7.4297e+02, 7.5272e+02],\n",
      "        [1.7720e+03, 1.6760e+03, 1.4230e+03, 1.7510e+03, 1.3850e+03, 1.2290e+03,\n",
      "         1.1020e+03, 1.1890e+03, 1.3220e+03, 1.1420e+03, 1.5620e+03],\n",
      "        [4.9021e+02, 6.9110e+02, 7.8539e+02, 6.7249e+02, 8.1042e+02, 9.1592e+02,\n",
      "         1.0107e+03, 1.5846e+03, 1.8051e+03, 2.1386e+03, 2.6821e+03],\n",
      "        [6.4410e+05, 7.1700e+05, 8.3610e+05, 7.9690e+05, 8.6920e+05, 8.8100e+05,\n",
      "         8.3910e+05, 7.0490e+05, 1.0448e+06, 1.1103e+06, 1.1115e+06],\n",
      "        [5.7860e+03, 6.4140e+03, 6.4120e+03, 6.9880e+03, 8.0250e+03, 7.8890e+03,\n",
      "         6.4840e+03, 5.9280e+03, 6.8760e+03, 7.9260e+03, 9.5590e+03],\n",
      "        [8.9015e+03, 1.2857e+04, 1.7549e+04, 1.8148e+04, 1.9361e+04, 2.0079e+04,\n",
      "         2.4802e+04, 2.9609e+04, 3.2452e+04, 3.7068e+04, 4.0850e+04],\n",
      "        [2.2281e+05, 2.3931e+05, 2.4708e+05, 2.6695e+05, 2.8573e+05, 2.9598e+05,\n",
      "         2.8685e+05, 2.5338e+05, 2.5116e+05, 2.6604e+05, 2.5339e+05]])\n",
      "y.shape torch.Size([32, 11])\n",
      "idxs tensor([331, 467, 249, 342, 110, 391, 245, 246, 224, 272, 223, 340, 408, 199,\n",
      "        454, 375, 440, 514, 439, 317, 115, 399, 463, 226, 248,  64, 464,   2,\n",
      "        215, 270,   5,  69])\n",
      "idxs.shape torch.Size([32])\n",
      "batch_len_series [16 27 27 27 11 15 16 27 16 27 27 30 27 27 29 14 27 11 16 27 47 27 32 16\n",
      " 27 20 27 29 29 22 27 27]\n",
      "y tensor([[1.6700e+04, 1.4500e+04, 1.8800e+04, 1.7800e+04, 1.9200e+04, 1.4800e+04,\n",
      "         1.5900e+04, 1.7200e+04, 1.8700e+04, 1.8000e+04, 2.2400e+04],\n",
      "        [5.3100e+02, 6.7200e+02, 4.9700e+02, 4.7400e+02, 6.4000e+02, 5.7300e+02,\n",
      "         5.4900e+02, 6.1700e+02, 8.2500e+02, 6.0400e+02, 7.4000e+02],\n",
      "        [7.5270e+03, 7.0570e+03, 7.4470e+03, 7.9200e+03, 9.2960e+03, 1.1019e+04,\n",
      "         9.3450e+03, 1.0223e+04, 1.0879e+04, 1.2540e+04, 1.1586e+04],\n",
      "        [2.5184e+06, 2.5345e+06, 2.2186e+06, 2.5027e+06, 2.6570e+06, 2.4843e+06,\n",
      "         2.4014e+06, 2.4406e+06, 2.6851e+06, 2.9521e+06, 2.8865e+06],\n",
      "        [2.5025e+07, 3.3825e+07, 3.3758e+07, 3.1291e+07, 2.5253e+07, 1.9567e+07,\n",
      "         1.9045e+07, 1.9451e+07, 2.2302e+07, 2.7671e+07, 2.8666e+07],\n",
      "        [6.5650e+03, 6.7800e+03, 7.1810e+03, 7.5050e+03, 8.1600e+03, 8.7710e+03,\n",
      "         8.4260e+03, 8.6920e+03, 9.9250e+03, 1.0279e+04, 1.0912e+04],\n",
      "        [6.4000e+03, 6.6000e+03, 7.4000e+03, 7.9000e+03, 8.5000e+03, 9.0000e+03,\n",
      "         8.3000e+03, 6.6000e+03, 8.7000e+03, 1.0200e+04, 9.7000e+03],\n",
      "        [2.7566e+04, 3.3716e+04, 3.0817e+04, 2.5612e+04, 2.8512e+04, 3.0939e+04,\n",
      "         3.2313e+04, 2.7888e+04, 2.7415e+04, 2.3174e+04, 2.3407e+04],\n",
      "        [6.1600e+04, 6.6100e+04, 6.8100e+04, 7.4800e+04, 8.0900e+04, 8.2200e+04,\n",
      "         8.0700e+04, 9.8400e+04, 1.2520e+05, 1.4720e+05, 1.6000e+05],\n",
      "        [2.4537e+04, 2.5044e+04, 2.3739e+04, 2.7189e+04, 3.0235e+04, 3.3130e+04,\n",
      "         4.0781e+04, 3.8125e+04, 4.3744e+04, 4.4819e+04, 4.7792e+04],\n",
      "        [2.0150e+03, 1.7280e+03, 3.1920e+03, 2.4480e+03, 2.5780e+03, 2.4530e+03,\n",
      "         2.6180e+03, 2.6110e+03, 2.3380e+03, 2.7730e+03, 3.0830e+03],\n",
      "        [4.6940e+03, 4.9400e+03, 5.2360e+03, 5.8750e+03, 6.4480e+03, 7.0330e+03,\n",
      "         7.2820e+03, 7.6540e+03, 8.9140e+03, 9.6720e+03, 1.0715e+04],\n",
      "        [1.7192e+04, 1.7708e+04, 1.6348e+04, 1.8758e+04, 2.1036e+04, 2.4312e+04,\n",
      "         3.1589e+04, 2.8854e+04, 2.7943e+04, 2.6442e+04, 2.9282e+04],\n",
      "        [4.1319e+04, 4.5324e+04, 6.5708e+04, 6.4109e+04, 8.1277e+04, 8.0698e+04,\n",
      "         7.3167e+04, 6.6751e+04, 7.3703e+04, 6.3295e+04, 7.0156e+04],\n",
      "        [1.2580e+00, 1.2284e+00, 1.3367e+00, 1.3382e+00, 1.3976e+00, 1.4997e+00,\n",
      "         1.5469e+00, 1.5357e+00, 1.4215e+00, 1.6309e+00, 1.7844e+00],\n",
      "        [9.6523e+03, 1.0140e+04, 9.2138e+03, 8.5971e+03, 1.1913e+04, 1.4657e+04,\n",
      "         1.5938e+04, 2.3450e+04, 2.3078e+04, 2.3919e+04, 2.6798e+04],\n",
      "        [1.0682e+05, 1.0462e+05, 1.0821e+05, 1.1524e+05, 1.3115e+05, 1.3246e+05,\n",
      "         1.3796e+05, 1.4570e+05, 1.5109e+05, 1.5592e+05, 1.6459e+05],\n",
      "        [5.9910e+06, 6.2020e+06, 6.8096e+06, 7.1150e+06, 7.0494e+06, 6.5503e+06,\n",
      "         6.7277e+06, 6.1009e+06, 6.3184e+06, 6.8095e+06, 6.7504e+06],\n",
      "        [5.8300e+04, 6.2600e+04, 6.3100e+04, 6.4200e+04, 7.2900e+04, 7.4700e+04,\n",
      "         7.5100e+04, 7.1100e+04, 9.1500e+04, 9.2400e+04, 1.0880e+05],\n",
      "        [1.5287e+06, 1.4972e+06, 1.4845e+06, 1.6072e+06, 1.7868e+06, 1.9094e+06,\n",
      "         2.0451e+06, 2.1044e+06, 2.3342e+06, 2.3655e+06, 2.4089e+06],\n",
      "        [3.1004e+06, 3.3010e+06, 3.6169e+06, 4.0077e+06, 3.4293e+06, 3.2594e+06,\n",
      "         2.7227e+06, 3.1911e+06, 3.5172e+06, 3.7580e+06, 3.9859e+06],\n",
      "        [9.0270e+03, 9.6950e+03, 1.1288e+04, 1.1019e+04, 1.0743e+04, 1.1988e+04,\n",
      "         1.3075e+04, 1.3248e+04, 1.4291e+04, 1.5826e+04, 1.7254e+04],\n",
      "        [8.1314e+05, 8.1390e+05, 7.5108e+05, 7.0745e+05, 7.2099e+05, 6.7356e+05,\n",
      "         7.1545e+05, 6.2774e+05, 7.1034e+05, 6.8534e+05, 6.5107e+05],\n",
      "        [2.8000e+04, 3.4600e+04, 3.9600e+04, 4.2900e+04, 5.0600e+04, 4.6000e+04,\n",
      "         5.1000e+04, 5.0600e+04, 6.3900e+04, 7.0500e+04, 7.2600e+04],\n",
      "        [5.3969e+04, 6.5866e+04, 7.6516e+04, 9.2585e+04, 1.2024e+05, 1.5801e+05,\n",
      "         1.9004e+05, 1.7614e+05, 2.5129e+05, 2.8502e+05, 3.0845e+05],\n",
      "        [1.1930e+03, 9.8400e+02, 1.4170e+03, 1.2920e+03, 1.4110e+03, 1.7020e+03,\n",
      "         1.5940e+03, 1.4900e+03, 1.2770e+03, 1.4490e+03, 1.6960e+03],\n",
      "        [1.1681e+04, 1.3224e+04, 1.0864e+04, 1.2518e+04, 1.3872e+04, 1.3412e+04,\n",
      "         1.4248e+04, 1.3473e+04, 1.4667e+04, 1.6321e+04, 1.7375e+04],\n",
      "        [6.1042e+00, 5.4272e+00, 6.3056e+00, 6.4000e+00, 6.7200e+00, 5.8800e+00,\n",
      "         4.6370e+00, 4.4283e+00, 4.6480e+00, 5.8668e+00, 7.2167e+00],\n",
      "        [1.6950e-01, 1.6960e-01, 1.8020e-01, 2.0000e-01, 3.1500e-01, 2.7300e-01,\n",
      "         3.6500e-01, 4.5740e-01, 6.2750e-01, 5.5630e-01, 4.6470e-01],\n",
      "        [5.6780e+01, 4.5100e+01, 2.5820e+01, 4.7340e+01, 2.2620e+01, 3.3440e+01,\n",
      "         4.4260e+01, 1.3890e+01, 1.8100e+01, 2.3420e+01, 2.7570e+01],\n",
      "        [1.0927e+04, 1.0672e+04, 1.1615e+04, 1.0524e+04, 1.1437e+04, 1.1626e+04,\n",
      "         1.1466e+04, 1.1339e+04, 1.2162e+04, 1.4292e+04, 1.4573e+04],\n",
      "        [3.2531e+04, 3.4426e+04, 3.5140e+04, 3.7694e+04, 4.4610e+04, 3.9604e+04,\n",
      "         3.4790e+04, 3.5078e+04, 3.7066e+04, 4.7527e+04, 5.5152e+04]])\n",
      "y.shape torch.Size([32, 11])\n",
      "idxs tensor([183, 339, 114, 104, 500, 256, 193, 151, 230, 361, 373, 178, 316, 163,\n",
      "        490,  54, 418, 497, 192, 452, 487, 417,  66, 180,  75, 459, 130, 510,\n",
      "        506,  41, 383, 143])\n",
      "idxs.shape torch.Size([32])\n",
      "batch_len_series [27 27 27 27 27 16 27 27 27 27 22 33 27 22 14 16 27 27 27 29 22 21 27 27\n",
      " 16 22 16 16 16 27 11 27]\n",
      "y tensor([[6.8120e+03, 6.6620e+03, 6.5870e+03, 6.7690e+03, 7.3910e+03, 6.9650e+03,\n",
      "         6.5380e+03, 5.7230e+03, 6.1020e+03, 5.6610e+03, 5.2630e+03],\n",
      "        [5.8038e+04, 4.1235e+04, 2.3865e+04, 3.4874e+04, 3.8559e+04, 3.8083e+04,\n",
      "         3.6965e+04, 2.9936e+04, 3.2538e+04, 3.0121e+04, 2.8669e+04],\n",
      "        [7.4911e+04, 8.0516e+04, 7.9136e+04, 8.6459e+04, 9.3000e+04, 8.3612e+04,\n",
      "         7.5429e+04, 7.7529e+04, 8.5938e+04, 9.2664e+04, 9.2924e+04],\n",
      "        [8.9807e+04, 9.5255e+04, 1.0237e+05, 1.0997e+05, 1.1394e+05, 1.1894e+05,\n",
      "         1.1712e+05, 1.2440e+05, 1.4603e+05, 1.5013e+05, 1.5698e+05],\n",
      "        [1.2746e+04, 1.3046e+04, 1.1220e+04, 1.2738e+04, 1.6313e+04, 2.1582e+04,\n",
      "         2.6603e+04, 2.2022e+04, 2.4792e+04, 2.0638e+04, 2.0121e+04],\n",
      "        [1.0600e+04, 1.1200e+04, 1.5100e+04, 1.5500e+04, 1.6900e+04, 2.0300e+04,\n",
      "         1.7700e+04, 2.1800e+04, 3.3500e+04, 3.5400e+04, 4.0800e+04],\n",
      "        [2.1208e+04, 2.1861e+04, 2.4129e+04, 2.8312e+04, 3.2512e+04, 3.1153e+04,\n",
      "         3.6375e+04, 3.7821e+04, 4.1461e+04, 4.3720e+04, 4.6887e+04],\n",
      "        [1.3646e+04, 1.7551e+04, 1.6410e+04, 2.3241e+04, 3.3502e+04, 5.3174e+04,\n",
      "         7.6534e+04, 6.5989e+04, 8.4368e+04, 8.7850e+04, 1.0572e+05],\n",
      "        [3.3240e+03, 3.3750e+03, 3.4400e+03, 4.2890e+03, 4.7910e+03, 4.3350e+03,\n",
      "         5.5350e+03, 5.8410e+03, 7.6590e+03, 7.7130e+03, 7.9290e+03],\n",
      "        [8.8140e+03, 9.1200e+03, 1.0292e+04, 1.0134e+04, 9.0050e+03, 9.1330e+03,\n",
      "         8.9700e+03, 1.0587e+04, 1.1986e+04, 1.3124e+04, 1.2177e+04],\n",
      "        [1.0405e+03, 8.5702e+02, 4.2910e+02, 3.2286e+02, 3.7026e+02, 3.2784e+02,\n",
      "         2.8614e+02, 1.5611e+02, 2.4598e+02, 3.3531e+02, 3.6045e+02],\n",
      "        [2.2232e+05, 2.6367e+05, 2.6934e+05, 2.3593e+05, 2.6294e+05, 3.1486e+05,\n",
      "         2.8639e+05, 3.1885e+05, 2.0869e+05, 3.3971e+05, 3.9205e+05],\n",
      "        [8.3600e+02, 8.4400e+02, 1.0950e+03, 1.0680e+03, 1.2250e+03, 2.4530e+03,\n",
      "         3.3110e+03, 2.0770e+03, 1.9420e+03, 2.8660e+03, 3.0270e+03],\n",
      "        [7.6051e+02, 6.1412e+02, 3.3627e+02, 3.0238e+02, 2.6071e+02, 2.6381e+02,\n",
      "         2.5721e+02, 1.4389e+02, 2.2615e+02, 3.0631e+02, 4.2055e+02],\n",
      "        [1.7069e+03, 1.8363e+03, 1.3956e+03, 1.6451e+03, 2.1754e+03, 2.9493e+03,\n",
      "         3.8734e+03, 4.0501e+03, 4.1448e+03, 4.8980e+03, 4.6394e+03],\n",
      "        [5.7000e+03, 6.3000e+03, 8.7000e+03, 8.8000e+03, 9.7000e+03, 9.1000e+03,\n",
      "         8.3000e+03, 1.0300e+04, 1.4700e+04, 1.3100e+04, 1.5900e+04],\n",
      "        [2.1790e+03, 2.0420e+03, 2.4690e+03, 1.9880e+03, 2.7600e+03, 2.6050e+03,\n",
      "         2.7400e+03, 2.9430e+03, 3.0620e+03, 2.9380e+03, 2.8580e+03],\n",
      "        [2.6625e+04, 3.0817e+04, 2.9030e+04, 3.7993e+04, 3.7346e+04, 3.7594e+04,\n",
      "         3.3443e+04, 3.2951e+04, 3.7070e+04, 4.4599e+04, 4.8760e+04],\n",
      "        [2.7550e+03, 3.0590e+03, 2.1980e+03, 2.9830e+03, 2.6330e+03, 2.5620e+03,\n",
      "         2.7380e+03, 3.0730e+03, 3.5960e+03, 3.4410e+03, 3.3050e+03],\n",
      "        [5.0860e-01, 5.0880e-01, 7.2060e-01, 6.0000e-01, 8.4000e-01, 1.2600e+00,\n",
      "         1.2480e+00, 1.4137e+00, 1.6733e+00, 1.6437e+00, 1.5582e+00],\n",
      "        [2.3242e+02, 1.5206e+02, 1.0170e+02, 1.2462e+02, 1.2151e+02, 1.0137e+02,\n",
      "         2.5270e+01, 2.9200e+00, 3.2900e+00, 5.8650e+01, 1.0640e+01],\n",
      "        [1.0008e+05, 8.5634e+04, 7.4972e+04, 8.5576e+04, 9.4688e+04, 8.3126e+04,\n",
      "         9.0278e+04, 7.3132e+04, 9.5867e+04, 1.0081e+05, 1.0638e+05],\n",
      "        [3.6310e+03, 2.9070e+03, 1.6680e+03, 3.3470e+03, 4.0580e+03, 3.9250e+03,\n",
      "         5.6850e+03, 4.0410e+03, 4.4580e+03, 4.6000e+03, 4.4810e+03],\n",
      "        [4.4840e+03, 6.2630e+03, 7.0950e+03, 7.4700e+03, 8.3660e+03, 8.4250e+03,\n",
      "         8.4620e+03, 9.3620e+03, 9.9290e+03, 1.0773e+04, 1.1325e+04],\n",
      "        [1.5610e+05, 1.6530e+05, 1.7830e+05, 1.7180e+05, 2.0560e+05, 1.9590e+05,\n",
      "         1.9640e+05, 1.9240e+05, 2.3900e+05, 2.5550e+05, 2.7920e+05],\n",
      "        [5.4324e+02, 3.9533e+02, 2.0979e+02, 2.2864e+02, 1.4569e+02, 1.7458e+02,\n",
      "         1.3515e+02, 1.0233e+02, 1.8266e+02, 1.9718e+02, 1.8620e+02],\n",
      "        [4.7000e+03, 5.7000e+03, 6.5000e+03, 7.7000e+03, 5.2000e+03, 5.7000e+03,\n",
      "         5.0000e+03, 6.2000e+03, 6.7000e+03, 8.0000e+03, 8.2000e+03],\n",
      "        [3.4300e+04, 3.8700e+04, 4.2200e+04, 3.7600e+04, 4.5800e+04, 4.0000e+04,\n",
      "         3.7500e+04, 3.3300e+04, 4.1200e+04, 4.8700e+04, 4.5400e+04],\n",
      "        [1.8600e+04, 2.1100e+04, 2.0800e+04, 2.4700e+04, 2.6700e+04, 2.6700e+04,\n",
      "         2.5600e+04, 2.5700e+04, 2.8000e+04, 3.0800e+04, 3.3200e+04],\n",
      "        [7.4005e+04, 7.8195e+04, 8.7325e+04, 9.2190e+04, 1.1199e+05, 1.1145e+05,\n",
      "         1.1985e+05, 1.3358e+05, 1.4851e+05, 1.5360e+05, 1.5937e+05],\n",
      "        [2.3729e+05, 2.7204e+05, 2.4424e+05, 2.8008e+05, 7.5876e+05, 9.0674e+05,\n",
      "         8.2219e+05, 1.0002e+06, 1.3344e+06, 1.4763e+06, 1.5576e+06],\n",
      "        [1.2820e+04, 1.2925e+04, 1.2498e+04, 1.4749e+04, 1.6400e+04, 1.6175e+04,\n",
      "         1.8262e+04, 1.9539e+04, 2.0067e+04, 1.9550e+04, 1.9665e+04]])\n",
      "y.shape torch.Size([32, 11])\n",
      "idxs tensor([293,  93, 122, 255, 153, 204, 407, 313, 404, 368,  61,  78, 346,  39,\n",
      "         57, 229, 398,  96, 348, 513,  52, 100, 335, 426, 197,  60, 231, 191,\n",
      "        182, 420, 468, 370])\n",
      "idxs.shape torch.Size([32])\n",
      "batch_len_series [22 11 27 16 27 35 16 27 30 16 16 27 16 16 27 29 27 27 16 22 27 29 30 27\n",
      " 27 22 16 21 27 27 22 30]\n",
      "y tensor([[1.5817e+03, 9.6585e+02, 6.3651e+02, 8.2777e+02, 8.9567e+02, 7.3410e+02,\n",
      "         6.8361e+02, 3.6517e+02, 4.9628e+02, 5.9376e+02, 6.1042e+02],\n",
      "        [2.0670e+06, 2.2720e+06, 2.4755e+06, 2.4008e+06, 2.1713e+06, 2.1821e+06,\n",
      "         2.2545e+06, 2.1167e+06, 2.1751e+06, 1.4686e+06, 1.5225e+06],\n",
      "        [1.7820e+04, 1.9495e+04, 1.9477e+04, 1.7601e+04, 2.1556e+04, 2.4266e+04,\n",
      "         3.5934e+04, 2.2213e+04, 2.5117e+04, 2.2481e+04, 2.4059e+04],\n",
      "        [3.4400e+04, 3.4400e+04, 3.8600e+04, 3.8000e+04, 4.7100e+04, 3.8800e+04,\n",
      "         4.0500e+04, 4.4100e+04, 5.5500e+04, 5.7800e+04, 6.4900e+04],\n",
      "        [1.3648e+05, 1.4818e+05, 1.5529e+05, 1.6827e+05, 2.0025e+05, 2.1165e+05,\n",
      "         2.3699e+05, 2.6482e+05, 2.8370e+05, 3.0679e+05, 2.9481e+05],\n",
      "        [2.1478e+05, 1.3040e+05, 1.7644e+05, 2.2877e+05, 2.4148e+05, 2.5934e+05,\n",
      "         1.8540e+05, 3.1691e+05, 3.8041e+05, 3.9553e+05, 3.8722e+05],\n",
      "        [6.6000e+03, 8.4000e+03, 6.6000e+03, 6.3000e+03, 8.1000e+03, 4.8000e+03,\n",
      "         6.0000e+03, 7.8000e+03, 9.5000e+03, 9.2000e+03, 1.0300e+04],\n",
      "        [1.8090e+04, 1.8943e+04, 1.9444e+04, 2.0949e+04, 2.0718e+04, 1.7631e+04,\n",
      "         1.8917e+04, 1.8524e+04, 1.9807e+04, 2.1649e+04, 2.1373e+04],\n",
      "        [1.2140e+03, 1.3050e+03, 1.5890e+03, 1.8810e+03, 1.8980e+03, 2.0330e+03,\n",
      "         1.9320e+03, 1.9760e+03, 1.9920e+03, 2.2530e+03, 2.3210e+03],\n",
      "        [8.1200e+04, 8.9200e+04, 1.3590e+05, 1.3700e+05, 1.5150e+05, 1.6610e+05,\n",
      "         1.6900e+05, 1.2830e+05, 1.8800e+05, 2.0290e+05, 2.8800e+05],\n",
      "        [1.8000e+04, 2.4400e+04, 3.2300e+04, 3.2200e+04, 3.6100e+04, 3.7200e+04,\n",
      "         2.8800e+04, 2.7300e+04, 3.4700e+04, 3.7900e+04, 4.0300e+04],\n",
      "        [9.7070e+03, 8.6020e+03, 8.2550e+03, 8.3820e+03, 9.0970e+03, 9.9390e+03,\n",
      "         1.0575e+04, 1.1006e+04, 1.0000e+04, 9.0440e+03, 9.3120e+03],\n",
      "        [5.1000e+03, 5.2000e+03, 4.9000e+03, 5.1000e+03, 6.6000e+03, 6.1000e+03,\n",
      "         5.5000e+03, 8.4000e+03, 8.6000e+03, 1.0100e+04, 1.0500e+04],\n",
      "        [4.6000e+03, 6.4000e+03, 5.0000e+03, 5.2000e+03, 4.8000e+03, 4.4000e+03,\n",
      "         3.8000e+03, 5.5000e+03, 6.6000e+03, 8.0000e+03, 7.9000e+03],\n",
      "        [1.3567e+04, 1.5228e+04, 1.9262e+04, 2.0461e+04, 2.1538e+04, 2.6616e+04,\n",
      "         2.7062e+04, 3.3240e+04, 3.3960e+04, 3.2707e+04, 3.3644e+04],\n",
      "        [3.8999e+00, 3.3920e+00, 3.4238e+00, 4.0000e+00, 4.8070e+00, 4.3470e+00,\n",
      "         4.9200e+00, 4.1580e+00, 4.8804e+00, 5.3868e+00, 5.7406e+00],\n",
      "        [9.3100e+02, 9.0800e+02, 7.7600e+02, 9.2500e+02, 9.8500e+02, 9.0300e+02,\n",
      "         9.5000e+02, 1.0190e+03, 1.1460e+03, 1.1100e+03, 1.1240e+03],\n",
      "        [1.7968e+04, 1.9845e+04, 2.1880e+04, 2.4943e+04, 3.3375e+04, 2.8899e+04,\n",
      "         2.3807e+04, 2.1381e+04, 2.3582e+04, 2.7979e+04, 3.5388e+04],\n",
      "        [2.8000e+03, 3.7000e+03, 3.1000e+03, 2.9000e+03, 4.7000e+03, 4.7000e+03,\n",
      "         5.0000e+03, 4.6000e+03, 6.3000e+03, 7.3000e+03, 7.1000e+03],\n",
      "        [2.4156e+03, 2.2739e+03, 1.7090e+03, 1.5747e+03, 1.8351e+03, 1.8827e+03,\n",
      "         1.3530e+03, 9.4409e+02, 1.6024e+03, 1.8963e+03, 2.1673e+03],\n",
      "        [2.3510e+03, 2.2640e+03, 1.8900e+03, 2.0150e+03, 1.7400e+03, 1.5170e+03,\n",
      "         1.7540e+03, 1.6040e+03, 2.0440e+03, 1.9230e+03, 1.5200e+03],\n",
      "        [8.6980e-01, 8.6820e-01, 1.0280e+00, 1.0168e+00, 1.3283e+00, 1.8614e+00,\n",
      "         1.4583e+00, 1.5599e+00, 1.7151e+00, 1.6611e+00, 1.6347e+00],\n",
      "        [2.7180e+03, 2.8710e+03, 3.1010e+03, 3.2350e+03, 3.6330e+03, 3.9270e+03,\n",
      "         3.7920e+03, 3.9830e+03, 4.1070e+03, 4.1190e+03, 4.5020e+03],\n",
      "        [1.0598e+04, 7.2470e+03, 6.6170e+03, 6.3670e+03, 8.5840e+03, 9.2450e+03,\n",
      "         9.7650e+03, 1.0978e+04, 1.1615e+04, 1.3636e+04, 1.3593e+04],\n",
      "        [6.7910e+03, 9.3140e+03, 1.6313e+04, 2.0898e+04, 2.7003e+04, 3.6171e+04,\n",
      "         5.0118e+04, 4.1813e+04, 5.9019e+04, 6.1367e+04, 6.1924e+04],\n",
      "        [7.1888e+02, 6.3075e+02, 4.0219e+02, 4.3283e+02, 3.7670e+02, 3.6880e+02,\n",
      "         3.2929e+02, 3.8637e+02, 1.3872e+03, 8.7897e+02, 9.9540e+02],\n",
      "        [8.2000e+03, 8.6000e+03, 9.0000e+03, 1.3200e+04, 1.4000e+04, 1.2200e+04,\n",
      "         1.2400e+04, 1.7100e+04, 1.8700e+04, 2.0900e+04, 2.1100e+04],\n",
      "        [1.3212e+05, 1.1751e+05, 1.2319e+05, 1.3116e+05, 1.3769e+05, 1.3934e+05,\n",
      "         1.6309e+05, 1.3294e+05, 1.7077e+05, 1.8245e+05, 1.9240e+05],\n",
      "        [9.6692e+04, 9.7541e+04, 1.0865e+05, 1.3210e+05, 1.2820e+05, 1.2019e+05,\n",
      "         1.0402e+05, 1.0596e+05, 1.0931e+05, 1.2632e+05, 1.3361e+05],\n",
      "        [8.4855e+05, 8.0604e+05, 7.4169e+05, 8.1997e+05, 9.2925e+05, 9.8943e+05,\n",
      "         1.0739e+06, 1.0837e+06, 1.1904e+06, 1.1920e+06, 1.1955e+06],\n",
      "        [1.3749e+04, 1.2609e+04, 1.0738e+04, 1.0950e+04, 1.1432e+04, 1.0696e+04,\n",
      "         2.6125e+03, 2.1070e+03, 2.6186e+03, 3.0474e+03, 3.6066e+03],\n",
      "        [1.3910e+03, 1.5980e+03, 1.6710e+03, 1.6960e+03, 1.7830e+03, 1.9670e+03,\n",
      "         1.9660e+03, 2.0710e+03, 2.0130e+03, 2.1860e+03, 2.2550e+03]])\n",
      "y.shape torch.Size([32, 11])\n",
      "idxs tensor([ 29, 501, 148, 196, 369,  23, 240, 261, 345, 212, 175, 397, 235, 168,\n",
      "        106, 515, 350, 252, 236,  48, 410, 489, 200, 133, 155,  21, 238, 112,\n",
      "        103, 448,  53, 156])\n",
      "idxs.shape torch.Size([32])\n",
      "batch_len_series [27 27 27 27 16 22 27 27 16 16 30 27 27 27 20 16 32 22 27 21 16 32 27 27\n",
      " 16 16 20 16 16 27 27 27]\n",
      "y tensor([[1.6583e+04, 1.7390e+04, 1.8098e+04, 2.1711e+04, 2.1541e+04, 2.0269e+04,\n",
      "         2.0967e+04, 2.5217e+04, 2.4044e+04, 2.2822e+04, 2.4203e+04, 2.5419e+04,\n",
      "         2.7220e+04, 2.9417e+04, 3.2818e+04, 3.3618e+04],\n",
      "        [2.5250e+03, 3.1460e+03, 3.9760e+03, 4.2730e+03, 3.8420e+03, 2.8120e+03,\n",
      "         3.1380e+03, 3.5650e+03, 2.8880e+03, 3.0480e+03, 3.5590e+03, 3.1060e+03,\n",
      "         3.5740e+03, 4.5450e+03, 5.0650e+03, 5.7900e+03],\n",
      "        [1.5710e+03, 1.3400e+03, 1.5250e+03, 1.7700e+03, 1.5690e+03, 1.7890e+03,\n",
      "         1.7680e+03, 2.2300e+03, 1.9300e+03, 1.6630e+03, 1.9320e+03, 2.0690e+03,\n",
      "         1.5710e+03, 1.8780e+03, 2.1010e+03, 1.7970e+03],\n",
      "        [7.5630e+03, 9.0020e+03, 9.0790e+03, 1.0212e+04, 1.2089e+04, 1.4254e+04,\n",
      "         1.7579e+04, 2.2856e+04, 1.9550e+04, 2.0705e+04, 2.8485e+04, 3.1491e+04,\n",
      "         2.8593e+04, 3.2021e+04, 2.8171e+04, 2.8303e+04],\n",
      "        [8.8000e+03, 1.4300e+04, 1.1000e+04, 1.0000e+04, 1.1200e+04, 1.3900e+04,\n",
      "         1.5200e+04, 1.6000e+04, 1.3800e+04, 1.4800e+04, 1.3700e+04, 1.5200e+04,\n",
      "         1.5700e+04, 1.9800e+04, 2.2600e+04, 2.6700e+04],\n",
      "        [1.6657e+02, 2.0055e+02, 1.8941e+02, 1.9970e+02, 1.9782e+02, 2.2873e+02,\n",
      "         1.7235e+02, 1.8498e+02, 2.1552e+02, 2.3210e+02, 2.3954e+02, 1.6326e+02,\n",
      "         1.3184e+02, 4.9628e+02, 2.8993e+02, 3.3308e+02],\n",
      "        [4.6100e+02, 6.0900e+02, 5.2100e+02, 8.1200e+02, 8.8500e+02, 7.0900e+02,\n",
      "         9.7700e+02, 9.9200e+02, 1.0190e+03, 1.2270e+03, 1.0820e+03, 1.1670e+03,\n",
      "         8.7700e+02, 1.0720e+03, 1.4240e+03, 1.3200e+03],\n",
      "        [1.2510e+03, 1.1210e+03, 1.4910e+03, 1.4700e+03, 1.7950e+03, 1.8490e+03,\n",
      "         1.8320e+03, 2.2960e+03, 2.0610e+03, 2.3420e+03, 1.9930e+03, 2.6400e+03,\n",
      "         2.5900e+03, 2.2980e+03, 2.4920e+03, 2.9050e+03],\n",
      "        [3.1840e+05, 3.4070e+05, 3.4740e+05, 3.5340e+05, 3.7140e+05, 4.1490e+05,\n",
      "         4.0680e+05, 4.7020e+05, 4.8900e+05, 5.2760e+05, 5.9960e+05, 5.9740e+05,\n",
      "         6.6280e+05, 8.1580e+05, 8.3570e+05, 8.6470e+05],\n",
      "        [1.4800e+04, 1.9900e+04, 2.6400e+04, 3.9200e+04, 5.3200e+04, 5.4800e+04,\n",
      "         7.2400e+04, 8.1900e+04, 8.3400e+04, 9.2800e+04, 1.0940e+05, 1.3690e+05,\n",
      "         1.1420e+05, 1.8200e+05, 2.3500e+05, 2.5100e+05],\n",
      "        [1.0244e+04, 9.6970e+03, 1.1428e+04, 1.2015e+04, 1.2174e+04, 1.3250e+04,\n",
      "         1.4402e+04, 1.5657e+04, 1.7152e+04, 1.8254e+04, 1.9211e+04, 1.6551e+04,\n",
      "         1.6881e+04, 1.6587e+04, 1.8005e+04, 1.8310e+04],\n",
      "        [3.9170e+03, 4.5440e+03, 5.7960e+03, 7.1390e+03, 7.2520e+03, 8.9040e+03,\n",
      "         9.6250e+03, 9.9750e+03, 1.1171e+04, 1.2469e+04, 1.2968e+04, 1.1559e+04,\n",
      "         1.0847e+04, 1.2890e+04, 1.5146e+04, 1.8847e+04],\n",
      "        [3.4670e+03, 4.2700e+03, 4.9590e+03, 6.5440e+03, 7.2470e+03, 7.4720e+03,\n",
      "         6.8340e+03, 6.3570e+03, 6.6840e+03, 6.8100e+03, 6.8280e+03, 7.0080e+03,\n",
      "         7.1310e+03, 7.4910e+03, 7.8470e+03, 6.1750e+03],\n",
      "        [3.7580e+03, 4.2100e+03, 4.6910e+03, 6.0730e+03, 7.8520e+03, 8.2480e+03,\n",
      "         8.8640e+03, 8.0490e+03, 9.8980e+03, 9.6840e+03, 9.9640e+03, 1.0168e+04,\n",
      "         1.0617e+04, 1.1056e+04, 1.2091e+04, 1.3374e+04],\n",
      "        [1.2850e+03, 1.6040e+03, 1.4910e+03, 1.3240e+03, 1.4680e+03, 1.1860e+03,\n",
      "         1.4940e+03, 1.6220e+03, 1.6470e+03, 1.3660e+03, 1.4300e+03, 1.6590e+03,\n",
      "         1.4710e+03, 1.4140e+03, 1.5590e+03, 1.1940e+03],\n",
      "        [9.0000e+02, 3.6000e+03, 3.7000e+03, 4.1000e+03, 2.5000e+03, 1.5000e+03,\n",
      "         2.8000e+03, 2.8000e+03, 4.9000e+03, 6.4000e+03, 7.9000e+03, 7.0000e+03,\n",
      "         7.7000e+03, 1.2400e+04, 1.2600e+04, 1.6800e+04],\n",
      "        [2.7173e+05, 2.6286e+05, 2.8127e+05, 2.8968e+05, 3.0489e+05, 3.1690e+05,\n",
      "         3.2959e+05, 3.7390e+05, 4.1706e+05, 4.8809e+05, 4.4647e+05, 4.3451e+05,\n",
      "         4.2212e+05, 4.3330e+05, 4.4628e+05, 4.5608e+05],\n",
      "        [2.2632e+02, 2.2706e+02, 4.1576e+02, 4.5180e+02, 7.2840e+02, 8.9733e+02,\n",
      "         5.3544e+02, 3.8763e+02, 3.2834e+02, 2.9117e+02, 1.8696e+02, 2.4004e+02,\n",
      "         3.0160e+01, 4.3120e+01, 6.3160e+01, 7.0570e+01],\n",
      "        [3.3851e+05, 3.6364e+05, 3.5898e+05, 3.8606e+05, 4.0266e+05, 4.3586e+05,\n",
      "         4.3301e+05, 5.0189e+05, 5.2343e+05, 5.7386e+05, 6.3055e+05, 6.3247e+05,\n",
      "         7.0216e+05, 8.5593e+05, 8.7474e+05, 9.0350e+05],\n",
      "        [9.1320e+03, 1.1546e+04, 1.3298e+04, 1.3188e+04, 1.3020e+04, 1.5277e+04,\n",
      "         1.5424e+04, 1.4280e+04, 1.4929e+04, 1.7057e+04, 1.5798e+04, 4.3986e+04,\n",
      "         3.6467e+04, 5.0038e+04, 4.6986e+04, 4.9532e+04],\n",
      "        [5.6000e+03, 7.4000e+03, 9.2000e+03, 9.8000e+03, 1.1000e+04, 1.2500e+04,\n",
      "         1.5400e+04, 1.7700e+04, 1.5300e+04, 1.6100e+04, 1.8500e+04, 1.8300e+04,\n",
      "         1.3100e+04, 1.6400e+04, 2.0400e+04, 2.2900e+04],\n",
      "        [2.4666e+04, 3.3560e+04, 4.6490e+04, 6.6842e+04, 8.1357e+04, 8.8909e+04,\n",
      "         6.8585e+04, 4.9149e+04, 6.1899e+04, 7.2752e+04, 7.9929e+04, 8.2678e+04,\n",
      "         7.3159e+04, 7.9824e+04, 7.6935e+04, 7.3969e+04],\n",
      "        [1.0934e+05, 1.0389e+05, 1.1864e+05, 1.2993e+05, 1.2177e+05, 1.1504e+05,\n",
      "         1.1037e+05, 1.2318e+05, 1.4305e+05, 1.5268e+05, 1.5018e+05, 1.6709e+05,\n",
      "         1.6679e+05, 1.6816e+05, 1.6252e+05, 1.7531e+05],\n",
      "        [5.6580e+03, 7.2040e+03, 9.7200e+03, 1.1878e+04, 1.2934e+04, 1.2820e+04,\n",
      "         1.2027e+04, 6.5810e+03, 6.2070e+03, 9.0470e+03, 9.6540e+03, 8.2480e+03,\n",
      "         8.5570e+03, 8.0050e+03, 7.2130e+03, 6.9290e+03],\n",
      "        [9.2000e+03, 6.3000e+03, 1.2900e+04, 1.5700e+04, 2.4200e+04, 2.6600e+04,\n",
      "         3.0000e+04, 2.9200e+04, 3.2400e+04, 3.5000e+04, 3.2800e+04, 4.1400e+04,\n",
      "         4.4400e+04, 4.6400e+04, 4.8800e+04, 5.7300e+04],\n",
      "        [4.0000e+03, 3.1000e+03, 4.3000e+03, 4.3000e+03, 5.1000e+03, 5.8000e+03,\n",
      "         6.0000e+03, 7.7000e+03, 8.1000e+03, 8.8000e+03, 8.9000e+03, 8.0000e+03,\n",
      "         1.0500e+04, 1.1100e+04, 1.4200e+04, 2.0200e+04],\n",
      "        [5.2217e+06, 5.3281e+06, 5.7862e+06, 6.9394e+06, 7.1688e+06, 7.1528e+06,\n",
      "         7.7136e+06, 8.5048e+06, 9.7996e+06, 8.5758e+06, 8.4929e+06, 7.7736e+06,\n",
      "         8.9057e+06, 9.1479e+06, 9.2721e+06, 9.6554e+06],\n",
      "        [8.7000e+03, 1.0000e+04, 8.4000e+03, 1.0400e+04, 9.5000e+03, 1.0900e+04,\n",
      "         9.8000e+03, 1.1700e+04, 1.2300e+04, 1.3100e+04, 1.1600e+04, 1.3200e+04,\n",
      "         1.3900e+04, 1.6500e+04, 1.6000e+04, 1.9000e+04],\n",
      "        [1.7500e+04, 1.5400e+04, 1.6300e+04, 1.4700e+04, 1.3300e+04, 1.4000e+04,\n",
      "         1.4300e+04, 1.2400e+04, 1.1900e+04, 1.7600e+04, 2.0000e+04, 1.7300e+04,\n",
      "         1.5300e+04, 1.5600e+04, 1.5400e+04, 1.3900e+04],\n",
      "        [1.4950e+03, 1.8100e+03, 1.6650e+03, 2.3730e+03, 3.2020e+03, 4.2690e+03,\n",
      "         4.7640e+03, 5.3150e+03, 6.6020e+03, 8.3270e+03, 1.2665e+04, 1.7270e+04,\n",
      "         1.4790e+04, 1.5694e+04, 1.7761e+04, 2.0265e+04],\n",
      "        [2.8420e+03, 2.2160e+03, 2.8170e+03, 4.9690e+03, 8.6930e+03, 1.0829e+04,\n",
      "         1.2211e+04, 1.3442e+04, 1.4699e+04, 1.7213e+04, 1.8402e+04, 2.0004e+04,\n",
      "         2.1170e+04, 2.6798e+04, 3.0835e+04, 3.7686e+04],\n",
      "        [1.1527e+04, 1.0959e+04, 1.2417e+04, 1.2754e+04, 1.2963e+04, 2.0566e+04,\n",
      "         1.9845e+04, 2.3742e+04, 2.5827e+04, 2.2331e+04, 2.7000e+04, 2.7226e+04,\n",
      "         2.3173e+04, 2.4404e+04, 2.0744e+04, 2.0543e+04]])\n",
      "y.shape torch.Size([32, 16])\n",
      "idxs tensor([423, 390, 372, 150, 242,  24, 395, 388, 170, 216, 211, 274, 298, 352,\n",
      "        457, 206,  77,  30, 258, 134, 202,  73, 281, 341, 241, 173, 486, 185,\n",
      "        169, 347, 116, 436])\n",
      "idxs.shape torch.Size([32])\n",
      "batch_len_series [27 27 27 21 22 27 27 27 27 11 27 16 27 22 21 27 27 22 27 35 27 21 27 20\n",
      " 27 27 16 27 16 27 27 11]\n",
      "y tensor([[2.7596e+04, 3.2117e+04, 4.6125e+04, 5.3529e+04, 7.7834e+04, 8.4170e+04,\n",
      "         9.8623e+04, 8.7415e+04, 8.7236e+04, 6.1952e+04, 6.2252e+04],\n",
      "        [7.9604e+04, 8.8226e+04, 8.6471e+04, 1.0026e+05, 1.2006e+05, 1.3218e+05,\n",
      "         1.5567e+05, 1.7336e+05, 1.8209e+05, 2.0068e+05, 1.8298e+05],\n",
      "        [6.0500e+03, 5.6260e+03, 5.7740e+03, 5.6650e+03, 6.3910e+03, 5.6690e+03,\n",
      "         6.4520e+03, 7.1060e+03, 6.9870e+03, 7.5220e+03, 6.3760e+03],\n",
      "        [1.5455e+05, 1.3913e+05, 1.6615e+05, 3.1426e+05, 3.1300e+05, 2.7871e+05,\n",
      "         2.7711e+05, 2.5327e+05, 2.2003e+05, 2.2004e+05, 2.3383e+05],\n",
      "        [6.0680e+01, 3.6360e+01, 1.3100e+01, 2.1690e+01, 1.7780e+01, 1.7040e+01,\n",
      "         1.4270e+01, 8.2600e+00, 4.3120e+01, 2.1970e+01, 2.3490e+01],\n",
      "        [9.3630e+03, 9.6180e+03, 1.0440e+04, 1.0661e+04, 1.1718e+04, 1.5149e+04,\n",
      "         1.3448e+04, 1.2809e+04, 1.3751e+04, 1.3291e+04, 1.4948e+04],\n",
      "        [4.0698e+04, 3.7091e+04, 3.5737e+04, 3.6457e+04, 4.0561e+04, 4.1927e+04,\n",
      "         3.7622e+04, 4.0107e+04, 4.1351e+04, 4.1764e+04, 4.2412e+04],\n",
      "        [1.1010e+03, 1.0740e+03, 1.1720e+03, 1.2190e+03, 1.5480e+03, 1.2100e+03,\n",
      "         1.3180e+03, 1.3470e+03, 1.1610e+03, 1.5910e+03, 1.7150e+03],\n",
      "        [5.2530e+03, 3.5910e+03, 3.6430e+03, 3.7800e+03, 3.2310e+03, 3.5360e+03,\n",
      "         4.9170e+03, 3.9520e+03, 4.6750e+03, 5.4010e+03, 5.9390e+03],\n",
      "        [6.1693e+06, 6.0639e+06, 6.0874e+06, 5.6175e+06, 5.2056e+06, 4.3996e+06,\n",
      "         4.5135e+06, 3.9958e+06, 4.7585e+06, 5.1054e+06, 5.0585e+06],\n",
      "        [3.9494e+05, 3.6366e+05, 2.5684e+05, 2.8456e+05, 3.2279e+05, 3.5654e+05,\n",
      "         4.2774e+05, 3.8126e+05, 3.0064e+05, 2.5504e+05, 2.6892e+05],\n",
      "        [3.9000e+03, 3.3000e+03, 3.7000e+03, 5.4000e+03, 5.3000e+03, 5.5000e+03,\n",
      "         4.5000e+03, 7.7000e+03, 7.8000e+03, 7.4000e+03, 1.0200e+04],\n",
      "        [1.9398e+04, 1.9324e+04, 2.1178e+04, 2.2386e+04, 2.4665e+04, 2.4190e+04,\n",
      "         2.6702e+04, 2.8104e+04, 2.8742e+04, 2.9090e+04, 2.9772e+04],\n",
      "        [1.5209e+04, 1.5579e+04, 1.4253e+04, 1.3477e+04, 1.8288e+04, 2.2994e+04,\n",
      "         2.6776e+04, 3.4258e+04, 3.3941e+04, 3.6570e+04, 3.9679e+04],\n",
      "        [9.6137e+04, 7.6376e+04, 7.8519e+04, 8.8534e+04, 9.7115e+04, 9.6381e+04,\n",
      "         1.1733e+05, 9.5638e+04, 1.1732e+05, 1.1361e+05, 1.3776e+05],\n",
      "        [8.6217e+04, 8.0966e+04, 7.7991e+04, 7.7675e+04, 8.3219e+04, 8.1715e+04,\n",
      "         6.8266e+04, 6.2233e+04, 5.9827e+04, 7.7993e+04, 6.9365e+04],\n",
      "        [9.8169e+04, 9.5480e+04, 3.8385e+04, 3.8221e+04, 4.4129e+04, 4.2461e+04,\n",
      "         3.5187e+04, 3.9491e+04, 3.4145e+04, 3.4754e+04, 3.5014e+04],\n",
      "        [1.8222e+03, 1.4857e+03, 7.3220e+02, 9.5632e+02, 8.8260e+02, 6.3155e+02,\n",
      "         6.2126e+02, 4.5643e+02, 6.1397e+02, 6.1498e+02, 6.3171e+02],\n",
      "        [2.8937e+04, 2.9682e+04, 3.1016e+04, 3.3296e+04, 3.2971e+04, 3.6694e+04,\n",
      "         3.9669e+04, 3.9940e+04, 4.0602e+04, 4.2182e+04, 4.5955e+04],\n",
      "        [8.0054e+05, 7.7331e+05, 8.0270e+05, 9.6601e+05, 9.3572e+05, 1.0008e+06,\n",
      "         6.8379e+05, 1.0517e+06, 1.1431e+06, 1.1590e+06, 1.2309e+06],\n",
      "        [1.5939e+04, 1.6344e+04, 1.7217e+04, 1.8336e+04, 2.1089e+04, 2.0374e+04,\n",
      "         2.2152e+04, 2.4043e+04, 2.7829e+04, 2.8225e+04, 2.9119e+04],\n",
      "        [2.8333e+05, 3.0386e+05, 2.9444e+05, 3.1123e+05, 3.0219e+05, 3.3933e+05,\n",
      "         3.3191e+05, 3.1428e+05, 2.6852e+05, 3.1021e+05, 3.6746e+05],\n",
      "        [2.5410e+03, 2.5900e+03, 2.4720e+03, 3.6860e+03, 5.6500e+03, 7.3360e+03,\n",
      "         1.0078e+04, 1.0267e+04, 1.1889e+04, 1.1691e+04, 1.2565e+04],\n",
      "        [1.8370e+03, 1.6290e+03, 1.6030e+03, 1.5870e+03, 1.5560e+03, 1.9890e+03,\n",
      "         1.9020e+03, 1.7850e+03, 1.7910e+03, 1.5310e+03, 1.4900e+03],\n",
      "        [2.1478e+04, 2.2426e+04, 2.5363e+04, 2.3417e+04, 2.4407e+04, 2.5439e+04,\n",
      "         2.5669e+04, 2.9280e+04, 2.8940e+04, 3.0784e+04, 3.3302e+04],\n",
      "        [9.6300e+02, 6.7500e+02, 7.4500e+02, 9.1800e+02, 8.8500e+02, 7.7700e+02,\n",
      "         1.2050e+03, 9.5400e+02, 1.3620e+03, 1.3150e+03, 1.4500e+03],\n",
      "        [1.7200e+04, 1.8900e+04, 1.8100e+04, 2.0800e+04, 2.1900e+04, 1.8400e+04,\n",
      "         2.1000e+04, 2.3700e+04, 3.3000e+04, 2.7300e+04, 2.0600e+04],\n",
      "        [5.9458e+04, 6.3778e+04, 6.6569e+04, 6.9003e+04, 7.4277e+04, 6.9307e+04,\n",
      "         5.8831e+04, 6.2482e+04, 6.5442e+04, 7.2064e+04, 7.6681e+04],\n",
      "        [2.8920e+05, 3.2240e+05, 3.2240e+05, 3.1270e+05, 3.3880e+05, 3.0080e+05,\n",
      "         3.1840e+05, 3.1290e+05, 3.7510e+05, 4.0440e+05, 4.1280e+05],\n",
      "        [4.5446e+04, 4.2519e+04, 4.6838e+04, 4.9922e+04, 5.1058e+04, 6.0058e+04,\n",
      "         5.9553e+04, 5.5672e+04, 5.6756e+04, 5.6693e+04, 5.7525e+04],\n",
      "        [4.7930e+03, 5.8280e+03, 8.4220e+03, 8.3020e+03, 9.9770e+03, 1.2104e+04,\n",
      "         1.5449e+04, 1.1807e+04, 1.3664e+04, 9.9920e+03, 1.1565e+04],\n",
      "        [4.0930e+06, 3.8940e+06, 4.3126e+06, 4.6507e+06, 4.5878e+06, 4.2803e+06,\n",
      "         4.6319e+06, 5.2676e+06, 5.6973e+06, 5.6712e+06, 5.3090e+06]])\n",
      "y.shape torch.Size([32, 11])\n",
      "idxs tensor([158, 381, 288, 477,  25, 443, 386, 327, 295, 445, 318, 174, 374,  59,\n",
      "        123,  87,  91,  38, 269, 222, 415, 476, 309, 460, 284, 332, 201, 124,\n",
      "        186, 441, 159, 498])\n",
      "idxs.shape torch.Size([32])\n",
      "batch_len_series [16 16 27 16 27 27 27 29 22 27 27 16 27 16 22 16 35 27 27 21 27 27 27 32\n",
      " 27 26 27 27 30 16 27 27]\n",
      "y tensor([[3.3700e+04, 4.3900e+04, 4.7200e+04, 5.1700e+04, 5.4300e+04, 6.0200e+04,\n",
      "         6.7700e+04, 7.3400e+04, 7.5700e+04, 8.3600e+04, 8.2000e+04, 8.2900e+04,\n",
      "         8.7700e+04, 1.2400e+05, 1.3590e+05, 1.3700e+05],\n",
      "        [5.2230e+05, 5.4660e+05, 5.4120e+05, 5.5140e+05, 5.5620e+05, 6.0280e+05,\n",
      "         6.1520e+05, 7.1350e+05, 7.5240e+05, 7.6340e+05, 8.4910e+05, 8.6590e+05,\n",
      "         9.5970e+05, 1.1511e+06, 1.2007e+06, 1.2496e+06],\n",
      "        [5.6260e+03, 5.1640e+03, 6.0200e+03, 6.5840e+03, 6.1920e+03, 6.6160e+03,\n",
      "         7.1790e+03, 1.1413e+04, 1.2292e+04, 1.3810e+04, 1.8907e+04, 2.2178e+04,\n",
      "         2.1593e+04, 2.2838e+04, 1.4188e+04, 1.6098e+04],\n",
      "        [2.9000e+04, 3.2300e+04, 3.1700e+04, 3.9700e+04, 4.0500e+04, 4.4200e+04,\n",
      "         5.0500e+04, 4.8600e+04, 5.6000e+04, 7.3800e+04, 7.1000e+04, 6.8800e+04,\n",
      "         6.6700e+04, 7.9300e+04, 8.8300e+04, 9.0400e+04],\n",
      "        [1.5730e+03, 1.9350e+03, 2.0610e+03, 2.4330e+03, 1.8670e+03, 2.2400e+03,\n",
      "         2.5520e+03, 2.8360e+03, 3.7290e+03, 6.2050e+03, 4.6690e+03, 5.4070e+03,\n",
      "         5.2730e+03, 5.0790e+03, 5.3400e+03, 6.0030e+03],\n",
      "        [9.2980e+03, 8.5420e+03, 8.0950e+03, 1.0966e+04, 1.1295e+04, 1.1242e+04,\n",
      "         1.1272e+04, 1.3929e+04, 1.3025e+04, 1.2079e+04, 1.2215e+04, 1.2344e+04,\n",
      "         1.3972e+04, 1.5126e+04, 1.6992e+04, 1.6364e+04],\n",
      "        [2.7536e+04, 2.7801e+04, 2.3136e+04, 2.5425e+04, 3.0495e+04, 4.7113e+04,\n",
      "         4.8524e+04, 4.9876e+04, 4.9114e+04, 4.2886e+04, 5.1854e+04, 5.3137e+04,\n",
      "         5.3507e+04, 4.5702e+04, 3.5295e+04, 3.7178e+04],\n",
      "        [1.1471e+00, 1.0737e+00, 9.4110e-01, 1.0205e+00, 9.4420e-01, 9.1210e-01,\n",
      "         8.7470e-01, 9.5420e-01, 1.0400e+00, 9.7580e-01, 1.0015e+00, 9.9830e-01,\n",
      "         1.1195e+00, 1.0472e+00, 1.0505e+00, 1.0425e+00],\n",
      "        [9.7280e+01, 9.0720e+01, 1.2283e+02, 1.2560e+02, 1.0940e+02, 8.7720e+01,\n",
      "         1.1024e+02, 9.7620e+01, 9.6080e+01, 5.6890e+01, 1.5890e+02, 6.2760e+01,\n",
      "         1.3060e+01, 3.3290e+01, 3.2330e+01, 2.6560e+01],\n",
      "        [1.8371e+05, 1.9212e+05, 2.1622e+05, 2.5778e+05, 2.7426e+05, 3.0022e+05,\n",
      "         3.2867e+05, 3.5083e+05, 3.9300e+05, 4.6329e+05, 4.0831e+05, 3.9848e+05,\n",
      "         3.9828e+05, 4.2405e+05, 4.5151e+05, 4.6284e+05],\n",
      "        [2.6162e+04, 2.3659e+04, 2.3761e+04, 3.0525e+04, 3.5176e+04, 3.3295e+04,\n",
      "         3.6959e+04, 3.1291e+04, 3.1058e+04, 3.4746e+04, 3.3498e+04, 3.5914e+04,\n",
      "         3.8726e+04, 3.7975e+04, 4.0302e+04, 4.3834e+04],\n",
      "        [9.0100e+04, 8.6700e+04, 7.8400e+04, 8.3000e+04, 7.4500e+04, 7.1900e+04,\n",
      "         7.5900e+04, 9.9100e+04, 1.1530e+05, 7.5100e+04, 9.4200e+04, 1.2820e+05,\n",
      "         1.4510e+05, 1.7520e+05, 1.9690e+05, 2.0240e+05],\n",
      "        [8.4900e+02, 1.0500e+03, 1.8120e+03, 1.6050e+03, 1.7130e+03, 1.7010e+03,\n",
      "         1.8750e+03, 2.0000e+03, 2.2500e+03, 2.5900e+03, 3.1870e+03, 3.0380e+03,\n",
      "         2.8240e+03, 2.9140e+03, 4.0000e+03, 6.0350e+03],\n",
      "        [4.1000e+03, 5.6000e+03, 4.5000e+03, 3.8000e+03, 3.8000e+03, 4.8000e+03,\n",
      "         4.0000e+03, 4.6000e+03, 8.1000e+03, 1.2100e+04, 1.0000e+04, 8.0000e+03,\n",
      "         8.3000e+03, 1.0100e+04, 9.5000e+03, 8.9000e+03],\n",
      "        [5.5866e+02, 5.3337e+02, 3.8680e+02, 4.8381e+02, 4.5075e+02, 5.1096e+02,\n",
      "         5.9204e+02, 4.7031e+02, 3.6692e+02, 4.7051e+02, 3.7507e+02, 3.6340e+02,\n",
      "         3.1950e+02, 5.6951e+02, 5.9531e+02, 6.1616e+02],\n",
      "        [4.4000e+03, 6.1000e+03, 5.5000e+03, 5.8000e+03, 6.2000e+03, 7.1000e+03,\n",
      "         8.1000e+03, 7.4000e+03, 9.6000e+03, 1.2400e+04, 9.9000e+03, 9.9000e+03,\n",
      "         9.7000e+03, 1.4200e+04, 1.7700e+04, 1.5000e+04],\n",
      "        [1.6400e+06, 1.7773e+06, 1.6653e+06, 1.7611e+06, 1.8213e+06, 1.7826e+06,\n",
      "         1.8126e+06, 2.0002e+06, 2.3857e+06, 2.4188e+06, 2.4288e+06, 1.8524e+06,\n",
      "         2.0748e+06, 2.1306e+06, 2.1772e+06, 2.2387e+06],\n",
      "        [7.6760e+03, 8.0070e+03, 9.7530e+03, 1.3031e+04, 1.5201e+04, 1.7137e+04,\n",
      "         1.8878e+04, 1.6529e+04, 1.7674e+04, 2.0150e+04, 1.9732e+04, 1.7093e+04,\n",
      "         1.5362e+04, 1.7771e+04, 2.0326e+04, 2.1531e+04],\n",
      "        [1.1492e+05, 1.2606e+05, 1.3629e+05, 1.4048e+05, 1.3401e+05, 1.4631e+05,\n",
      "         1.6738e+05, 1.9334e+05, 2.4888e+05, 2.7742e+05, 2.9541e+05, 3.0884e+05,\n",
      "         3.3310e+05, 3.3273e+05, 3.7382e+05, 3.9072e+05],\n",
      "        [3.8470e+05, 3.4361e+05, 3.1668e+05, 3.0903e+05, 3.5185e+05, 3.6782e+05,\n",
      "         4.4516e+05, 5.2119e+05, 5.7694e+05, 4.8588e+05, 5.2812e+05, 5.5998e+05,\n",
      "         5.7110e+05, 6.9120e+05, 7.2388e+05, 6.1508e+05],\n",
      "        [9.9618e+04, 1.1238e+05, 1.2010e+05, 1.3571e+05, 1.5133e+05, 1.6375e+05,\n",
      "         1.7018e+05, 1.7955e+05, 1.9164e+05, 2.0064e+05, 2.0762e+05, 2.1395e+05,\n",
      "         2.1756e+05, 2.4882e+05, 2.5917e+05, 2.7009e+05],\n",
      "        [5.3490e+03, 5.8410e+03, 6.6200e+03, 7.8410e+03, 9.9120e+03, 1.0480e+04,\n",
      "         1.1351e+04, 1.1538e+04, 1.2548e+04, 1.4596e+04, 1.2537e+04, 1.1167e+04,\n",
      "         1.1756e+04, 1.3731e+04, 1.5560e+04, 1.6437e+04],\n",
      "        [9.6750e+03, 1.0940e+04, 1.2537e+04, 1.6573e+04, 1.9513e+04, 1.8951e+04,\n",
      "         2.1095e+04, 1.9253e+04, 1.9202e+04, 2.0932e+04, 1.8842e+04, 1.5949e+04,\n",
      "         1.7347e+04, 1.8393e+04, 2.0089e+04, 1.9777e+04],\n",
      "        [6.2753e+04, 7.4745e+04, 9.1993e+04, 1.0954e+05, 1.3171e+05, 1.5320e+05,\n",
      "         1.5175e+05, 1.4344e+05, 1.3956e+05, 1.5410e+05, 1.5415e+05, 1.5093e+05,\n",
      "         1.2924e+05, 1.3718e+05, 1.5955e+05, 1.5480e+05],\n",
      "        [1.1808e+05, 1.3026e+05, 1.3098e+05, 1.4026e+05, 1.3900e+05, 1.4748e+05,\n",
      "         1.4099e+05, 1.7478e+05, 1.7862e+05, 2.0843e+05, 2.4213e+05, 2.4294e+05,\n",
      "         2.7410e+05, 3.4071e+05, 3.3898e+05, 3.5065e+05],\n",
      "        [1.0120e+03, 1.0130e+03, 1.1780e+03, 1.1670e+03, 1.1860e+03, 1.0630e+03,\n",
      "         9.4800e+02, 1.0290e+03, 1.0450e+03, 9.9300e+02, 1.0220e+03, 1.0060e+03,\n",
      "         1.0250e+03, 1.0540e+03, 1.0960e+03, 1.1100e+03],\n",
      "        [1.3212e+04, 1.4205e+04, 1.8536e+04, 1.7609e+04, 1.5108e+04, 1.6247e+04,\n",
      "         1.9034e+04, 1.7611e+04, 2.4119e+04, 2.5219e+04, 2.2180e+04, 2.2335e+04,\n",
      "         2.1891e+04, 2.0870e+04, 1.6824e+04, 1.5405e+04],\n",
      "        [1.5914e+04, 1.7967e+04, 1.9276e+04, 2.0085e+04, 2.2780e+04, 2.5871e+04,\n",
      "         3.2435e+04, 5.9211e+04, 5.2655e+04, 7.8212e+04, 7.2269e+04, 8.4555e+04,\n",
      "         7.6027e+04, 8.2173e+04, 5.4586e+04, 5.6664e+04],\n",
      "        [1.2980e+05, 1.4600e+05, 1.4946e+05, 1.5878e+05, 1.6806e+05, 2.1154e+05,\n",
      "         3.4408e+05, 1.0800e+06, 1.1170e+06, 1.3282e+06, 1.2848e+06, 1.6252e+06,\n",
      "         1.7624e+06, 1.6501e+06, 1.7762e+06, 1.8203e+06],\n",
      "        [3.2710e+05, 3.5820e+05, 3.5590e+05, 3.7350e+05, 3.9350e+05, 4.2250e+05,\n",
      "         4.6420e+05, 4.8320e+05, 4.8020e+05, 5.2470e+05, 4.7530e+05, 5.0190e+05,\n",
      "         4.9260e+05, 5.9080e+05, 6.3730e+05, 6.7460e+05],\n",
      "        [1.3269e+05, 1.3136e+05, 1.4360e+05, 1.5849e+05, 1.5220e+05, 1.4739e+05,\n",
      "         1.4357e+05, 1.6234e+05, 1.8088e+05, 1.9578e+05, 1.8738e+05, 2.0529e+05,\n",
      "         2.1162e+05, 2.1834e+05, 2.1451e+05, 2.2563e+05],\n",
      "        [1.0217e+05, 1.0856e+05, 1.2399e+05, 1.4263e+05, 1.4665e+05, 1.5861e+05,\n",
      "         1.7057e+05, 1.6784e+05, 2.0644e+05, 2.3410e+05, 2.0899e+05, 1.9549e+05,\n",
      "         2.0317e+05, 2.1137e+05, 2.5128e+05, 2.5781e+05]])\n",
      "y.shape torch.Size([32, 16])\n",
      "idxs tensor([205, 177, 161, 233, 251, 384, 446, 493,  14,  82, 127, 166, 273, 165,\n",
      "         16, 198,  34, 128,  99, 475, 450, 141, 121,  68, 253, 322, 319, 162,\n",
      "          9, 188, 263, 102])\n",
      "idxs.shape torch.Size([32])\n",
      "batch_len_series [35 27 27 27 32 27]\n",
      "y tensor([[1.0137e+05, 1.2390e+05, 1.4945e+05, 1.6867e+05, 2.0098e+05, 1.9298e+05,\n",
      "         2.0567e+05, 1.8745e+05, 1.6561e+05, 2.0087e+05, 2.2939e+05, 2.8323e+05,\n",
      "         2.8972e+05, 2.7058e+05, 2.7951e+05, 3.4977e+05, 3.3969e+05, 3.3161e+05,\n",
      "         3.5118e+05, 4.5057e+05, 3.6058e+05, 3.7996e+05, 2.8132e+05, 4.1129e+05,\n",
      "         4.6460e+05, 5.1650e+05, 6.0117e+05],\n",
      "        [2.4780e+03, 2.7870e+03, 2.7880e+03, 4.2920e+03, 5.7880e+03, 6.1630e+03,\n",
      "         9.4150e+03, 1.2313e+04, 1.4442e+04, 1.5884e+04, 1.8474e+04, 2.5297e+04,\n",
      "         4.9090e+04, 8.7307e+04, 1.1792e+05, 1.2333e+05, 1.2388e+05, 1.1470e+05,\n",
      "         1.0984e+05, 1.0912e+05, 9.4830e+04, 7.6263e+04, 5.8472e+04, 5.8258e+04,\n",
      "         6.6445e+04, 8.2692e+04, 6.6745e+04],\n",
      "        [3.5680e+03, 4.3280e+03, 4.8000e+03, 6.0190e+03, 7.4200e+03, 8.5720e+03,\n",
      "         1.4181e+04, 1.4932e+04, 1.4679e+04, 1.4808e+04, 1.5207e+04, 1.7342e+04,\n",
      "         1.9044e+04, 2.3883e+04, 2.3784e+04, 2.1332e+04, 2.4185e+04, 2.7527e+04,\n",
      "         2.6743e+04, 3.3903e+04, 3.5725e+04, 3.2808e+04, 3.4019e+04, 3.2603e+04,\n",
      "         3.2920e+04, 2.9735e+04, 2.8168e+04],\n",
      "        [7.3840e+03, 7.6920e+03, 8.6560e+03, 9.6000e+03, 9.8760e+03, 1.1124e+04,\n",
      "         1.0898e+04, 1.2147e+04, 1.4804e+04, 1.3039e+04, 1.5895e+04, 1.3059e+04,\n",
      "         1.4235e+04, 1.5054e+04, 1.8461e+04, 1.9241e+04, 1.9188e+04, 1.8929e+04,\n",
      "         2.0478e+04, 1.9647e+04, 2.0632e+04, 1.9360e+04, 2.0988e+04, 2.0710e+04,\n",
      "         2.1927e+04, 2.5457e+04, 2.4988e+04],\n",
      "        [1.2679e+04, 1.5072e+04, 1.5596e+04, 1.3301e+04, 1.4230e+04, 1.5293e+04,\n",
      "         1.7744e+04, 2.1475e+04, 2.9572e+04, 2.9015e+04, 3.4345e+04, 3.6934e+04,\n",
      "         4.5863e+04, 7.1542e+04, 1.0569e+05, 1.3498e+05, 1.5447e+05, 1.6036e+05,\n",
      "         9.3042e+04, 9.0981e+04, 9.8107e+04, 9.7878e+04, 8.9419e+04, 9.0253e+04,\n",
      "         8.4354e+04, 8.3499e+04, 8.3538e+04],\n",
      "        [4.6000e+02, 4.2000e+02, 6.2400e+02, 6.8000e+02, 7.7200e+02, 8.3600e+02,\n",
      "         1.0290e+03, 1.3270e+03, 1.8160e+03, 1.4380e+03, 1.7060e+03, 1.9940e+03,\n",
      "         1.9650e+03, 1.9220e+03, 2.2690e+03, 2.3850e+03, 2.7740e+03, 2.9030e+03,\n",
      "         3.2580e+03, 3.2590e+03, 4.0240e+03, 3.6300e+03, 3.7530e+03, 4.3830e+03,\n",
      "         4.4590e+03, 4.6660e+03, 5.2270e+03]])\n",
      "y.shape torch.Size([6, 27])\n",
      "idxs tensor([474,  92, 324, 421,  71, 371])\n",
      "idxs.shape torch.Size([6])\n",
      "Epoch: 4, Time: 1.818, Insample loss: 4.86516\n"
     ]
    }
   ],
   "source": [
    "esrnn.fit(train_ts_loader=train_loader, eval_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = esrnn.predict(ts_loader=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_plot = Y_df[Y_df['unique_id']=='Y1']['y']\n",
    "y_hat_plot = y_hat[y_hat['unique_id']=='Y1']['y_hat']\n",
    "plt.plot(range(len(y_plot)), y_plot)\n",
    "plt.plot(range(len(y_plot), len(y_plot)+len(y_hat_plot)), y_hat_plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "52b0028e7074a0058398558ea661882eddbb489e66a28504cc0449d2f54d6290"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}