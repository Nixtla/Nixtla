{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.esrnn.esrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESRNN model\n",
    "\n",
    "> API details."
   ]
  },
  {
   "source": [
    "#TODO: validation loader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "from nixtla.models.esrnn.utils.esrnn import _ESRNN\n",
    "from nixtla.models.esrnn.utils.losses import SmylLoss, PinballLoss\n",
    "from nixtla.models.esrnn.utils.data import Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ESRNN(object):\n",
    "    \"\"\" Exponential Smoothing Recurrent Neural Network\n",
    "\n",
    "    Pytorch Implementation of the M4 time series forecasting competition winner.\n",
    "    Proposed by Smyl. The model uses a hybrid approach of Machine Learning and\n",
    "    statistical methods by combining recurrent neural networks to model a common\n",
    "    trend with shared parameters across series, and multiplicative Holt-Winter\n",
    "    exponential smoothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_epochs: int\n",
    "        maximum number of complete passes to train data during fit\n",
    "    freq_of_test: int\n",
    "        period for the diagnostic evaluation of the model.\n",
    "    learning_rate: float\n",
    "        size of the stochastic gradient descent steps\n",
    "    lr_scheduler_step_size: int\n",
    "        this step_size is the period for each learning rate decay\n",
    "    per_series_lr_multip: float\n",
    "        multiplier for per-series parameters smoothing and initial\n",
    "        seasonalities learning rate (default 1.0)\n",
    "    gradient_eps: float\n",
    "        term added to the Adam optimizer denominator to improve\n",
    "        numerical stability (default: 1e-8)\n",
    "    gradient_clipping_threshold: float\n",
    "        max norm of gradient vector, with all parameters treated\n",
    "        as a single vector\n",
    "    rnn_weight_decay: float\n",
    "        parameter to control classic L2/Tikhonov regularization\n",
    "        of the rnn parameters\n",
    "    noise_std: float\n",
    "        standard deviation of white noise added to input during\n",
    "        fit to avoid the model from memorizing the train data\n",
    "    level_variability_penalty: float\n",
    "        this parameter controls the strength of the penalization\n",
    "        to the wigglines of the level vector, induces smoothness\n",
    "        in the output\n",
    "    testing_percentile: float\n",
    "        This value is only for diagnostic evaluation.\n",
    "        In case of percentile predictions this parameter controls\n",
    "        for the value predicted, when forecasting point value,\n",
    "        the forecast is the median, so percentile=50.\n",
    "    training_percentile: float\n",
    "        To reduce the model's tendency to over estimate, the\n",
    "        training_percentile can be set to fit a smaller value\n",
    "        through the Pinball Loss.\n",
    "    seasonality: int list\n",
    "        list of seasonalities of the time series\n",
    "        Hourly [24, 168], Daily [7], Weekly [52], Monthly [12],\n",
    "        Quarterly [4], Yearly [].\n",
    "    input_size: int\n",
    "        input size of the recurrent neural network, usually a\n",
    "        multiple of seasonality\n",
    "    output_size: int\n",
    "        output_size or forecast horizon of the recurrent neural\n",
    "        network, usually multiple of seasonality\n",
    "    random_seed: int\n",
    "        random_seed for pseudo random pytorch initializer and\n",
    "        numpy random generator\n",
    "    min_inp_seq_length: int\n",
    "        description\n",
    "    cell_type: str\n",
    "        Type of RNN cell, available GRU, LSTM, RNN, ResidualLSTM.\n",
    "    state_hsize: int\n",
    "        dimension of hidden state of the recurrent neural network\n",
    "    dilations: int list\n",
    "        each list represents one chunk of Dilated LSTMS, connected in\n",
    "        standard ResNet fashion\n",
    "    add_nl_layer: bool\n",
    "        whether to insert a tanh() layer between the RNN stack and the\n",
    "        linear adaptor (output) layers\n",
    "    device: str\n",
    "        pytorch device either 'cpu' or 'cuda'\n",
    "    Notes\n",
    "    -----\n",
    "    **References:**\n",
    "    `M4 Competition Conclusions\n",
    "    <https://rpubs.com/fotpetr/m4competition>`__\n",
    "    `Original Dynet Implementation of ESRNN\n",
    "    <https://github.com/M4Competition/M4-methods/tree/master/118%20-%20slaweks17>`__\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_size=4,\n",
    "                 output_size=8,\n",
    "                 max_epochs=15,\n",
    "                 freq_of_test=-1,\n",
    "                 learning_rate=1e-3,\n",
    "                 lr_scheduler_step_size=9,\n",
    "                 lr_decay=0.9,\n",
    "                 per_series_lr_multip=1.0,\n",
    "                 gradient_eps=1e-8,\n",
    "                 gradient_clipping_threshold=20,\n",
    "                 rnn_weight_decay=0,\n",
    "                 noise_std=0.001,\n",
    "                 level_variability_penalty=80,\n",
    "                 testing_percentile=50,\n",
    "                 training_percentile=50,\n",
    "                 cell_type='LSTM',\n",
    "                 state_hsize=40,\n",
    "                 dilations=[[1, 2], [4, 8]],\n",
    "                 add_nl_layer=False,\n",
    "                 seasonality=[4],\n",
    "                 random_seed=1,\n",
    "                 device='cpu', root_dir='./'):\n",
    "        super(ESRNN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.cell_type = cell_type\n",
    "        self.state_hsize = state_hsize\n",
    "        self.dilations = dilations\n",
    "        self.add_nl_layer = add_nl_layer\n",
    "        self.seasonality = seasonality\n",
    "\n",
    "        self.max_epochs = max_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_scheduler_step_size = lr_scheduler_step_size\n",
    "        self.lr_decay = lr_decay\n",
    "        self.per_series_lr_multip = per_series_lr_multip\n",
    "        self.gradient_eps = gradient_eps\n",
    "        self.gradient_clipping_threshold = gradient_clipping_threshold\n",
    "        self.freq_of_test = freq_of_test\n",
    "\n",
    "        self.rnn_weight_decay = rnn_weight_decay\n",
    "        self.noise_std = noise_std\n",
    "        self.level_variability_penalty = level_variability_penalty\n",
    "        self.testing_percentile = testing_percentile\n",
    "        self.training_percentile = training_percentile\n",
    "\n",
    "        self.random_seed = random_seed\n",
    "        self.device = device\n",
    "        self.root_dir = root_dir\n",
    "        self._fitted = False\n",
    "\n",
    "    def to_tensor(self, x: np.ndarray, dtype = t.float32) -> t.Tensor:\n",
    "        tensor = t.as_tensor(x, dtype=dtype).to(self.device)\n",
    "        return tensor\n",
    "\n",
    "    def __loss_fn(self, loss_name: str):\n",
    "        def loss(x, freq, forecast, target, mask):\n",
    "            if loss_name == 'SMYL':\n",
    "                return \n",
    "            if loss_name == 'MAPE':\n",
    "                return MAPELoss(y=target, y_hat=forecast, mask=mask)\n",
    "            elif loss_name == 'MASE':\n",
    "                return MASELoss(y=target, y_hat=forecast, y_insample=x, seasonality=freq, mask=mask)\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return SMAPELoss(y=target, y_hat=forecast, mask=mask)\n",
    "            elif loss_name == 'MSE':\n",
    "                return MSELoss(y=target, y_hat=forecast, mask=mask)\n",
    "            elif loss_name == 'MAE':\n",
    "                return MAELoss(y=target, y_hat=forecast, mask=mask)\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "       \n",
    "    # def evaluate_performance(self, ts_loader, validation_loss_fn):\n",
    "    #     \"\"\"\n",
    "    #     Auxiliary function, evaluate ESRNN model for training\n",
    "    #     procedure supervision.\n",
    "\n",
    "    #     Parameters\n",
    "    #     ----------\n",
    "    #     dataloader: pytorch dataloader\n",
    "    #     criterion: pytorch test criterion\n",
    "\n",
    "    #     Returns\n",
    "    #     -------\n",
    "    #     model_loss: float\n",
    "    #         loss for train supervision purpose.\n",
    "    #     \"\"\"\n",
    "    #     #TODO: FALTA\n",
    "    #     with t.no_grad():\n",
    "    #         model_loss = 0.0\n",
    "    #         for batch in iter(ts_loader):\n",
    "    #             windows_y, windows_y_hat, _ = self.esrnn(batch)\n",
    "    #             loss = validation_loss_fn(windows_y, windows_y_hat)\n",
    "    #             model_loss += loss.data.cpu().numpy()\n",
    "    #         model_loss /= dataloader.n_batches\n",
    "    #     return model_loss\n",
    "\n",
    "    def fit(self, train_ts_loader, val_ts_loader=None, max_epochs=None, verbose=True, eval_epochs=1):\n",
    "        \"\"\"\n",
    "        Fit ESRNN model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "\n",
    "        # Random Seeds (model initialization)\n",
    "        t.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "\n",
    "        # Exogenous variables\n",
    "        self.n_x, self.n_s = train_ts_loader.get_n_variables()\n",
    "        self.frequency = train_ts_loader.get_frequency()\n",
    "        print(\"Infered frequency: {}\".format(self.frequency))\n",
    "\n",
    "        # Initialize model\n",
    "        self.n_series = train_ts_loader.get_n_series()\n",
    "        self.instantiate_esrnn()\n",
    "\n",
    "        # Train model\n",
    "        self._fitted = True\n",
    "\n",
    "        # dataloader, max_epochs, shuffle, verbose\n",
    "        if verbose: print(15*'='+' Training ESRNN  ' + 15*'=' + '\\n')\n",
    "\n",
    "        # Optimizers\n",
    "        self.es_optimizer = optim.Adam(params=self.esrnn.es.parameters(),\n",
    "                                        lr=self.learning_rate*self.per_series_lr_multip,\n",
    "                                        betas=(0.9, 0.999), eps=self.gradient_eps)\n",
    "\n",
    "        self.es_scheduler = StepLR(optimizer=self.es_optimizer,\n",
    "                                    step_size=self.lr_scheduler_step_size,\n",
    "                                    gamma=0.9)\n",
    "\n",
    "        self.rnn_optimizer = optim.Adam(params=self.esrnn.rnn.parameters(),\n",
    "                                        lr=self.learning_rate,\n",
    "                                        betas=(0.9, 0.999), eps=self.gradient_eps,\n",
    "                                        weight_decay=self.rnn_weight_decay)\n",
    "\n",
    "        self.rnn_scheduler = StepLR(optimizer=self.rnn_optimizer,\n",
    "                                    step_size=self.lr_scheduler_step_size,\n",
    "                                    gamma=self.lr_decay)\n",
    "\n",
    "        # Loss Functions\n",
    "        #TODO: cambiar por similar a Nbeats\n",
    "        train_tau = self.training_percentile / 100\n",
    "        train_loss = SmylLoss(tau=train_tau,\n",
    "                              level_variability_penalty=self.level_variability_penalty)\n",
    "\n",
    "        eval_tau = self.testing_percentile / 100\n",
    "        eval_loss = PinballLoss(tau=eval_tau)\n",
    "\n",
    "        # Overwrite n_iterations and train datasets\n",
    "        if max_epochs is None:\n",
    "            max_epochs = self.max_epochs\n",
    "\n",
    "        start = time.time()\n",
    "        self.trajectories = {'epoch':[],'train_loss':[], 'val_loss':[]}\n",
    "\n",
    "        # for epoch in range(max_epochs):\n",
    "        #     self.esrnn.train()\n",
    "        #     start = time.time()\n",
    "\n",
    "        #     losses = []\n",
    "        # Training Loop\n",
    "        for epoch in range(max_epochs):\n",
    "            losses = []\n",
    "            for batch in iter(train_ts_loader):\n",
    "                self.esrnn.train()\n",
    "                self.es_optimizer.zero_grad()\n",
    "                self.rnn_optimizer.zero_grad()\n",
    "                \n",
    "                insample_y  = self.to_tensor(x=batch['insample_y'])\n",
    "                s_matrix    = self.to_tensor(x=batch['s_matrix'])\n",
    "                idxs        = self.to_tensor(x=batch['idxs'], dtype=t.long)\n",
    "\n",
    "                windows_y, windows_y_hat, levels = self.esrnn(insample_y=insample_y, s_matrix=s_matrix, idxs=idxs)\n",
    "                \n",
    "                # Pinball loss on normalized values\n",
    "                training_loss = train_loss(windows_y, windows_y_hat, levels)\n",
    "                training_loss.backward()\n",
    "                losses.append(training_loss.cpu().data.numpy())\n",
    "                \n",
    "                t.nn.utils.clip_grad_norm_(self.esrnn.rnn.parameters(),\n",
    "                                            self.gradient_clipping_threshold)\n",
    "                t.nn.utils.clip_grad_norm_(self.esrnn.es.parameters(),\n",
    "                                            self.gradient_clipping_threshold)\n",
    "                self.rnn_optimizer.step()\n",
    "                self.es_optimizer.step()\n",
    "\n",
    "                # Decay learning rate\n",
    "                self.es_scheduler.step()\n",
    "                self.rnn_scheduler.step()\n",
    "\n",
    "            # Evaluation\n",
    "            if (epoch % eval_epochs == 0):\n",
    "                display_string = 'Epoch: {}, Time: {:03.3f}, Insample loss: {:.5f}'.format(epoch,\n",
    "                                                                                time.time()-start,\n",
    "                                                                                np.mean(losses))\n",
    "                self.trajectories['epoch'].append(epoch)\n",
    "                self.trajectories['train_loss'].append(training_loss.cpu().data.numpy())\n",
    "\n",
    "                if val_ts_loader is not None:\n",
    "                    loss = self.evaluate_performance(ts_loader=val_ts_loader, \n",
    "                                                        validation_loss_fn=eval_loss)\n",
    "                    display_string += \", Outsample loss: {:.5f}\".format(loss)\n",
    "                    self.trajectories['val_loss'].append(loss)\n",
    "                \n",
    "                print(display_string)\n",
    "\n",
    "                self.esrnn.train()\n",
    "                train_ts_loader.train()\n",
    "\n",
    "    def instantiate_esrnn(self):\n",
    "        \"\"\"Auxiliary function used at beginning of train to instantiate ESRNN\"\"\"\n",
    "        self.esrnn = _ESRNN(n_series=self.n_series, input_size=self.input_size, output_size=self.output_size,\n",
    "                            n_s=self.n_s, seasonality=self.seasonality,\n",
    "                            noise_std=self.noise_std, cell_type=self.cell_type,\n",
    "                            dilations=self.dilations, state_hsize=self.state_hsize,\n",
    "                            add_nl_layer=self.add_nl_layer, device=self.device).to(self.device)\n",
    "\n",
    "\n",
    "    def predict(self, ts_loader, X_test=None):\n",
    "        assert self._fitted, \"Model not fitted yet\"\n",
    "        self.esrnn.eval()\n",
    "        ts_loader.eval()\n",
    "        frequency = ts_loader.get_frequency()\n",
    "\n",
    "        # Build forecasts\n",
    "        unique_ids = ts_loader.get_meta_data_col('unique_id')\n",
    "        last_ds = ts_loader.get_meta_data_col('last_ds') #TODO: ajustar of offset\n",
    "\n",
    "        with t.no_grad():\n",
    "            forecasts = []\n",
    "            for batch in iter(ts_loader):\n",
    "                insample_y  = self.to_tensor(x=batch['insample_y'])\n",
    "                s_matrix    = self.to_tensor(x=batch['s_matrix'])\n",
    "                idxs        = self.to_tensor(x=batch['idxs'], dtype=t.long)\n",
    "\n",
    "                forecast = self.esrnn.predict(insample_y=insample_y, s_matrix=s_matrix, idxs=idxs)\n",
    "                forecasts += [forecast.cpu().data.numpy()]\n",
    "        forecasts = np.vstack(forecasts)\n",
    "\n",
    "        # Predictions for panel\n",
    "        Y_hat_panel = pd.DataFrame(columns=['unique_id', 'ds'])\n",
    "        for i, unique_id in enumerate(unique_ids):\n",
    "            Y_hat_id = pd.DataFrame([unique_id]*self.output_size, columns=[\"unique_id\"])\n",
    "            ds = pd.date_range(start=last_ds[i], periods=self.output_size+1, freq=frequency)\n",
    "            Y_hat_id[\"ds\"] = ds[1:]\n",
    "            Y_hat_panel = Y_hat_panel.append(Y_hat_id, sort=False).reset_index(drop=True)\n",
    "\n",
    "        Y_hat_panel['y_hat'] = forecasts.flatten()\n",
    "\n",
    "        if X_test is not None:\n",
    "            Y_hat_panel = X_test.merge(Y_hat_panel, on=['unique_id', 'ds'], how='left')\n",
    "        \n",
    "        return Y_hat_panel\n",
    "\n",
    "    def save(self, model_dir=None, copy=None):\n",
    "        \"\"\"Auxiliary function to save ESRNN model\"\"\"\n",
    "        if copy is not None:\n",
    "                self.copy = copy\n",
    "\n",
    "        if not model_dir:\n",
    "            assert self.root_dir\n",
    "            model_dir = self.get_dir_name()\n",
    "\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        rnn_filepath = os.path.join(model_dir, \"rnn.model\")\n",
    "        es_filepath = os.path.join(model_dir, \"es.model\")\n",
    "\n",
    "        print('Saving model to:\\n {}'.format(model_dir)+'\\n')\n",
    "        t.save({'model_state_dict': self.es.state_dict()}, es_filepath)\n",
    "        t.save({'model_state_dict': self.rnn.state_dict()}, rnn_filepath)\n",
    "\n",
    "    def load(self, model_dir=None, copy=None):\n",
    "        \"\"\"Auxiliary function to load ESRNN model\"\"\"\n",
    "        if copy is not None:\n",
    "            self.copy = copy\n",
    "\n",
    "        if not model_dir:\n",
    "            assert self.root_dir\n",
    "            model_dir = self.get_dir_name()\n",
    "\n",
    "        rnn_filepath = os.path.join(model_dir, \"rnn.model\")\n",
    "        es_filepath = os.path.join(model_dir, \"es.model\")\n",
    "        path = Path(es_filepath)\n",
    "\n",
    "        if path.is_file():\n",
    "            print('Loading model from:\\n {}'.format(model_dir)+'\\n')\n",
    "\n",
    "            checkpoint = t.load(es_filepath, map_location=self.device)\n",
    "            self.es.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.es.to(self.device)\n",
    "\n",
    "            checkpoint = t.load(rnn_filepath, map_location=self.device)\n",
    "            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.rnn.to(self.device)\n",
    "        else:\n",
    "            print('Model path {} does not exist'.format(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nixtla.data.tsdataset import TimeSeriesDataset\n",
    "from nixtla.data.tsloader_general import TimeSeriesLoader as TimeSeriesLoaderGeneral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TourismInfo.groups[0] Yearly\n",
      "Processing dataframes ...\n",
      "Creating ts tensor ...\n"
     ]
    }
   ],
   "source": [
    "from nixtla.data.datasets.tourism import Tourism, TourismInfo\n",
    "group = TourismInfo.groups[0]\n",
    "print(\"TourismInfo.groups[0]\", group)\n",
    "Y_df, _ = Tourism.load(directory='data', group=group)\n",
    "tourism_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=None, X_df=None, ts_train_mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = TimeSeriesLoaderGeneral(ts_dataset=tourism_dataset,\n",
    "                                            model='esrnn',\n",
    "                                            offset=4,\n",
    "                                            window_sampling_limit=20*4, \n",
    "                                            input_size=2*4,\n",
    "                                            output_size=4,\n",
    "                                            idx_to_sample_freq=1,\n",
    "                                            batch_size= 32,\n",
    "                                            n_series_per_batch=32,\n",
    "                                            is_train_loader=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "esrnn = ESRNN(input_size=1*4,\n",
    "              output_size=4,\n",
    "              max_epochs=5,\n",
    "              freq_of_test=-1,\n",
    "              learning_rate=1e-3,\n",
    "              lr_scheduler_step_size=9,\n",
    "              lr_decay=0.9,\n",
    "              per_series_lr_multip=1.0,\n",
    "              gradient_eps=1e-8,\n",
    "              gradient_clipping_threshold=20,\n",
    "              rnn_weight_decay=0,\n",
    "              noise_std=0.001,\n",
    "              level_variability_penalty=200,\n",
    "              testing_percentile=50,\n",
    "              training_percentile=50,\n",
    "              cell_type='LSTM',\n",
    "              state_hsize=40,\n",
    "              dilations=[[1, 2], [4, 8]],\n",
    "              add_nl_layer=False,\n",
    "              seasonality=[],\n",
    "              random_seed=1,\n",
    "              device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Infered frequency: A-DEC\n",
      "=============== Training ESRNN  ===============\n",
      "\n",
      "Epoch: 0, Time: 0.227, Insample loss: 5.07064\n",
      "Epoch: 1, Time: 0.458, Insample loss: 4.94714\n",
      "Epoch: 2, Time: 0.680, Insample loss: 5.24259\n",
      "Epoch: 3, Time: 0.900, Insample loss: 4.84759\n",
      "Epoch: 4, Time: 1.149, Insample loss: 4.86516\n"
     ]
    }
   ],
   "source": [
    "esrnn.fit(train_ts_loader=train_loader, eval_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = esrnn.predict(ts_loader=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "52b0028e7074a0058398558ea661882eddbb489e66a28504cc0449d2f54d6290"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}