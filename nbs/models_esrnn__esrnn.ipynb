{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.esrnn.esrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESRNN model\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "from nixtla.models.esrnn.utils.config import ModelConfig\n",
    "from nixtla.models.esrnn.utils.esrnn import _ESRNN\n",
    "from nixtla.models.esrnn.utils.losses import SmylLoss, PinballLoss\n",
    "from nixtla.models.esrnn.utils.data import Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ESRNN(object):\n",
    "    \"\"\" Exponential Smoothing Recurrent Neural Network\n",
    "\n",
    "    Pytorch Implementation of the M4 time series forecasting competition winner.\n",
    "    Proposed by Smyl. The model uses a hybrid approach of Machine Learning and\n",
    "    statistical methods by combining recurrent neural networks to model a common\n",
    "    trend with shared parameters across series, and multiplicative Holt-Winter\n",
    "    exponential smoothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_epochs: int\n",
    "        maximum number of complete passes to train data during fit\n",
    "    freq_of_test: int\n",
    "        period for the diagnostic evaluation of the model.\n",
    "    learning_rate: float\n",
    "        size of the stochastic gradient descent steps\n",
    "    lr_scheduler_step_size: int\n",
    "        this step_size is the period for each learning rate decay\n",
    "    per_series_lr_multip: float\n",
    "        multiplier for per-series parameters smoothing and initial\n",
    "        seasonalities learning rate (default 1.0)\n",
    "    gradient_eps: float\n",
    "        term added to the Adam optimizer denominator to improve\n",
    "        numerical stability (default: 1e-8)\n",
    "    gradient_clipping_threshold: float\n",
    "        max norm of gradient vector, with all parameters treated\n",
    "        as a single vector\n",
    "    rnn_weight_decay: float\n",
    "        parameter to control classic L2/Tikhonov regularization\n",
    "        of the rnn parameters\n",
    "    noise_std: float\n",
    "        standard deviation of white noise added to input during\n",
    "        fit to avoid the model from memorizing the train data\n",
    "    level_variability_penalty: float\n",
    "        this parameter controls the strength of the penalization\n",
    "        to the wigglines of the level vector, induces smoothness\n",
    "        in the output\n",
    "    testing_percentile: float\n",
    "        This value is only for diagnostic evaluation.\n",
    "        In case of percentile predictions this parameter controls\n",
    "        for the value predicted, when forecasting point value,\n",
    "        the forecast is the median, so percentile=50.\n",
    "    training_percentile: float\n",
    "        To reduce the model's tendency to over estimate, the\n",
    "        training_percentile can be set to fit a smaller value\n",
    "        through the Pinball Loss.\n",
    "    batch_size: int\n",
    "        number of training examples for the stochastic gradient steps\n",
    "    seasonality: int list\n",
    "        list of seasonalities of the time series\n",
    "        Hourly [24, 168], Daily [7], Weekly [52], Monthly [12],\n",
    "        Quarterly [4], Yearly [].\n",
    "    input_size: int\n",
    "        input size of the recurrent neural network, usually a\n",
    "        multiple of seasonality\n",
    "    output_size: int\n",
    "        output_size or forecast horizon of the recurrent neural\n",
    "        network, usually multiple of seasonality\n",
    "    random_seed: int\n",
    "        random_seed for pseudo random pytorch initializer and\n",
    "        numpy random generator\n",
    "    exogenous_size: int\n",
    "        size of one hot encoded categorical variable, invariannt\n",
    "        per time series of the panel\n",
    "    min_inp_seq_length: int\n",
    "        description\n",
    "    max_periods: int\n",
    "        Parameter to chop longer series, to last max_periods,\n",
    "        max e.g. 40 years\n",
    "    cell_type: str\n",
    "        Type of RNN cell, available GRU, LSTM, RNN, ResidualLSTM.\n",
    "    state_hsize: int\n",
    "        dimension of hidden state of the recurrent neural network\n",
    "    dilations: int list\n",
    "        each list represents one chunk of Dilated LSTMS, connected in\n",
    "        standard ResNet fashion\n",
    "    add_nl_layer: bool\n",
    "        whether to insert a tanh() layer between the RNN stack and the\n",
    "        linear adaptor (output) layers\n",
    "    device: str\n",
    "        pytorch device either 'cpu' or 'cuda'\n",
    "    Notes\n",
    "    -----\n",
    "    **References:**\n",
    "    `M4 Competition Conclusions\n",
    "    <https://rpubs.com/fotpetr/m4competition>`__\n",
    "    `Original Dynet Implementation of ESRNN\n",
    "    <https://github.com/M4Competition/M4-methods/tree/master/118%20-%20slaweks17>`__\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_size=4,\n",
    "                 output_size=8,\n",
    "                 max_epochs=15,\n",
    "                 batch_size=1,\n",
    "                 batch_size_test=64,\n",
    "                 freq_of_test=-1,\n",
    "                 learning_rate=1e-3,\n",
    "                 lr_scheduler_step_size=9,\n",
    "                 lr_decay=0.9,\n",
    "                 per_series_lr_multip=1.0,\n",
    "                 gradient_eps=1e-8,\n",
    "                 gradient_clipping_threshold=20,\n",
    "                 rnn_weight_decay=0,\n",
    "                 noise_std=0.001,\n",
    "                 level_variability_penalty=80,\n",
    "                 testing_percentile=50,\n",
    "                 training_percentile=50,\n",
    "                 cell_type='LSTM',\n",
    "                 state_hsize=40,\n",
    "                 dilations=[[1, 2], [4, 8]],\n",
    "                 add_nl_layer=False,\n",
    "                 seasonality=[4],\n",
    "                 frequency=None,\n",
    "                 max_periods=20,\n",
    "                 random_seed=1,\n",
    "                 device='cpu', root_dir='./'):\n",
    "        super(ESRNN, self).__init__()\n",
    "        self.mc = ModelConfig(max_epochs=max_epochs, batch_size=batch_size, batch_size_test=batch_size_test,\n",
    "                            freq_of_test=freq_of_test, learning_rate=learning_rate,\n",
    "                            lr_scheduler_step_size=lr_scheduler_step_size, lr_decay=lr_decay,\n",
    "                            per_series_lr_multip=per_series_lr_multip,\n",
    "                            gradient_eps=gradient_eps, gradient_clipping_threshold=gradient_clipping_threshold,\n",
    "                            rnn_weight_decay=rnn_weight_decay, noise_std=noise_std,\n",
    "                            level_variability_penalty=level_variability_penalty,\n",
    "                            testing_percentile=testing_percentile, training_percentile=training_percentile,\n",
    "                            cell_type=cell_type,\n",
    "                            state_hsize=state_hsize, dilations=dilations, add_nl_layer=add_nl_layer,\n",
    "                            seasonality=seasonality, input_size=input_size, output_size=output_size,\n",
    "                            frequency=frequency, max_periods=max_periods, random_seed=random_seed,\n",
    "                            device=device, root_dir=root_dir)\n",
    "        self._fitted = False\n",
    "\n",
    "    def to_tensor(self, x: np.ndarray, dtype = t.float32) -> t.Tensor:\n",
    "        tensor = t.as_tensor(x, dtype=dtype).to(self.mc.device)\n",
    "        return tensor\n",
    "\n",
    "    def __loss_fn(self, loss_name: str):\n",
    "        def loss(x, freq, forecast, target, mask):\n",
    "            if loss_name == 'SMYL':\n",
    "                return \n",
    "            if loss_name == 'MAPE':\n",
    "                return MAPELoss(y=target, y_hat=forecast, mask=mask)\n",
    "            elif loss_name == 'MASE':\n",
    "                return MASELoss(y=target, y_hat=forecast, y_insample=x, seasonality=freq, mask=mask)\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return SMAPELoss(y=target, y_hat=forecast, mask=mask)\n",
    "            elif loss_name == 'MSE':\n",
    "                return MSELoss(y=target, y_hat=forecast, mask=mask)\n",
    "            elif loss_name == 'MAE':\n",
    "                return MAELoss(y=target, y_hat=forecast, mask=mask)\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def __val_loss_fn(self, loss_name='MAE'):\n",
    "        #TODO: mase not implemented\n",
    "        def loss(forecast, target, weights):\n",
    "            if loss_name == 'MAPE':\n",
    "                return mape(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return smape(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MSE':\n",
    "                return mse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'RMSE':\n",
    "                return rmse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MAE':\n",
    "                return mae(y=target, y_hat=forecast, weights=weights)\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "       \n",
    "    def evaluate_performance(self, ts_loader, validation_loss_fn):\n",
    "        \"\"\"\n",
    "        Auxiliary function, evaluate ESRNN model for training\n",
    "        procedure supervision.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataloader: pytorch dataloader\n",
    "        criterion: pytorch test criterion\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model_loss: float\n",
    "            loss for train supervision purpose.\n",
    "        \"\"\"\n",
    "\n",
    "        with t.no_grad():\n",
    "            # Create fast dataloader\n",
    "            # if self.mc.n_series < self.mc.batch_size_test: new_batch_size = self.mc.n_series\n",
    "            # else: new_batch_size = self.mc.batch_size_test\n",
    "            # dataloader.update_batch_size(new_batch_size)\n",
    "\n",
    "            model_loss = 0.0\n",
    "            for batch in iter(ts_loader):\n",
    "                windows_y, windows_y_hat, _ = self.esrnn(batch)\n",
    "                loss = validation_loss_fn(windows_y, windows_y_hat)\n",
    "                model_loss += loss.data.cpu().numpy()\n",
    "\n",
    "            model_loss /= dataloader.n_batches\n",
    "            dataloader.update_batch_size(self.mc.batch_size)\n",
    "        return model_loss\n",
    "\n",
    "\n",
    "    #def fit(self, X_df, y_df, X_test_df=None, shuffle=True, verbose=True):\n",
    "    def fit(self, train_ts_loader, val_ts_loader=None, n_iterations=None, verbose=True, eval_steps=1):\n",
    "        \"\"\"\n",
    "        Fit ESRNN model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_df : pandas dataframe\n",
    "            Train dataframe in long format with columns 'unique_id', 'ds'\n",
    "            and 'x'.\n",
    "            - 'unique_id' an identifier of each independent time series.\n",
    "            - 'ds' is a datetime column\n",
    "            - 'x' is a single exogenous variable\n",
    "        y_df : pandas dataframe\n",
    "            Train dataframe in long format with columns 'unique_id', 'ds' and 'y'.\n",
    "            - 'unique_id' an identifier of each independent time series.\n",
    "            - 'ds' is a datetime column\n",
    "            - 'y' is the column with the target values\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "\n",
    "        # Exogenous variables\n",
    "        exogenous_size = 0 #TODO: mementaneo, no lo queremos ahora\n",
    "        max_epochs = 10 #TODO: momentaneo\n",
    "\n",
    "        # Random Seeds (model initialization)\n",
    "        t.manual_seed(self.mc.random_seed)\n",
    "        np.random.seed(self.mc.random_seed)\n",
    "\n",
    "        # Initialize model\n",
    "        n_series = train_ts_loader.get_n_series()\n",
    "        self.instantiate_esrnn(exogenous_size, n_series)\n",
    "\n",
    "        self.mc.frequency = train_ts_loader.get_frequency()\n",
    "        print(\"Infered frequency: {}\".format(self.mc.frequency))\n",
    "\n",
    "        # Train model\n",
    "        self._fitted = True\n",
    "\n",
    "        # dataloader, max_epochs, shuffle, verbose\n",
    "        if verbose: print(15*'='+' Training ESRNN  ' + 15*'=' + '\\n')\n",
    "\n",
    "        # Optimizers\n",
    "        self.es_optimizer = optim.Adam(params=self.esrnn.es.parameters(),\n",
    "                                        lr=self.mc.learning_rate*self.mc.per_series_lr_multip,\n",
    "                                        betas=(0.9, 0.999), eps=self.mc.gradient_eps)\n",
    "\n",
    "        self.es_scheduler = StepLR(optimizer=self.es_optimizer,\n",
    "                                    step_size=self.mc.lr_scheduler_step_size,\n",
    "                                    gamma=0.9)\n",
    "\n",
    "        self.rnn_optimizer = optim.Adam(params=self.esrnn.rnn.parameters(),\n",
    "                                        lr=self.mc.learning_rate,\n",
    "                                        betas=(0.9, 0.999), eps=self.mc.gradient_eps,\n",
    "                                        weight_decay=self.mc.rnn_weight_decay)\n",
    "\n",
    "        self.rnn_scheduler = StepLR(optimizer=self.rnn_optimizer,\n",
    "                                    step_size=self.mc.lr_scheduler_step_size,\n",
    "                                    gamma=self.mc.lr_decay)\n",
    "\n",
    "        # Loss Functions\n",
    "        #TODO: cambiar por similar a Nbeats\n",
    "        train_tau = self.mc.training_percentile / 100\n",
    "        train_loss = SmylLoss(tau=train_tau,\n",
    "                              level_variability_penalty=self.mc.level_variability_penalty)\n",
    "\n",
    "        eval_tau = self.mc.testing_percentile / 100\n",
    "        eval_loss = PinballLoss(tau=eval_tau)\n",
    "\n",
    "        # Overwrite n_iterations and train datasets\n",
    "        if n_iterations is None:\n",
    "            n_iterations = self.mc.max_epochs\n",
    "\n",
    "        train_dataloader = iter(train_ts_loader)\n",
    "\n",
    "        start = time.time()\n",
    "        self.trajectories = {'step':[],'train_loss':[], 'val_loss':[]}\n",
    "\n",
    "        # for epoch in range(max_epochs):\n",
    "        #     self.esrnn.train()\n",
    "        #     start = time.time()\n",
    "\n",
    "        #     losses = []\n",
    "        # Training Loop\n",
    "        for step in range(n_iterations):\n",
    "            self.es_optimizer.zero_grad()\n",
    "            self.rnn_optimizer.zero_grad()\n",
    "            \n",
    "            batch = next(train_dataloader)\n",
    "            y = self.to_tensor(x=batch['y'])\n",
    "            idxs = self.to_tensor(x=batch['idxs'], dtype=t.long)\n",
    "            categories = self.to_tensor(x=batch['categories'])\n",
    "            windows_y, windows_y_hat, levels = self.esrnn(y=y, idxs=idxs, categories=categories)\n",
    "            \n",
    "            # Pinball loss on normalized values\n",
    "            training_loss = train_loss(windows_y, windows_y_hat, levels)\n",
    "            training_loss.backward()\n",
    "            \n",
    "            t.nn.utils.clip_grad_norm_(self.esrnn.rnn.parameters(),\n",
    "                                        self.mc.gradient_clipping_threshold)\n",
    "            t.nn.utils.clip_grad_norm_(self.esrnn.es.parameters(),\n",
    "                                        self.mc.gradient_clipping_threshold)\n",
    "            self.rnn_optimizer.step()\n",
    "            self.es_optimizer.step()\n",
    "\n",
    "            # Decay learning rate\n",
    "            self.es_scheduler.step()\n",
    "            self.rnn_scheduler.step()\n",
    "\n",
    "            # Evaluation\n",
    "            if (step % eval_steps == 0):\n",
    "                display_string = 'Step: {}, Time: {:03.3f}, Insample loss: {:.5f}'.format(step,\n",
    "                                                                                time.time()-start,\n",
    "                                                                                training_loss.cpu().data.numpy())\n",
    "                self.trajectories['step'].append(step)\n",
    "                self.trajectories['train_loss'].append(training_loss.cpu().data.numpy())\n",
    "\n",
    "                if val_ts_loader is not None:\n",
    "                    loss = self.evaluate_performance(ts_loader=val_ts_loader, \n",
    "                                                        validation_loss_fn=eval_loss)\n",
    "                    display_string += \", Outsample loss: {:.5f}\".format(loss)\n",
    "                    self.trajectories['val_loss'].append(loss)\n",
    "                \n",
    "                print(display_string)\n",
    "\n",
    "                self.esrnn.train()\n",
    "                train_ts_loader.train()\n",
    "\n",
    "    def instantiate_esrnn(self, exogenous_size, n_series):\n",
    "        \"\"\"Auxiliary function used at beginning of train to instantiate ESRNN\"\"\"\n",
    "\n",
    "        self.mc.exogenous_size = exogenous_size\n",
    "        self.mc.n_series = n_series\n",
    "        self.esrnn = _ESRNN(self.mc).to(self.mc.device)\n",
    "\n",
    "    def predict(self, X_df, decomposition=False):\n",
    "        \"\"\"\n",
    "        Predict using the ESRNN model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_df : pandas dataframe\n",
    "            Dataframe in LONG format with columns 'unique_id', 'ds'\n",
    "            and 'x'.\n",
    "            - 'unique_id' an identifier of each independent time series.\n",
    "            - 'ds' is a datetime column\n",
    "            - 'x' is a single exogenous variable\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Y_hat_panel : pandas dataframe\n",
    "            Dataframe in LONG format with columns 'unique_id', 'ds'\n",
    "            and 'x'.\n",
    "            - 'unique_id' an identifier of each independent time series.\n",
    "            - 'ds' datetime columnn that matches the dates in X_df\n",
    "            - 'y_hat' is the column with the predicted target values\n",
    "        \"\"\"\n",
    "\n",
    "        #print(9*'='+' Predicting ESRNN ' + 9*'=' + '\\n')\n",
    "        assert type(X_df) == pd.core.frame.DataFrame\n",
    "        assert 'unique_id' in X_df\n",
    "        assert self._fitted, \"Model not fitted yet\"\n",
    "\n",
    "        self.esrnn.eval()\n",
    "\n",
    "        # Create fast dataloader\n",
    "        if self.mc.n_series < self.mc.batch_size_test: new_batch_size = self.mc.n_series\n",
    "        else: new_batch_size = self.mc.batch_size_test\n",
    "        self.train_dataloader.update_batch_size(new_batch_size)\n",
    "        dataloader = self.train_dataloader\n",
    "\n",
    "        # Create Y_hat_panel placeholders\n",
    "        output_size = self.mc.output_size\n",
    "        n_unique_id = len(dataloader.sort_key['unique_id'])\n",
    "        panel_unique_id = pd.Series(dataloader.sort_key['unique_id']).repeat(output_size)\n",
    "\n",
    "        #access column with last train date\n",
    "        panel_last_ds = pd.Series(dataloader.X[:, 2])\n",
    "        panel_ds = []\n",
    "        for i in range(len(panel_last_ds)):\n",
    "            ranges = pd.date_range(start=panel_last_ds[i], periods=output_size+1, freq=self.mc.frequency)\n",
    "            panel_ds += list(ranges[1:])\n",
    "\n",
    "        panel_y_hat= np.zeros((output_size * n_unique_id))\n",
    "\n",
    "        # Predict\n",
    "        count = 0\n",
    "        for j in range(dataloader.n_batches):\n",
    "            batch = dataloader.get_batch()\n",
    "            batch_size = batch.y.shape[0]\n",
    "\n",
    "            y_hat = self.esrnn.predict(batch)\n",
    "            y_hat = y_hat.data.cpu().numpy()\n",
    "\n",
    "            panel_y_hat[count:count+output_size*batch_size] = y_hat.flatten()\n",
    "            count += output_size*batch_size\n",
    "\n",
    "        Y_hat_panel_dict = {'unique_id': panel_unique_id,\n",
    "                                                'ds': panel_ds,\n",
    "                                                'y_hat': panel_y_hat}\n",
    "\n",
    "        assert len(panel_ds) == len(panel_y_hat) == len(panel_unique_id)\n",
    "\n",
    "        Y_hat_panel = pd.DataFrame.from_dict(Y_hat_panel_dict)\n",
    "\n",
    "        if 'ds' in X_df:\n",
    "            Y_hat_panel = X_df.merge(Y_hat_panel, on=['unique_id', 'ds'], how='left')\n",
    "        else:\n",
    "            Y_hat_panel = X_df.merge(Y_hat_panel, on=['unique_id'], how='left')\n",
    "\n",
    "        self.train_dataloader.update_batch_size(self.mc.batch_size)\n",
    "        return Y_hat_panel\n",
    "\n",
    "    def save(self, model_dir=None, copy=None):\n",
    "        \"\"\"Auxiliary function to save ESRNN model\"\"\"\n",
    "        if copy is not None:\n",
    "                self.mc.copy = copy\n",
    "\n",
    "        if not model_dir:\n",
    "            assert self.mc.root_dir\n",
    "            model_dir = self.get_dir_name()\n",
    "\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        rnn_filepath = os.path.join(model_dir, \"rnn.model\")\n",
    "        es_filepath = os.path.join(model_dir, \"es.model\")\n",
    "\n",
    "        print('Saving model to:\\n {}'.format(model_dir)+'\\n')\n",
    "        t.save({'model_state_dict': self.es.state_dict()}, es_filepath)\n",
    "        t.save({'model_state_dict': self.rnn.state_dict()}, rnn_filepath)\n",
    "\n",
    "    def load(self, model_dir=None, copy=None):\n",
    "        \"\"\"Auxiliary function to load ESRNN model\"\"\"\n",
    "        if copy is not None:\n",
    "            self.mc.copy = copy\n",
    "\n",
    "        if not model_dir:\n",
    "            assert self.mc.root_dir\n",
    "            model_dir = self.get_dir_name()\n",
    "\n",
    "        rnn_filepath = os.path.join(model_dir, \"rnn.model\")\n",
    "        es_filepath = os.path.join(model_dir, \"es.model\")\n",
    "        path = Path(es_filepath)\n",
    "\n",
    "        if path.is_file():\n",
    "            print('Loading model from:\\n {}'.format(model_dir)+'\\n')\n",
    "\n",
    "            checkpoint = t.load(es_filepath, map_location=self.mc.device)\n",
    "            self.es.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.es.to(self.mc.device)\n",
    "\n",
    "            checkpoint = t.load(rnn_filepath, map_location=self.mc.device)\n",
    "            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.rnn.to(self.mc.device)\n",
    "        else:\n",
    "            print('Model path {} does not exist'.format(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch as t\n",
    "import copy\n",
    "from fastcore.foundation import patch\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset\n",
    "# from nixtla.data.tsloader_fast import TimeSeriesLoader as TimeSeriesLoaderFast\n",
    "# from nixtla.data.tsloader_pinche import TimeSeriesLoader as TimeSeriesLoaderPinche\n",
    "from nixtla.data.tsloader_general import TimeSeriesLoader as TimeSeriesLoaderGeneral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing dataframes ...\nCreating ts tensor ...\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  unique_id                  ds      y\n",
       "0        NP 2013-01-01 00:00:00  31.05\n",
       "1        NP 2013-01-01 01:00:00  30.47\n",
       "2        NP 2013-01-01 02:00:00  28.92\n",
       "3        NP 2013-01-01 03:00:00  27.88\n",
       "4        NP 2013-01-01 04:00:00  26.96"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unique_id</th>\n      <th>ds</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NP</td>\n      <td>2013-01-01 00:00:00</td>\n      <td>31.05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NP</td>\n      <td>2013-01-01 01:00:00</td>\n      <td>30.47</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NP</td>\n      <td>2013-01-01 02:00:00</td>\n      <td>28.92</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NP</td>\n      <td>2013-01-01 03:00:00</td>\n      <td>27.88</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NP</td>\n      <td>2013-01-01 04:00:00</td>\n      <td>26.96</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "Y_df, X_df = EPF.load(directory='data', group=EPFInfo.groups[0])\n",
    "train_outsample_mask = np.ones(len(Y_df))\n",
    "train_outsample_mask[-365 * 24:] = 0\n",
    "sum(train_outsample_mask)\n",
    "epf_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=None, X_df=X_df, ts_train_mask=train_outsample_mask)\n",
    "# print('X: time series features, of shape (#series,#times,#features): \\t' + str(X.shape))\n",
    "# print('Y: target series (in X), of shape (#series,#times): \\t \\t' + str(Y.shape))\n",
    "# print('S: static features, of shape (#series,#features): \\t \\t' + str(S.shape))\n",
    "Y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = TimeSeriesLoaderGeneral(ts_dataset=epf_dataset,\n",
    "                                    model='esrnn',\n",
    "                                    offset=0,\n",
    "                                    window_sampling_limit= 60 * 24, #365*4*24\n",
    "                                    input_size=3*24,\n",
    "                                    output_size=24,\n",
    "                                    idx_to_sample_freq=1,\n",
    "                                    batch_size=1,\n",
    "                                    is_train_loader=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "esrnn = ESRNN(input_size=4,\n",
    "              output_size=8,\n",
    "              max_epochs=15,\n",
    "              batch_size=1,\n",
    "              batch_size_test=64,\n",
    "              freq_of_test=-1,\n",
    "              learning_rate=1e-3,\n",
    "              lr_scheduler_step_size=9,\n",
    "              lr_decay=0.9,\n",
    "              per_series_lr_multip=1.0,\n",
    "              gradient_eps=1e-8,\n",
    "              gradient_clipping_threshold=20,\n",
    "              rnn_weight_decay=0,\n",
    "              noise_std=0.001,\n",
    "              level_variability_penalty=80,\n",
    "              testing_percentile=50,\n",
    "              training_percentile=50,\n",
    "              cell_type='LSTM',\n",
    "              state_hsize=40,\n",
    "              dilations=[[1, 2], [4, 8]],\n",
    "              add_nl_layer=False,\n",
    "              seasonality=[4],\n",
    "              frequency=None,\n",
    "              max_periods=20,\n",
    "              random_seed=1,\n",
    "              device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Infered frequency: H\n=============== Training ESRNN  ===============\n\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-72efd75e1a8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mesrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ts_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ts_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-afe8b6897c5a>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_ts_loader, val_ts_loader, n_iterations, verbose, eval_steps)\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mval_ts_loader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     loss = self.evaluate_performance(ts_loader=val_ts_loader, \n\u001b[0;32m--> 329\u001b[0;31m                                                         validation_loss_fn=eval_loss)\n\u001b[0m\u001b[1;32m    330\u001b[0m                     \u001b[0mdisplay_string\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\", Outsample loss: {:.5f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-afe8b6897c5a>\u001b[0m in \u001b[0;36mevaluate_performance\u001b[0;34m(self, ts_loader, validation_loss_fn)\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mmodel_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m                 \u001b[0mwindows_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindows_y_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mesrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindows_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindows_y_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "esrnn.fit(train_ts_loader=train_loader, val_ts_loader=train_loader, n_iterations= 100, eval_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "52b0028e7074a0058398558ea661882eddbb489e66a28504cc0449d2f54d6290"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}