{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.ts_loader_pinche_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch as t\n",
    "import copy\n",
    "from fastcore.foundation import patch\n",
    "from nixtla.data.ts_dataset import TimeSeriesDataset\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TimeSeriesLoader(object):\n",
    "    def __init__(self,\n",
    "                 ts_dataset:TimeSeriesDataset,\n",
    "                 model:str,\n",
    "                 offset:int,\n",
    "                 window_sampling_limit: int, \n",
    "                 input_size: int,\n",
    "                 output_size: int,\n",
    "                 idx_to_sample_freq: int, #TODO: not active yet\n",
    "                 batch_size: int,\n",
    "                 train_loader: bool):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.ts_dataset = ts_dataset # Pass by reference\n",
    "        self.model = model\n",
    "        self.offset = offset\n",
    "        self.window_sampling_limit = window_sampling_limit\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.idx_to_sample_freq = idx_to_sample_freq\n",
    "        self.batch_size = batch_size\n",
    "        self.train_loader = train_loader\n",
    "        self._is_train = True\n",
    "\n",
    "        assert offset < self.ts_dataset.max_len, 'Offset must be smaller than max_len'\n",
    "\n",
    "        self.window_sampling_idx = self._update_windows_sampling_idx()\n",
    "\n",
    "    def _update_windows_sampling_idx(self):\n",
    "        # Filter sampling_mask with offset and window_sampling_limit\n",
    "        last_ds = self.ts_dataset.max_len - self.offset\n",
    "        first_ds = max(last_ds - self.window_sampling_limit, 0)\n",
    "        filtered_outsample_mask = self.ts_dataset.ts_tensor[:, self.ts_dataset.t_cols.index('outsample_mask'), first_ds:last_ds]\n",
    "        filtered_ts_train_mask = self.ts_dataset.ts_train_mask[first_ds:last_ds]\n",
    "\n",
    "        # Get indices of train/validation windows\n",
    "        if self.train_loader:\n",
    "            train_mask =  filtered_outsample_mask * filtered_ts_train_mask\n",
    "            indices = np.argwhere(train_mask > 0)\n",
    "        else:\n",
    "            val_mask = filtered_outsample_mask * (1-filtered_ts_train_mask)\n",
    "            indices = np.argwhere(val_mask > 0)\n",
    "\n",
    "        #To change relative position of filtered tensor to global position\n",
    "        indices[:, 1] += first_ds\n",
    "\n",
    "        #Loop for each serie to extract window_sampling_idx\n",
    "        window_sampling_idx = []\n",
    "        for i in range(self.ts_dataset.n_series):\n",
    "            ts_idx = indices[indices[:, 0] == i]\n",
    "            window_sampling_idx.append(list(ts_idx[:, 1]))\n",
    "        return window_sampling_idx\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            if self._is_train:\n",
    "                sampled_ts_indices = np.random.randint(self.ts_dataset.n_series, size=self.batch_size)\n",
    "            else:\n",
    "                sampled_ts_indices = range(self.ts_dataset.n_series)\n",
    "\n",
    "            batch_dict = defaultdict(list)\n",
    "            for index in sampled_ts_indices:\n",
    "                batch_i = self.__get_item__(index)\n",
    "                for key in batch_i:\n",
    "                    batch_dict[key].append(batch_i[key])\n",
    "\n",
    "            batch = defaultdict(list)\n",
    "            for key in batch_dict:\n",
    "                batch[key] = np.stack(batch_dict[key])\n",
    "\n",
    "            yield batch\n",
    "\n",
    "    def __get_item__(self, index):\n",
    "        if self.model == 'nbeats':\n",
    "            return self._nbeats_batch(index)\n",
    "        elif self.model == 'esrnn':\n",
    "            assert 1<0, 'hacer esrnn'\n",
    "        else:\n",
    "            assert 1<0, 'error'\n",
    "\n",
    "    def _nbeats_batch(self, index):\n",
    "        insample = np.zeros((self.ts_dataset.n_channels-1, self.input_size), dtype=float)\n",
    "        insample_mask = np.zeros(self.input_size)\n",
    "        outsample = np.zeros((self.ts_dataset.n_channels-1, self.output_size), dtype=float)\n",
    "        outsample_mask = np.zeros(self.output_size)\n",
    "            \n",
    "        ts = self.ts_dataset.ts_tensor[index]\n",
    "        len_ts = self.ts_dataset.len_series[index]\n",
    "        init_ts = max(self.ts_dataset.max_len-len_ts, self.ts_dataset.max_len-self.offset-self.window_sampling_limit) #TODO: precomputar en loader\n",
    "\n",
    "        assert self.ts_dataset.max_len-self.offset > init_ts, f'Offset too big for serie {index}'\n",
    "        if self._is_train:\n",
    "            cut_point = np.random.choice(self.window_sampling_idx[index],1)[0] # Sampling from available cuts for ts \"index\"\n",
    "        else:\n",
    "            cut_point = max(self.ts_dataset.max_len-self.offset, self.input_size)\n",
    "        \n",
    "        insample_window = ts[:-2, max(0, cut_point - self.input_size):cut_point] # se sacan mask channels y Y outsample\n",
    "        insample_window = np.delete(insample_window, 0, 0)\n",
    "        insample_mask_start = min(self.input_size, cut_point - init_ts) #In case cut_point is close to init_ts, because series are padded\n",
    "        insample[:, -insample_window.shape[1]:] = insample_window\n",
    "        insample_mask[-insample_mask_start:] = 1.0\n",
    "\n",
    "        if self._is_train:\n",
    "            #se saca mask channel del final\n",
    "            outsample_window = ts[:-2, cut_point:min(self.ts_dataset.max_len - self.offset, cut_point + self.output_size)]\n",
    "            outsample_window = np.delete(outsample_window, 1, 0)\n",
    "\n",
    "        else:\n",
    "            #se saca mask channel del final\n",
    "            outsample_window = ts[:-2, cut_point:min(self.ts_dataset.max_len, cut_point + self.output_size)]\n",
    "            outsample_window = np.delete(outsample_window, 1, 0)\n",
    "\n",
    "        # First mask is to filter after offset, second mask to filter ts validation\n",
    "        outsample[:, :outsample_window.shape[1]] = outsample_window \n",
    "        outsample_mask[:outsample_window.shape[1]] = 1.0\n",
    "        if self.train_loader:\n",
    "            outsample_mask[:outsample_window.shape[1]] = outsample_mask[:outsample_window.shape[1]] * \\\n",
    "                                                        self.ts_dataset.ts_train_mask[cut_point:(cut_point+outsample_window.shape[1])]\n",
    "        else:\n",
    "            outsample_mask[:outsample_window.shape[1]] = outsample_mask[:outsample_window.shape[1]] * \\\n",
    "                                                        (1-self.ts_dataset.ts_train_mask[cut_point:(cut_point+outsample_window.shape[1])])\n",
    "\n",
    "        insample_y = insample[0, :]\n",
    "        insample_x_t = insample[1:, :]\n",
    "\n",
    "        outsample_y = outsample[0, :]\n",
    "        outsample_x_t = outsample[1:, :]\n",
    "\n",
    "        x_s = self.ts_dataset.x_s[index, :]\n",
    "\n",
    "        sample = {'insample_y':insample_y, 'insample_x_t':insample_x_t, 'insample_mask':insample_mask,\n",
    "                  'outsample_y':outsample_y, 'outsample_x_t':outsample_x_t, 'outsample_mask':outsample_mask,\n",
    "                  'x_s':x_s}\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def update_offset(self, offset):\n",
    "        if offset == self.offset:\n",
    "            return # Avoid extra computation\n",
    "        self.offset = offset\n",
    "\n",
    "    def get_meta_data_var(self, var):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return self.ts_dataset.get_meta_data_var(var)\n",
    "\n",
    "    def get_n_variables(self):\n",
    "        return self.ts_dataset.n_x_t-1, self.ts_dataset.n_s_t\n",
    "\n",
    "    def get_n_series(self):\n",
    "        return self.ts_dataset.n_series\n",
    "\n",
    "    def get_max_len(self):\n",
    "        return self.ts_dataset.max_len\n",
    "\n",
    "    def get_n_channels(self):\n",
    "        return self.ts_dataset.n_channels-1\n",
    "\n",
    "    def get_frequency(self):\n",
    "        return self.ts_dataset.frequency\n",
    "\n",
    "    def train(self):\n",
    "        self._is_train = True\n",
    "\n",
    "    def eval(self):\n",
    "        self._is_train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing dataframes ...\nCreating ts tensor ...\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[3.1050e+01, 3.0470e+01, 2.8920e+01, ..., 2.6820e+01,\n",
       "         2.6650e+01, 2.5680e+01],\n",
       "        [4.2497e+04, 4.1463e+04, 4.0812e+04, ..., 4.5471e+04,\n",
       "         4.4386e+04, 4.3017e+04],\n",
       "        [2.7980e+03, 2.4170e+03, 2.0360e+03, ..., 2.1290e+03,\n",
       "         1.8270e+03, 1.6890e+03],\n",
       "        ...,\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [1.0000e+00, 1.0000e+00, 1.0000e+00, ..., 1.0000e+00,\n",
       "         1.0000e+00, 1.0000e+00],\n",
       "        [0.0000e+00, 1.0000e+00, 1.0000e+00, ..., 1.0000e+00,\n",
       "         1.0000e+00, 1.0000e+00]]])"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "dataset = EPF.load(directory='data', group=EPFInfo.groups[0])\n",
    "dataset.ts_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_loader = TimeSeriesLoader(ts_dataset= dataset,\n",
    "                             model='nbeats',\n",
    "                             offset = 0,\n",
    "                             window_sampling_limit=1000, \n",
    "                             input_size=8,\n",
    "                             output_size=4,\n",
    "                             idx_to_sample_freq=1,\n",
    "                             batch_size=1024,\n",
    "                             train_loader=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader = iter(ts_loader)\n",
    "batch = next(dataloader)\n",
    "insample_y = batch['insample_y']\n",
    "insample_x_t = batch['insample_x_t']\n",
    "insample_mask = batch['insample_mask']\n",
    "outsample_x_t = batch['outsample_x_t']\n",
    "outsample_y = batch['outsample_y']\n",
    "outsample_mask = batch['outsample_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[48120., 52172., 52840., ..., 51408., 51353., 51506.],\n",
       "       [57691., 56917., 56719., ..., 56336., 56412., 56555.],\n",
       "       [55610., 56645., 57669., ..., 55793., 54120., 52557.],\n",
       "       ...,\n",
       "       [49373., 49872., 51044., ..., 52385., 51018., 49533.],\n",
       "       [58271., 57240., 55642., ..., 49097., 47188., 46064.],\n",
       "       [53185., 54105., 53209., ..., 52588., 52731., 52750.]])"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "insample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[41.21, 41.69, 42.88, 42.87],\n",
       "       [38.95, 39.64, 39.41, 37.91],\n",
       "       [32.58, 31.23, 30.97, 30.19],\n",
       "       ...,\n",
       "       [31.45, 30.99, 30.18, 29.94],\n",
       "       [30.21, 30.64, 31.12, 32.3 ],\n",
       "       [33.05, 33.38, 33.6 , 32.98]])"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "outsample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "outsample_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9990234375"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "outsample_mask.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "52b0028e7074a0058398558ea661882eddbb489e66a28504cc0449d2f54d6290"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}