{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c68c0f618c4f787b1e02bfee278e48d25b62407cb335aab5258cedc4db4ced"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.ontsdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch as t\n",
    "\n",
    "from fastcore.foundation import patch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 Y_df: pd.DataFrame,\n",
    "                 X_df: pd.DataFrame=None,\n",
    "                 f_cols: list=None,\n",
    "                 S_df: pd.DataFrame=None,\n",
    "                 ts_train_mask: list=None,\n",
    "                 loss_weights: list=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        assert type(Y_df) == pd.core.frame.DataFrame\n",
    "        assert all([(col in Y_df) for col in ['unique_id', 'ds', 'y']])\n",
    "        if X_df is not None:\n",
    "            assert type(X_df) == pd.core.frame.DataFrame\n",
    "            assert all([(col in X_df) for col in ['unique_id', 'ds']])\n",
    "            assert f_cols is not None, \"Define f_cols\"\n",
    "\n",
    "        print('Processing dataframes ...')\n",
    "        # Pandas dataframes to data lists\n",
    "        ts_data, s_data, self.meta_data, self.t_cols, self.X_cols = self._df_to_lists(Y_df=Y_df, S_df=S_df, X_df=X_df)\n",
    "\n",
    "        # Dataset attributes\n",
    "        self.n_series   = len(ts_data)\n",
    "        self.max_len    = max([len(ts['y']) for ts in ts_data])\n",
    "        self.n_channels = len(self.t_cols) # y, X_cols, insample_mask and outsample_mask\n",
    "        self.frequency  = pd.infer_freq(Y_df.head()['ds']) #TODO: improve, can die with head\n",
    "        self.f_cols     = f_cols\n",
    "        \n",
    "        # Number of X and S features\n",
    "        self.n_x = 0 if X_df is None else len(self.X_cols)\n",
    "        self.n_s = 0 if S_df is None else S_df.shape[1]-1 # -1 for unique_id\n",
    "\n",
    "        print('Creating ts tensor ...')\n",
    "        # Balances panel and creates\n",
    "        # numpy  s_matrix of shape (n_series, n_s)\n",
    "        # numpy ts_tensor of shape (n_series, n_channels, max_len) n_channels = y + X_cols + masks\n",
    "        self.ts_tensor, self.s_matrix, self.len_series = self._create_tensor(ts_data, s_data)\n",
    "        if ts_train_mask is None: ts_train_mask = np.ones(self.max_len)\n",
    "        assert len(ts_train_mask)==self.max_len, f'Outsample mask must have {self.max_len} length'\n",
    "        self.ts_train_mask = ts_train_mask\n",
    "\n",
    "        if loss_weights is None: loss_weights = np.ones(self.max_len)\n",
    "        assert len(loss_weights)==self.max_len, f'Loss weights must have {self.max_len} length'\n",
    "        self.loss_weights = loss_weights\n",
    "\n",
    "    def _df_to_lists(self, Y_df, S_df, X_df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        unique_ids = Y_df['unique_id'].unique()\n",
    "\n",
    "        if X_df is not None:\n",
    "            X_cols = [col for col in X_df.columns if col not in ['unique_id','ds']]\n",
    "        else:\n",
    "            X_cols = []\n",
    "\n",
    "        if S_df is not None:\n",
    "            S_cols = [col for col in S_df.columns if col not in ['unique_id']]\n",
    "        else:\n",
    "            S_cols = []\n",
    "\n",
    "        ts_data = []\n",
    "        s_data = []\n",
    "        meta_data = []\n",
    "        for i, u_id in enumerate(unique_ids):\n",
    "            top_row = np.asscalar(Y_df['unique_id'].searchsorted(u_id, 'left'))\n",
    "            bottom_row = np.asscalar(Y_df['unique_id'].searchsorted(u_id, 'right'))\n",
    "            serie = Y_df[top_row:bottom_row]['y'].values\n",
    "            last_ds_i = Y_df[top_row:bottom_row]['ds'].max()\n",
    "\n",
    "            # Y values\n",
    "            ts_data_i = {'y': serie}\n",
    "\n",
    "            # X values\n",
    "            for X_col in X_cols:\n",
    "                serie =  X_df[top_row:bottom_row][X_col].values\n",
    "                ts_data_i[X_col] = serie\n",
    "            ts_data.append(ts_data_i)\n",
    "\n",
    "            # S values\n",
    "            s_data_i = defaultdict(list)\n",
    "            for S_col in S_cols:\n",
    "                s_data_i[S_col] = S_df.loc[S_df['unique_id']==u_id, S_col].values\n",
    "            s_data.append(s_data_i)\n",
    "\n",
    "            # Metadata\n",
    "            meta_data_i = {'unique_id': u_id,\n",
    "                           'last_ds': last_ds_i}\n",
    "            meta_data.append(meta_data_i)\n",
    "\n",
    "        assert X_cols[0] == 'ejecutado', 'First exogenous must be ejecutado'\n",
    "\n",
    "        t_cols = ['y'] + X_cols + ['insample_mask', 'outsample_mask']\n",
    "        X_cols = X_cols[1:] # First variable is ejecutado we skip it\n",
    "\n",
    "        return ts_data, s_data, meta_data, t_cols, X_cols\n",
    "\n",
    "    def _create_tensor(self, ts_data, s_data):\n",
    "        \"\"\"\n",
    "        s_matrix of shape (n_series, n_s)\n",
    "        ts_tensor of shape (n_series, n_channels, max_len) n_channels = y + X_cols + masks\n",
    "        \"\"\"\n",
    "        s_matrix  = np.zeros((self.n_series, self.n_s))\n",
    "        ts_tensor = np.zeros((self.n_series, self.n_channels, self.max_len))\n",
    "\n",
    "        len_series = []\n",
    "        for idx in range(self.n_series):\n",
    "            ts_idx = np.array(list(ts_data[idx].values()))\n",
    "            ts_tensor[idx, :self.t_cols.index('insample_mask'), -ts_idx.shape[1]:] = ts_idx\n",
    "            ts_tensor[idx,  self.t_cols.index('insample_mask'), -ts_idx.shape[1]:] = 1\n",
    "\n",
    "            # To avoid sampling windows without inputs available to predict we shift -1\n",
    "            # outsample_mask will be completed with the train_mask, this ensures available data\n",
    "            ts_tensor[idx,  self.t_cols.index('outsample_mask'), -(ts_idx.shape[1]-1):] = 1\n",
    "            s_matrix[idx, :] = list(s_data[idx].values())\n",
    "            len_series.append(ts_idx.shape[1])\n",
    "\n",
    "        return ts_tensor, s_matrix, np.array(len_series)\n",
    "\n",
    "    def get_meta_data_col(self, col):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        col_values = [x[col] for x in self.meta_data]\n",
    "        return col_values\n",
    "\n",
    "    def get_filtered_ts_tensor(self, offset, output_size, window_sampling_limit, ts_idxs=None):\n",
    "        \"\"\"\n",
    "        Esto te da todo lo que tenga el tensor, el futuro incluido esto orque se usa exogenoas del futuro\n",
    "        La mascara se hace despues\n",
    "        \"\"\"\n",
    "        last_outsample_ds = self.max_len - offset + output_size\n",
    "        first_ds = max(last_outsample_ds - window_sampling_limit - output_size, 0)\n",
    "        if ts_idxs is None:\n",
    "            filtered_ts_tensor = self.ts_tensor[:, :, first_ds:last_outsample_ds]\n",
    "        else:\n",
    "            filtered_ts_tensor = self.ts_tensor[ts_idxs, :, first_ds:last_outsample_ds]\n",
    "        right_padding = max(last_outsample_ds - self.max_len, 0) #To padd with zeros if there is \"nothing\" to the right\n",
    "        ts_train_mask = self.ts_train_mask[first_ds:last_outsample_ds]\n",
    "        loss_weights = self.loss_weights[first_ds:last_outsample_ds]\n",
    "\n",
    "        assert np.sum(np.isnan(filtered_ts_tensor))<1.0, \\\n",
    "            f'The balanced balanced filtered_tensor has {np.sum(np.isnan(filtered_ts_tensor))} nan values'\n",
    "        return filtered_ts_tensor, right_padding, ts_train_mask, loss_weights\n",
    "\n",
    "    def get_f_idxs(self, cols):\n",
    "        # Check if cols are available f_cols and return the idxs\n",
    "        assert all(col in self.f_cols for col in cols), f'Some variables in {cols} are not available in f_cols.'\n",
    "        f_idxs = [self.X_cols.index(col) for col in cols]\n",
    "        return f_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing dataframes ...\n",
      "Creating ts tensor ...\n",
      "dataset.ts_tensor.shape (1, 332, 104641)\n"
     ]
    }
   ],
   "source": [
    "from nixtla.data.datasets.on import load_on_data\n",
    "\n",
    "Y_insample_df, X_insample_df, Y_outsample_df, X_outsample_df, f_cols = load_on_data('2020-11-1')\n",
    "dataset = TimeSeriesDataset(Y_df=Y_insample_df, S_df=None, X_df=X_insample_df, ts_train_mask=np.ones(len(Y_insample_df)))\n",
    "print(\"dataset.ts_tensor.shape\", dataset.ts_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['y',\n",
       " 'ejecutado',\n",
       " 'DECIMAL_HOUR',\n",
       " 'DAY_OF_YEAR',\n",
       " 'DAY_OF_WEEK',\n",
       " 'IS_WEEKDAY',\n",
       " 'IS_HOLIDAY',\n",
       " 'L_1',\n",
       " 'L_2',\n",
       " 'L_3',\n",
       " 'L_4',\n",
       " 'L_5',\n",
       " 'L_6',\n",
       " 'L_7',\n",
       " 'PE_1',\n",
       " 'PD_1',\n",
       " 'PS_1',\n",
       " 'PE_7',\n",
       " 'PD_7',\n",
       " 'PS_7',\n",
       " 'PD_0',\n",
       " 'PS_0',\n",
       " 'P_1',\n",
       " 'P_2',\n",
       " 'P_3',\n",
       " 'P_4',\n",
       " 'P_5',\n",
       " 'P_6',\n",
       " 'P_7',\n",
       " 'AVG_1',\n",
       " 'AVG_2',\n",
       " 'AVG_3',\n",
       " 'AVG_4',\n",
       " 'AVG_5',\n",
       " 'AVG_6',\n",
       " 'AVG_7',\n",
       " 'MOV_7',\n",
       " 'MOV_14',\n",
       " 'DELTA_1',\n",
       " 'DELTA_7',\n",
       " 'PEAK_TIME_1',\n",
       " 'PEAK_TIME_2',\n",
       " 'PEAK_TIME_3',\n",
       " 'PEAK_TIME_4',\n",
       " 'PEAK_TIME_5',\n",
       " 'PEAK_TIME_6',\n",
       " 'PEAK_TIME_7',\n",
       " 'PE_PP1',\n",
       " 'PE_PP2',\n",
       " 'PE_PP3',\n",
       " 'PE_PP4',\n",
       " 'PE_PP5',\n",
       " 'PE_PP6',\n",
       " 'PE_PP7',\n",
       " 'PE_PP8',\n",
       " 'PE_PP9',\n",
       " 'PE_PP10',\n",
       " 'PE_PP11',\n",
       " 'PE_PP12',\n",
       " 'PE_PP13',\n",
       " 'PE_PP14',\n",
       " 'PE_PP15',\n",
       " 'PE_PP16',\n",
       " 'PE_PP17',\n",
       " 'PE_PP18',\n",
       " 'PE_PP19',\n",
       " 'PE_PP20',\n",
       " 'PE_PP21',\n",
       " 'PE_PP22',\n",
       " 'PE_PP23',\n",
       " 'PE_PP24',\n",
       " 'PE_PP25',\n",
       " 'PE_PP26',\n",
       " 'PE_PP27',\n",
       " 'PE_PP28',\n",
       " 'PE_PP29',\n",
       " 'PE_PP30',\n",
       " 'PE_PP31',\n",
       " 'PE_PP32',\n",
       " 'PE_PP33',\n",
       " 'PE_PP34',\n",
       " 'PE_PP35',\n",
       " 'PE_PP36',\n",
       " 'PE_PP37',\n",
       " 'PE_PP38',\n",
       " 'PE_PP39',\n",
       " 'PE_PP40',\n",
       " 'PE_PP41',\n",
       " 'PE_PP42',\n",
       " 'PE_PP43',\n",
       " 'PE_PP44',\n",
       " 'PE_PP45',\n",
       " 'PE_PP46',\n",
       " 'PE_PP47',\n",
       " 'PE_PP48',\n",
       " 'PD_PP1',\n",
       " 'PD_PP2',\n",
       " 'PD_PP3',\n",
       " 'PD_PP4',\n",
       " 'PD_PP5',\n",
       " 'PD_PP6',\n",
       " 'PD_PP7',\n",
       " 'PD_PP8',\n",
       " 'PD_PP9',\n",
       " 'PD_PP10',\n",
       " 'PD_PP11',\n",
       " 'PD_PP12',\n",
       " 'PD_PP13',\n",
       " 'PD_PP14',\n",
       " 'PD_PP15',\n",
       " 'PD_PP16',\n",
       " 'PD_PP17',\n",
       " 'PD_PP18',\n",
       " 'PD_PP19',\n",
       " 'PD_PP20',\n",
       " 'PD_PP21',\n",
       " 'PD_PP22',\n",
       " 'PD_PP23',\n",
       " 'PD_PP24',\n",
       " 'PD_PP25',\n",
       " 'PD_PP26',\n",
       " 'PD_PP27',\n",
       " 'PD_PP28',\n",
       " 'PD_PP29',\n",
       " 'PD_PP30',\n",
       " 'PD_PP31',\n",
       " 'PD_PP32',\n",
       " 'PD_PP33',\n",
       " 'PD_PP34',\n",
       " 'PD_PP35',\n",
       " 'PD_PP36',\n",
       " 'PD_PP37',\n",
       " 'PD_PP38',\n",
       " 'PD_PP39',\n",
       " 'PD_PP40',\n",
       " 'PD_PP41',\n",
       " 'PD_PP42',\n",
       " 'PD_PP43',\n",
       " 'PD_PP44',\n",
       " 'PD_PP45',\n",
       " 'PD_PP46',\n",
       " 'PD_PP47',\n",
       " 'PD_PP48',\n",
       " 'PS_PP1',\n",
       " 'PS_PP2',\n",
       " 'PS_PP3',\n",
       " 'PS_PP4',\n",
       " 'PS_PP5',\n",
       " 'PS_PP6',\n",
       " 'PS_PP7',\n",
       " 'PS_PP8',\n",
       " 'PS_PP9',\n",
       " 'PS_PP10',\n",
       " 'PS_PP11',\n",
       " 'PS_PP12',\n",
       " 'PS_PP13',\n",
       " 'PS_PP14',\n",
       " 'PS_PP15',\n",
       " 'PS_PP16',\n",
       " 'PS_PP17',\n",
       " 'PS_PP18',\n",
       " 'PS_PP19',\n",
       " 'PS_PP20',\n",
       " 'PS_PP21',\n",
       " 'PS_PP22',\n",
       " 'PS_PP23',\n",
       " 'PS_PP24',\n",
       " 'PS_PP25',\n",
       " 'PS_PP26',\n",
       " 'PS_PP27',\n",
       " 'PS_PP28',\n",
       " 'PS_PP29',\n",
       " 'PS_PP30',\n",
       " 'PS_PP31',\n",
       " 'PS_PP32',\n",
       " 'PS_PP33',\n",
       " 'PS_PP34',\n",
       " 'PS_PP35',\n",
       " 'PS_PP36',\n",
       " 'PS_PP37',\n",
       " 'PS_PP38',\n",
       " 'PS_PP39',\n",
       " 'PS_PP40',\n",
       " 'PS_PP41',\n",
       " 'PS_PP42',\n",
       " 'PS_PP43',\n",
       " 'PS_PP44',\n",
       " 'PS_PP45',\n",
       " 'PS_PP46',\n",
       " 'PS_PP47',\n",
       " 'PS_PP48',\n",
       " 'DELTA_PP1',\n",
       " 'DELTA_PP2',\n",
       " 'DELTA_PP3',\n",
       " 'DELTA_PP4',\n",
       " 'DELTA_PP5',\n",
       " 'DELTA_PP6',\n",
       " 'DELTA_PP7',\n",
       " 'DELTA_PP8',\n",
       " 'DELTA_PP9',\n",
       " 'DELTA_PP10',\n",
       " 'DELTA_PP11',\n",
       " 'DELTA_PP12',\n",
       " 'DELTA_PP13',\n",
       " 'DELTA_PP14',\n",
       " 'DELTA_PP15',\n",
       " 'DELTA_PP16',\n",
       " 'DELTA_PP17',\n",
       " 'DELTA_PP18',\n",
       " 'DELTA_PP19',\n",
       " 'DELTA_PP20',\n",
       " 'DELTA_PP21',\n",
       " 'DELTA_PP22',\n",
       " 'DELTA_PP23',\n",
       " 'DELTA_PP24',\n",
       " 'DELTA_PP25',\n",
       " 'DELTA_PP26',\n",
       " 'DELTA_PP27',\n",
       " 'DELTA_PP28',\n",
       " 'DELTA_PP29',\n",
       " 'DELTA_PP30',\n",
       " 'DELTA_PP31',\n",
       " 'DELTA_PP32',\n",
       " 'DELTA_PP33',\n",
       " 'DELTA_PP34',\n",
       " 'DELTA_PP35',\n",
       " 'DELTA_PP36',\n",
       " 'DELTA_PP37',\n",
       " 'DELTA_PP38',\n",
       " 'DELTA_PP39',\n",
       " 'DELTA_PP40',\n",
       " 'DELTA_PP41',\n",
       " 'DELTA_PP42',\n",
       " 'DELTA_PP43',\n",
       " 'DELTA_PP44',\n",
       " 'DELTA_PP45',\n",
       " 'DELTA_PP46',\n",
       " 'DELTA_PP47',\n",
       " 'DELTA_PP48',\n",
       " 'temp_Ancash',\n",
       " 'temp_Arequipa',\n",
       " 'temp_Cusco',\n",
       " 'temp_Ica',\n",
       " 'temp_Lima',\n",
       " 'feels_like_Ancash',\n",
       " 'feels_like_Arequipa',\n",
       " 'feels_like_Cusco',\n",
       " 'feels_like_Ica',\n",
       " 'feels_like_Lima',\n",
       " 'rain_1h_Ancash',\n",
       " 'rain_1h_Arequipa',\n",
       " 'rain_1h_Cusco',\n",
       " 'rain_1h_Ica',\n",
       " 'rain_1h_Lima',\n",
       " 'pressure_Ancash',\n",
       " 'pressure_Arequipa',\n",
       " 'pressure_Cusco',\n",
       " 'pressure_Ica',\n",
       " 'pressure_Lima',\n",
       " 'humidity_Ancash',\n",
       " 'humidity_Arequipa',\n",
       " 'humidity_Cusco',\n",
       " 'humidity_Ica',\n",
       " 'humidity_Lima',\n",
       " 'wind_speed_Ancash',\n",
       " 'wind_speed_Arequipa',\n",
       " 'wind_speed_Cusco',\n",
       " 'wind_speed_Ica',\n",
       " 'wind_speed_Lima',\n",
       " 'clouds_all_Ancash',\n",
       " 'clouds_all_Arequipa',\n",
       " 'clouds_all_Cusco',\n",
       " 'clouds_all_Ica',\n",
       " 'clouds_all_Lima',\n",
       " 'temperature_0',\n",
       " 'temperature_1',\n",
       " 'temperature_2',\n",
       " 'temperature_3',\n",
       " 'temperature_4',\n",
       " 'pressure_0',\n",
       " 'pressure_1',\n",
       " 'pressure_2',\n",
       " 'pressure_3',\n",
       " 'pressure_4',\n",
       " 'humidity_0',\n",
       " 'humidity_1',\n",
       " 'humidity_2',\n",
       " 'humidity_3',\n",
       " 'humidity_4',\n",
       " 'wind_speed_0',\n",
       " 'wind_speed_1',\n",
       " 'wind_speed_2',\n",
       " 'wind_speed_3',\n",
       " 'wind_speed_4',\n",
       " 'rain_0',\n",
       " 'rain_1',\n",
       " 'rain_2',\n",
       " 'rain_3',\n",
       " 'rain_4',\n",
       " 'hour_0',\n",
       " 'hour_1',\n",
       " 'hour_2',\n",
       " 'hour_3',\n",
       " 'hour_4',\n",
       " 'hour_5',\n",
       " 'hour_6',\n",
       " 'hour_7',\n",
       " 'hour_8',\n",
       " 'hour_9',\n",
       " 'hour_10',\n",
       " 'hour_11',\n",
       " 'hour_12',\n",
       " 'hour_13',\n",
       " 'hour_14',\n",
       " 'hour_15',\n",
       " 'hour_16',\n",
       " 'hour_17',\n",
       " 'hour_18',\n",
       " 'hour_19',\n",
       " 'hour_20',\n",
       " 'hour_21',\n",
       " 'hour_22',\n",
       " 'hour_23',\n",
       " 'day_0',\n",
       " 'day_1',\n",
       " 'day_2',\n",
       " 'day_3',\n",
       " 'day_4',\n",
       " 'day_5',\n",
       " 'day_6',\n",
       " 'insample_mask',\n",
       " 'outsample_mask']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "dataset.t_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "dataset.ts_train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([5545.98338, 5448.66753, 5392.92989, ..., 5886.83837, 5825.31859,\n",
       "       5730.61466])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "dataset.ts_tensor[0, dataset.t_cols.index('y'), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[[6.44549894e+03, 6.42243587e+03, 6.39460725e+03, ...,\n",
       "          5.88683837e+03, 5.82531859e+03, 5.73061466e+03],\n",
       "         [6.42098288e+03, 6.41277336e+03, 6.41277336e+03, ...,\n",
       "          5.90676623e+03, 5.90676623e+03, 5.79786651e+03],\n",
       "         [1.27500000e+01, 1.30000000e+01, 1.32500000e+01, ...,\n",
       "          2.35000000e+01, 2.37500000e+01, 0.00000000e+00],\n",
       "         ...,\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "          1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "          1.00000000e+00, 1.00000000e+00, 1.00000000e+00]]]),\n",
       " 2,\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1.]))"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "dataset.get_filtered_ts_tensor(offset=10, output_size=12, window_sampling_limit=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}