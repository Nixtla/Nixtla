{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "52b0028e7074a0058398558ea661882eddbb489e66a28504cc0449d2f54d6290"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiments.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import time\n",
    "import os\n",
    "# Limit number of threads in numpy and others to avoid throttling\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\" # export OMP_NUM_THREADS=4\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"2\" # export OPENBLAS_NUM_THREADS=4 \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"3\" # export MKL_NUM_THREADS=6\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"2\" # export VECLIB_MAXIMUM_THREADS=4\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"3\" # export NUMEXPR_NUM_THREADS=6\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import pickle\n",
    "import glob\n",
    "import itertools\n",
    "import random\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "from nixtla.data.scalers import Scaler\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset\n",
    "from nixtla.data.tsloader_fast import TimeSeriesLoader\n",
    "from nixtla.losses.numpy import mae, mape, smape, rmse, pinball_loss\n",
    "\n",
    "# Models\n",
    "from nixtla.models.nbeats.nbeats import Nbeats\n",
    "from nixtla.models.esrnn.esrnn import ESRNN\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mask_df(Y_df, timestamps_in_outsample):\n",
    "    # Creates outsample_mask\n",
    "    # train 1 validation 0\n",
    "\n",
    "    last_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    last_df.sort_values(by=['unique_id', 'ds'], inplace=True, ascending=False)\n",
    "    last_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    last_df = last_df.groupby('unique_id').head(timestamps_in_outsample)\n",
    "    last_df['sample_mask'] = 0\n",
    "\n",
    "    last_df = last_df[['unique_id', 'ds', 'sample_mask']]\n",
    "\n",
    "    mask_df = Y_df.merge(last_df, on=['unique_id', 'ds'], how='left')\n",
    "    mask_df['sample_mask'] = mask_df['sample_mask'].fillna(1)    # The first len(Y)-n_hours used as train\n",
    "\n",
    "    mask_df = mask_df[['unique_id', 'ds', 'sample_mask']]\n",
    "    mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "    mask_df['available_mask'] = 1\n",
    "\n",
    "    assert len(mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "\n",
    "    return mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(Y_df, X_df, mask_df, normalizer_y, normalizer_x):\n",
    "    y_shift = None\n",
    "    y_scale = None\n",
    "\n",
    "    # mask = mask.astype(int)\n",
    "    mask = mask_df['available_mask'].values * mask_df['sample_mask'].values\n",
    "    \n",
    "    if normalizer_y is not None:\n",
    "        scaler_y = Scaler(normalizer=normalizer_y)\n",
    "        Y_df['y'] = scaler_y.scale(x=Y_df['y'].values, mask=mask)\n",
    "    else:\n",
    "        scaler_y = None\n",
    "    # Exogenous are always scaled to help learning\n",
    "    if normalizer_x is not None:\n",
    "        scaler_x = Scaler(normalizer=normalizer_x)\n",
    "        X_df['Exogenous1'] = scaler_x.scale(x=X_df['Exogenous1'].values, mask=mask)\n",
    "\n",
    "        scaler_x = Scaler(normalizer=normalizer_x)\n",
    "        X_df['Exogenous2'] = scaler_x.scale(x=X_df['Exogenous2'].values, mask=mask)\n",
    "\n",
    "    filter_variables = ['unique_id', 'ds', 'Exogenous1', 'Exogenous2', 'week_day'] + \\\n",
    "                       [col for col in X_df if (col.startswith('day'))]\n",
    "                       #[col for col in X_df if (col.startswith('_hour_'))]\n",
    "    X_df = X_df[filter_variables]\n",
    "\n",
    "    return Y_df, X_df, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def train_val_split(len_series, offset, window_sampling_limit, n_val_weeks, ds_per_day):\n",
    "    last_ds = len_series - offset\n",
    "    first_ds = max(last_ds - window_sampling_limit, 0)\n",
    "\n",
    "    last_day = int(last_ds/ds_per_day)\n",
    "    first_day = int(first_ds/ds_per_day)\n",
    "\n",
    "    days = set(range(first_day, last_day)) # All days, to later get train days\n",
    "    # Sample weeks from here, -7 to avoid sampling from last week\n",
    "    # To not sample first week and have inputs\n",
    "    sampling_days = set(range(first_day + 7, last_day - 7))\n",
    "    validation_days = set({}) # Val days set\n",
    "    \n",
    "    # For loop for n of weeks in validation\n",
    "    for i in range(n_val_weeks):\n",
    "        # Sample random day, init of week\n",
    "        init_day = random.sample(sampling_days, 1)[0]\n",
    "        # Select days of sampled init of week\n",
    "        sampled_days = list(range(init_day, min(init_day+7, last_day)))\n",
    "        # Add days to validation days\n",
    "        validation_days.update(sampled_days)\n",
    "        # Remove days from sampling_days, including overlapping resulting previous week\n",
    "        days_to_remove = set(range(init_day-6, min(init_day+7, last_day)))\n",
    "        sampling_days = sampling_days.difference(days_to_remove)\n",
    "\n",
    "    train_days = days.difference(validation_days)\n",
    "\n",
    "    train_days = sorted(list(train_days))\n",
    "    validation_days = sorted(list(validation_days))\n",
    "\n",
    "    train_idx = []\n",
    "    for day in train_days:\n",
    "        hours_idx = range(day*ds_per_day,(day+1)*ds_per_day)\n",
    "        train_idx += hours_idx\n",
    "\n",
    "    val_idx = []\n",
    "    for day in validation_days:\n",
    "        hours_idx = range(day*ds_per_day,(day+1)*ds_per_day)\n",
    "        val_idx += hours_idx\n",
    "\n",
    "    assert all([idx < last_ds for idx in val_idx]), 'Leakage!!!!'\n",
    "    \n",
    "    return train_idx, val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def prepare_dataset(mc, Y_df, X_df, S_df, timestamps_in_outsample, shuffle_outsample, offset):\n",
    "    #TODO: offset not implemented\n",
    "    #TODO: shuffle_outsample\n",
    "\n",
    "    # n_timestamps_pred defines number of hours ahead to predict\n",
    "    # offset defines the shift of the data to simulate rolling window\n",
    "    # assert offset % n_timestamps_pred == 0, 'Avoid overlap of predictions, redefine n_timestamps_pred or offset' <-- restriccion poco general\n",
    "\n",
    "    #------------------------------------- Available and Validation Mask ------------------------------------#\n",
    "    # mask: 1 last_n_timestamps, 0 timestamps until last_n_timestamps\n",
    "    train_mask_df = get_mask_df(Y_df=Y_df, timestamps_in_outsample=timestamps_in_outsample)\n",
    "    outsample_mask_df = train_mask_df.copy()\n",
    "    outsample_mask_df['sample_mask'] = 1 - outsample_mask_df['sample_mask']\n",
    "\n",
    "    #---------------------------------------------- Scale Data ----------------------------------------------#\n",
    "    # Scale data # TODO: write sample_mask conditional/groupby(['unique_id]) scaling\n",
    "    Y_df, X_df, scaler_y = scale_data(Y_df=Y_df, X_df=X_df, mask_df=train_mask_df,\n",
    "                                                    normalizer_y=mc['normalizer_y'], normalizer_x=mc['normalizer_x'])\n",
    "\n",
    "    #----------------------------------------- Declare Dataset and Loaders ----------------------------------#\n",
    "    train_ts_dataset = TimeSeriesDataset(Y_df=Y_df, X_df=X_df, S_df=S_df, mask_df=train_mask_df)\n",
    "    if timestamps_in_outsample == 0:\n",
    "        outsample_ts_dataset = None\n",
    "    else:\n",
    "        outsample_ts_dataset = TimeSeriesDataset(Y_df=Y_df, X_df=X_df, S_df=S_df, mask_df=outsample_mask_df)\n",
    "\n",
    "    mc['t_cols'] = train_ts_dataset.t_cols\n",
    "    return mc, train_ts_dataset, outsample_ts_dataset, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_nbeats(mc, train_ts_dataset, outsample_ts_dataset, scaler_y):\n",
    "    #TODO: window_sampling_limit interaccion con offset\n",
    "    #TODO: idx_to_sample_freq de validation??\n",
    "    #TODO: que time_series_loader???\n",
    "    #TODO: n_hidden e include_var_dict cambiar por parser\n",
    "\n",
    "    mc['n_hidden'] = len(mc['stack_types']) * [ [int(mc['n_hidden']), int(mc['n_hidden'])] ]\n",
    "\n",
    "    include_var_dict = {'y': [],\n",
    "                    'Exogenous1': [],\n",
    "                    'Exogenous2': [],\n",
    "                    'week_day': []}\n",
    "\n",
    "    if mc['incl_pr1']: include_var_dict['y'].append(-2)\n",
    "    if mc['incl_pr2']: include_var_dict['y'].append(-3)\n",
    "    if mc['incl_pr3']: include_var_dict['y'].append(-4)\n",
    "    if mc['incl_pr7']: include_var_dict['y'].append(-8)\n",
    "        \n",
    "    if mc['incl_ex1_0']: include_var_dict['Exogenous1'].append(-1)\n",
    "    if mc['incl_ex1_1']: include_var_dict['Exogenous1'].append(-2)\n",
    "    if mc['incl_ex1_7']: include_var_dict['Exogenous1'].append(-8)\n",
    "        \n",
    "    if mc['incl_ex2_0']: include_var_dict['Exogenous2'].append(-1)\n",
    "    if mc['incl_ex2_1']: include_var_dict['Exogenous2'].append(-2)\n",
    "    if mc['incl_ex2_7']: include_var_dict['Exogenous2'].append(-8)\n",
    "\n",
    "    if mc['incl_day']: include_var_dict['week_day'].append(-1)\n",
    "\n",
    "    mc['include_var_dict'] = include_var_dict\n",
    "\n",
    "    train_ts_loader = TimeSeriesLoader(ts_dataset=train_ts_dataset,\n",
    "                                       model='nbeats',\n",
    "                                       offset=0,\n",
    "                                       window_sampling_limit=int(mc['window_sampling_limit']),\n",
    "                                       input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                                       output_size=int(mc['output_size']),\n",
    "                                       idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                       batch_size=int(mc['batch_size']),\n",
    "                                       complete_inputs=mc['complete_inputs'],\n",
    "                                       complete_sample=False,\n",
    "                                       shuffle=True)\n",
    "\n",
    "    if outsample_ts_dataset is not None:\n",
    "        val_ts_loader = TimeSeriesLoader(ts_dataset=outsample_ts_dataset,\n",
    "                                        model='nbeats',\n",
    "                                        offset=0,\n",
    "                                        window_sampling_limit=int(mc['window_sampling_limit']),\n",
    "                                        input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                                        output_size=int(mc['output_size']),\n",
    "                                        idx_to_sample_freq=24,\n",
    "                                        batch_size=int(mc['batch_size']),\n",
    "                                        complete_inputs=False,\n",
    "                                        complete_sample=False,\n",
    "                                        shuffle=False)\n",
    "    else:\n",
    "        val_ts_loader = None\n",
    "    \n",
    "    model = Nbeats(input_size_multiplier=mc['input_size_multiplier'],\n",
    "                    output_size=int(mc['output_size']),\n",
    "                    shared_weights=mc['shared_weights'],\n",
    "                    initialization=mc['initialization'],\n",
    "                    activation=mc['activation'],\n",
    "                    stack_types=mc['stack_types'],\n",
    "                    n_blocks=mc['n_blocks'],\n",
    "                    n_layers=mc['n_layers'],\n",
    "                    n_hidden=mc['n_hidden'],\n",
    "                    n_harmonics=int(mc['n_harmonics']),\n",
    "                    n_polynomials=int(mc['n_polynomials']),\n",
    "                    x_s_n_hidden=int(mc['x_s_n_hidden']),\n",
    "                    exogenous_n_channels=int(mc['exogenous_n_channels']),\n",
    "                    include_var_dict=mc['include_var_dict'],\n",
    "                    t_cols=mc['t_cols'],\n",
    "                    batch_normalization = mc['batch_normalization'],\n",
    "                    dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                    dropout_prob_exogenous=mc['dropout_prob_exogenous'],\n",
    "                    learning_rate=float(mc['learning_rate']),\n",
    "                    lr_decay=float(mc['lr_decay']),\n",
    "                    n_lr_decay_steps=float(mc['n_lr_decay_steps']),\n",
    "                    weight_decay=mc['weight_decay'],\n",
    "                    l1_theta=mc['l1_theta'],\n",
    "                    n_iterations=int(mc['n_iterations']),\n",
    "                    early_stopping=int(mc['early_stopping']),\n",
    "                    #scaler_y=scaler_y,\n",
    "                    loss=mc['loss'],\n",
    "                    loss_hypar=float(mc['loss_hypar']),\n",
    "                    val_loss=mc['val_loss'],\n",
    "                    frequency=mc['frequency'],\n",
    "                    seasonality=int(mc['seasonality']),\n",
    "                    random_seed=int(mc['random_seed']))\n",
    "\n",
    "    return train_ts_loader, val_ts_loader, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def model_fit_predict(mc, Y_df, X_df, S_df, timestamps_in_outsample, shuffle_outsample, offsets):\n",
    "    #TODO: pensar si mejorar for loop\n",
    "    #TODO: no me convence la funcion como esta, hace demasiado, tal vez separar diferente\n",
    "    \n",
    "    X_df = X_df.copy()\n",
    "    Y_df = Y_df.copy()\n",
    "\n",
    "    #-------------------------------------- Rolling prediction on outsample --------------------------------------#\n",
    "    y_true = []\n",
    "    y_hat = []\n",
    "    mask = []\n",
    "    n_splits = len(offsets)\n",
    "    for split, offset in enumerate(offsets):\n",
    "        print(10*'-', f'Split {split+1}/{n_splits}', 10*'-')\n",
    "\n",
    "        #----------------------------------------------- Datasets -----------------------------------------------#\n",
    "        #TODO: offset verdadero no hecho, hackeado por fuera\n",
    "        Y_split_df = Y_df.head(len(Y_df) - offset)\n",
    "        X_split_df = X_df.head(len(X_df) - offset)\n",
    "        mc, train_ts_dataset, outsample_ts_dataset, scaler_y = prepare_dataset(mc=mc, Y_df=Y_split_df, X_df=X_split_df,\n",
    "                                                                               S_df=S_df,\n",
    "                                                                               timestamps_in_outsample=timestamps_in_outsample,\n",
    "                                                                               shuffle_outsample=shuffle_outsample,\n",
    "                                                                               offset=offset)\n",
    "\n",
    "        #--------------------------------------- Instantiate, fit, predict ---------------------------------------#\n",
    "        train_ts_loader, val_ts_loader, model = instantiate_nbeats(mc=mc, train_ts_dataset=train_ts_dataset,\n",
    "                                                                   outsample_ts_dataset=outsample_ts_dataset,\n",
    "                                                                   scaler_y=scaler_y)\n",
    "\n",
    "        model.fit(train_ts_loader=train_ts_loader, val_ts_loader=val_ts_loader, eval_steps=mc['eval_steps'])\n",
    "\n",
    "        y_true_split, y_hat_split, mask_split = model.predict(ts_loader=val_ts_loader, eval_mode=True)\n",
    "\n",
    "        y_true.append(y_true_split)\n",
    "        y_hat.append(y_hat_split)\n",
    "        mask.append(mask_split)\n",
    "\n",
    "    y_true = np.vstack(y_true)\n",
    "    y_hat = np.vstack(y_hat)\n",
    "    mask = np.vstack(mask)\n",
    "\n",
    "    print(f'y_true.shape (#n_windows, #lt) {y_true.shape}')\n",
    "    print(f'y_hat.shape (#n_windows, #lt) {y_hat.shape}')\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Reshape for univariate and panel model compatibility\n",
    "    n_series = train_ts_loader.ts_dataset.n_series\n",
    "    n_fcds = len(y_true) // n_series\n",
    "    y_true = y_true.reshape(n_series, n_fcds, mc['output_size'])\n",
    "    y_hat = y_hat.reshape(n_series, n_fcds, mc['output_size'])\n",
    "\n",
    "    print(\"y_true.shape (#n_series, #n_fcds, #lt) \", y_true.shape)\n",
    "    print(\"y_hat.shape (#n_series, #n_fcds, #lt) \", y_hat.shape)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    meta_data = val_ts_loader.ts_dataset.meta_data\n",
    "\n",
    "    return y_true, y_hat, mask, meta_data, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def evaluate_model(mc, loss_function, Y_df, X_df, S_df, timestamps_in_outsample, shuffle_outsample, offsets):\n",
    "    #TODO: mask shape esta mal\n",
    "\n",
    "    # Make predictions\n",
    "    start = time.time()\n",
    "    y_true, y_hat, mask, meta_data, model = model_fit_predict(mc, Y_df, X_df, S_df, timestamps_in_outsample, shuffle_outsample, offsets)\n",
    "    run_time = time.time() - start\n",
    "    # Evaluate predictions\n",
    "    loss = loss_function(y=y_true, y_hat=y_hat) #weights=mask\n",
    "\n",
    "    result =  {'loss': loss,\n",
    "                'mc': mc,\n",
    "                'trajectories': model.trajectories,\n",
    "                'run_time': run_time,\n",
    "                'status': STATUS_OK}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def hyperopt_tunning(space, hyperopt_iters, loss_function, Y_df, X_df, S_df, timestamps_in_outsample, shuffle_outsample, offsets, save_trials=False):\n",
    "    #TODO: mc parser!!!!!!\n",
    "\n",
    "    trials = Trials()\n",
    "    fmin_objective = partial(evaluate_model, loss_function=loss_function, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "                             timestamps_in_outsample=timestamps_in_outsample,\n",
    "                             shuffle_outsample=shuffle_outsample, offsets=offsets)\n",
    "\n",
    "    fmin(fmin_objective, space=space, algo=tpe.suggest, max_evals=hyperopt_iters, trials=trials, verbose=True)\n",
    "\n",
    "    return trials"
   ]
  },
  {
   "source": [
    "# TESTS (POR AHORA)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= {# Architecture parameters\n",
    "        'input_size_multiplier': hp.choice('input_size_multiplier', [7]),\n",
    "        'output_size': hp.choice('output_size', [24]),\n",
    "        'shared_weights': hp.choice('shared_weights', [False]),\n",
    "        'activation': hp.choice('activation', ['selu']),\n",
    "        'initialization':  hp.choice('initialization', ['glorot_normal','he_normal']),\n",
    "        'stack_types': hp.choice('stack_types', [2*['identity'],\n",
    "                                                    1*['identity']+1*['exogenous_tcn'],\n",
    "                                                    1*['exogenous_tcn']+1*['identity'] ]),\n",
    "        'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),\n",
    "        'n_layers': hp.choice('n_layers', [ [2, 2] ]),\n",
    "        'n_hidden': hp.choice('n_hidden', [ 364 ]),\n",
    "        'n_harmonics': hp.choice('n_harmonics', [1]),\n",
    "        'n_polynomials': hp.choice('n_polynomials', [2]),\n",
    "        'exogenous_n_channels': hp.quniform('exogenous_n_channels', 1, 10, 1),\n",
    "        'x_s_n_hidden': hp.choice('x_s_n_hidden', [0]),\n",
    "        # Regularization and optimization parameters\n",
    "        'batch_normalization': hp.choice('batch_normalization', [False]),\n",
    "        'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 0.5),\n",
    "        'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.001)),\n",
    "        'lr_decay': hp.uniform('lr_decay', 0.3, 0.5),\n",
    "        'n_lr_decay_steps': hp.choice('n_lr_decay_steps', [3]),\n",
    "        'weight_decay': hp.loguniform('weight_decay', np.log(5e-5), np.log(5e-3)),\n",
    "        'n_iterations': hp.choice('n_iterations', [100]), #[args.max_epochs]),\n",
    "        'early_stopping': hp.choice('early_stopping', [16]),\n",
    "        'eval_steps': hp.choice('eval_steps', [50]),\n",
    "        'n_val_weeks': hp.choice('n_val_weeks', [52*2]),\n",
    "        'loss': hp.choice('loss', ['MAE']),\n",
    "        'loss_hypar': hp.choice('loss_hypar', [0.5]),                \n",
    "        'val_loss': hp.choice('val_loss', ['MAE']), #[args.val_loss]),\n",
    "        'l1_theta': hp.choice('l1_theta', [0]),\n",
    "        # Data parameters\n",
    "        'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "        'normalizer_x': hp.choice('normalizer_x', ['median']),\n",
    "        'window_sampling_limit': hp.choice('window_sampling_limit', [50_000]),\n",
    "        'complete_inputs': hp.choice('complete_inputs', [True]),                \n",
    "        'frequency': hp.choice('frequency', ['H']),\n",
    "        'seasonality': hp.choice('seasonality', [24]),      \n",
    "        'incl_pr1': hp.choice('incl_pr1', [True]),\n",
    "        'incl_pr2': hp.choice('incl_pr2', [True, False]),\n",
    "        'incl_pr3': hp.choice('incl_pr3', [True, False]),\n",
    "        'incl_pr7': hp.choice('incl_pr7', [True, False]),\n",
    "        'incl_ex1_0': hp.choice('incl_ex1_0', [True, False]),\n",
    "        'incl_ex1_1': hp.choice('incl_ex1_1', [True, False]),\n",
    "        'incl_ex1_7': hp.choice('incl_ex1_7', [True, False]),\n",
    "        'incl_ex2_0': hp.choice('incl_ex2_0', [True, False]),\n",
    "        'incl_ex2_1': hp.choice('incl_ex2_1', [True, False]),\n",
    "        'incl_ex2_7': hp.choice('incl_ex2_7', [True, False]),\n",
    "        'incl_day': hp.choice('incl_day', [True, False]),\n",
    "        'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [24]),\n",
    "        'batch_size': hp.choice('batch_size', [256]),\n",
    "        'random_seed': hp.quniform('random_seed', 10, 20, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = {# Architecture parameters\n",
    "      'input_size_multiplier': 7,\n",
    "      'output_size': 24,\n",
    "      'shared_weights': False,\n",
    "      'activation': 'selu',\n",
    "      'initialization': 'he_normal',\n",
    "      'stack_types': ['exogenous_tcn']+1*['identity'],\n",
    "      'n_blocks': [1, 1],\n",
    "      'n_layers': [2, 2],\n",
    "      'n_hidden': 364,\n",
    "      'n_polynomials': 2,\n",
    "      'n_harmonics': 1,\n",
    "      'exogenous_n_channels': 3,\n",
    "      'x_s_n_hidden': 0,\n",
    "      # Regularization and optimization parameters\n",
    "      'batch_normalization': False,\n",
    "      'dropout_prob_theta': 0.2,\n",
    "      'dropout_prob_exogenous': 0.2,\n",
    "      'learning_rate': 0.0005, #0.002,\n",
    "      'lr_decay': 0.64,\n",
    "      'n_lr_decay_steps': 3,\n",
    "      'weight_decay': 0.00015,\n",
    "      'n_iterations': 100,\n",
    "      'early_stopping': 8,\n",
    "      'eval_steps': 50,\n",
    "      'n_val_weeks': 52*2,\n",
    "      'loss': 'PINBALL',\n",
    "      'loss_hypar': 0.5, #0.49,\n",
    "      'val_loss': 'MAE',\n",
    "      'l1_theta': 0,\n",
    "      # Data parameters\n",
    "      'normalizer_y': None,\n",
    "      'normalizer_x': 'median',\n",
    "      'window_sampling_limit': 50_000,\n",
    "      'complete_inputs': True,\n",
    "      'frequency':'H',\n",
    "      'seasonality': 24,\n",
    "      'idx_to_sample_freq': 24,\n",
    "      'incl_pr1': True,\n",
    "      'incl_pr2': True,\n",
    "      'incl_pr3': True,\n",
    "      'incl_pr7': True,\n",
    "      'incl_ex1_0': True,\n",
    "      'incl_ex1_1': True,\n",
    "      'incl_ex1_7': True,\n",
    "      'incl_ex2_0': True,\n",
    "      'incl_ex2_1': True,\n",
    "      'incl_ex2_7': True,\n",
    "      'incl_day': True,\n",
    "      'batch_size': 256,\n",
    "      'random_seed': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = ['NP']\n",
    "\n",
    "Y_df, X_df, S_df = EPF.load_groups(directory='data', groups=dataset)\n",
    "\n",
    "plt.plot(Y_df.y.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate_model(loss_function=mae, mc=mc, Y_df=Y_df, X_df=X_df, S_df=S_df, timestamps_in_outsample=728*24,\n",
    "                        shuffle_outsample=False, offsets=[0])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperopt_tunning(space=space, hyperopt_iters=3, loss_function=mae, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "                 timestamps_in_outsample=728*24, shuffle_outsample=False, offsets=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}