{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiments.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "ENV_VARS = dict(OMP_NUM_THREADS='2',\n",
    "                OPENBLAS_NUM_THREADS='2',\n",
    "                MKL_NUM_THREADS='3',\n",
    "                VECLIB_MAXIMUM_THREADS='2',\n",
    "                NUMEXPR_NUM_THREADS='3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "# Limit number of threads in numpy and others to avoid throttling\n",
    "os.environ.update(ENV_VARS)\n",
    "import random\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "from nixtla.data.scalers import Scaler\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset\n",
    "from nixtla.data.tsloader_general import TimeSeriesLoader\n",
    "from nixtla.models.esrnn.esrnn import ESRNN\n",
    "from nixtla.models.esrnn.mqesrnn import MQESRNN\n",
    "from nixtla.models.esrnn.rnn import RNN\n",
    "from nixtla.models.nbeats.nbeats import Nbeats\n",
    "from nixtla.models.tcn.tcn import TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_default_mask_df(Y_df, ds_in_test, is_test):\n",
    "    # Creates outsample_mask\n",
    "    # train 1 validation 0\n",
    "    last_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    last_df.sort_values(by=['unique_id', 'ds'], inplace=True, ascending=False)\n",
    "    last_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    last_df = last_df.groupby('unique_id').head(ds_in_test)\n",
    "    last_df['sample_mask'] = 0\n",
    "\n",
    "    last_df = last_df[['unique_id', 'ds', 'sample_mask']]\n",
    "\n",
    "    mask_df = Y_df.merge(last_df, on=['unique_id', 'ds'], how='left')\n",
    "    mask_df['sample_mask'] = mask_df['sample_mask'].fillna(1)\n",
    "\n",
    "    mask_df = mask_df[['unique_id', 'ds', 'sample_mask']]\n",
    "    mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "    mask_df['available_mask'] = 1\n",
    "\n",
    "    assert len(mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "\n",
    "    if is_test:\n",
    "        mask_df['sample_mask'] = 1 - mask_df['sample_mask']\n",
    "\n",
    "    return mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def scale_data(Y_df, X_df, mask_df, normalizer_y, normalizer_x):\n",
    "    y_shift = None\n",
    "    y_scale = None\n",
    "\n",
    "    # mask = mask.astype(int)\n",
    "    mask = mask_df['available_mask'].values * mask_df['sample_mask'].values\n",
    "    \n",
    "    if normalizer_y is not None:\n",
    "        scaler_y = Scaler(normalizer=normalizer_y)\n",
    "        Y_df['y'] = scaler_y.scale(x=Y_df['y'].values, mask=mask)\n",
    "    else:\n",
    "        scaler_y = None\n",
    "\n",
    "    if normalizer_x is not None:\n",
    "        X_cols = [col for col in X_df.columns if col not in ['unique_id','ds']]\n",
    "        for col in X_cols:\n",
    "            scaler_x = Scaler(normalizer=normalizer_x)\n",
    "            X_df[col] = scaler_x.scale(x=X_df[col].values, mask=mask)\n",
    "\n",
    "    return Y_df, X_df, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def train_val_split(len_series, offset, window_sampling_limit, n_val_weeks, ds_per_day):\n",
    "    last_ds = len_series - offset\n",
    "    first_ds = max(last_ds - window_sampling_limit, 0)\n",
    "\n",
    "    last_day = int(last_ds/ds_per_day)\n",
    "    first_day = int(first_ds/ds_per_day)\n",
    "\n",
    "    days = set(range(first_day, last_day)) # All days, to later get train days\n",
    "    # Sample weeks from here, -7 to avoid sampling from last week\n",
    "    # To not sample first week and have inputs\n",
    "    sampling_days = set(range(first_day + 7, last_day - 7))\n",
    "    validation_days = set({}) # Val days set\n",
    "    \n",
    "    # For loop for n of weeks in validation\n",
    "    for i in range(n_val_weeks):\n",
    "        # Sample random day, init of week\n",
    "        init_day = random.sample(sampling_days, 1)[0]\n",
    "        # Select days of sampled init of week\n",
    "        sampled_days = list(range(init_day, min(init_day+7, last_day)))\n",
    "        # Add days to validation days\n",
    "        validation_days.update(sampled_days)\n",
    "        # Remove days from sampling_days, including overlapping resulting previous week\n",
    "        days_to_remove = set(range(init_day-6, min(init_day+7, last_day)))\n",
    "        sampling_days = sampling_days.difference(days_to_remove)\n",
    "\n",
    "    train_days = days.difference(validation_days)\n",
    "\n",
    "    train_days = sorted(list(train_days))\n",
    "    validation_days = sorted(list(validation_days))\n",
    "\n",
    "    train_idx = []\n",
    "    for day in train_days:\n",
    "        hours_idx = range(day*ds_per_day,(day+1)*ds_per_day)\n",
    "        train_idx += hours_idx\n",
    "\n",
    "    val_idx = []\n",
    "    for day in validation_days:\n",
    "        hours_idx = range(day*ds_per_day,(day+1)*ds_per_day)\n",
    "        val_idx += hours_idx\n",
    "\n",
    "    assert all([idx < last_ds for idx in val_idx]), 'Leakage!!!!'\n",
    "    \n",
    "    return train_idx, val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_datasets(mc, Y_df, X_df, S_df, ds_in_test, shuffle_outsample):\n",
    "    #TODO: shuffle_outsample\n",
    "\n",
    "    # n_timestamps_pred defines number of hours ahead to predict\n",
    "    \n",
    "    #------------------------------------- Available and Validation Mask ------------------------------------#\n",
    "    # mask: 1 last_n_timestamps, 0 timestamps until last_n_timestamps\n",
    "    train_mask_df = get_default_mask_df(Y_df=Y_df, ds_in_test=ds_in_test, is_test=False)\n",
    "    outsample_mask_df = get_default_mask_df(Y_df=Y_df, ds_in_test=ds_in_test, is_test=True)\n",
    "\n",
    "    #---------------------------------------------- Scale Data ----------------------------------------------#\n",
    "    # Scale data # TODO: write sample_mask conditional/groupby(['unique_id]) scaling\n",
    "    Y_df, X_df, scaler_y = scale_data(Y_df=Y_df, X_df=X_df, mask_df=train_mask_df,\n",
    "                                      normalizer_y=mc['normalizer_y'], normalizer_x=mc['normalizer_x'])\n",
    "\n",
    "    #----------------------------------------- Declare Dataset and Loaders ----------------------------------#\n",
    "    train_ts_dataset = TimeSeriesDataset(Y_df=Y_df, X_df=X_df, S_df=S_df, mask_df=train_mask_df, verbose=True)\n",
    "    if ds_in_test == 0:\n",
    "        outsample_ts_dataset = None\n",
    "    else:\n",
    "        outsample_ts_dataset = TimeSeriesDataset(Y_df=Y_df, X_df=X_df, S_df=S_df, \n",
    "                                                 mask_df=outsample_mask_df, verbose=True)\n",
    "\n",
    "#     train_ts_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=None, X_df=X_df,\n",
    "#                                      ds_in_test=728*24, verbose=True)\n",
    "\n",
    "#     outsample_ts_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=None, X_df=X_df,\n",
    "#                                             ds_in_test=728*24, is_test=True, verbose=True)\n",
    "\n",
    "#     scaler_y = None\n",
    "\n",
    "    return train_ts_dataset, outsample_ts_dataset, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_loaders(mc, train_ts_dataset, outsample_ts_dataset):\n",
    "    train_ts_loader = TimeSeriesLoader(ts_dataset=train_ts_dataset,\n",
    "                                       model=mc['model'],\n",
    "                                       window_sampling_limit=int(mc['window_sampling_limit']),\n",
    "                                       input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                                       output_size=int(mc['output_size']),\n",
    "                                       idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                       len_sample_chunks=mc['len_sample_chunks'],\n",
    "                                       batch_size=int(mc['batch_size']),\n",
    "                                       n_series_per_batch=mc['n_series_per_batch'],\n",
    "                                       complete_inputs=mc['complete_inputs'],\n",
    "                                       complete_sample=mc['complete_sample'],\n",
    "                                       shuffle=True)\n",
    "\n",
    "    if outsample_ts_dataset is not None:\n",
    "        val_ts_loader = TimeSeriesLoader(ts_dataset=outsample_ts_dataset,\n",
    "                                        model=mc['model'],\n",
    "                                        window_sampling_limit=int(mc['window_sampling_limit']),\n",
    "                                        input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                                        output_size=int(mc['output_size']),\n",
    "                                        idx_to_sample_freq=mc['val_idx_to_sample_freq'],\n",
    "                                        len_sample_chunks=mc['len_sample_chunks'],\n",
    "                                        batch_size=1,\n",
    "                                        n_series_per_batch=mc['n_series_per_batch'],\n",
    "                                        complete_inputs=mc['complete_inputs'],\n",
    "                                        complete_sample=mc['complete_sample'],\n",
    "                                        shuffle=False)\n",
    "    else:\n",
    "        val_ts_loader = None\n",
    "\n",
    "    return train_ts_loader, val_ts_loader\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_nbeats(mc):\n",
    "    mc['n_hidden_list'] = len(mc['stack_types']) * [ mc['n_layers'][0]*[mc['n_hidden']] ]\n",
    "    model = Nbeats(input_size_multiplier=mc['input_size_multiplier'],\n",
    "                   output_size=int(mc['output_size']),\n",
    "                   shared_weights=mc['shared_weights'],\n",
    "                   initialization=mc['initialization'],\n",
    "                   activation=mc['activation'],\n",
    "                   stack_types=mc['stack_types'],\n",
    "                   n_blocks=mc['n_blocks'],\n",
    "                   n_layers=mc['n_layers'],\n",
    "                   n_hidden=mc['n_hidden_list'],\n",
    "                   n_harmonics=int(mc['n_harmonics']),\n",
    "                   n_polynomials=int(mc['n_polynomials']),\n",
    "                   x_s_n_hidden=int(mc['x_s_n_hidden']),\n",
    "                   exogenous_n_channels=int(mc['exogenous_n_channels']),\n",
    "                   batch_normalization = mc['batch_normalization'],\n",
    "                   dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                   dropout_prob_exogenous=mc['dropout_prob_exogenous'],\n",
    "                   learning_rate=float(mc['learning_rate']),\n",
    "                   lr_decay=float(mc['lr_decay']),\n",
    "                   n_lr_decay_steps=float(mc['n_lr_decay_steps']),\n",
    "                   weight_decay=mc['weight_decay'],\n",
    "                   l1_theta=mc['l1_theta'],\n",
    "                   n_iterations=int(mc['n_iterations']),\n",
    "                   early_stopping=int(mc['early_stopping']),\n",
    "                   loss=mc['loss'],\n",
    "                   loss_hypar=float(mc['loss_hypar']),\n",
    "                   val_loss=mc['val_loss'],\n",
    "                   frequency=mc['frequency'],\n",
    "                   seasonality=int(mc['seasonality']),\n",
    "                   random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_esrnn(mc):    \n",
    "    model = ESRNN(# Architecture parameters\n",
    "                  input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                  output_size=int(mc['output_size']),\n",
    "                  es_component=mc['es_component'],\n",
    "                  cell_type=mc['cell_type'],\n",
    "                  state_hsize=int(mc['state_hsize']),\n",
    "                  dilations=mc['dilations'],\n",
    "                  add_nl_layer=mc['add_nl_layer'],\n",
    "                  # Optimization parameters\n",
    "                  n_iterations=int(mc['n_iterations']),\n",
    "                  early_stopping=int(mc['early_stopping']),                  \n",
    "                  learning_rate=mc['learning_rate'],\n",
    "                  lr_scheduler_step_size=int(mc['lr_scheduler_step_size']),\n",
    "                  lr_decay=mc['lr_decay'],\n",
    "                  per_series_lr_multip=mc['per_series_lr_multip'],\n",
    "                  gradient_eps=mc['gradient_eps'],\n",
    "                  gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                  rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                  noise_std=mc['noise_std'],\n",
    "                  level_variability_penalty=mc['level_variability_penalty'],\n",
    "                  testing_percentile=mc['testing_percentile'],\n",
    "                  training_percentile=mc['training_percentile'],\n",
    "                  loss=mc['loss'],\n",
    "                  val_loss=mc['val_loss'],\n",
    "                  seasonality=mc['seasonality'],\n",
    "                  random_seed=int(mc['random_seed'])\n",
    "                  # Data parameters\n",
    "                  ) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_mqesrnn(mc):    \n",
    "    model = MQESRNN(# Architecture parameters\n",
    "                    input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                    output_size=int(mc['output_size']),\n",
    "                    es_component=mc['es_component'],\n",
    "                    cell_type=mc['cell_type'],\n",
    "                    state_hsize=int(mc['state_hsize']),\n",
    "                    dilations=mc['dilations'],\n",
    "                    add_nl_layer=mc['add_nl_layer'],\n",
    "                    # Optimization parameters\n",
    "                    n_iterations=int(mc['n_iterations']),\n",
    "                    early_stopping=int(mc['early_stopping']),                  \n",
    "                    learning_rate=mc['learning_rate'],\n",
    "                    lr_scheduler_step_size=int(mc['lr_scheduler_step_size']),\n",
    "                    lr_decay=mc['lr_decay'],\n",
    "                    gradient_eps=mc['gradient_eps'],\n",
    "                    gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                    rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                    noise_std=mc['noise_std'],\n",
    "                    testing_percentiles=list(mc['testing_percentiles']),\n",
    "                    training_percentiles=list(mc['training_percentiles']),\n",
    "                    loss=mc['loss'],\n",
    "                    val_loss=mc['val_loss'],\n",
    "                    random_seed=int(mc['random_seed'])\n",
    "                    # Data parameters\n",
    "                  ) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_rnn(mc):\n",
    "    model = RNN(input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                output_size=int(mc['output_size']),\n",
    "                max_epochs=int(mc['max_epochs']),\n",
    "                learning_rate=mc['learning_rate'],\n",
    "                lr_scheduler_step_size=int(mc['lr_scheduler_step_size']),\n",
    "                lr_decay=mc['lr_decay'],\n",
    "                gradient_eps=mc['gradient_eps'],\n",
    "                gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                noise_std=mc['noise_std'],\n",
    "                testing_percentile=mc['testing_percentile'],\n",
    "                training_percentile=mc['training_percentile'],\n",
    "                cell_type=mc['cell_type'],\n",
    "                state_hsize=int(mc['state_hsize']),\n",
    "                dilations=mc['dilations'],\n",
    "                add_nl_layer=mc['add_nl_layer'],\n",
    "                loss=mc['loss'],\n",
    "                random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_tcn(mc):\n",
    "    model = TCN(output_size=int(mc['output_size']),\n",
    "                n_channels=mc['n_channels'],\n",
    "                kernel_size=int(mc['kernel_size']),\n",
    "                initialization=mc['initialization'],\n",
    "                learning_rate=mc['learning_rate'],\n",
    "                lr_decay=mc['lr_decay'],\n",
    "                n_lr_decay_steps=mc['n_lr_decay_steps'],\n",
    "                weight_decay=mc['weight_decay'],\n",
    "                dropout_prob=mc['dropout_prob'],\n",
    "                n_iterations=int(mc['n_iterations']),\n",
    "                early_stopping=int(mc['early_stopping']),\n",
    "                loss=mc['loss'],\n",
    "                val_loss=mc['val_loss'],\n",
    "                frequency=mc['frequency'],\n",
    "                random_seed=int(mc['random_seed']),\n",
    "                seasonality=mc['seasonality'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_model(mc):\n",
    "    MODEL_DICT = {'nbeats': instantiate_nbeats,\n",
    "                  'esrnn': instantiate_esrnn,\n",
    "                  'new_rnn': instantiate_esrnn,\n",
    "                  'mqesrnn': instantiate_mqesrnn,\n",
    "                  'rnn': instantiate_rnn,\n",
    "                  'tcn': instantiate_tcn}\n",
    "    return MODEL_DICT[mc['model']](mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def model_fit_predict(mc, Y_df, X_df, S_df, ds_in_test, shuffle_outsample):\n",
    "    #TODO: rolling forecast\n",
    "    #TODO: expected_fcds\n",
    "    \n",
    "    Y_df = Y_df.copy()\n",
    "    if X_df is not None:\n",
    "        X_df = X_df.copy()\n",
    "    if S_df is not None:\n",
    "        S_df = S_df.copy()\n",
    "\n",
    "    #----------------------------------------------- Datasets -----------------------------------------------#\n",
    "    train_ts_dataset, outsample_ts_dataset, scaler_y = create_datasets(mc=mc, Y_df=Y_df, X_df=X_df,\n",
    "                                                                       S_df=S_df,\n",
    "                                                                       ds_in_test=ds_in_test,\n",
    "                                                                       shuffle_outsample=shuffle_outsample)\n",
    "\n",
    "    #--------------------------------------- Instantiate, fit, predict ---------------------------------------#\n",
    "    train_ts_loader, val_ts_loader = instantiate_loaders(mc=mc, train_ts_dataset=train_ts_dataset,\n",
    "                                                         outsample_ts_dataset=outsample_ts_dataset)\n",
    "    model = instantiate_model(mc=mc)\n",
    "\n",
    "    # Val loader not implemented during training for ESRNN and RNN\n",
    "    model.fit(train_ts_loader=train_ts_loader, val_ts_loader=val_ts_loader, verbose=True,\n",
    "                eval_freq=mc['eval_freq'])\n",
    "    y_true, y_hat, mask = model.predict(ts_loader=val_ts_loader, return_decomposition=False)\n",
    "\n",
    "    print(\"y_true.shape (#n_series, #n_fcds, #lt) \", y_true.shape)\n",
    "    print(\"y_hat.shape (#n_series, #n_fcds, #lt) \", y_hat.shape)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    meta_data = val_ts_loader.ts_dataset.meta_data\n",
    "\n",
    "    return y_true, y_hat, mask, meta_data, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def evaluate_model(mc, loss_function, Y_df, X_df, S_df, ds_in_test, shuffle_outsample,\n",
    "                   kwargs_loss):\n",
    "    \n",
    "    # Some asserts due to work in progress\n",
    "    assert mc['normalizer_y'] is None, 'Scaling Y not iplemented (inverse Y missing for loss)'\n",
    "\n",
    "    n_series = Y_df['unique_id'].nunique()\n",
    "    if n_series > 1:\n",
    "        assert mc['normalizer_x'] is None, 'Data scaling not implemented with multiple time series'\n",
    "    assert shuffle_outsample == False, 'Shuffle outsample not implemented'\n",
    "\n",
    "    assert ds_in_test % mc['val_idx_to_sample_freq']==0, 'outsample size should be multiple of val_idx_to_sample_freq'\n",
    "\n",
    "    # Make predictions\n",
    "    start = time.time()\n",
    "    y_true, y_hat, mask, meta_data, model = model_fit_predict(mc=mc, Y_df=Y_df, X_df=X_df, \n",
    "                                                              S_df=S_df, ds_in_test=ds_in_test,\n",
    "                                                              shuffle_outsample=shuffle_outsample)\n",
    "    run_time = time.time() - start\n",
    "\n",
    "    # Evaluate predictions\n",
    "    loss = loss_function(y=y_true, y_hat=y_hat, weights=mask, **kwargs_loss)\n",
    "\n",
    "    result =  {'loss': loss,\n",
    "               'mc': mc,\n",
    "               'y_true': y_true,\n",
    "               'y_hat': y_hat,\n",
    "               'trajectories': model.trajectories,\n",
    "               'run_time': run_time,\n",
    "               'status': STATUS_OK}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def hyperopt_tunning(space, hyperopt_iters, loss_function, Y_df, X_df, S_df, ds_in_test,\n",
    "                     shuffle_outsample, save_trials=False,\n",
    "                     kwargs_loss=None):\n",
    "    trials = Trials()\n",
    "    fmin_objective = partial(evaluate_model, loss_function=loss_function, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "                             ds_in_test=ds_in_test,\n",
    "                             shuffle_outsample=shuffle_outsample,\n",
    "                             kwargs_loss=kwargs_loss or {})\n",
    "\n",
    "    fmin(fmin_objective, space=space, algo=tpe.suggest, max_evals=hyperopt_iters, trials=trials, verbose=True)\n",
    "\n",
    "    return trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nixtla.losses.numpy import mae, mape, smape, rmse, pinball_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbeats_space= {# Architecture parameters\n",
    "               'model':'nbeats',\n",
    "               'input_size_multiplier': hp.choice('input_size_multiplier', [7]),\n",
    "               'output_size': hp.choice('output_size', [24]),\n",
    "               'shared_weights': hp.choice('shared_weights', [False]),\n",
    "               'activation': hp.choice('activation', ['selu']),\n",
    "               'initialization':  hp.choice('initialization', ['glorot_normal','he_normal']),\n",
    "               'stack_types': hp.choice('stack_types', [2*['identity'],\n",
    "                                                        1*['identity']+1*['exogenous_tcn'],\n",
    "                                                        1*['exogenous_tcn']+1*['identity'] ]),\n",
    "               'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),\n",
    "               'n_layers': hp.choice('n_layers', [ [2, 2] ]),\n",
    "               'n_hidden': hp.choice('n_hidden', [ 364 ]),\n",
    "               'n_harmonics': hp.choice('n_harmonics', [1]),\n",
    "               'n_polynomials': hp.choice('n_polynomials', [2]),\n",
    "               'exogenous_n_channels': hp.quniform('exogenous_n_channels', 1, 10, 1),\n",
    "               'x_s_n_hidden': hp.choice('x_s_n_hidden', [0]),\n",
    "               # Regularization and optimization parameters\n",
    "               'batch_normalization': hp.choice('batch_normalization', [False]),\n",
    "               'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 0.5),\n",
    "               'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),\n",
    "               'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.001)),\n",
    "               'lr_decay': hp.uniform('lr_decay', 0.3, 0.5),\n",
    "               'n_lr_decay_steps': hp.choice('n_lr_decay_steps', [3]),\n",
    "               'weight_decay': hp.loguniform('weight_decay', np.log(5e-5), np.log(5e-3)),\n",
    "               'n_iterations': hp.choice('n_iterations', [10]), #[args.max_epochs]),\n",
    "               'early_stopping': hp.choice('early_stopping', [16]),\n",
    "               'eval_freq': hp.choice('eval_freq', [50]),\n",
    "               'n_val_weeks': hp.choice('n_val_weeks', [52*2]),\n",
    "               'loss': hp.choice('loss', ['MAE']),\n",
    "               'loss_hypar': hp.choice('loss_hypar', [0.5]),                \n",
    "               'val_loss': hp.choice('val_loss', ['MAE']), #[args.val_loss]),\n",
    "               'l1_theta': hp.choice('l1_theta', [0]),\n",
    "               # Data parameters\n",
    "               'len_sample_chunks': hp.choice('len_sample_chunks', [None]),\n",
    "               'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "               'normalizer_x': hp.choice('normalizer_x', ['median']),\n",
    "               'window_sampling_limit': hp.choice('window_sampling_limit', [100_000]),\n",
    "               'complete_inputs': hp.choice('complete_inputs', [False]),\n",
    "               'complete_sample': hp.choice('complete_sample', [False]),                \n",
    "               'frequency': hp.choice('frequency', ['H']),\n",
    "               'seasonality': hp.choice('seasonality', [24]),      \n",
    "               'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [24]),\n",
    "               'val_idx_to_sample_freq': hp.choice('val_idx_to_sample_freq', [24]),\n",
    "               'batch_size': hp.choice('batch_size', [256]),\n",
    "               'n_series_per_batch': hp.choice('n_series_per_batch', [1]),\n",
    "               'random_seed': hp.quniform('random_seed', 10, 20, 1)}\n",
    "\n",
    "mc = {'model':'nbeats',\n",
    "      # Architecture parameters\n",
    "      'input_size_multiplier': 7,\n",
    "      'output_size': 24,\n",
    "      'shared_weights': False,\n",
    "      'activation': 'selu',\n",
    "      'initialization': 'he_normal',\n",
    "      'stack_types': ['exogenous_tcn']+1*['identity'],\n",
    "      'n_blocks': [1, 1],\n",
    "      'n_layers': [2, 2],\n",
    "      'n_hidden': 364,\n",
    "      'n_polynomials': 2,\n",
    "      'n_harmonics': 1,\n",
    "      'exogenous_n_channels': 3,\n",
    "      'x_s_n_hidden': 0,\n",
    "      # Regularization and optimization parameters\n",
    "      'batch_normalization': False,\n",
    "      'dropout_prob_theta': 0.2,\n",
    "      'dropout_prob_exogenous': 0.2,\n",
    "      'learning_rate': 0.0005, #0.002,\n",
    "      'lr_decay': 0.64,\n",
    "      'n_lr_decay_steps': 3,\n",
    "      'weight_decay': 0.00015,\n",
    "      'n_iterations': 100,\n",
    "      'early_stopping': 8,\n",
    "      'eval_freq': 50,\n",
    "      'n_val_weeks': 52*2,\n",
    "      'loss': 'PINBALL',\n",
    "      'loss_hypar': 0.5, #0.49,\n",
    "      'val_loss': 'MAE',\n",
    "      'l1_theta': 0,\n",
    "      # Data parameters\n",
    "      'normalizer_y': None,\n",
    "      'normalizer_x': 'median',\n",
    "      'window_sampling_limit': 100_000,\n",
    "      'complete_inputs': False,\n",
    "      'complete_sample': False,\n",
    "      'frequency':'H',\n",
    "      'seasonality': 24,\n",
    "      'idx_to_sample_freq': 24,\n",
    "      'val_idx_to_sample_freq': 24,\n",
    "      'batch_size': 256,\n",
    "      'n_series_per_batch': 1,\n",
    "      'random_seed': 10}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esrnn_space = {'model': hp.choice('model', ['new_rnn']),\n",
    "               # Architecture parameters\n",
    "               'input_size_multiplier': hp.choice('input_size_multiplier', [7]),\n",
    "               'output_size': hp.choice('output_size', [24]),\n",
    "               'dilations': hp.choice('dilations', [ [[1, 2]], [[1,2], [7, 14]] ]),\n",
    "               'es_component': hp.choice('es_component', ['multiplicative']),\n",
    "               'cell_type': hp.choice('cell_type', ['LSTM']),\n",
    "               'state_hsize': hp.quniform('state_hsize', 10, 100, 10),\n",
    "               'add_nl_layer': hp.choice('add_nl_layer', [True, False]),\n",
    "               'seasonality': hp.choice('seasonality', [ [24] ]),\n",
    "               # Regularization and optimization parameters\n",
    "               'n_iterations':hp.choice('n_iterations', [10]),\n",
    "               'early_stopping':hp.choice('early_stopping', [10]),\n",
    "               'eval_freq': hp.choice('eval_freq', [10]),\n",
    "               'batch_size': hp.choice('batch_size', [32]),\n",
    "               'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.01)),\n",
    "               'lr_scheduler_step_size': hp.choice('lr_scheduler_step_size', [100]),\n",
    "               'lr_decay': hp.quniform('lr_decay', 0.5, 0.8, 0.1),\n",
    "               'per_series_lr_multip': hp.choice('per_series_lr_multip', [0.5, 1.0, 1.5, 2.0, 3.0]),\n",
    "               'gradient_eps': hp.choice('gradient_eps', [1e-8]),\n",
    "               'gradient_clipping_threshold': hp.choice('gradient_clipping_threshold', [10, 50]),\n",
    "               'rnn_weight_decay': hp.choice('rnn_weight_decay', [0, 0.0005, 0.005]),\n",
    "               'noise_std': hp.loguniform('noise_std', np.log(0.0001), np.log(0.001)),\n",
    "               'level_variability_penalty': hp.quniform('level_variability_penalty', 0, 100, 10),\n",
    "               'testing_percentile': hp.choice('testing_percentile', [50]),\n",
    "               'training_percentile': hp.choice('training_percentile', [48, 49, 50, 51]),\n",
    "               'random_seed': hp.quniform('random_seed', 1, 1000, 1),\n",
    "               'loss': hp.choice('loss', ['SMYL']),\n",
    "               'val_loss': hp.choice('val_loss', ['MAE']),\n",
    "               # Data parameters\n",
    "               'len_sample_chunks': hp.choice('len_sample_chunks', [7*3*24]),\n",
    "               'window_sampling_limit': hp.choice('window_sampling_limit', [500_000]),\n",
    "               'complete_inputs': hp.choice('complete_inputs', [True]),\n",
    "               'complete_sample': hp.choice('complete_sample', [True]),\n",
    "               'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [24]),\n",
    "               'val_idx_to_sample_freq': hp.choice('val_idx_to_sample_freq', [24]),\n",
    "               'n_series_per_batch': hp.choice('n_series_per_batch', [1]),\n",
    "               'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "               'normalizer_x': hp.choice('normalizer_x',  [None])}\n",
    "\n",
    "mc = {'model':'new_rnn',\n",
    "      # Architecture parameters\n",
    "      'input_size_multiplier': 7,\n",
    "      'output_size': 24,\n",
    "      'dilations': [[1,2], [7]],\n",
    "      'es_component': 'multiplicative',\n",
    "      'cell_type': 'LSTM',\n",
    "      'state_hsize': 50,\n",
    "      'add_nl_layer': False,\n",
    "      'seasonality': [24],\n",
    "      # Regularization and optimization parameters\n",
    "      'n_iterations': 10,#00,\n",
    "      'early_stopping': 10,\n",
    "      'eval_freq': 10,\n",
    "      'batch_size': 32,\n",
    "      'learning_rate': 0.0005,\n",
    "      'lr_scheduler_step_size': 100,\n",
    "      'lr_decay': 0.8,\n",
    "      'per_series_lr_multip': 1.5,\n",
    "      'gradient_eps': 1e-8, \n",
    "      'gradient_clipping_threshold': 20,\n",
    "      'rnn_weight_decay': 0.0,\n",
    "      'noise_std': 0.0005,\n",
    "      'level_variability_penalty': 10,\n",
    "      'testing_percentile': 50,\n",
    "      'training_percentile': 50,\n",
    "      'random_seed': 1,\n",
    "      'loss': 'SMYL',\n",
    "      'val_loss': 'MAE',\n",
    "      # Data parameters\n",
    "      'len_sample_chunks': 7*4*24,\n",
    "      'window_sampling_limit': 500_000,\n",
    "      'complete_inputs': False,\n",
    "      'complete_sample': False,\n",
    "      'idx_to_sample_freq': 24,\n",
    "      'val_idx_to_sample_freq': 24,\n",
    "      'n_series_per_batch': 1,\n",
    "      'normalizer_y': None,\n",
    "      'normalizer_x': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = instantiate_esrnn(mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = ['NP']\n",
    "\n",
    "Y_df, X_df, S_df = EPF.load_groups(directory='../data', groups=dataset)\n",
    "\n",
    "X_df = X_df[['unique_id', 'ds', 'week_day']]\n",
    "Y_min = Y_df.y.min()\n",
    "#Y_df.y = Y_df.y - Y_min + 20\n",
    "\n",
    "plt.plot(Y_df.y.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0.0         2016-12-27 2018-12-24 23:00:00\n",
      "          1.0         2013-01-01 2016-12-26 23:00:00\n",
      "INFO:root:Total data \t\t\t52416 time stamps\n",
      "INFO:root:Available percentage=100.0, \t52416 time stamps\n",
      "INFO:root:Insample  percentage=66.67, \t34944.0 time stamps\n",
      "INFO:root:Outsample percentage=33.33, \t17472.0 time stamps\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0.0         2013-01-01 2016-12-26 23:00:00\n",
      "          1.0         2016-12-27 2018-12-24 23:00:00\n",
      "INFO:root:Total data \t\t\t52416 time stamps\n",
      "INFO:root:Available percentage=100.0, \t52416 time stamps\n",
      "INFO:root:Insample  percentage=33.33, \t17472.0 time stamps\n",
      "INFO:root:Outsample percentage=66.67, \t34944.0 time stamps\n",
      "INFO:root:\n",
      "\n",
      "WARNING:root:Batch size will be ignored (shuffle=False). All windows constructed will be used to train.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================== Start fitting ==============================\n",
      "Step: 10, Time: 2.874, Insample SMYL: 0.05373, Outsample MAE: 3.52510\n",
      "Step: 10, Time: 3.200, Insample SMYL: 0.05373, Outsample MAE: 3.52510\n",
      "==============================  End fitting  ==============================\n",
      "\n",
      "\n",
      "y_true.shape (#n_series, #n_fcds, #lt)  (728, 24)\n",
      "y_hat.shape (#n_series, #n_fcds, #lt)  (728, 24)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 3.525100053189111,\n",
       " 'mc': {'model': 'new_rnn',\n",
       "  'input_size_multiplier': 7,\n",
       "  'output_size': 24,\n",
       "  'dilations': [[1, 2], [7]],\n",
       "  'es_component': 'multiplicative',\n",
       "  'cell_type': 'LSTM',\n",
       "  'state_hsize': 50,\n",
       "  'add_nl_layer': False,\n",
       "  'seasonality': [24],\n",
       "  'n_iterations': 10,\n",
       "  'early_stopping': 10,\n",
       "  'eval_freq': 10,\n",
       "  'batch_size': 32,\n",
       "  'learning_rate': 0.0005,\n",
       "  'lr_scheduler_step_size': 100,\n",
       "  'lr_decay': 0.8,\n",
       "  'per_series_lr_multip': 1.5,\n",
       "  'gradient_eps': 1e-08,\n",
       "  'gradient_clipping_threshold': 20,\n",
       "  'rnn_weight_decay': 0.0,\n",
       "  'noise_std': 0.0005,\n",
       "  'level_variability_penalty': 10,\n",
       "  'testing_percentile': 50,\n",
       "  'training_percentile': 50,\n",
       "  'random_seed': 1,\n",
       "  'loss': 'SMYL',\n",
       "  'val_loss': 'MAE',\n",
       "  'len_sample_chunks': 672,\n",
       "  'window_sampling_limit': 500000,\n",
       "  'complete_inputs': False,\n",
       "  'complete_sample': False,\n",
       "  'idx_to_sample_freq': 24,\n",
       "  'val_idx_to_sample_freq': 24,\n",
       "  'n_series_per_batch': 1,\n",
       "  'normalizer_y': None,\n",
       "  'normalizer_x': None},\n",
       " 'y_true': array([[24.08, 22.52, 20.13, ..., 28.37, 27.24, 25.73],\n",
       "        [26.45, 26.26, 26.24, ..., 30.65, 30.02, 29.37],\n",
       "        [29.26, 28.72, 28.29, ..., 30.01, 29.44, 28.76],\n",
       "        ...,\n",
       "        [48.39, 47.72, 47.23, ..., 52.05, 51.09, 50.47],\n",
       "        [51.49, 50.83, 50.74, ..., 53.99, 53.86, 52.32],\n",
       "        [51.09, 50.19, 48.98, ..., 49.09, 49.02, 48.1 ]], dtype=float32),\n",
       " 'y_hat': array([[24.497166, 23.753738, 21.575783, ..., 28.64855 , 27.814072,\n",
       "         24.719896],\n",
       "        [24.206263, 23.51362 , 21.387358, ..., 29.257362, 27.245247,\n",
       "         24.602047],\n",
       "        [27.119228, 27.55813 , 24.60927 , ..., 32.431866, 30.095257,\n",
       "         28.377064],\n",
       "        ...,\n",
       "        [47.92579 , 50.50907 , 48.934116, ..., 52.614826, 49.15002 ,\n",
       "         46.778122],\n",
       "        [49.50737 , 52.01791 , 48.179058, ..., 53.624588, 52.51216 ,\n",
       "         49.16685 ],\n",
       "        [50.415997, 53.542316, 51.976524, ..., 56.046535, 52.904808,\n",
       "         48.989334]], dtype=float32),\n",
       " 'trajectories': {'iteration': [10],\n",
       "  'train_loss': [0.05372771993279457],\n",
       "  'val_loss': [3.5251002]},\n",
       " 'run_time': 3.819559335708618,\n",
       " 'status': 'ok'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = evaluate_model(loss_function=mae, mc=mc, Y_df=Y_df, X_df=X_df, S_df=S_df, \n",
    "                        ds_in_test=728*24, shuffle_outsample=False, kwargs_loss={})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.016478 seconds\n",
      "INFO:hyperopt.tpe:TPE using 0 trials\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0.0         2016-12-27 2018-12-24 23:00:00\n",
      "          1.0         2013-01-01 2016-12-26 23:00:00\n",
      "INFO:root:Total data \t\t\t52416 time stamps\n",
      "INFO:root:Available percentage=100.0, \t52416 time stamps\n",
      "INFO:root:Insample  percentage=66.67, \t34944.0 time stamps\n",
      "INFO:root:Outsample percentage=33.33, \t17472.0 time stamps\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0.0         2013-01-01 2016-12-26 23:00:00\n",
      "          1.0         2016-12-27 2018-12-24 23:00:00\n",
      "INFO:root:Total data \t\t\t52416 time stamps\n",
      "INFO:root:Available percentage=100.0, \t52416 time stamps\n",
      "INFO:root:Insample  percentage=33.33, \t17472.0 time stamps\n",
      "INFO:root:Outsample percentage=66.67, \t34944.0 time stamps\n",
      "INFO:root:\n",
      "\n",
      "WARNING:root:Batch size will be ignored (shuffle=False). All windows constructed will be used to train.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Start fitting ==============================\n",
      "Step: 10, Time: 1.164, Insample MAE: 6.03131, Outsample MAE: 6.36892\n",
      "==============================  End fitting  ==============================\n",
      "y_true.shape (#n_series, #n_fcds, #lt)               \n",
      "(1, 728, 24)                                         \n",
      "y_hat.shape (#n_series, #n_fcds, #lt)                \n",
      "(1, 728, 24)                                         \n",
      " 50%|     | 1/2 [00:01<00:01,  1.73s/trial, best loss: 6.368921756744385]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.021106 seconds\n",
      "INFO:hyperopt.tpe:TPE using 1/1 trials with best loss 6.368922\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0.0         2016-12-27 2018-12-24 23:00:00\n",
      "          1.0         2013-01-01 2016-12-26 23:00:00\n",
      "INFO:root:Total data \t\t\t52416 time stamps\n",
      "INFO:root:Available percentage=100.0, \t52416 time stamps\n",
      "INFO:root:Insample  percentage=66.67, \t34944.0 time stamps\n",
      "INFO:root:Outsample percentage=33.33, \t17472.0 time stamps\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0.0         2013-01-01 2016-12-26 23:00:00\n",
      "          1.0         2016-12-27 2018-12-24 23:00:00\n",
      "INFO:root:Total data \t\t\t52416 time stamps\n",
      "INFO:root:Available percentage=100.0, \t52416 time stamps\n",
      "INFO:root:Insample  percentage=33.33, \t17472.0 time stamps\n",
      "INFO:root:Outsample percentage=66.67, \t34944.0 time stamps\n",
      "INFO:root:\n",
      "\n",
      "WARNING:root:Batch size will be ignored (shuffle=False). All windows constructed will be used to train.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Start fitting ==============================   \n",
      "Step: 10, Time: 0.294, Insample MAE: 2.25357, Outsample MAE: 2.81257          \n",
      "==============================  End fitting  ==============================   \n",
      "y_true.shape (#n_series, #n_fcds, #lt)                                        \n",
      "(1, 728, 24)                                                                  \n",
      "y_hat.shape (#n_series, #n_fcds, #lt)                                         \n",
      "(1, 728, 24)                                                                  \n",
      "100%|| 2/2 [00:02<00:00,  1.22s/trial, best loss: 2.812570810317993]\n"
     ]
    }
   ],
   "source": [
    "trials = hyperopt_tunning(space=nbeats_space, hyperopt_iters=2, loss_function=mae, Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "                          ds_in_test=728*24, shuffle_outsample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'state': 2,\n",
       "  'tid': 0,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 4.577489376068115,\n",
       "   'mc': {'activation': 'selu',\n",
       "    'batch_normalization': False,\n",
       "    'batch_size': 256,\n",
       "    'complete_inputs': False,\n",
       "    'complete_sample': False,\n",
       "    'dropout_prob_exogenous': 0.3680239643206844,\n",
       "    'dropout_prob_theta': 0.009343118485134028,\n",
       "    'early_stopping': 16,\n",
       "    'eval_freq': 50,\n",
       "    'exogenous_n_channels': 4.0,\n",
       "    'frequency': 'H',\n",
       "    'idx_to_sample_freq': 24,\n",
       "    'initialization': 'he_normal',\n",
       "    'input_size_multiplier': 7,\n",
       "    'l1_theta': 0,\n",
       "    'learning_rate': 0.000900505803667316,\n",
       "    'len_sample_chunks': None,\n",
       "    'loss': 'MAE',\n",
       "    'loss_hypar': 0.5,\n",
       "    'lr_decay': 0.38412961772271936,\n",
       "    'model': 'nbeats',\n",
       "    'n_blocks': (1, 1),\n",
       "    'n_harmonics': 1,\n",
       "    'n_hidden': 364,\n",
       "    'n_iterations': 10,\n",
       "    'n_layers': (2, 2),\n",
       "    'n_lr_decay_steps': 3,\n",
       "    'n_polynomials': 2,\n",
       "    'n_series_per_batch': 1,\n",
       "    'n_val_weeks': 104,\n",
       "    'normalizer_x': 'median',\n",
       "    'normalizer_y': None,\n",
       "    'output_size': 24,\n",
       "    'random_seed': 14.0,\n",
       "    'seasonality': 24,\n",
       "    'shared_weights': False,\n",
       "    'stack_types': ('exogenous_tcn', 'identity'),\n",
       "    'val_idx_to_sample_freq': 24,\n",
       "    'val_loss': 'MAE',\n",
       "    'weight_decay': 0.0010122670939110975,\n",
       "    'window_sampling_limit': 100000,\n",
       "    'x_s_n_hidden': 0,\n",
       "    'n_hidden_list': [[364, 364], [364, 364]]},\n",
       "   'y_true': array([[[24.08, 22.52, 20.13, ..., 28.37, 27.24, 25.73],\n",
       "           [26.45, 26.26, 26.24, ..., 30.65, 30.02, 29.37],\n",
       "           [29.26, 28.72, 28.29, ..., 30.01, 29.44, 28.76],\n",
       "           ...,\n",
       "           [48.39, 47.72, 47.23, ..., 52.05, 51.09, 50.47],\n",
       "           [51.49, 50.83, 50.74, ..., 53.99, 53.86, 52.32],\n",
       "           [51.09, 50.19, 48.98, ..., 49.09, 49.02, 48.1 ]]], dtype=float32),\n",
       "   'y_hat': array([[[22.102705, 23.134613, 22.273325, ..., 22.822798, 26.419062,\n",
       "            24.368002],\n",
       "           [21.848896, 22.70428 , 22.002983, ..., 22.47703 , 26.06789 ,\n",
       "            24.353512],\n",
       "           [25.606987, 26.319887, 25.431143, ..., 26.051361, 28.91377 ,\n",
       "            27.955185],\n",
       "           ...,\n",
       "           [40.360424, 41.269714, 40.116203, ..., 41.016075, 48.248856,\n",
       "            44.58606 ],\n",
       "           [40.28879 , 41.47557 , 40.193848, ..., 41.897472, 49.791866,\n",
       "            45.401882],\n",
       "           [45.722824, 46.39514 , 45.11126 , ..., 46.977417, 53.854076,\n",
       "            50.914886]]], dtype=float32),\n",
       "   'trajectories': {'iteration': [], 'train_loss': [], 'val_loss': []},\n",
       "   'run_time': 0.6965718269348145,\n",
       "   'status': 'ok'},\n",
       "  'misc': {'tid': 0,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'activation': [0],\n",
       "    'batch_normalization': [0],\n",
       "    'batch_size': [0],\n",
       "    'complete_inputs': [0],\n",
       "    'complete_sample': [0],\n",
       "    'dropout_prob_exogenous': [0],\n",
       "    'dropout_prob_theta': [0],\n",
       "    'early_stopping': [0],\n",
       "    'eval_freq': [0],\n",
       "    'exogenous_n_channels': [0],\n",
       "    'frequency': [0],\n",
       "    'idx_to_sample_freq': [0],\n",
       "    'initialization': [0],\n",
       "    'input_size_multiplier': [0],\n",
       "    'l1_theta': [0],\n",
       "    'learning_rate': [0],\n",
       "    'len_sample_chunks': [0],\n",
       "    'loss': [0],\n",
       "    'loss_hypar': [0],\n",
       "    'lr_decay': [0],\n",
       "    'n_blocks': [0],\n",
       "    'n_harmonics': [0],\n",
       "    'n_hidden': [0],\n",
       "    'n_iterations': [0],\n",
       "    'n_layers': [0],\n",
       "    'n_lr_decay_steps': [0],\n",
       "    'n_polynomials': [0],\n",
       "    'n_series_per_batch': [0],\n",
       "    'n_val_weeks': [0],\n",
       "    'normalizer_x': [0],\n",
       "    'normalizer_y': [0],\n",
       "    'output_size': [0],\n",
       "    'random_seed': [0],\n",
       "    'seasonality': [0],\n",
       "    'shared_weights': [0],\n",
       "    'stack_types': [0],\n",
       "    'val_idx_to_sample_freq': [0],\n",
       "    'val_loss': [0],\n",
       "    'weight_decay': [0],\n",
       "    'window_sampling_limit': [0],\n",
       "    'x_s_n_hidden': [0]},\n",
       "   'vals': {'activation': [0],\n",
       "    'batch_normalization': [0],\n",
       "    'batch_size': [0],\n",
       "    'complete_inputs': [0],\n",
       "    'complete_sample': [0],\n",
       "    'dropout_prob_exogenous': [0.3680239643206844],\n",
       "    'dropout_prob_theta': [0.009343118485134028],\n",
       "    'early_stopping': [0],\n",
       "    'eval_freq': [0],\n",
       "    'exogenous_n_channels': [4.0],\n",
       "    'frequency': [0],\n",
       "    'idx_to_sample_freq': [0],\n",
       "    'initialization': [1],\n",
       "    'input_size_multiplier': [0],\n",
       "    'l1_theta': [0],\n",
       "    'learning_rate': [0.000900505803667316],\n",
       "    'len_sample_chunks': [0],\n",
       "    'loss': [0],\n",
       "    'loss_hypar': [0],\n",
       "    'lr_decay': [0.38412961772271936],\n",
       "    'n_blocks': [0],\n",
       "    'n_harmonics': [0],\n",
       "    'n_hidden': [0],\n",
       "    'n_iterations': [0],\n",
       "    'n_layers': [0],\n",
       "    'n_lr_decay_steps': [0],\n",
       "    'n_polynomials': [0],\n",
       "    'n_series_per_batch': [0],\n",
       "    'n_val_weeks': [0],\n",
       "    'normalizer_x': [0],\n",
       "    'normalizer_y': [0],\n",
       "    'output_size': [0],\n",
       "    'random_seed': [14.0],\n",
       "    'seasonality': [0],\n",
       "    'shared_weights': [0],\n",
       "    'stack_types': [2],\n",
       "    'val_idx_to_sample_freq': [0],\n",
       "    'val_loss': [0],\n",
       "    'weight_decay': [0.0010122670939110975],\n",
       "    'window_sampling_limit': [0],\n",
       "    'x_s_n_hidden': [0]}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2021, 3, 16, 0, 45, 13, 633000),\n",
       "  'refresh_time': datetime.datetime(2021, 3, 16, 0, 45, 14, 339000)},\n",
       " {'state': 2,\n",
       "  'tid': 1,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 2.999371290206909,\n",
       "   'mc': {'activation': 'selu',\n",
       "    'batch_normalization': False,\n",
       "    'batch_size': 256,\n",
       "    'complete_inputs': False,\n",
       "    'complete_sample': False,\n",
       "    'dropout_prob_exogenous': 0.22674402023168583,\n",
       "    'dropout_prob_theta': 0.3754955891490347,\n",
       "    'early_stopping': 16,\n",
       "    'eval_freq': 50,\n",
       "    'exogenous_n_channels': 8.0,\n",
       "    'frequency': 'H',\n",
       "    'idx_to_sample_freq': 24,\n",
       "    'initialization': 'he_normal',\n",
       "    'input_size_multiplier': 7,\n",
       "    'l1_theta': 0,\n",
       "    'learning_rate': 0.0006223292463060922,\n",
       "    'len_sample_chunks': None,\n",
       "    'loss': 'MAE',\n",
       "    'loss_hypar': 0.5,\n",
       "    'lr_decay': 0.4876099208261274,\n",
       "    'model': 'nbeats',\n",
       "    'n_blocks': (1, 1),\n",
       "    'n_harmonics': 1,\n",
       "    'n_hidden': 364,\n",
       "    'n_iterations': 10,\n",
       "    'n_layers': (2, 2),\n",
       "    'n_lr_decay_steps': 3,\n",
       "    'n_polynomials': 2,\n",
       "    'n_series_per_batch': 1,\n",
       "    'n_val_weeks': 104,\n",
       "    'normalizer_x': 'median',\n",
       "    'normalizer_y': None,\n",
       "    'output_size': 24,\n",
       "    'random_seed': 19.0,\n",
       "    'seasonality': 24,\n",
       "    'shared_weights': False,\n",
       "    'stack_types': ('exogenous_tcn', 'identity'),\n",
       "    'val_idx_to_sample_freq': 24,\n",
       "    'val_loss': 'MAE',\n",
       "    'weight_decay': 0.000971793150819528,\n",
       "    'window_sampling_limit': 100000,\n",
       "    'x_s_n_hidden': 0,\n",
       "    'n_hidden_list': [[364, 364], [364, 364]]},\n",
       "   'y_true': array([[[24.08, 22.52, 20.13, ..., 28.37, 27.24, 25.73],\n",
       "           [26.45, 26.26, 26.24, ..., 30.65, 30.02, 29.37],\n",
       "           [29.26, 28.72, 28.29, ..., 30.01, 29.44, 28.76],\n",
       "           ...,\n",
       "           [48.39, 47.72, 47.23, ..., 52.05, 51.09, 50.47],\n",
       "           [51.49, 50.83, 50.74, ..., 53.99, 53.86, 52.32],\n",
       "           [51.09, 50.19, 48.98, ..., 49.09, 49.02, 48.1 ]]], dtype=float32),\n",
       "   'y_hat': array([[[28.085384, 27.007774, 27.315134, ..., 29.374865, 29.39265 ,\n",
       "            28.502396],\n",
       "           [27.28584 , 26.342508, 26.776764, ..., 28.9014  , 28.781723,\n",
       "            27.999695],\n",
       "           [30.933008, 29.714657, 29.659342, ..., 32.454147, 32.59295 ,\n",
       "            31.660177],\n",
       "           ...,\n",
       "           [48.727055, 46.794212, 45.745358, ..., 48.793808, 49.765366,\n",
       "            49.016956],\n",
       "           [51.05863 , 49.70736 , 49.608307, ..., 52.240364, 52.638515,\n",
       "            52.926228],\n",
       "           [52.873352, 51.75296 , 51.66349 , ..., 55.169926, 55.209515,\n",
       "            54.453312]]], dtype=float32),\n",
       "   'trajectories': {'iteration': [], 'train_loss': [], 'val_loss': []},\n",
       "   'run_time': 0.8031067848205566,\n",
       "   'status': 'ok'},\n",
       "  'misc': {'tid': 1,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'activation': [1],\n",
       "    'batch_normalization': [1],\n",
       "    'batch_size': [1],\n",
       "    'complete_inputs': [1],\n",
       "    'complete_sample': [1],\n",
       "    'dropout_prob_exogenous': [1],\n",
       "    'dropout_prob_theta': [1],\n",
       "    'early_stopping': [1],\n",
       "    'eval_freq': [1],\n",
       "    'exogenous_n_channels': [1],\n",
       "    'frequency': [1],\n",
       "    'idx_to_sample_freq': [1],\n",
       "    'initialization': [1],\n",
       "    'input_size_multiplier': [1],\n",
       "    'l1_theta': [1],\n",
       "    'learning_rate': [1],\n",
       "    'len_sample_chunks': [1],\n",
       "    'loss': [1],\n",
       "    'loss_hypar': [1],\n",
       "    'lr_decay': [1],\n",
       "    'n_blocks': [1],\n",
       "    'n_harmonics': [1],\n",
       "    'n_hidden': [1],\n",
       "    'n_iterations': [1],\n",
       "    'n_layers': [1],\n",
       "    'n_lr_decay_steps': [1],\n",
       "    'n_polynomials': [1],\n",
       "    'n_series_per_batch': [1],\n",
       "    'n_val_weeks': [1],\n",
       "    'normalizer_x': [1],\n",
       "    'normalizer_y': [1],\n",
       "    'output_size': [1],\n",
       "    'random_seed': [1],\n",
       "    'seasonality': [1],\n",
       "    'shared_weights': [1],\n",
       "    'stack_types': [1],\n",
       "    'val_idx_to_sample_freq': [1],\n",
       "    'val_loss': [1],\n",
       "    'weight_decay': [1],\n",
       "    'window_sampling_limit': [1],\n",
       "    'x_s_n_hidden': [1]},\n",
       "   'vals': {'activation': [0],\n",
       "    'batch_normalization': [0],\n",
       "    'batch_size': [0],\n",
       "    'complete_inputs': [0],\n",
       "    'complete_sample': [0],\n",
       "    'dropout_prob_exogenous': [0.22674402023168583],\n",
       "    'dropout_prob_theta': [0.3754955891490347],\n",
       "    'early_stopping': [0],\n",
       "    'eval_freq': [0],\n",
       "    'exogenous_n_channels': [8.0],\n",
       "    'frequency': [0],\n",
       "    'idx_to_sample_freq': [0],\n",
       "    'initialization': [1],\n",
       "    'input_size_multiplier': [0],\n",
       "    'l1_theta': [0],\n",
       "    'learning_rate': [0.0006223292463060922],\n",
       "    'len_sample_chunks': [0],\n",
       "    'loss': [0],\n",
       "    'loss_hypar': [0],\n",
       "    'lr_decay': [0.4876099208261274],\n",
       "    'n_blocks': [0],\n",
       "    'n_harmonics': [0],\n",
       "    'n_hidden': [0],\n",
       "    'n_iterations': [0],\n",
       "    'n_layers': [0],\n",
       "    'n_lr_decay_steps': [0],\n",
       "    'n_polynomials': [0],\n",
       "    'n_series_per_batch': [0],\n",
       "    'n_val_weeks': [0],\n",
       "    'normalizer_x': [0],\n",
       "    'normalizer_y': [0],\n",
       "    'output_size': [0],\n",
       "    'random_seed': [19.0],\n",
       "    'seasonality': [0],\n",
       "    'shared_weights': [0],\n",
       "    'stack_types': [2],\n",
       "    'val_idx_to_sample_freq': [0],\n",
       "    'val_loss': [0],\n",
       "    'weight_decay': [0.000971793150819528],\n",
       "    'window_sampling_limit': [0],\n",
       "    'x_s_n_hidden': [0]}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2021, 3, 16, 0, 45, 14, 382000),\n",
       "  'refresh_time': datetime.datetime(2021, 3, 16, 0, 45, 15, 196000)}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nixtla",
   "language": "python",
   "name": "nixtla"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
