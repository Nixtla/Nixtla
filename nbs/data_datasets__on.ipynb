{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "52b0028e7074a0058398558ea661882eddbb489e66a28504cc0449d2f54d6290"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.datasets.on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from dataclasses import dataclass\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 100\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "from nixtla.data.datasets.utils import download_file, Info, TimeSeriesDataclass\n",
    "from nixtla.data.ontsdataset import TimeSeriesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_on_data(root_dir, test_date, last_date):\n",
    "    # Declare future variables\n",
    "    f_cols = ['PD_0','PS_0','day_0','day_1','day_2','day_3','day_4','day_5','day_6',\n",
    "              'hour_0', 'hour_1','hour_2','hour_3','hour_4','hour_5','hour_6','hour_7','hour_8',\n",
    "              'hour_9','hour_10','hour_11','hour_12','hour_13','hour_14','hour_15','hour_16',\n",
    "              'hour_17','hour_18','hour_19','hour_20','hour_21','hour_22','hour_23',\n",
    "              'temperature_0', 'temperature_1', 'temperature_2', 'temperature_3', 'temperature_4',\n",
    "              'pressure_0', 'pressure_1', 'pressure_2', 'pressure_3', 'pressure_4', 'humidity_0',\n",
    "              'humidity_1', 'humidity_2', 'humidity_3', 'humidity_4', 'wind_speed_0', 'wind_speed_1',\n",
    "              'wind_speed_2', 'wind_speed_3', 'wind_speed_4', 'rain_0', 'rain_1', 'rain_2', 'rain_3', 'rain_4', 'total_demand']\n",
    "\n",
    "    # Demanda Final\n",
    "    demanda_final = pd.read_csv(f'{root_dir}/Demanda_Final.csv')\n",
    "    demanda_final.columns = ['ds','y']\n",
    "    demanda_final['unique_id'] = 'demanda_final'\n",
    "    demanda_final['ds'] = pd.to_datetime(demanda_final['ds'])\n",
    "    demanda_final = demanda_final[demanda_final['ds']<last_date]\n",
    "    demanda_final = demanda_final[demanda_final['ds']>='2017-10-07'].reset_index(drop=True)\n",
    "\n",
    "    # Ejecutado\n",
    "    ejecutado = pd.read_csv(f'{root_dir}/Despacho_Reprograma.csv')\n",
    "    ejecutado = ejecutado[['DATE_TIME','EJECUTADO']]\n",
    "    ejecutado.columns = ['ds','ejecutado']\n",
    "    ejecutado['ds'] = pd.to_datetime(ejecutado['ds'])\n",
    "    ejecutado = ejecutado[ejecutado['ds']<last_date]\n",
    "    ejecutado = ejecutado[ejecutado['ds']>='2017-10-07'].reset_index(drop=True)\n",
    "\n",
    "    # Train data (ON exogenous variables)\n",
    "    train_data = pd.read_csv(f'{root_dir}/Training_Data.csv')\n",
    "    train_data = train_data.rename(columns={'CURRENT_TIME':'ds'})\n",
    "    train_data['ds'] = pd.to_datetime(train_data['ds'])\n",
    "    train_data = train_data[train_data['ds']<last_date]\n",
    "    train_data = train_data[train_data['ds']>='2017-10-07'].reset_index(drop=True)\n",
    "\n",
    "    # Historic Weather\n",
    "    data_weather = pd.read_csv(f'{root_dir}/HistoricalWeather.csv')\n",
    "    data_weather['dt_iso'] = data_weather['dt_iso'].str[:-10]\n",
    "    data_weather['dt_iso'] = pd.to_datetime(data_weather['dt_iso'])\n",
    "    data_weather = data_weather[data_weather['dt_iso']>='2017-10-07'].reset_index(drop=True)\n",
    "    data_weather = data_weather[['dt_iso', 'city_name', 'temp', 'feels_like', 'rain_1h', 'pressure', 'humidity', 'wind_speed', 'clouds_all']]\n",
    "    data_weather = data_weather.drop_duplicates(subset=['dt_iso', 'city_name']).reset_index(drop=True)\n",
    "    data_weather = data_weather.pivot(index='dt_iso', columns='city_name').reset_index()\n",
    "    data_weather.columns = ['_'.join(col).strip() for col in data_weather.columns.values]\n",
    "    data_weather = data_weather.rename(columns = {'dt_iso_':'ds'})\n",
    "    data_weather = data_weather.fillna(0) # Fill rain nans with 0\n",
    "\n",
    "    # Forecast Weather\n",
    "    data_f_weather = pd.read_csv(f'{root_dir}/Historical ForecastWeather.csv')\n",
    "    # Between 1 and 21,600 takes next 6h of forecasts. Always uses last available forecast for each ds\n",
    "    data_f_weather = data_f_weather[(data_f_weather['slice dt unixtime']-data_f_weather['forecast dt unixtime']). \\\n",
    "                                    between(1, 21600)].reset_index(drop=True)\n",
    "    data_f_weather['forecast dt iso'] = data_f_weather['forecast dt iso'].str[:-10]\n",
    "    data_f_weather['forecast dt iso'] = pd.to_datetime(data_f_weather['forecast dt iso'])\n",
    "    data_f_weather = data_f_weather[data_f_weather['forecast dt iso']>='2017-10-07'].reset_index(drop=True)\n",
    "    data_f_weather = data_f_weather.rename(columns = {'forecast dt iso':'ds'})\n",
    "    data_f_weather['city'] = data_f_weather.groupby(['lat']).ngroup()\n",
    "    data_f_weather['city'] = data_f_weather['city'].astype(str)\n",
    "    data_f_weather = data_f_weather[['ds', 'city', 'temperature', 'pressure', 'humidity', 'wind_speed', 'rain']]\n",
    "    data_f_weather = data_f_weather.drop_duplicates(subset=['ds', 'city']).reset_index(drop=True)\n",
    "    data_f_weather = data_f_weather.pivot(index='ds', columns='city').reset_index()\n",
    "    data_f_weather.columns = ['_'.join(col).strip() for col in data_f_weather.columns.values]\n",
    "    data_f_weather = data_f_weather.rename(columns = {'ds_':'ds'})\n",
    "    data_f_weather = data_f_weather.fillna(0) # Fill rain nans with 0\n",
    "\n",
    "    # Total demand of large clients\n",
    "    large_demand = pd.read_csv(f'{root_dir}/usuarios_libres_demanda_prevista_semanal.csv')\n",
    "    large_demand['EQUIPO'] = pd.to_datetime(large_demand['EQUIPO'])\n",
    "    large_demand = large_demand[large_demand['EQUIPO']>='2017-10-07'].reset_index(drop=True)\n",
    "    large_demand = large_demand.fillna(0)\n",
    "    large_demand['total_demand'] = np.sum(large_demand.values[:,1:], axis=1)\n",
    "    large_demand = large_demand[['EQUIPO', 'total_demand']]\n",
    "    large_demand.columns = ['ds', 'total_demand']\n",
    "\n",
    "    # Merge\n",
    "    data = demanda_final.merge(ejecutado, how='left', indicator=False) #ejecutado needs to be first exogenous variable\n",
    "    data = data.merge(train_data, how='left', indicator=False)\n",
    "    data[['ejecutado']] = data[['ejecutado']].fillna(method='ffill') #ffill to avoid leakage from immediate next ejecutado\n",
    "    data = data.merge(data_weather, how='left', on='ds')\n",
    "    data = data.merge(data_f_weather, how='left', on='ds')\n",
    "    data = data.merge(large_demand, how='left', on='ds')\n",
    "    data = data.fillna(method='ffill') # ffill 15-mintues with last available forecast\n",
    "\n",
    "    data['hour'] = data['ds'].dt.hour\n",
    "    dummies = pd.get_dummies(data['hour'],prefix='hour')\n",
    "    data = pd.concat((data,dummies), axis=1)\n",
    "\n",
    "    data['day'] = data['ds'].dt.dayofweek\n",
    "    dummies = pd.get_dummies(data['day'],prefix='day')\n",
    "    data = pd.concat((data,dummies), axis=1)\n",
    "\n",
    "    dummies_cols = [col for col in data if (col.startswith('day') or col.startswith('hour_'))]\n",
    "\n",
    "    X_t_df = data.drop(columns=['hour','day','y'])\n",
    "    y_df = data[['unique_id','ds','y']]\n",
    "\n",
    "    Y_insample_df = y_df[y_df['ds'] < test_date].reset_index(drop=True)\n",
    "    Y_outsample_df = y_df[y_df['ds'] >= test_date].reset_index(drop=True)\n",
    "\n",
    "    X_insample_df = X_t_df[X_t_df['ds'] < test_date].reset_index(drop=True)\n",
    "    X_outsample_df = X_t_df[X_t_df['ds'] >= test_date].reset_index(drop=True)\n",
    "\n",
    "    return Y_insample_df, X_insample_df, Y_outsample_df, X_outsample_df, f_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_insample_df, X_insample_df, Y_outsample_df, X_outsample_df, f_cols = load_on_data('2020-11-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_insample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_insample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}