{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.ts_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch as t\n",
    "\n",
    "from fastcore.foundation import patch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# TODO: rename x_s -> s_list\n",
    "# TODO: rename ts_data -> ts_list\n",
    "# TODO: resolver t_cols y X_cols duplicados t_cols se usa en dataloader X_cols para indexar con f_cols\n",
    "#.      idea mantenemos solo X_cols y en el dataloader corregimos con 'y' y 'insample_mask' \n",
    "# TODO: assert no nans in numpy tensors en get_filter_tensor\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, #TODO: poner hint the tipo\n",
    "                 y_df,\n",
    "                 X_df = None,\n",
    "                 S_df = None,\n",
    "                 f_cols = None,\n",
    "                 ts_train_mask = None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        assert type(y_df) == pd.core.frame.DataFrame\n",
    "        assert all([(col in y_df) for col in ['unique_id', 'ds', 'y']])\n",
    "        if X_df is not None:\n",
    "            assert type(X_df) == pd.core.frame.DataFrame\n",
    "            assert all([(col in X_df) for col in ['unique_id', 'ds']])\n",
    "\n",
    "        print('Processing dataframes ...')\n",
    "        # Pandas dataframes to lists\n",
    "        ts_data, x_s, self.meta_data, self.t_cols, self.X_cols = self._df_to_lists(y_df=y_df, S_df=S_df, X_df=X_df)\n",
    "\n",
    "        # Attributes\n",
    "        self.n_series = len(ts_data)\n",
    "        self.max_len = max([len(ts['y']) for ts in ts_data])\n",
    "        self.n_channels = len(ts_data[0].values())\n",
    "        self.frequency = pd.infer_freq(y_df.head()['ds']) #TODO: improve, can die with head\n",
    "        self.f_cols = f_cols\n",
    "\n",
    "        # Number of X and S features\n",
    "        self.n_x, self.n_s = 0, 0\n",
    "        if X_df is not None:\n",
    "            self.n_x = len(self.X_cols)\n",
    "        if S_df is not None:\n",
    "            self.n_s = S_df.shape[1]-1 # 1 for unique_id\n",
    "\n",
    "        print('Creating ts tensor ...')\n",
    "        self.ts_tensor, self.x_s, self.len_series = self._create_tensor(ts_data, x_s)\n",
    "\n",
    "        if ts_train_mask is not None:\n",
    "            assert len(ts_train_mask)==self.max_len, f'Outsample mask must have length {self.max_len}'\n",
    "        else:\n",
    "            ts_train_mask = np.ones(self.max_len)\n",
    "\n",
    "        self._declare_outsample_train_mask(ts_train_mask)\n",
    "\n",
    "\n",
    "    def _df_to_lists(self, y_df, S_df, X_df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        unique_ids = y_df['unique_id'].unique()\n",
    "\n",
    "        if X_df is not None:\n",
    "            X_cols = [col for col in X_df.columns if col not in ['unique_id','ds']]\n",
    "        else:\n",
    "            X_cols = []\n",
    "\n",
    "        if S_df is not None:\n",
    "            S_cols = [col for col in S_df.columns if col not in ['unique_id']]\n",
    "        else:\n",
    "            S_cols = []\n",
    "\n",
    "        ts_data = []\n",
    "        x_s = []\n",
    "        meta_data = []\n",
    "        for i, u_id in enumerate(unique_ids):\n",
    "            top_row = np.asscalar(y_df['unique_id'].searchsorted(u_id, 'left'))\n",
    "            bottom_row = np.asscalar(y_df['unique_id'].searchsorted(u_id, 'right'))\n",
    "            serie = y_df[top_row:bottom_row]['y'].values\n",
    "            last_ds_i = y_df[top_row:bottom_row]['ds'].max()\n",
    "            \n",
    "            # Y values\n",
    "            ts_data_i = {'y': serie}\n",
    "            \n",
    "            # X values\n",
    "            for X_col in X_cols:\n",
    "                serie =  X_df[top_row:bottom_row][X_col].values\n",
    "                ts_data_i[X_col] = serie\n",
    "            ts_data.append(ts_data_i)\n",
    "\n",
    "            # S values\n",
    "            s_data_i = defaultdict(list)\n",
    "            for S_col in S_cols:\n",
    "                s_data_i[S_col] = S_df.loc[S_df['unique_id']==u_id, S_col].values\n",
    "            x_s.append(s_data_i)\n",
    "\n",
    "            # Metadata\n",
    "            meta_data_i = {'unique_id': u_id,\n",
    "                           'last_ds': last_ds_i}\n",
    "            meta_data.append(meta_data_i)\n",
    "\n",
    "        t_cols = ['y'] + X_cols + ['insample_mask', 'outsample_mask']\n",
    "\n",
    "        return ts_data, x_s, meta_data, t_cols, X_cols\n",
    "\n",
    "    def _create_tensor(self, ts_data, x_s):\n",
    "        \"\"\"\n",
    "        ts_tensor: n_series x n_channels x max_len\n",
    "        \"\"\"\n",
    "        ts_tensor = np.zeros((self.n_series, self.n_channels + 2, self.max_len)) # +2 for the masks\n",
    "        static_tensor = np.zeros((self.n_series, len(x_s[0])))\n",
    "\n",
    "        len_series = []\n",
    "        for idx in range(self.n_series):\n",
    "            ts_idx = np.array(list(ts_data[idx].values()))\n",
    "            ts_tensor[idx, :self.t_cols.index('insample_mask'), -ts_idx.shape[1]:] = ts_idx\n",
    "            ts_tensor[idx, self.t_cols.index('insample_mask'), -ts_idx.shape[1]:] = 1\n",
    "            # To avoid sampling windows without inputs to predict\n",
    "            # Outsample mask will be later completed with the 'train_mask'\n",
    "            ts_tensor[idx, self.t_cols.index('outsample_mask'), -(ts_idx.shape[1]-1):] = 1\n",
    "            static_tensor[idx, :] = list(x_s[idx].values())\n",
    "            len_series.append(ts_idx.shape[1])\n",
    "\n",
    "        return ts_tensor, static_tensor, np.array(len_series)\n",
    "\n",
    "    def _declare_outsample_train_mask(self, ts_train_mask):\n",
    "        # Update attribute and ts_tensor\n",
    "        self.ts_train_mask = ts_train_mask\n",
    "\n",
    "    def get_meta_data_col(self, col):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        col_values = [x[col] for x in self.meta_data]\n",
    "        return col_values\n",
    "\n",
    "def get_filtered_tensor(self, offset, output_size, window_sampling_limit, ts_idxs=None):\n",
    "        \"\"\"\n",
    "        Esto te da todo lo que tenga el tensor, el futuro incluido esto orque se usa exogenoas del futuro\n",
    "        La mascara se hace despues\n",
    "        \"\"\"\n",
    "        last_outsample_ds = self.max_len - offset + output_size\n",
    "        first_ds = max(last_outsample_ds - window_sampling_limit - output_size, 0)\n",
    "        if ts_idxs is None:\n",
    "            filtered_tensor = self.ts_tensor[:, :, first_ds:last_outsample_ds]\n",
    "        else:\n",
    "            filtered_tensor = self.ts_tensor[ts_idxs, :, first_ds:last_outsample_ds]\n",
    "        right_padding = max(last_outsample_ds - self.max_len, 0) #To padd with zeros if there is \"nothing\" to the right\n",
    "\n",
    "        train_mask = self.ts_train_mask[first_ds:last_outsample_ds]\n",
    "\n",
    "        return filtered_tensor, right_padding, train_mask\n",
    "\n",
    "def get_f_idxs(self, cols):\n",
    "        # Check if cols are available future variables and return idxs\n",
    "        assert all(col in self.f_cols for col in cols), f'Some variables in {cols} are not available in f_cols.'\n",
    "        f_idxs = [self.X_cols.index(col) for col in cols]\n",
    "        return f_idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing dataframes ...\nCreating ts tensor ...\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         3.73856371e+04, 3.84319699e+04, 4.03453300e+04],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         3.91049000e+05, 4.54041000e+05, 5.52942000e+05],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.80507000e+03, 2.13865000e+03, 2.68214000e+03],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         4.88040000e+00, 5.38680000e+00, 5.74060000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         2.32400000e+01, 2.52900000e+01, 2.73360000e+01],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         9.33115600e+06, 1.01999940e+07, 1.17027350e+07],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00]]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from nixtla.data.datasets.tourism import Tourism, TourismInfo\n",
    "tourism_dataset = Tourism.load(directory='data', group=TourismInfo.groups[0])\n",
    "tourism_dataset.ts_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['y', 'insample_mask', 'outsample_mask']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "tourism_dataset.t_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "tourism_dataset.ts_train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "tourism_dataset.ts_tensor[0,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([    0.    ,     0.    ,     0.    ,     0.    ,     0.    ,\n",
       "           0.    ,     0.    ,     0.    ,     0.    ,     0.    ,\n",
       "           0.    ,     0.    ,     0.    ,     0.    ,     0.    ,\n",
       "           0.    ,     0.    ,     0.    ,     0.    ,     0.    ,\n",
       "           0.    ,     0.    ,     0.    ,     0.    ,     0.    ,\n",
       "           0.    ,     0.    ,     0.    ,     0.    ,     0.    ,\n",
       "           0.    ,     0.    , 25092.2284, 24271.5134, 25828.9883,\n",
       "       27697.5047, 27956.2276, 29924.4321, 30216.8321, 32613.4968,\n",
       "       36053.1674, 38472.7532, 38420.894 , 36555.6156, 37385.6371,\n",
       "       38431.9699, 40345.33  ])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "tourism_dataset.ts_tensor[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "tourism_dataset.ts_train_mask[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rolling_sum(a, window_size=4) :\n",
    "#     ret = np.cumsum(np.concatenate((M, np.zeros(shape=(len(M), window_size-1))), axis=1), \n",
    "#                     axis=1, dtype=float)\n",
    "#     ret[:, window_size:] = ret[:, window_size:] - ret[:, :-window_size]\n",
    "#     return ret[:, window_size-1:]\n",
    "\n",
    "# M = np.array([[0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  1.],\n",
    "#               [0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "#               [1.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.]])\n",
    "\n",
    "# print(rolling_sum(M))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c68c0f618c4f787b1e02bfee278e48d25b62407cb335aab5258cedc4db4ced"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}