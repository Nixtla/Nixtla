{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c68c0f618c4f787b1e02bfee278e48d25b62407cb335aab5258cedc4db4ced"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.wnbeats.nbeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "import torch as t\n",
    "from torch import optim\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "from nixtla.models.wnbeats.nbeats_model import NBeats, NBeatsBlock, IdentityBasis, TrendBasis, SeasonalityBasis\n",
    "from nixtla.models.wnbeats.nbeats_model import XBasisTCN, XBasisWavenet\n",
    "from nixtla.losses.pytorch import MAPELoss, MASELoss, SMAPELoss, MSELoss, MAELoss, RMSELoss\n",
    "from nixtla.losses.numpy import mae, mse, mape, smape, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_weights(module, initialization):\n",
    "    if type(module) == t.nn.Linear:\n",
    "        if initialization == 'Orthogonal':\n",
    "            t.nn.init.orthogonal_(module.weight)\n",
    "        elif initialization == 'he_uniform':\n",
    "            t.nn.init.kaiming_uniform_(module.weight)\n",
    "        elif initialization == 'he_normal':\n",
    "            t.nn.init.kaiming_normal_(module.weight)\n",
    "        elif initialization == 'glorot_uniform':\n",
    "            t.nn.init.xavier_uniform_(module.weight)\n",
    "        elif initialization == 'glorot_normal':\n",
    "            t.nn.init.xavier_normal_(module.weight)\n",
    "        elif initialization == 'lecun_normal':\n",
    "            t.nn.init.normal_(module.weight, 0.0, std=1/np.sqrt(module.weight.numel()))\n",
    "        else:\n",
    "            assert 1<0, f'Initialization {initialization} not found'\n",
    "\n",
    "class Nbeats(object):\n",
    "    \"\"\"\n",
    "    Future documentation\n",
    "    \"\"\"\n",
    "    SEASONALITY_BLOCK = 'seasonality'\n",
    "    TREND_BLOCK = 'trend'\n",
    "    IDENTITY_BLOCK = 'identity'\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_x_t,\n",
    "                 n_x_s,\n",
    "                 t_cols,\n",
    "                 include_var_dict,\n",
    "                 input_size_multiplier,\n",
    "                 output_size,\n",
    "                 shared_weights,\n",
    "                 activation,\n",
    "                 initialization,\n",
    "                 stack_types,\n",
    "                 n_blocks,\n",
    "                 n_theta_hidden_list,\n",
    "                 n_xbasis_layers,\n",
    "                 n_xbasis_channels,\n",
    "                 n_s_hidden,\n",
    "                 scaler,\n",
    "                 batch_normalization,\n",
    "                 learning_rate,\n",
    "                 lr_decay,\n",
    "                 n_lr_decay_steps,\n",
    "                 dropout_prob_theta,\n",
    "                 dropout_prob_xbasis,\n",
    "                 lambda_l1_theta,\n",
    "                 lambda_l1_input,\n",
    "                 lambda_l2_xbasis,\n",
    "                 max_epochs,\n",
    "                 early_stopping,\n",
    "                 loss,\n",
    "                 frequency,\n",
    "                 seasonality,\n",
    "                 random_seed,\n",
    "                 device=None):\n",
    "        super(Nbeats, self).__init__()\n",
    "\n",
    "        if activation == 'selu': initialization = 'lecun_normal'\n",
    "\n",
    "        # Attributes of ts_dataset\n",
    "        # TODO: pensar como quitar redundancia\n",
    "        self.n_x_t  = n_x_t\n",
    "        self.n_x_s  = n_x_s\n",
    "        self.t_cols = t_cols\n",
    "        self.include_var_dict = include_var_dict\n",
    "\n",
    "        # Architecture parameters\n",
    "        self.input_size            = int(input_size_multiplier*output_size)\n",
    "        self.output_size           = output_size\n",
    "        self.shared_weights        = shared_weights\n",
    "        self.activation            = activation\n",
    "        self.initialization        = initialization\n",
    "        self.stack_types           = stack_types\n",
    "        self.n_blocks              = n_blocks\n",
    "        self.n_theta_hidden_list   = n_theta_hidden_list\n",
    "        self.n_xbasis_layers       = n_xbasis_layers\n",
    "        self.n_xbasis_channels     = n_xbasis_channels\n",
    "        self.n_s_hidden            = n_s_hidden\n",
    "\n",
    "        # Regularization and optimization parameters\n",
    "        self.scaler                = scaler\n",
    "        self.batch_normalization   = batch_normalization\n",
    "        self.learning_rate         = learning_rate\n",
    "        self.lr_decay              = lr_decay\n",
    "        self.n_lr_decay_steps      = n_lr_decay_steps\n",
    "        # self.weight_decay         = weight_decay\n",
    "\n",
    "        self.loss                  = loss\n",
    "        self.dropout_prob_theta    = dropout_prob_theta\n",
    "        self.dropout_prob_xbasis   = dropout_prob_xbasis\n",
    "        self.lambda_l1_theta       = lambda_l1_theta\n",
    "        self.lambda_l1_input       = lambda_l1_input\n",
    "        self.lambda_l2_xbasis      = lambda_l2_xbasis\n",
    "        self.max_epochs            = max_epochs\n",
    "        self.early_stopping        = early_stopping\n",
    "\n",
    "        # Regularization and optimization parameters\n",
    "        self.frequency   = frequency\n",
    "        self.seasonality = seasonality\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        if device is None: device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
    "        self.device = device\n",
    "        self._is_instantiated = False\n",
    "\n",
    "    def create_stack(self):\n",
    "        if self.include_var_dict is not None:\n",
    "            x_t_n_inputs = self.output_size * int(sum([len(x) for x in self.include_var_dict.values()]))\n",
    "\n",
    "            # Correction because week_day only adds 1 no output_size\n",
    "            if len(self.include_var_dict['week_day'])>0:\n",
    "                x_t_n_inputs = x_t_n_inputs - self.output_size + 1\n",
    "        else:\n",
    "            x_t_n_inputs = self.input_size\n",
    "\n",
    "        # Architecture definition\n",
    "        block_list = []\n",
    "        for i in range(len(self.stack_types)):\n",
    "            #print(f'| --  Stack {self.stack_types[i]} (#{i})')\n",
    "            for block_id in range(self.n_blocks[i]):\n",
    "                # Batch norm only on first block\n",
    "                if (len(block_list)==0) and (self.batch_normalization):\n",
    "                    batch_normalization_block = True\n",
    "                else:\n",
    "                    batch_normalization_block = False\n",
    "\n",
    "                # Shared weights\n",
    "                if self.shared_weights and block_id>0:\n",
    "                    nbeats_block = block_list[-1]\n",
    "\n",
    "                else:\n",
    "                    if self.stack_types[i] == 'identity':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.n_s_hidden,\n",
    "                                                   theta_n_dim=self.input_size + self.output_size,\n",
    "                                                   basis=IdentityBasis(backcast_size=self.input_size,\n",
    "                                                                       forecast_size=self.output_size),\n",
    "                                                   n_theta_hidden_list=self.n_theta_hidden_list[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous_tcn':\n",
    "                        #assert len(self.f_cols)>0, 'If Xbasis, provide f_cols hyperparameter'\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden = self.n_s_hidden,\n",
    "                                                   theta_n_dim = 2*(self.n_xbasis_channels),\n",
    "                                                   basis=XBasisTCN(out_features=self.n_xbasis_channels,\n",
    "                                                                   in_features=self.n_x_t,\n",
    "                                                                   #f_idxs=self.f_idxs,\n",
    "                                                                   num_levels=self.n_xbasis_layers,\n",
    "                                                                   dropout_prob=self.dropout_prob_xbasis),\n",
    "                                                   n_theta_hidden_list=self.n_theta_hidden_list[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous_wavenet':\n",
    "                        #assert len(self.f_cols)>0, 'If Xbasis, provide f_cols hyperparameter'\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden = self.n_s_hidden,\n",
    "                                                   theta_n_dim = 2*(self.n_xbasis_channels),\n",
    "                                                   basis=XBasisWavenet(out_features=self.n_xbasis_channels,\n",
    "                                                                       in_features=self.n_x_t,\n",
    "                                                                       #f_idxs=self.f_idxs,\n",
    "                                                                       num_levels=self.n_xbasis_layers,\n",
    "                                                                       dropout_prob=self.dropout_prob_xbasis),\n",
    "                                                   n_theta_hidden_list=self.n_theta_hidden_list[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "\n",
    "                # Select type of evaluation and apply it to all layers of block\n",
    "                init_function = partial(init_weights, initialization=self.initialization)\n",
    "                nbeats_block.layers.apply(init_function)\n",
    "                block_list.append(nbeats_block)\n",
    "        return block_list\n",
    "\n",
    "    def __loss_fn(self, loss_name: str):\n",
    "        def loss(x, freq, forecast, target, mask):\n",
    "            if loss_name == 'MAPE':\n",
    "                return MAPELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.reg_loss_theta() + self.reg_loss_xbasis()\n",
    "            elif loss_name == 'MASE':\n",
    "                return MASELoss(y=target, y_hat=forecast, y_insample=x, seasonality=freq, mask=mask) + \\\n",
    "                       self.reg_loss_theta() + self.reg_loss_xbasis()\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return SMAPELoss(y=target, y_hat=forecast, mask=mask) + self.l1_regularization() + \\\n",
    "                       self.reg_loss_theta() + self.reg_loss_xbasis()\n",
    "            elif loss_name == 'MSE':\n",
    "                return MSELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.reg_loss_theta() + self.reg_loss_xbasis()\n",
    "            elif loss_name == 'RMSE':\n",
    "                return RMSELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.reg_loss_theta() + self.reg_loss_xbasis()\n",
    "            elif loss_name == 'MAE':\n",
    "                return MAELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.reg_loss_theta() + self.reg_loss_xbasis()\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def __val_loss_fn(self, loss_name: str):\n",
    "        #TODO: mase not implemented\n",
    "        def loss(forecast, target, weights):\n",
    "            if loss_name == 'MAPE':\n",
    "                return mape(y=target, y_hat=forecast, weights=weights) #TODO: faltan weights\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return smape(y=target, y_hat=forecast, weights=weights) #TODO: faltan weights\n",
    "            elif loss_name == 'MSE':\n",
    "                return mse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'RMSE':\n",
    "                return rmse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MAE':\n",
    "                return mae(y=target, y_hat=forecast, weights=weights)\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def reg_loss_xbasis(self):\n",
    "        loss = 0.0\n",
    "        for i in range(len(self.stack_types)):\n",
    "            for block_id in range(self.n_blocks[i]):\n",
    "                if self.stack_types[i] == 'exogenous_tcn':\n",
    "                    for layer in self.model.blocks[i].modules():\n",
    "                        if isinstance(layer, t.nn.Conv1d):\n",
    "                            loss += self.lambda_l2_xbasis * t.norm(layer.weight)\n",
    "        return loss\n",
    "\n",
    "    def reg_loss_theta(self):\n",
    "        # L1 loss for initial exogenous input\n",
    "        loss = self.lambda_l1_input * t.sum(t.abs(self.model.l1_weight))\n",
    "\n",
    "        # L2/L1 regularization for thetas\n",
    "        for i in range(len(self.stack_types)):\n",
    "            for block_id in range(self.n_blocks[i]):\n",
    "                for layer in self.model.blocks[i].modules():\n",
    "                    if isinstance(layer, t.nn.Linear):\n",
    "                        # loss += self.lambda_reg_theta * t.norm(layer.weight)\n",
    "                        loss += self.lambda_l1_theta * layer.weight.abs().sum()\n",
    "        return loss\n",
    "\n",
    "    def to_tensor(self, x: np.ndarray) -> t.Tensor:\n",
    "        tensor = t.as_tensor(x, dtype=t.float32).to(self.device)\n",
    "        return tensor\n",
    "\n",
    "    def predict(self, ts_loader, eval_mode=False):\n",
    "        self.model.eval()\n",
    "        \n",
    "        forecasts = []\n",
    "        outsample_ys = []\n",
    "        with t.no_grad():\n",
    "            for batch in iter(ts_loader):\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                forecast = self.model(insample_y=insample_y, insample_x_t=insample_x,\n",
    "                                      insample_mask=insample_mask, outsample_x_t=outsample_x, x_s=s_matrix)\n",
    "                \n",
    "                forecasts += [forecast.cpu().data.numpy()]\n",
    "                outsample_ys += [outsample_y.cpu().data.numpy()]\n",
    "\n",
    "        forecasts = np.vstack(forecasts)\n",
    "        outsample_ys = np.vstack(outsample_ys)\n",
    "\n",
    "        if self.scaler is not None:\n",
    "            if len(outsample_ys.shape) == 1:\n",
    "                forecasts = forecasts.reshape(-1, 1)\n",
    "                outsample_ys = outsample_ys.reshape(-1, 1)\n",
    "            forecasts = self.scaler.inverse_transform(forecasts)\n",
    "            if eval_mode: # Otherwise outsample_ys is None from batch\n",
    "                outsample_ys = self.scaler.inverse_transform(outsample_ys)\n",
    "\n",
    "        self.model.train()\n",
    "        if eval_mode:\n",
    "            return forecasts, outsample_ys\n",
    "        else:\n",
    "            return forecasts\n",
    "\n",
    "    def evaluate_performance(self, ts_loader, validation_loss_fn):\n",
    "        y_hat, y = self.predict(ts_loader=ts_loader, eval_mode=True)\n",
    "\n",
    "        y = y.flatten()\n",
    "        y_hat = y_hat.flatten()\n",
    "\n",
    "        loss = validation_loss_fn(target=y, forecast=y_hat, weights=np.ones(len(y)))\n",
    "        return loss\n",
    "\n",
    "    def fit(self, train_ts_loader, val_ts_loader=None, max_epochs=None, verbose=True, eval_steps=1):\n",
    "        # Asserts\n",
    "        assert self.t_cols[0] == 'y', f'First variable must be y not {self.t_cols[0]}'\n",
    "        #assert (self.input_size)==train_ts_loader.input_size, \\\n",
    "        #    f'model input_size {self.input_size} data input_size {train_ts_loader.input_size}'\n",
    "\n",
    "        # Random Seeds (model initialization)\n",
    "        t.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        #random.seed(self.random_seed) #TODO: interaccion rara con window_sampling de validacion\n",
    "\n",
    "        # Instantiate model\n",
    "        #if not self._is_instantiated:\n",
    "        block_list = self.create_stack()\n",
    "        self.model = NBeats(blocks=t.nn.ModuleList(block_list), in_features=self.n_x_t).to(self.device)\n",
    "        self._is_instantiated = True\n",
    "\n",
    "        # Overwrite max_epochs and train datasets\n",
    "        if max_epochs is None:\n",
    "            max_epochs = self.max_epochs\n",
    "\n",
    "        lr_decay_step_size = max_epochs // self.n_lr_decay_steps\n",
    "\n",
    "        #optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_step_size, gamma=self.lr_decay)\n",
    "\n",
    "        training_loss_fn = self.__loss_fn(self.loss)\n",
    "        validation_loss_fn = self.__val_loss_fn(self.loss) #Uses numpy losses\n",
    "\n",
    "        if verbose and (max_epochs > 0):\n",
    "            print('='*30+' Start fitting '+'='*30)\n",
    "            print(f'Number of exogenous variables: {self.n_x_t}')\n",
    "            print(f'Number of static variables: {self.n_x_s} , with dim_hidden: {self.n_s_hidden}')\n",
    "            print(f'Number of iterations: {max_epochs}')\n",
    "            print(f'Number of blocks: {len(self.model.blocks)}')\n",
    "\n",
    "        #self.loss_dict = {} # Restart self.loss_dict\n",
    "        start = time.time()\n",
    "        self.trajectories = {'step':[],'train_loss':[], 'val_loss':[]}\n",
    "        self.final_insample_loss = None\n",
    "        self.final_outsample_loss = None\n",
    "\n",
    "        # Training Loop\n",
    "        best_val_loss = np.inf\n",
    "        best_insample_loss = np.inf\n",
    "        break_flag = False\n",
    "        early_stopping_counter = 0\n",
    "        # Save initial weights as best_state_dict\n",
    "        best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "        for step in range(max_epochs):\n",
    "            self.model.train()\n",
    "            for batch in iter(train_ts_loader):\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                forecast = self.model(insample_y=insample_y, insample_x_t=insample_x,\n",
    "                                    insample_mask=insample_mask, outsample_x_t=outsample_x, x_s=s_matrix)\n",
    "\n",
    "                training_loss = training_loss_fn(x=insample_y, freq=self.seasonality, forecast=forecast,\n",
    "                                                target=outsample_y, mask=outsample_mask)\n",
    "\n",
    "                training_loss.backward()\n",
    "                t.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            lr_scheduler.step()\n",
    "            if (step % eval_steps == 0):\n",
    "                display_string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(step,\n",
    "                                                                                time.time()-start,\n",
    "                                                                                self.loss,\n",
    "                                                                                training_loss.cpu().data.numpy())\n",
    "                self.trajectories['step'].append(step)\n",
    "                self.trajectories['train_loss'].append(training_loss.cpu().data.numpy())\n",
    "\n",
    "                if val_ts_loader is not None:\n",
    "                    loss = self.evaluate_performance(val_ts_loader, validation_loss_fn=validation_loss_fn)\n",
    "                    display_string += \", Outsample {}: {:.5f}\".format(self.loss, loss)\n",
    "                    self.trajectories['val_loss'].append(loss)\n",
    "\n",
    "                    if self.early_stopping:\n",
    "                        if loss < best_val_loss:\n",
    "                            # Save current model if improves outsample loss\n",
    "                            best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "                            best_insample_loss = training_loss.cpu().data.numpy()\n",
    "                            early_stopping_counter = 0\n",
    "                            best_val_loss = loss\n",
    "                        else:\n",
    "                            early_stopping_counter += 1\n",
    "                        if early_stopping_counter >= self.early_stopping:\n",
    "                            break_flag = True\n",
    "\n",
    "                print(display_string)\n",
    "\n",
    "                self.model.train()\n",
    "                #train_ts_loader.train()\n",
    "\n",
    "            if break_flag:\n",
    "                print(\"\\n\")\n",
    "                print(17*'-',' Stopped training with early stopping', 17*'-')\n",
    "                self.model.load_state_dict(best_state_dict)\n",
    "                break\n",
    "\n",
    "        #End of fitting\n",
    "        if max_epochs > 0:\n",
    "            self.final_insample_loss = training_loss.cpu().data.numpy() if not break_flag else best_insample_loss #This is batch!\n",
    "            string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(step,\n",
    "                                                                            time.time()-start,\n",
    "                                                                            self.loss,\n",
    "                                                                            self.final_insample_loss)\n",
    "            if val_ts_loader is not None:\n",
    "                self.final_outsample_loss = self.evaluate_performance(val_ts_loader, validation_loss_fn=validation_loss_fn)\n",
    "                string += \", Outsample {}: {:.5f}\".format(self.loss, self.final_outsample_loss)\n",
    "            print(string)\n",
    "            print('='*30+' End  fitting '+'='*30)\n",
    "            print(\"\\n\")\n",
    "\n",
    "    def save(self, model_dir, model_id):\n",
    "\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        print('Saving model to:\\n {}'.format(model_file)+'\\n')\n",
    "        t.save({'model_state_dict': self.model.state_dict()}, model_file)\n",
    "\n",
    "    def load(self, model_dir, model_id):\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        path = Path(model_file)\n",
    "\n",
    "        assert path.is_file(), 'No model_*.model file found in this path!'\n",
    "\n",
    "        print('Loading model from:\\n {}'.format(model_file)+'\\n')\n",
    "\n",
    "        checkpoint = t.load(model_file, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(self.device)\n",
    "\n",
    "\n",
    "    def recalibrate_and_forecast_next_day(self, mc, df, next_day_date):\n",
    "        \"\"\"Method that builds an easy-to-use interface for daily recalibration and forecasting of the DNN model\n",
    "        \n",
    "        The method receives a pandas dataframe ``df`` and a day ``next_day_date``. Then, it \n",
    "        recalibrates the model using data up to the day before ``next_day_date`` and makes a prediction\n",
    "        for day ``next_day_date``.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            Dataframe of historical data containing prices and N exogenous inputs. The index of the \n",
    "            dataframe should be dates with hourly frequency. The columns should have the following \n",
    "            names ['Price', 'Exogenous 1', 'Exogenous 2', ...., 'Exogenous N']\n",
    "        next_day_date : TYPE\n",
    "            Date of the day-ahead\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.array\n",
    "            An array containing the predictions in the provided date\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        #---------------------------------------- Parse data ----------------------------------------#\n",
    "\n",
    "        # We define the new training dataset considering the last calibration_window years of data \n",
    "        df_train = df.loc[:next_day_date - pd.Timedelta(hours=1)]\n",
    "        df_train = df_train.loc[next_day_date - pd.Timedelta(hours=mc['calibration_window'] * 364 * 24):]\n",
    "\n",
    "        # We define the test dataset as the next day (they day of interest) plus the last two weeks\n",
    "        # in order to be able to build the necessary input features.\n",
    "        df_test = df.loc[next_day_date - pd.Timedelta(weeks=2):, :]\n",
    "\n",
    "        # Generating training, validation, and test input and outpus. For the test dataset,\n",
    "        # even though the dataframe contains 15 days of data (next day + last 2 weeks),\n",
    "        # we provide as parameter the date of interest so that Xtest and Ytest only reflect that\n",
    "        Xtrain, Ytrain, Xval, Yval, Xtest, _, _ = \\\n",
    "            _build_and_split_XYs(dfTrain=df_train, features=mc, percentage_val=0.25,\n",
    "                                 shuffle_train=True, dfTest=df_test, date_test=next_day_date,\n",
    "                                 data_augmentation=mc['data_augmentation'], \n",
    "                                 n_exogenous_inputs=len(df_train.columns) - 1)\n",
    "\n",
    "        #---------------------------------------- Scale data ----------------------------------------#\n",
    "        # If required, datasets are scaled\n",
    "        if mc['scaleX'] in ['Norm', 'Norm1', 'Std', 'Median', 'Invariant']:\n",
    "            [Xtrain, Xval, Xtest], _ = scaling([Xtrain, Xval, Xtest], mc['scaleX'])\n",
    "\n",
    "        if mc['scaleY'] in ['Norm', 'Norm1', 'Std', 'Median', 'Invariant']:\n",
    "            [Ytrain, Yval], self.scaler = scaling([Ytrain, Yval], mc['scaleY'])\n",
    "        else:\n",
    "            self.scaler = None\n",
    "\n",
    "        train_dataset = TimeSeriesDataset(X=Xtrain, Y=Ytrain)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=mc['batch_size'], shuffle=True, num_workers=0)\n",
    "\n",
    "        val_dataset = TimeSeriesDataset(X=Xval, Y=Yval)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=len(Xval), shuffle=False, num_workers=0)\n",
    "\n",
    "        test_dataset = TimeSeriesDataset(X=Xtest, Y=None)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=len(Xtest), shuffle=False, num_workers=0)\n",
    "\n",
    "        #---------------------------------------- Recalibrate ----------------------------------------#\n",
    "        # Initialize and fit model\n",
    "        self.fit(train_loader, val_ts_loader=val_loader, verbose=True, eval_steps=mc['eval_steps']) \n",
    "\n",
    "        #---------------------------------------- Predict ----------------------------------------#\n",
    "        # Make prediction\n",
    "        y_hat = self.predict(ts_loader=test_loader, eval_mode=False)\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model(model):\n",
    "    print(120*'=')\n",
    "    line = '{:<40} {:<40} {:<40}'.format('Weights', 'Parameter shape', 'Number of parameters')\n",
    "    print(line)\n",
    "    print(120*'-')\n",
    "    total_params = 0\n",
    "    for model_param_name, model_param_value in model.model.named_parameters():\n",
    "        line = '{:<50} {:<40} {:<40}'.format(model_param_name, str(model_param_value.shape), len(model_param_value.flatten()))\n",
    "        total_params += len(model_param_value.flatten())\n",
    "        print(line)\n",
    "        print(120*'-')\n",
    "    print(f'Total parameters: {total_params}')\n",
    "    print(120*'=')\n",
    "    print(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, Y=None):\n",
    "        \"\"\"\n",
    "        \"\"\"        \n",
    "        self.insample_y  = X[:, (7*0)*24:(7*1)*24]\n",
    "\n",
    "        insample_x1 = X[:, (7*1)*24:(7*2)*24]\n",
    "        insample_x2 = X[:, (7*2)*24:(7*3)*24]\n",
    "        insample_x3 = X[:, (7*3)*24:(7*4)*24]\n",
    "        \n",
    "        self.insample_x  = np.concatenate([insample_x1[:,None,:], \n",
    "                                           insample_x2[:,None,:], \n",
    "                                           insample_x3[:,None,:]], axis=1)\n",
    "\n",
    "        self.insample_mask = np.ones(shape=self.insample_y.shape)\n",
    "        \n",
    "        outsample_x1 = X[:, (7*4+0)*24:(7*4+1)*24]\n",
    "        outsample_x2 = X[:, (7*4+1)*24:(7*4+2)*24]\n",
    "        outsample_x3 = X[:, (7*4+2)*24:(7*4+3)*24]\n",
    "        \n",
    "        self.outsample_x  = np.concatenate([outsample_x1[:,None,:], \n",
    "                                            outsample_x2[:,None,:], \n",
    "                                            outsample_x3[:,None,:]], axis=1)\n",
    "\n",
    "        self.outsample_y = Y\n",
    "        self.outsample_mask = np.ones(shape=(self.outsample_x.shape[0], self.outsample_x.shape[2]))\n",
    "\n",
    "        assert np.sum(np.isnan(X))<1.0, \\\n",
    "            f' X has {np.sum(np.isnan(X))} nan values'\n",
    "\n",
    "        if Y is not None:\n",
    "            assert np.sum(np.isnan(Y))<1.0, \\\n",
    "                f'Y has {np.sum(np.isnan(Y))} nan values'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.outsample_x)\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        # insample y\n",
    "        insample_y     = self.insample_y[idx, :]\n",
    "\n",
    "        # insample and outsample x\n",
    "        insample_x     = self.insample_x[idx, :, :]\n",
    "        outsample_x    = self.outsample_x[idx, :, :]\n",
    "        \n",
    "        # train masks\n",
    "        insample_mask  = self.insample_mask[idx, :]\n",
    "        outsample_mask = self.outsample_mask[idx, :]\n",
    "\n",
    "        # outsample y\n",
    "        if self.outsample_y is not None:\n",
    "            outsample_y    = self.outsample_y[idx, :]\n",
    "        else:\n",
    "            outsample_y    = []\n",
    "\n",
    "        batch = {'s_matrix': [],\n",
    "                 'insample_y': insample_y, 'insample_x':insample_x, 'insample_mask':insample_mask,\n",
    "                 'outsample_y': outsample_y, 'outsample_x':outsample_x, 'outsample_mask':outsample_mask}\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X: time series features, of shape (#series,#times,#features): \t(1087, 744)\nY: target series (in X), of shape (#series,#times): \t \t(1087, 24)\n\n\n\nDataloaderFast batch time: 0.001127004623413086\ninsample_y.shape torch.Size([4, 168])\ninsample_x.shape torch.Size([4, 3, 168])\ninsample_mask.shape torch.Size([4, 168])\noutsample_y.shape torch.Size([4, 24])\noutsample_x.shape torch.Size([4, 3, 24])\noutsample_mask.shape torch.Size([4, 24])\nt.max(insample_y) tensor(0.2681, dtype=torch.float64)\nt.max(outsample_y) tensor(35.5800, dtype=torch.float64)\n\n\n\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch as t\n",
    "import copy\n",
    "from fastcore.foundation import patch\n",
    "from nixtla.data.scalers import scaling\n",
    "# from nixtla.data.ontsdataset import TimeSeriesDataset\n",
    "# from nixtla.data.ontsloader_fast import TimeSeriesLoader as TimeSeriesLoaderFast\n",
    "\n",
    "np.random.seed(1)\n",
    "t.manual_seed(1)\n",
    "\n",
    "Xtrain = np.genfromtxt('../data/epf/Xtrain.csv')\n",
    "Ytrain = np.genfromtxt('../data/epf/Ytrain.csv')\n",
    "\n",
    "[Xtrain], _ = scaling(datasets=[Xtrain], normalize='Norm1')\n",
    "[Y_train], scaler = scaling(datasets=[Ytrain], normalize='Norm1')\n",
    "\n",
    "print('X: time series features, of shape (#series,#times,#features): \\t' + str(Xtrain.shape))\n",
    "print('Y: target series (in X), of shape (#series,#times): \\t \\t' + str(Ytrain.shape))\n",
    "# print('S: static features, of shape (#series,#features): \\t \\t' + str(S.shape))\n",
    "#Y_insample_df.head()\n",
    "#Y_insample_df.tail()\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X=Xtrain, Y=Ytrain)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4,\n",
    "                          shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = TimeSeriesDataset(X=Xtrain, Y=Ytrain)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)                        \n",
    "\n",
    "start = time.time()\n",
    "dataloader = iter(train_loader)\n",
    "batch = next(dataloader)\n",
    "insample_y = batch['insample_y']\n",
    "insample_x = batch['insample_x']\n",
    "insample_mask = batch['insample_mask']\n",
    "outsample_x = batch['outsample_x']\n",
    "outsample_y = batch['outsample_y']\n",
    "outsample_mask = batch['outsample_mask']\n",
    "\n",
    "print(\"DataloaderFast batch time:\", time.time()-start)\n",
    "print(\"insample_y.shape\", insample_y.shape)\n",
    "print(\"insample_x.shape\", insample_x.shape)\n",
    "print(\"insample_mask.shape\", insample_mask.shape)\n",
    "\n",
    "print(\"outsample_y.shape\", outsample_y.shape)\n",
    "print(\"outsample_x.shape\", outsample_x.shape)\n",
    "print(\"outsample_mask.shape\", outsample_mask.shape)\n",
    "\n",
    "print(\"t.max(insample_y)\", t.max(insample_y))\n",
    "print(\"t.max(outsample_y)\", t.max(outsample_y * outsample_mask))\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================== Start fitting ==============================\n",
      "Number of exogenous variables: 3\n",
      "Number of static variables: 0 , with dim_hidden: 10\n",
      "Number of iterations: 20\n",
      "Number of blocks: 4\n",
      "Step: 0, Time: 3.722, Insample MAE: 5.18549, Outsample MAE: 139.05465\n",
      "Step: 1, Time: 8.133, Insample MAE: 4.82959, Outsample MAE: 114.44603\n",
      "Step: 2, Time: 12.571, Insample MAE: 4.15717, Outsample MAE: 122.55745\n",
      "Step: 3, Time: 16.998, Insample MAE: 5.20711, Outsample MAE: 103.47394\n",
      "Step: 4, Time: 21.431, Insample MAE: 4.87293, Outsample MAE: 103.24238\n",
      "Step: 5, Time: 25.856, Insample MAE: 4.04834, Outsample MAE: 99.55143\n",
      "Step: 6, Time: 31.311, Insample MAE: 4.44104, Outsample MAE: 102.26979\n",
      "Step: 7, Time: 36.075, Insample MAE: 4.15949, Outsample MAE: 102.07084\n",
      "Step: 8, Time: 40.512, Insample MAE: 4.87403, Outsample MAE: 104.04875\n",
      "Step: 9, Time: 45.148, Insample MAE: 3.43742, Outsample MAE: 97.02498\n",
      "Step: 10, Time: 49.568, Insample MAE: 4.34725, Outsample MAE: 102.09178\n",
      "Step: 11, Time: 54.047, Insample MAE: 3.01757, Outsample MAE: 99.40977\n",
      "Step: 12, Time: 58.450, Insample MAE: 2.69785, Outsample MAE: 97.00988\n",
      "Step: 13, Time: 62.861, Insample MAE: 3.10275, Outsample MAE: 96.66817\n",
      "Step: 14, Time: 67.286, Insample MAE: 3.32968, Outsample MAE: 96.53252\n",
      "Step: 15, Time: 71.718, Insample MAE: 2.38081, Outsample MAE: 100.57180\n",
      "Step: 16, Time: 76.189, Insample MAE: 2.59379, Outsample MAE: 103.70461\n",
      "Step: 17, Time: 80.588, Insample MAE: 2.39455, Outsample MAE: 100.47330\n",
      "Step: 18, Time: 84.975, Insample MAE: 1.86696, Outsample MAE: 97.95220\n",
      "Step: 19, Time: 89.369, Insample MAE: 2.84628, Outsample MAE: 97.14386\n",
      "Step: 19, Time: 90.053, Insample MAE: 2.84628, Outsample MAE: 97.14386\n",
      "============================== End  fitting ==============================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nbeatsx = Nbeats(n_x_t=3,\n",
    "                 n_x_s=0,\n",
    "                 t_cols=['y', 'Exogenous1', 'Exogenous2', 'week_day'],\n",
    "                 include_var_dict={'y': [-2, -8], \n",
    "                                   'Exogenous1': [-2, -8],\n",
    "                                   'Exogenous2': [-2, -8],\n",
    "                                   'week_day': [-1]},\n",
    "                 input_size_multiplier=7,\n",
    "                 output_size=24,\n",
    "                 shared_weights=False,\n",
    "                 activation='relu',\n",
    "                 initialization='lecun_normal',                \n",
    "                 stack_types=['exogenous_wavenet']+3*['identity'],\n",
    "                 n_blocks=4*[1],\n",
    "                 n_theta_hidden_list=4*[[256,256]],\n",
    "                 n_xbasis_layers=4,\n",
    "                 n_xbasis_channels=2,\n",
    "                 n_s_hidden=10,\n",
    "                 scaler=scaler, # It is updated in recalibrate_and_forecast_next_day method\n",
    "                 batch_normalization=False,\n",
    "                 learning_rate=0.001,\n",
    "                 lr_decay=0.5,\n",
    "                 n_lr_decay_steps=3,\n",
    "                 dropout_prob_theta=0.1,\n",
    "                 dropout_prob_xbasis=0.1,\n",
    "                 lambda_l1_input=0.01,\n",
    "                 lambda_l1_theta=0.01,\n",
    "                 lambda_l2_xbasis=0.01,\n",
    "                 max_epochs=100,\n",
    "                 early_stopping=20,\n",
    "                 loss='MAE',\n",
    "                 frequency=24,\n",
    "                 seasonality='H',\n",
    "                 random_seed=1)\n",
    "\n",
    "nbeatsx.fit(train_ts_loader=train_loader, val_ts_loader=val_loader, max_epochs=20, verbose=True, eval_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}