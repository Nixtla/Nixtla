{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c68c0f618c4f787b1e02bfee278e48d25b62407cb335aab5258cedc4db4ced"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.wnbeats.nbeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "import torch as t\n",
    "from torch import optim\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "from nixtla.models.wnbeats.nbeats_model import NBeats, NBeatsBlock, IdentityBasis, TrendBasis, SeasonalityBasis\n",
    "from nixtla.models.wnbeats.nbeats_model import XBasisTCN, XBasisWavenet\n",
    "from nixtla.losses.pytorch import MAPELoss, MASELoss, SMAPELoss, MSELoss, MAELoss, RMSELoss\n",
    "from nixtla.losses.numpy import mae, mse, mape, smape, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_weights(module, initialization):\n",
    "    if type(module) == t.nn.Linear:\n",
    "        if initialization == 'Orthogonal':\n",
    "            t.nn.init.orthogonal_(module.weight)\n",
    "        elif initialization == 'he_uniform':\n",
    "            t.nn.init.kaiming_uniform_(module.weight)\n",
    "        elif initialization == 'he_normal':\n",
    "            t.nn.init.kaiming_normal_(module.weight)\n",
    "        elif initialization == 'glorot_uniform':\n",
    "            t.nn.init.xavier_uniform_(module.weight)\n",
    "        elif initialization == 'glorot_normal':\n",
    "            t.nn.init.xavier_normal_(module.weight)\n",
    "        elif initialization == 'lecun_normal':\n",
    "            pass #t.nn.init.normal_(module.weight, 0.0, std=1/np.sqrt(module.weight.numel()))\n",
    "        else:\n",
    "            assert 1<0, f'Initialization {initialization} not found'\n",
    "\n",
    "class Nbeats(object):\n",
    "    \"\"\"\n",
    "    Future documentation\n",
    "    \"\"\"\n",
    "    SEASONALITY_BLOCK = 'seasonality'\n",
    "    TREND_BLOCK = 'trend'\n",
    "    IDENTITY_BLOCK = 'identity'\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_x_t, \n",
    "                 n_x_s,\n",
    "                 t_cols,\n",
    "                 include_var_dict,\n",
    "                 input_size_multiplier,\n",
    "                 output_size,\n",
    "                 shared_weights,\n",
    "                 activation,\n",
    "                 initialization,\n",
    "                 stack_types,\n",
    "                 n_blocks,\n",
    "                 n_theta_hidden_list,\n",
    "                 n_xbasis_layers,\n",
    "                 n_xbasis_channels,\n",
    "                 n_s_hidden,\n",
    "                 scaler,\n",
    "                 batch_normalization,\n",
    "                 learning_rate,\n",
    "                 lr_decay,\n",
    "                 n_lr_decay_steps,\n",
    "                 dropout_prob_theta,\n",
    "                 dropout_prob_xbasis,\n",
    "                 lambda_l1_theta,\n",
    "                 lambda_l1_input,\n",
    "                 lambda_l2_xbasis,\n",
    "                 max_iterations,\n",
    "                 early_stopping,\n",
    "                 loss,\n",
    "                 frequency,\n",
    "                 seasonality,\n",
    "                 random_seed,\n",
    "                 device=None):\n",
    "        super(Nbeats, self).__init__()\n",
    "\n",
    "        if activation == 'selu': initialization = 'lecun_normal'\n",
    "\n",
    "        # Attributes of ts_dataset\n",
    "        # TODO: pensar como quitar redundancia\n",
    "        self.n_x_t  = n_x_t\n",
    "        self.n_x_s  = n_x_s\n",
    "        self.t_cols = t_cols\n",
    "        self.include_var_dict = include_var_dict\n",
    "\n",
    "        # Architecture parameters\n",
    "        self.input_size            = int(input_size_multiplier*output_size)\n",
    "        self.output_size           = output_size\n",
    "        self.shared_weights        = shared_weights\n",
    "        self.activation            = activation\n",
    "        self.initialization        = initialization\n",
    "        self.stack_types           = stack_types\n",
    "        self.n_blocks              = n_blocks\n",
    "        self.n_theta_hidden_list   = n_theta_hidden_list\n",
    "        self.n_xbasis_layers       = n_xbasis_layers\n",
    "        self.n_xbasis_channels     = n_xbasis_channels\n",
    "        self.n_s_hidden            = n_s_hidden\n",
    "        \n",
    "        # Regularization and optimization parameters\n",
    "        self.scaler                = scaler\n",
    "        self.batch_normalization   = batch_normalization\n",
    "        self.learning_rate         = learning_rate\n",
    "        self.lr_decay              = lr_decay\n",
    "        self.n_lr_decay_steps      = n_lr_decay_steps\n",
    "        # self.weight_decay         = weight_decay\n",
    "\n",
    "        self.loss                  = loss\n",
    "        self.dropout_prob_theta    = dropout_prob_theta\n",
    "        self.dropout_prob_xbasis   = dropout_prob_xbasis\n",
    "        self.lambda_l1_theta       = lambda_l1_theta\n",
    "        self.lambda_l1_input       = lambda_l1_input\n",
    "        self.lambda_l2_xbasis      = lambda_l2_xbasis\n",
    "        self.max_iterations        = max_iterations\n",
    "        self.early_stopping        = early_stopping\n",
    "\n",
    "        # Regularization and optimization parameters\n",
    "        self.frequency   = frequency\n",
    "        self.seasonality = seasonality\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        if device is None: device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
    "        self.device = device\n",
    "        self._is_instantiated = False\n",
    "\n",
    "    def create_stack(self):\n",
    "        if self.include_var_dict is not None:\n",
    "            x_t_n_inputs = self.output_size * int(sum([len(x) for x in self.include_var_dict.values()]))\n",
    "            \n",
    "            # Correction because week_day only adds 1 no output_size\n",
    "            if len(self.include_var_dict['week_day'])>0:\n",
    "                x_t_n_inputs = x_t_n_inputs - self.output_size + 1 \n",
    "        else:\n",
    "            x_t_n_inputs = self.input_size\n",
    "        \n",
    "        # Architecture definition\n",
    "        block_list = []\n",
    "        for i in range(len(self.stack_types)):\n",
    "            #print(f'| --  Stack {self.stack_types[i]} (#{i})')\n",
    "            for block_id in range(self.n_blocks[i]):\n",
    "                # Batch norm only on first block\n",
    "                if (len(block_list)==0) and (self.batch_normalization):\n",
    "                    batch_normalization_block = True\n",
    "                else:\n",
    "                    batch_normalization_block = False              \n",
    "\n",
    "                # Shared weights\n",
    "                if self.shared_weights and block_id>0:\n",
    "                    nbeats_block = block_list[-1]\n",
    "\n",
    "                else:\n",
    "                    if self.stack_types[i] == 'identity':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.n_s_hidden,\n",
    "                                                   theta_n_dim=self.input_size + self.output_size,\n",
    "                                                   basis=IdentityBasis(backcast_size=self.input_size,\n",
    "                                                                       forecast_size=self.output_size),\n",
    "                                                   n_theta_hidden_list=self.n_theta_hidden_list[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous_tcn':\n",
    "                        #assert len(self.f_cols)>0, 'If Xbasis, provide f_cols hyperparameter'\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden = self.n_s_hidden,\n",
    "                                                   theta_n_dim = 2*(self.n_xbasis_channels),\n",
    "                                                   basis=XBasisTCN(out_features=self.n_xbasis_channels, \n",
    "                                                                   in_features=self.n_x_t,\n",
    "                                                                   #f_idxs=self.f_idxs,\n",
    "                                                                   num_levels=self.n_xbasis_layers,\n",
    "                                                                   dropout_prob=self.dropout_prob_xbasis),\n",
    "                                                   n_theta_hidden_list=self.n_theta_hidden_list[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous_wavenet':\n",
    "                        #assert len(self.f_cols)>0, 'If Xbasis, provide f_cols hyperparameter'\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden = self.n_s_hidden,\n",
    "                                                   theta_n_dim = 2*(self.n_xbasis_channels),\n",
    "                                                   basis=XBasisWavenet(out_features=self.n_xbasis_channels, \n",
    "                                                                       in_features=self.n_x_t,\n",
    "                                                                       #f_idxs=self.f_idxs,\n",
    "                                                                       num_levels=self.n_xbasis_layers,\n",
    "                                                                       dropout_prob=self.dropout_prob_xbasis),\n",
    "                                                   n_theta_hidden_list=self.n_theta_hidden_list[i],\n",
    "                                                   include_var_dict=self.include_var_dict,\n",
    "                                                   t_cols=self.t_cols,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "\n",
    "                # Select type of evaluation and apply it to all layers of block\n",
    "                init_function = partial(init_weights, initialization=self.initialization)\n",
    "                nbeats_block.layers.apply(init_function)\n",
    "                block_list.append(nbeats_block)\n",
    "        return block_list\n",
    "\n",
    "    def __loss_fn(self, loss_name: str):\n",
    "        def loss(x, freq, forecast, target, mask):\n",
    "            if loss_name == 'MAPE':\n",
    "                return MAPELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.reg_loss_theta() + self.reg_loss_xbasis()\n",
    "            elif loss_name == 'MASE':\n",
    "                return MASELoss(y=target, y_hat=forecast, y_insample=x, seasonality=freq, mask=mask) + \\\n",
    "                       self.reg_loss_theta() + self.reg_loss_xbasis()\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return SMAPELoss(y=target, y_hat=forecast, mask=mask) + self.l1_regularization() + \\\n",
    "                       self.reg_loss_theta() + self.reg_loss_xbasis()\n",
    "            elif loss_name == 'MSE':\n",
    "                return MSELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.reg_loss_theta() + self.reg_loss_xbasis()\n",
    "            elif loss_name == 'RMSE':\n",
    "                return RMSELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.reg_loss_theta() + self.reg_loss_xbasis()\n",
    "            elif loss_name == 'MAE':\n",
    "                return MAELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.reg_loss_theta() + self.reg_loss_xbasis()\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def __val_loss_fn(self, loss_name: str):\n",
    "        #TODO: mase not implemented\n",
    "        def loss(forecast, target, weights):\n",
    "            if loss_name == 'MAPE':\n",
    "                return mape(y=target, y_hat=forecast, weights=weights) #TODO: faltan weights\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return smape(y=target, y_hat=forecast, weights=weights) #TODO: faltan weights\n",
    "            elif loss_name == 'MSE':\n",
    "                return mse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'RMSE':\n",
    "                return rmse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MAE':\n",
    "                return mae(y=target, y_hat=forecast, weights=weights)\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def reg_loss_xbasis(self):\n",
    "        loss = 0.0\n",
    "        for i in range(len(self.stack_types)):\n",
    "            for block_id in range(self.n_blocks[i]):\n",
    "                if self.stack_types[i] == 'exogenous_tcn':\n",
    "                    for layer in self.model.blocks[i].modules():\n",
    "                        if isinstance(layer, t.nn.Conv1d):\n",
    "                            loss += self.lambda_l2_xbasis * t.norm(layer.weight)\n",
    "        return loss\n",
    "\n",
    "    def reg_loss_theta(self):\n",
    "        # L1 loss for initial exogenous input\n",
    "        loss = self.lambda_l1_input * t.sum(t.abs(self.model.l1_weight))\n",
    "\n",
    "        # L2/L1 regularization for thetas\n",
    "        for i in range(len(self.stack_types)):\n",
    "            for block_id in range(self.n_blocks[i]):\n",
    "                for layer in self.model.blocks[i].modules():\n",
    "                    if isinstance(layer, t.nn.Linear):\n",
    "                        # loss += self.lambda_reg_theta * t.norm(layer.weight)\n",
    "                        loss += self.lambda_l1_theta * layer.weight.abs().sum()\n",
    "        return loss\n",
    "\n",
    "    def to_tensor(self, x: np.ndarray) -> t.Tensor:\n",
    "        tensor = t.as_tensor(x, dtype=t.float32).to(self.device)\n",
    "        return tensor\n",
    "\n",
    "    def evaluate_performance(self, ts_loader, validation_loss_fn):\n",
    "        #TODO: mas opciones que mae\n",
    "        self.model.eval()\n",
    "\n",
    "        losses = []\n",
    "        with t.no_grad():\n",
    "            for batch in iter(ts_loader):\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                forecast = self.model(insample_y=insample_y, insample_x_t=insample_x,\n",
    "                                      insample_mask=insample_mask, outsample_x_t=outsample_x, x_s=s_matrix)\n",
    "                batch_loss = validation_loss_fn(target=forecast.cpu().data.numpy(),\n",
    "                                                forecast=outsample_y.cpu().data.numpy(),\n",
    "                                                weights=outsample_mask.cpu().data.numpy())\n",
    "                losses.append(batch_loss)\n",
    "                #break #TODO: remove this in future\n",
    "        loss = np.mean(losses)\n",
    "        self.model.train()\n",
    "        return loss\n",
    "\n",
    "    def fit(self, train_ts_loader, val_ts_loader=None, max_epochs=None, verbose=True, eval_steps=1):\n",
    "        # Asserts\n",
    "        assert self.t_cols[0] == 'y', f'First variable must be y not {self.t_cols[0]}'\n",
    "        #assert (self.input_size)==train_ts_loader.input_size, \\\n",
    "        #    f'model input_size {self.input_size} data input_size {train_ts_loader.input_size}'        \n",
    "\n",
    "        # Random Seeds (model initialization)\n",
    "        t.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed) #TODO: interaccion rara con window_sampling de validacion\n",
    "\n",
    "        # Instantiate model\n",
    "        if not self._is_instantiated:\n",
    "            block_list = self.create_stack()\n",
    "            self.model = NBeats(blocks=t.nn.ModuleList(block_list), in_features=self.n_x_t).to(self.device)\n",
    "            self._is_instantiated = True\n",
    "\n",
    "        # Overwrite max_epochs and train datasets\n",
    "        if max_epochs is None:\n",
    "            max_epochs = self.max_epochs\n",
    "\n",
    "        train_dataloader = iter(train_ts_loader)\n",
    "\n",
    "        lr_decay_steps = max_epochs // self.n_lr_decay_steps\n",
    "        if lr_decay_steps == 0:\n",
    "            lr_decay_steps = 1\n",
    "\n",
    "        #optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_steps, gamma=self.lr_decay)\n",
    "\n",
    "        training_loss_fn = self.__loss_fn(self.loss)\n",
    "        validation_loss_fn = self.__val_loss_fn(self.loss) #Uses numpy losses\n",
    "\n",
    "        if verbose and (max_epochs > 0):\n",
    "            print('='*30+' Start fitting '+'='*30)\n",
    "            print(f'Number of exogenous variables: {self.n_x_t}')\n",
    "            print(f'Number of static variables: {self.n_x_s} , with dim_hidden: {self.n_s_hidden}')\n",
    "            print(f'Number of iterations: {max_epochs}')\n",
    "            print(f'Number of blocks: {len(self.model.blocks)}')\n",
    "\n",
    "        #self.loss_dict = {} # Restart self.loss_dict\n",
    "        start = time.time()\n",
    "        self.trajectories = {'step':[],'train_loss':[], 'val_loss':[]}\n",
    "        self.final_insample_loss = None\n",
    "        self.final_outsample_loss = None\n",
    "\n",
    "        # Training Loop\n",
    "        best_val_loss = np.inf\n",
    "        break_flag = False\n",
    "        for step in range(max_epochs):\n",
    "            for batch in iter(train_dataloader):\n",
    "                self.model.train()\n",
    "                #train_ts_loader.train()\n",
    "\n",
    "                batch = next(train_dataloader)\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                forecast = self.model(insample_y=insample_y, insample_x_t=insample_x,\n",
    "                                    insample_mask=insample_mask, outsample_x_t=outsample_x, x_s=s_matrix)\n",
    "\n",
    "                training_loss = training_loss_fn(x=insample_y, freq=self.seasonality, forecast=forecast,\n",
    "                                                target=outsample_y, mask=outsample_mask)\n",
    "\n",
    "                if np.isnan(float(training_loss)):\n",
    "                    break\n",
    "\n",
    "                training_loss.backward()\n",
    "                t.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            lr_scheduler.step()\n",
    "            if (step % eval_steps == 0):\n",
    "                display_string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(step,\n",
    "                                                                                time.time()-start,\n",
    "                                                                                self.loss,\n",
    "                                                                                training_loss.cpu().data.numpy())\n",
    "                self.trajectories['step'].append(step)\n",
    "                self.trajectories['train_loss'].append(training_loss.cpu().data.numpy())\n",
    "\n",
    "                if val_ts_loader is not None:\n",
    "                    loss = self.evaluate_performance(val_ts_loader, validation_loss_fn=validation_loss_fn)\n",
    "                    display_string += \", Outsample {}: {:.5f}\".format(self.loss, loss)\n",
    "                    self.trajectories['val_loss'].append(loss)\n",
    "\n",
    "                    if self.early_stopping:\n",
    "                        if loss < best_val_loss:\n",
    "                            # Save current model if improves outsample loss\n",
    "                            best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "                            best_insample_loss = training_loss.cpu().data.numpy()\n",
    "                            early_stopping_counter = 0\n",
    "                            best_val_loss = loss\n",
    "                        else:\n",
    "                            early_stopping_counter += 1\n",
    "                        if early_stopping_counter >= self.early_stopping:\n",
    "                            break_flag = True\n",
    "                \n",
    "                print(display_string)\n",
    "\n",
    "                self.model.train()\n",
    "                #train_ts_loader.train()\n",
    "\n",
    "            if break_flag:\n",
    "                print(10*'-',' Stopped training by early stopping', 10*'-')\n",
    "                self.model.load_state_dict(best_state_dict)\n",
    "                break\n",
    "\n",
    "        #End of fitting\n",
    "        if max_epochs > 0:\n",
    "            self.final_insample_loss = training_loss.cpu().data.numpy() if not break_flag else best_insample_loss #This is batch!\n",
    "            string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(step,\n",
    "                                                                            time.time()-start,\n",
    "                                                                            self.loss,\n",
    "                                                                            self.final_insample_loss)\n",
    "            if val_ts_loader is not None:\n",
    "                self.final_outsample_loss = self.evaluate_performance(val_ts_loader, validation_loss_fn=validation_loss_fn)\n",
    "                string += \", Outsample {}: {:.5f}\".format(self.loss, self.final_outsample_loss)\n",
    "            print(string)\n",
    "            print('='*30+' End  fitting '+'='*30)\n",
    "\n",
    "    def predict(self, ts_loader, X_test=None, eval_mode=False):\n",
    "\n",
    "        ts_loader.eval()\n",
    "        frequency = ts_loader.get_frequency()\n",
    "\n",
    "        # Build forecasts\n",
    "        unique_ids = ts_loader.get_meta_data_var('unique_id')\n",
    "        last_ds = ts_loader.get_meta_data_var('last_ds') #TODO: ajustar of offset\n",
    "\n",
    "        batch = next(iter(ts_loader))\n",
    "        insample_y     = self.to_tensor(batch['insample_y'])\n",
    "        insample_x     = self.to_tensor(batch['insample_x'])\n",
    "        insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "        outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "        outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "        outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "        s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "        self.model.eval()\n",
    "        with t.no_grad():\n",
    "            forecast = self.model(insample_y=insample_y, insample_x_t=insample_x,\n",
    "                                  insample_mask=insample_mask, outsample_x_t=outsample_x, x_s=s_matrix)\n",
    "\n",
    "        if eval_mode:\n",
    "            return forecast, outsample_y, outsample_mask\n",
    "\n",
    "        # Predictions for panel\n",
    "        Y_hat_panel = pd.DataFrame(columns=['unique_id', 'ds'])\n",
    "        for i, unique_id in enumerate(unique_ids):\n",
    "            Y_hat_id = pd.DataFrame([unique_id]*self.output_size, columns=[\"unique_id\"])\n",
    "            ds = pd.date_range(start=last_ds[i], periods=self.output_size+1, freq=self.frequency)\n",
    "            Y_hat_id[\"ds\"] = ds[1:]\n",
    "            Y_hat_panel = Y_hat_panel.append(Y_hat_id, sort=False).reset_index(drop=True)\n",
    "\n",
    "        forecast = forecast.cpu().detach().numpy()\n",
    "        Y_hat_panel['y_hat'] = forecast.flatten()\n",
    "\n",
    "        if X_test is not None:\n",
    "            Y_hat_panel = X_test.merge(Y_hat_panel, on=['unique_id', 'ds'], how='left')\n",
    "\n",
    "        return Y_hat_panel\n",
    "\n",
    "\n",
    "    def save(self, model_dir, model_id):\n",
    "\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        print('Saving model to:\\n {}'.format(model_file)+'\\n')\n",
    "        t.save({'model_state_dict': self.model.state_dict()}, model_file)\n",
    "\n",
    "    def load(self, model_dir, model_id):\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        path = Path(model_file)\n",
    "\n",
    "        assert path.is_file(), 'No model_*.model file found in this path!'\n",
    "\n",
    "        print('Loading model from:\\n {}'.format(model_file)+'\\n')\n",
    "\n",
    "        checkpoint = t.load(model_file, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model(model):\n",
    "    print(120*'=')\n",
    "    line = '{:<40} {:<40} {:<40}'.format('Weights', 'Parameter shape', 'Number of parameters')\n",
    "    print(line)\n",
    "    print(120*'-')\n",
    "    total_params = 0\n",
    "    for model_param_name, model_param_value in model.model.named_parameters():\n",
    "        line = '{:<50} {:<40} {:<40}'.format(model_param_name, str(model_param_value.shape), len(model_param_value.flatten()))\n",
    "        total_params += len(model_param_value.flatten())\n",
    "        print(line)\n",
    "        print(120*'-')\n",
    "    print(f'Total parameters: {total_params}')\n",
    "    print(120*'=')\n",
    "    print(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        \"\"\"\n",
    "        \"\"\"        \n",
    "        self.insample_y  = X[:, (7*0)*24:(7*1)*24]\n",
    "        self.outsample_y = Y\n",
    "        \n",
    "        insample_x1 = X[:, (7*1)*24:(7*2)*24]\n",
    "        insample_x2 = X[:, (7*2)*24:(7*3)*24]\n",
    "        insample_x3 = X[:, (7*3)*24:(7*4)*24]\n",
    "        \n",
    "        self.insample_x  = np.concatenate([insample_x1[:,None,:], \n",
    "                                           insample_x2[:,None,:], \n",
    "                                           insample_x3[:,None,:]], axis=1)\n",
    "\n",
    "        self.insample_mask = np.ones(shape=self.insample_y.shape)\n",
    "        \n",
    "        outsample_x1 = X[:, (7*4+0)*24:(7*4+1)*24]\n",
    "        outsample_x2 = X[:, (7*4+1)*24:(7*4+2)*24]\n",
    "        outsample_x3 = X[:, (7*4+2)*24:(7*4+3)*24]\n",
    "        \n",
    "        self.outsample_x  = np.concatenate([outsample_x1[:,None,:], \n",
    "                                            outsample_x2[:,None,:], \n",
    "                                            outsample_x3[:,None,:]], axis=1)\n",
    "\n",
    "        self.outsample_mask = np.ones(shape=self.outsample_y.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.outsample_x)\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        # y variables\n",
    "        insample_y     = self.insample_y[idx, :]\n",
    "        outsample_y    = self.outsample_y[idx, :]\n",
    "\n",
    "        # outsample y\n",
    "        insample_x     = self.insample_x[idx, :, :]\n",
    "        outsample_x    = self.outsample_x[idx, :, :]\n",
    "        \n",
    "        # train masks\n",
    "        insample_mask  = self.insample_mask[idx, :]\n",
    "        outsample_mask = self.outsample_mask[idx, :]\n",
    "\n",
    "        batch = {'s_matrix': [],\n",
    "                 'insample_y': insample_y, 'insample_x':insample_x, 'insample_mask':insample_mask,\n",
    "                 'outsample_y': outsample_y, 'outsample_x':outsample_x, 'outsample_mask':outsample_mask}\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X: time series features, of shape (#series,#times,#features): \t(1087, 744)\nY: target series (in X), of shape (#series,#times): \t \t(1087, 24)\n\n\n\nDataloaderFast batch time: 0.0008358955383300781\ninsample_y.shape torch.Size([4, 168])\ninsample_x.shape torch.Size([4, 3, 168])\ninsample_mask.shape torch.Size([4, 168])\noutsample_y.shape torch.Size([4, 24])\noutsample_x.shape torch.Size([4, 3, 24])\noutsample_mask.shape torch.Size([4, 24])\nt.max(insample_y) tensor(35.3600, dtype=torch.float64)\nt.max(outsample_y) tensor(35.5800, dtype=torch.float64)\n\n\n\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch as t\n",
    "import copy\n",
    "from fastcore.foundation import patch\n",
    "# from nixtla.data.ontsdataset import TimeSeriesDataset\n",
    "# from nixtla.data.ontsloader_fast import TimeSeriesLoader as TimeSeriesLoaderFast\n",
    "\n",
    "np.random.seed(1)\n",
    "t.manual_seed(1)\n",
    "\n",
    "Xtrain = np.genfromtxt('../data/epf/Xtrain.csv')\n",
    "Ytrain = np.genfromtxt('../data/epf/Ytrain.csv')\n",
    "\n",
    "print('X: time series features, of shape (#series,#times,#features): \\t' + str(Xtrain.shape))\n",
    "print('Y: target series (in X), of shape (#series,#times): \\t \\t' + str(Ytrain.shape))\n",
    "# print('S: static features, of shape (#series,#features): \\t \\t' + str(S.shape))\n",
    "#Y_insample_df.head()\n",
    "#Y_insample_df.tail()\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X=Xtrain, Y=Ytrain)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4,\n",
    "                          shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = TimeSeriesDataset(X=Xtrain, Y=Ytrain)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)                        \n",
    "\n",
    "start = time.time()\n",
    "dataloader = iter(train_loader)\n",
    "batch = next(dataloader)\n",
    "insample_y = batch['insample_y']\n",
    "insample_x = batch['insample_x']\n",
    "insample_mask = batch['insample_mask']\n",
    "outsample_x = batch['outsample_x']\n",
    "outsample_y = batch['outsample_y']\n",
    "outsample_mask = batch['outsample_mask']\n",
    "\n",
    "print(\"DataloaderFast batch time:\", time.time()-start)\n",
    "print(\"insample_y.shape\", insample_y.shape)\n",
    "print(\"insample_x.shape\", insample_x.shape)\n",
    "print(\"insample_mask.shape\", insample_mask.shape)\n",
    "\n",
    "print(\"outsample_y.shape\", outsample_y.shape)\n",
    "print(\"outsample_x.shape\", outsample_x.shape)\n",
    "print(\"outsample_mask.shape\", outsample_mask.shape)\n",
    "\n",
    "print(\"t.max(insample_y)\", t.max(insample_y))\n",
    "print(\"t.max(outsample_y)\", t.max(outsample_y * outsample_mask))\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================== Start fitting ==============================\n",
      "Number of exogenous variables: 3\n",
      "Number of static variables: 0 , with dim_hidden: 0\n",
      "Number of iterations: 20\n",
      "Number of blocks: 4\n",
      "Step: 0, Time: 1.937, Insample MAE: 40.54165, Outsample MAE: 2.42813\n",
      "Step: 1, Time: 2.697, Insample MAE: 40.54165, Outsample MAE: 2.42905\n",
      "Step: 2, Time: 3.458, Insample MAE: 40.54165, Outsample MAE: 2.42824\n",
      "Step: 3, Time: 4.186, Insample MAE: 40.54165, Outsample MAE: 2.42893\n",
      "Step: 4, Time: 4.903, Insample MAE: 40.54165, Outsample MAE: 2.43081\n",
      "Step: 5, Time: 5.631, Insample MAE: 40.54165, Outsample MAE: 2.42843\n",
      "Step: 6, Time: 6.352, Insample MAE: 40.54165, Outsample MAE: 2.42750\n",
      "Step: 7, Time: 7.077, Insample MAE: 40.54165, Outsample MAE: 2.42853\n",
      "Step: 8, Time: 7.796, Insample MAE: 40.54165, Outsample MAE: 2.42832\n",
      "Step: 9, Time: 8.526, Insample MAE: 40.54165, Outsample MAE: 2.42924\n",
      "Step: 10, Time: 9.276, Insample MAE: 40.54165, Outsample MAE: 2.42835\n",
      "Step: 11, Time: 10.005, Insample MAE: 40.54165, Outsample MAE: 2.42957\n",
      "Step: 12, Time: 10.735, Insample MAE: 40.54165, Outsample MAE: 2.44254\n",
      "Step: 13, Time: 11.456, Insample MAE: 40.54165, Outsample MAE: 2.42851\n",
      "Step: 14, Time: 12.176, Insample MAE: 40.54165, Outsample MAE: 2.42883\n",
      "Step: 15, Time: 12.898, Insample MAE: 40.54165, Outsample MAE: 2.42804\n",
      "Step: 16, Time: 13.623, Insample MAE: 40.54165, Outsample MAE: 2.42767\n",
      "Step: 17, Time: 14.347, Insample MAE: 40.54165, Outsample MAE: 2.42804\n",
      "Step: 18, Time: 15.072, Insample MAE: 40.54165, Outsample MAE: 2.42937\n",
      "Step: 19, Time: 15.809, Insample MAE: 40.54165, Outsample MAE: 2.42901\n",
      "Step: 19, Time: 16.530, Insample MAE: 40.54165, Outsample MAE: 2.42789\n",
      "============================== End  fitting ==============================\n"
     ]
    }
   ],
   "source": [
    "nbeatsx = Nbeats(n_x_t=3,\n",
    "                 n_x_s=0,\n",
    "                 t_cols=['y', 'Exogenous1', 'Exogenous2', 'week_day'],\n",
    "                 include_var_dict={'y': [-2, -8], \n",
    "                                   'Exogenous1': [-2, -8],\n",
    "                                   'Exogenous2': [-2, -8],\n",
    "                                   'week_day': [-1]},\n",
    "                 input_size_multiplier=7,\n",
    "                 output_size=24,\n",
    "                 shared_weights=False,\n",
    "                 activation='relu',\n",
    "                 initialization='lecun_normal',                 \n",
    "                 stack_types=['exogenous_wavenet']+3*['identity'],\n",
    "                 n_blocks=4*[1],\n",
    "                 n_theta_hidden_list=4*[[256,256]],\n",
    "                 n_xbasis_layers=4,\n",
    "                 n_xbasis_channels=2,\n",
    "                 n_s_hidden=0,\n",
    "                 scaler='hola',\n",
    "                 batch_normalization=False,\n",
    "                 learning_rate=0.001,\n",
    "                 lr_decay=0.5,\n",
    "                 n_lr_decay_steps=3,\n",
    "                 dropout_prob_theta=0.1,\n",
    "                 dropout_prob_xbasis=0.1,\n",
    "                 lambda_l1_input=0.01,\n",
    "                 lambda_l1_theta=0.01,\n",
    "                 lambda_l2_xbasis=0.01,\n",
    "                 max_iterations=100,\n",
    "                 early_stopping=20,\n",
    "                 loss='MAE',\n",
    "                 frequency=24,\n",
    "                 seasonality='H',\n",
    "                 random_seed=1)\n",
    "\n",
    "nbeatsx.fit(train_ts_loader=train_loader, val_ts_loader=val_loader, max_epochs=20, verbose=True, eval_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}