{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c68c0f618c4f787b1e02bfee278e48d25b62407cb335aab5258cedc4db4ced"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.wnbeats.nbeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "import torch as t\n",
    "from torch import optim\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "from nixtla.models.wnbeats.nbeats_model import NBeats, NBeatsBlock, IdentityBasis, TrendBasis, SeasonalityBasis\n",
    "from nixtla.models.wnbeats.nbeats_model import XBasisTCN, XBasisWavenet\n",
    "from nixtla.losses.pytorch import MAPELoss, MASELoss, SMAPELoss, MSELoss, MAELoss, RMSELoss\n",
    "from nixtla.losses.numpy import mae, mse, mape, smape, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_weights(module, initialization):\n",
    "    if type(module) == t.nn.Linear:\n",
    "        if initialization == 'Orthogonal':\n",
    "            t.nn.init.orthogonal_(module.weight)\n",
    "        elif initialization == 'he_uniform':\n",
    "            t.nn.init.kaiming_uniform_(module.weight)\n",
    "        elif initialization == 'he_normal':\n",
    "            t.nn.init.kaiming_normal_(module.weight)\n",
    "        elif initialization == 'glorot_uniform':\n",
    "            t.nn.init.xavier_uniform_(module.weight)\n",
    "        elif initialization == 'glorot_normal':\n",
    "            t.nn.init.xavier_normal_(module.weight)\n",
    "        elif initialization == 'lecun_normal':\n",
    "            pass #t.nn.init.normal_(module.weight, 0.0, std=1/np.sqrt(module.weight.numel()))\n",
    "        else:\n",
    "            assert 1<0, f'Initialization {initialization} not found'\n",
    "\n",
    "class Nbeats(object):\n",
    "    \"\"\"\n",
    "    Future documentation\n",
    "    \"\"\"\n",
    "    SEASONALITY_BLOCK = 'seasonality'\n",
    "    TREND_BLOCK = 'trend'\n",
    "    IDENTITY_BLOCK = 'identity'\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size_multiplier,\n",
    "                 output_size,\n",
    "                 shared_weights,\n",
    "                 activation,\n",
    "                 initialization,\n",
    "                 stack_types,\n",
    "                 n_blocks,\n",
    "                 n_theta_hidden_list,\n",
    "                 n_xbasis_layers,\n",
    "                 n_xbasis_channels,\n",
    "                 n_s_hidden,\n",
    "                 f_cols,\n",
    "                 scaler,\n",
    "                 theta_with_exogenous,\n",
    "                 batch_normalization,\n",
    "                 learning_rate,\n",
    "                 lr_decay,\n",
    "                 n_lr_decay_steps,\n",
    "                 dropout_prob_theta,\n",
    "                 dropout_prob_xbasis,\n",
    "                 regularization,\n",
    "                 lambda_l1_theta,\n",
    "                 lambda_reg_theta,\n",
    "                 lambda_reg_xbasis,\n",
    "                 max_iterations,\n",
    "                 early_stopping,\n",
    "                 loss,\n",
    "                 frequency,\n",
    "                 seasonality,\n",
    "                 random_seed,\n",
    "                 device=None):\n",
    "        super(Nbeats, self).__init__()\n",
    "\n",
    "        if activation == 'selu': initialization = 'lecun_normal'\n",
    "\n",
    "        # Architecture parameters\n",
    "        self.input_size           = int(input_size_multiplier*output_size)\n",
    "        self.output_size          = output_size\n",
    "        self.shared_weights       = shared_weights\n",
    "        self.activation           = activation\n",
    "        self.initialization       = initialization\n",
    "        self.stack_types          = stack_types\n",
    "        self.n_blocks             = n_blocks\n",
    "        self.n_theta_hidden_list  = n_theta_hidden_list\n",
    "        self.n_xbasis_layers      = n_xbasis_layers\n",
    "        self.n_xbasis_channels    = n_xbasis_channels\n",
    "        self.n_s_hidden           = n_s_hidden\n",
    "        self.f_cols               = f_cols\n",
    "        \n",
    "        # Regularization and optimization parameters\n",
    "        self.scaler               = scaler\n",
    "        self.theta_with_exogenous = theta_with_exogenous\n",
    "        self.batch_normalization  = batch_normalization\n",
    "        self.learning_rate        = learning_rate\n",
    "        self.lr_decay             = lr_decay\n",
    "        self.n_lr_decay_steps     = n_lr_decay_steps\n",
    "        # self.weight_decay         = weight_decay\n",
    "\n",
    "        self.loss                 = loss\n",
    "        self.dropout_prob_theta   = dropout_prob_theta\n",
    "        self.dropout_prob_xbasis  = dropout_prob_xbasis\n",
    "        self.regularization       = regularization\n",
    "        self.lambda_l1_theta      = lambda_l1_theta\n",
    "        self.lambda_reg_theta     = lambda_reg_theta\n",
    "        self.lambda_reg_xbasis    = lambda_reg_xbasis\n",
    "        self.max_iterations       = max_iterations\n",
    "        self.early_stopping       = early_stopping\n",
    "\n",
    "        # Regularization and optimization parameters\n",
    "        self.frequency   = frequency\n",
    "        self.seasonality = seasonality\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        if device is None: device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
    "        self.device = device\n",
    "        self._is_instantiated = False\n",
    "\n",
    "    def create_stack(self):\n",
    "        # Declare model/parameter dimensions\n",
    "        if self.theta_with_exogenous:\n",
    "            x_t_n_inputs = self.input_size + self.n_x_t\n",
    "        else:\n",
    "            x_t_n_inputs = self.input_size # only y_lags in theta\n",
    "        \n",
    "        # Architecture definition\n",
    "        block_list = []\n",
    "        for i in range(len(self.stack_types)):\n",
    "            #print(f'| --  Stack {self.stack_types[i]} (#{i})')\n",
    "            for block_id in range(self.n_blocks[i]):\n",
    "                # Batch norm only on first block\n",
    "                if (len(block_list)==0) and (self.batch_normalization):\n",
    "                    batch_normalization_block = True\n",
    "                else:\n",
    "                    batch_normalization_block = False              \n",
    "\n",
    "                # Shared weights\n",
    "                if self.shared_weights and block_id>0:\n",
    "                    nbeats_block = block_list[-1]\n",
    "\n",
    "                else:\n",
    "                    if self.stack_types[i] == 'identity':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.n_s_hidden,\n",
    "                                                   theta_n_dim=self.input_size + self.output_size,\n",
    "                                                   basis=IdentityBasis(backcast_size=self.input_size,\n",
    "                                                                       forecast_size=self.output_size),\n",
    "                                                   n_theta_hidden_list=self.n_theta_hidden_list[i],\n",
    "                                                   theta_with_exogenous=self.theta_with_exogenous,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous_tcn':\n",
    "                        assert len(self.f_cols)>0, 'If Xbasis, provide f_cols hyperparameter'\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden = self.n_s_hidden,\n",
    "                                                   theta_n_dim = 2*(self.n_xbasis_channels),\n",
    "                                                   basis=XBasisTCN(out_features=self.n_xbasis_channels, \n",
    "                                                                   #in_features=self.n_x_t,\n",
    "                                                                   f_idxs=self.f_idxs,\n",
    "                                                                   num_levels=self.n_xbasis_layers,\n",
    "                                                                   dropout_prob=self.dropout_prob_xbasis),\n",
    "                                                   n_theta_hidden_list=self.n_theta_hidden_list[i],\n",
    "                                                   theta_with_exogenous=self.theta_with_exogenous,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "                    elif self.stack_types[i] == 'exogenous_wavenet':\n",
    "                        assert len(self.f_cols)>0, 'If Xbasis, provide f_cols hyperparameter'\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden = self.n_s_hidden,\n",
    "                                                   theta_n_dim = 2*(self.n_xbasis_channels),\n",
    "                                                   basis=XBasisWavenet(out_features=self.n_xbasis_channels, \n",
    "                                                                       #in_features=self.n_x_t,\n",
    "                                                                       f_idxs=self.f_idxs,\n",
    "                                                                       num_levels=self.n_xbasis_layers,\n",
    "                                                                       dropout_prob=self.dropout_prob_xbasis),\n",
    "                                                   n_theta_hidden_list=self.n_theta_hidden_list[i],\n",
    "                                                   theta_with_exogenous=self.theta_with_exogenous,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob_theta,\n",
    "                                                   activation=self.activation)\n",
    "\n",
    "                # Select type of evaluation and apply it to all layers of block\n",
    "                init_function = partial(init_weights, initialization=self.initialization)\n",
    "                nbeats_block.layers.apply(init_function)\n",
    "                block_list.append(nbeats_block)\n",
    "        return block_list\n",
    "\n",
    "    def __loss_fn(self, loss_name: str):\n",
    "        def loss(x, freq, forecast, target, mask):\n",
    "            if loss_name == 'MAPE':\n",
    "                return MAPELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.reg_loss_theta() + self.reg_loss_xbasis()\n",
    "            elif loss_name == 'MASE':\n",
    "                return MASELoss(y=target, y_hat=forecast, y_insample=x, seasonality=freq, mask=mask) + \\\n",
    "                       self.reg_loss_theta() + self.reg_loss_xbasis()\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return SMAPELoss(y=target, y_hat=forecast, mask=mask) + self.l1_regularization() + \\\n",
    "                       self.reg_loss_theta() + self.reg_loss_xbasis()\n",
    "            elif loss_name == 'MSE':\n",
    "                return MSELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.reg_loss_theta() + self.reg_loss_xbasis()\n",
    "            elif loss_name == 'RMSE':\n",
    "                return RMSELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.reg_loss_theta() + self.reg_loss_xbasis()\n",
    "            elif loss_name == 'MAE':\n",
    "                return MAELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.reg_loss_theta() + self.reg_loss_xbasis()\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def __val_loss_fn(self, loss_name: str):\n",
    "        #TODO: mase not implemented\n",
    "        def loss(forecast, target, weights):\n",
    "            if loss_name == 'MAPE':\n",
    "                return mape(y=target, y_hat=forecast, weights=weights) #TODO: faltan weights\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return smape(y=target, y_hat=forecast, weights=weights) #TODO: faltan weights\n",
    "            elif loss_name == 'MSE':\n",
    "                return mse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'RMSE':\n",
    "                return rmse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MAE':\n",
    "                return mae(y=target, y_hat=forecast, weights=weights)\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def reg_loss_xbasis(self):\n",
    "        loss = 0.0\n",
    "        for i in range(len(self.stack_types)):\n",
    "            for block_id in range(self.n_blocks[i]):\n",
    "                if self.stack_types[i] == 'exogenous_tcn':\n",
    "                    for layer in self.model.blocks[i].modules():\n",
    "                        if isinstance(layer, t.nn.Conv1d):\n",
    "                            loss += self.lambda_reg_xbasis * t.norm(layer.weight)\n",
    "        return loss\n",
    "\n",
    "    def reg_loss_theta(self):\n",
    "        # L1 loss for initial exogenous input\n",
    "        loss = self.lambda_l1_theta * t.sum(t.abs(self.model.l1_weight))\n",
    "\n",
    "        # L2/L1 regularization for thetas\n",
    "        for i in range(len(self.stack_types)):\n",
    "            for block_id in range(self.n_blocks[i]):\n",
    "                for layer in self.model.blocks[i].modules():\n",
    "                    if isinstance(layer, t.nn.Linear):\n",
    "                        # loss += self.lambda_reg_theta * t.norm(layer.weight)\n",
    "                        loss += self.lambda_reg_theta * layer.weight.abs().sum()\n",
    "        return loss\n",
    "\n",
    "    def to_tensor(self, x: np.ndarray) -> t.Tensor:\n",
    "        tensor = t.as_tensor(x, dtype=t.float32).to(self.device)\n",
    "        return tensor\n",
    "\n",
    "    def evaluate_performance(self, ts_loader, validation_loss_fn):\n",
    "        #TODO: mas opciones que mae\n",
    "        self.model.eval()\n",
    "\n",
    "        losses = []\n",
    "        with t.no_grad():\n",
    "            for batch in iter(ts_loader):\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                forecast = self.model(insample_y=insample_y, insample_x_t=insample_x,\n",
    "                                      insample_mask=insample_mask, outsample_x_t=outsample_x, x_s=s_matrix)\n",
    "                batch_loss = validation_loss_fn(target=forecast.cpu().data.numpy(),\n",
    "                                                forecast=outsample_y.cpu().data.numpy(),\n",
    "                                                weights=outsample_mask.cpu().data.numpy())\n",
    "                losses.append(batch_loss)\n",
    "                break #TODO: remove this in future\n",
    "        loss = np.mean(losses)\n",
    "        self.model.train()\n",
    "        return loss\n",
    "\n",
    "    def fit(self, train_ts_loader, val_ts_loader=None, max_epochs=None, verbose=True, eval_steps=1):\n",
    "        # Asserts\n",
    "        assert train_ts_loader.t_cols[0] == 'y', f'First variable must be y not {train_ts_loader.t_cols[0]}'\n",
    "        assert train_ts_loader.t_cols[1] == 'ejecutado', f'First exogenous variable must be ejecutado not {train_ts_loader.t_cols[1]}'\n",
    "        assert (self.input_size)==train_ts_loader.input_size, \\\n",
    "            f'model input_size {self.input_size} data input_size {train_ts_loader.input_size}'        \n",
    "\n",
    "        # Random Seeds (model initialization)\n",
    "        t.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed) #TODO: interaccion rara con window_sampling de validacion\n",
    "\n",
    "        # Attributes of ts_dataset\n",
    "        self.n_x_t, self.n_x_s = train_ts_loader.get_n_variables()\n",
    "        self.t_cols = train_ts_loader.t_cols\n",
    "        self.f_idxs = train_ts_loader.ts_dataset.get_f_idxs(self.f_cols)\n",
    "\n",
    "        # Instantiate model\n",
    "        if not self._is_instantiated:\n",
    "            block_list = self.create_stack()\n",
    "            self.model = NBeats(blocks=t.nn.ModuleList(block_list), in_features=self.n_x_t).to(self.device)\n",
    "            self._is_instantiated = True\n",
    "\n",
    "        # Overwrite max_epochs and train datasets\n",
    "        if max_epochs is None:\n",
    "            max_epochs = self.max_epochs\n",
    "\n",
    "        train_dataloader = iter(train_ts_loader)\n",
    "\n",
    "        lr_decay_steps = max_epochs // self.n_lr_decay_steps\n",
    "        if lr_decay_steps == 0:\n",
    "            lr_decay_steps = 1\n",
    "\n",
    "        #optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_steps, gamma=self.lr_decay)\n",
    "\n",
    "        training_loss_fn = self.__loss_fn(self.loss)\n",
    "        validation_loss_fn = self.__val_loss_fn(self.loss) #Uses numpy losses\n",
    "\n",
    "        if verbose and (max_epochs > 0):\n",
    "            print('='*30+' Start fitting '+'='*30)\n",
    "            print(f'Number of exogenous variables: {self.n_x_t}')\n",
    "            print(f'Number of static variables: {self.n_x_s} , with dim_hidden: {self.n_s_hidden}')\n",
    "            print(f'Number of iterations: {max_epochs}')\n",
    "            print(f'Number of blocks: {len(self.model.blocks)}')\n",
    "\n",
    "        #self.loss_dict = {} # Restart self.loss_dict\n",
    "        start = time.time()\n",
    "        self.trajectories = {'step':[],'train_loss':[], 'val_loss':[]}\n",
    "        self.final_insample_loss = None\n",
    "        self.final_outsample_loss = None\n",
    "\n",
    "        # Training Loop\n",
    "        best_val_loss = np.inf\n",
    "        break_flag = False\n",
    "        for step in range(max_epochs):\n",
    "            self.model.train()\n",
    "            train_ts_loader.train()\n",
    "\n",
    "            batch = next(train_dataloader)\n",
    "            insample_y     = self.to_tensor(batch['insample_y'])\n",
    "            insample_x     = self.to_tensor(batch['insample_x'])\n",
    "            insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "            outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "            outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "            outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "            s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            forecast = self.model(insample_y=insample_y, insample_x_t=insample_x,\n",
    "                                  insample_mask=insample_mask, outsample_x_t=outsample_x, x_s=s_matrix)\n",
    "\n",
    "            training_loss = training_loss_fn(x=insample_y, freq=self.seasonality, forecast=forecast,\n",
    "                                            target=outsample_y, mask=outsample_mask)\n",
    "\n",
    "            if np.isnan(float(training_loss)):\n",
    "                break\n",
    "\n",
    "            training_loss.backward()\n",
    "            t.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            lr_scheduler.step()\n",
    "            if (step % eval_steps == 0):\n",
    "                display_string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(step,\n",
    "                                                                                time.time()-start,\n",
    "                                                                                self.loss,\n",
    "                                                                                training_loss.cpu().data.numpy())\n",
    "                self.trajectories['step'].append(step)\n",
    "                self.trajectories['train_loss'].append(training_loss.cpu().data.numpy())\n",
    "\n",
    "                if val_ts_loader is not None:\n",
    "                    loss = self.evaluate_performance(val_ts_loader, validation_loss_fn=validation_loss_fn)\n",
    "                    display_string += \", Outsample {}: {:.5f}\".format(self.loss, loss)\n",
    "                    self.trajectories['val_loss'].append(loss)\n",
    "\n",
    "                    if self.early_stopping:\n",
    "                        if loss < best_val_loss:\n",
    "                            # Save current model if improves outsample loss\n",
    "                            best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "                            best_insample_loss = training_loss.cpu().data.numpy()\n",
    "                            early_stopping_counter = 0\n",
    "                            best_val_loss = loss\n",
    "                        else:\n",
    "                            early_stopping_counter += 1\n",
    "                        if early_stopping_counter >= self.early_stopping:\n",
    "                            break_flag = True\n",
    "                \n",
    "                print(display_string)\n",
    "\n",
    "                self.model.train()\n",
    "                train_ts_loader.train()\n",
    "\n",
    "            if break_flag:\n",
    "                print(10*'-',' Stopped training by early stopping', 10*'-')\n",
    "                self.model.load_state_dict(best_state_dict)\n",
    "                break\n",
    "\n",
    "        #End of fitting\n",
    "        if max_epochs >0:\n",
    "            self.final_insample_loss = training_loss.cpu().data.numpy() if not break_flag else best_insample_loss #This is batch!\n",
    "            string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(step,\n",
    "                                                                            time.time()-start,\n",
    "                                                                            self.loss,\n",
    "                                                                            self.final_insample_loss)\n",
    "            if val_ts_loader is not None:\n",
    "                self.final_outsample_loss = self.evaluate_performance(val_ts_loader, validation_loss_fn=validation_loss_fn)\n",
    "                string += \", Outsample {}: {:.5f}\".format(self.loss, self.final_outsample_loss)\n",
    "            print(string)\n",
    "            print('='*30+'End fitting '+'='*30)\n",
    "\n",
    "    def predict(self, ts_loader, X_test=None, eval_mode=False):\n",
    "\n",
    "        ts_loader.eval()\n",
    "        frequency = ts_loader.get_frequency()\n",
    "\n",
    "        # Build forecasts\n",
    "        unique_ids = ts_loader.get_meta_data_var('unique_id')\n",
    "        last_ds = ts_loader.get_meta_data_var('last_ds') #TODO: ajustar of offset\n",
    "\n",
    "        batch = next(iter(ts_loader))\n",
    "        insample_y     = self.to_tensor(batch['insample_y'])\n",
    "        insample_x     = self.to_tensor(batch['insample_x'])\n",
    "        insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "        outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "        outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "        outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "        s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "        self.model.eval()\n",
    "        with t.no_grad():\n",
    "            forecast = self.model(insample_y=insample_y, insample_x_t=insample_x,\n",
    "                                  insample_mask=insample_mask, outsample_x_t=outsample_x, x_s=s_matrix)\n",
    "\n",
    "        if eval_mode:\n",
    "            return forecast, outsample_y, outsample_mask\n",
    "\n",
    "        # Predictions for panel\n",
    "        Y_hat_panel = pd.DataFrame(columns=['unique_id', 'ds'])\n",
    "        for i, unique_id in enumerate(unique_ids):\n",
    "            Y_hat_id = pd.DataFrame([unique_id]*self.output_size, columns=[\"unique_id\"])\n",
    "            ds = pd.date_range(start=last_ds[i], periods=self.output_size+1, freq=self.frequency)\n",
    "            Y_hat_id[\"ds\"] = ds[1:]\n",
    "            Y_hat_panel = Y_hat_panel.append(Y_hat_id, sort=False).reset_index(drop=True)\n",
    "\n",
    "        forecast = forecast.cpu().detach().numpy()\n",
    "        Y_hat_panel['y_hat'] = forecast.flatten()\n",
    "\n",
    "        if X_test is not None:\n",
    "            Y_hat_panel = X_test.merge(Y_hat_panel, on=['unique_id', 'ds'], how='left')\n",
    "\n",
    "        return Y_hat_panel\n",
    "\n",
    "\n",
    "    def save(self, model_dir, model_id):\n",
    "\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        print('Saving model to:\\n {}'.format(model_file)+'\\n')\n",
    "        t.save({'model_state_dict': self.model.state_dict()}, model_file)\n",
    "\n",
    "    def load(self, model_dir, model_id):\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        path = Path(model_file)\n",
    "\n",
    "        assert path.is_file(), 'No model_*.model file found in this path!'\n",
    "\n",
    "        print('Loading model from:\\n {}'.format(model_file)+'\\n')\n",
    "\n",
    "        checkpoint = t.load(model_file, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model(model):\n",
    "    print(120*'=')\n",
    "    line = '{:<40} {:<40} {:<40}'.format('Weights', 'Parameter shape', 'Number of parameters')\n",
    "    print(line)\n",
    "    print(120*'-')\n",
    "    total_params = 0\n",
    "    for model_param_name, model_param_value in model.model.named_parameters():\n",
    "        line = '{:<50} {:<40} {:<40}'.format(model_param_name, str(model_param_value.shape), len(model_param_value.flatten()))\n",
    "        total_params += len(model_param_value.flatten())\n",
    "        print(line)\n",
    "        print(120*'-')\n",
    "    print(f'Total parameters: {total_params}')\n",
    "    print(120*'=')\n",
    "    print(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing dataframes ...\n",
      "Creating ts tensor ...\n",
      "X: time series features, of shape (#series,#times,#features): \t(101760, 331)\n",
      "Y: target series (in X), of shape (#series,#times): \t \t(101760, 3)\n",
      "\n",
      "\n",
      "\n",
      "DataloaderGeneral batch time: 0.016681194305419922\n",
      "insample_y.shape torch.Size([256, 48])\n",
      "insample_x.shape torch.Size([256, 328, 48])\n",
      "outsample_y.shape torch.Size([256, 16])\n",
      "outsample_x.shape torch.Size([256, 328, 16])\n",
      "t.max(insample_y) tensor(6632.9390)\n",
      "t.max(outsample_y) tensor(6552.0488)\n",
      "\n",
      "\n",
      "\n",
      "============================== Start fitting ==============================\n",
      "Number of exogenous variables: 328\n",
      "Number of static variables: 0 , with dim_hidden: 0\n",
      "Number of iterations: 20\n",
      "Number of blocks: 4\n",
      "Step: 0, Time: 0.466, Insample MAE: 595.04285, Outsample MAE: 647.11053\n",
      "Step: 1, Time: 1.096, Insample MAE: 859.62683, Outsample MAE: 504.06638\n",
      "Step: 2, Time: 1.719, Insample MAE: 697.37976, Outsample MAE: 346.00204\n",
      "Step: 3, Time: 2.369, Insample MAE: 552.47858, Outsample MAE: 286.38171\n",
      "Step: 4, Time: 2.984, Insample MAE: 508.32187, Outsample MAE: 306.50406\n",
      "Step: 5, Time: 3.594, Insample MAE: 495.08142, Outsample MAE: 282.67752\n",
      "Step: 6, Time: 4.214, Insample MAE: 469.30588, Outsample MAE: 273.58087\n",
      "Step: 7, Time: 4.832, Insample MAE: 466.62454, Outsample MAE: 259.87473\n",
      "Step: 8, Time: 5.452, Insample MAE: 437.33533, Outsample MAE: 257.52676\n",
      "Step: 9, Time: 6.069, Insample MAE: 445.14084, Outsample MAE: 258.45355\n",
      "Step: 10, Time: 6.805, Insample MAE: 433.03857, Outsample MAE: 243.32417\n",
      "Step: 11, Time: 7.510, Insample MAE: 411.84055, Outsample MAE: 253.95062\n",
      "Step: 12, Time: 8.188, Insample MAE: 409.05011, Outsample MAE: 246.90802\n",
      "Step: 13, Time: 8.838, Insample MAE: 404.04370, Outsample MAE: 237.58086\n",
      "Step: 14, Time: 9.457, Insample MAE: 429.05359, Outsample MAE: 251.31239\n",
      "Step: 15, Time: 10.066, Insample MAE: 424.30353, Outsample MAE: 247.69054\n",
      "Step: 16, Time: 10.675, Insample MAE: 409.64935, Outsample MAE: 263.96173\n",
      "Step: 17, Time: 11.293, Insample MAE: 407.67941, Outsample MAE: 225.27148\n",
      "Step: 18, Time: 11.913, Insample MAE: 393.79590, Outsample MAE: 241.62404\n",
      "Step: 19, Time: 12.523, Insample MAE: 396.36145, Outsample MAE: 238.25476\n",
      "Step: 19, Time: 12.684, Insample MAE: 396.36145, Outsample MAE: 238.89665\n",
      "==============================End fitting ==============================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch as t\n",
    "import copy\n",
    "from fastcore.foundation import patch\n",
    "from nixtla.data.ontsdataset import TimeSeriesDataset\n",
    "from nixtla.data.ontsloader_fast import TimeSeriesLoader as TimeSeriesLoaderFast\n",
    "# from nixtla.data.tsloader_pinche import TimeSeriesLoader as TimeSeriesLoaderPinche\n",
    "# from nixtla.data.tsloader_general import TimeSeriesLoader as TimeSeriesLoaderGeneral\n",
    "\n",
    "#from nixtla.models.nbeats.onnbeats import Nbeats\n",
    "from nixtla.data.datasets.on import load_on_data\n",
    "np.random.seed(1)\n",
    "t.manual_seed(1)\n",
    "\n",
    "Y_insample_df, X_insample_df, Y_outsample_df, X_outsample_df, f_cols = load_on_data(root_dir='../data/on/', test_date='2020-09-01')\n",
    "\n",
    "ts_train_mask = np.ones(len(Y_insample_df))\n",
    "ts_train_mask[-7*6*16:] = 0 # 16 fifteenminutales = 4 hours   (total = 1 week)\n",
    "dataset = TimeSeriesDataset(Y_df=Y_insample_df, S_df=None, X_df=X_insample_df, ts_train_mask=ts_train_mask, f_cols=f_cols)\n",
    "print('X: time series features, of shape (#series,#times,#features): \\t' + str(X_insample_df.shape))\n",
    "print('Y: target series (in X), of shape (#series,#times): \\t \\t' + str(Y_insample_df.shape))\n",
    "# print('S: static features, of shape (#series,#features): \\t \\t' + str(S.shape))\n",
    "#Y_insample_df.head()\n",
    "#Y_insample_df.tail()\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "train_loader = TimeSeriesLoaderFast(ts_dataset=dataset,\n",
    "                                    model='nbeats',\n",
    "                                    offset=0,\n",
    "                                    window_sampling_limit=60*6*16, \n",
    "                                    input_size=3*16,\n",
    "                                    output_size=16,\n",
    "                                    idx_to_sample_freq=1,\n",
    "                                    batch_size=256,\n",
    "                                    is_train_loader=True)\n",
    "\n",
    "val_loader = TimeSeriesLoaderFast(ts_dataset=dataset,\n",
    "                                  model='nbeats',\n",
    "                                  offset=0,\n",
    "                                  window_sampling_limit=60*6*16,\n",
    "                                  input_size=3*16,\n",
    "                                  output_size=16,\n",
    "                                  idx_to_sample_freq=1,\n",
    "                                  batch_size=256,\n",
    "                                  is_train_loader=False)\n",
    "\n",
    "start = time.time()\n",
    "dataloader = iter(train_loader)\n",
    "batch = next(dataloader)\n",
    "insample_y = batch['insample_y']\n",
    "insample_x = batch['insample_x']\n",
    "insample_mask = batch['insample_mask']\n",
    "outsample_x = batch['outsample_x']\n",
    "outsample_y = batch['outsample_y']\n",
    "outsample_mask = batch['outsample_mask']\n",
    "print(\"DataloaderGeneral batch time:\", time.time()-start)\n",
    "print(\"insample_y.shape\", insample_y.shape)\n",
    "print(\"insample_x.shape\", insample_x.shape)\n",
    "print(\"outsample_y.shape\", outsample_y.shape)\n",
    "print(\"outsample_x.shape\", outsample_x.shape)\n",
    "print(\"t.max(insample_y)\", t.max(insample_y))\n",
    "print(\"t.max(outsample_y)\", t.max(outsample_y * outsample_mask))\n",
    "print(\"\\n\\n\")\n",
    "insample_y\n",
    "\n",
    "nbeatsx = Nbeats(input_size_multiplier=3,\n",
    "                 output_size=16,\n",
    "                 shared_weights=False,\n",
    "                 activation='relu',\n",
    "                 initialization='lecun_normal',                 \n",
    "                 stack_types=['exogenous_wavenet']+3*['identity'],\n",
    "                 n_blocks=4*[1],\n",
    "                 n_theta_hidden_list=4*[[256,256]],\n",
    "                 n_xbasis_layers=4,\n",
    "                 n_xbasis_channels=2,\n",
    "                 n_s_hidden=0,\n",
    "                 f_cols=f_cols,\n",
    "                 scaler='hola',\n",
    "                 theta_with_exogenous=True,\n",
    "                 batch_normalization=False,\n",
    "                 learning_rate=0.001,\n",
    "                 lr_decay=0.5,\n",
    "                 n_lr_decay_steps=3,\n",
    "                 dropout_prob_theta=0.1,\n",
    "                 dropout_prob_xbasis=0.1,\n",
    "                 regularization=None,\n",
    "                 lambda_l1_theta=0.01,\n",
    "                 lambda_reg_theta=0.01,\n",
    "                 lambda_reg_xbasis=0.01,\n",
    "                 max_iterations=100,\n",
    "                 early_stopping=20,\n",
    "                 loss='MAE',\n",
    "                 frequency=24,\n",
    "                 seasonality='H',\n",
    "                 random_seed=1)\n",
    "\n",
    "nbeatsx.fit(train_ts_loader=train_loader, val_ts_loader=val_loader, max_epochs=20, verbose=True, eval_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}