{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.datasets.business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business datasets\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from nixtla.data.datasets.utils import download_file, Info, TimeSeriesDataclass\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business meta information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def remove_outliers(df: pd.DataFrame, seasonality: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes outliers from data.\n",
    "    \"\"\"\n",
    "    y = df['y'].values\n",
    "    q1, q3 = np.quantile(y, [0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    low = q1 - 1.5 * iqr\n",
    "    high = q3 + 1.5 * iqr\n",
    "\n",
    "    idx_to_replace, = np.where((y > high) | (y < low))\n",
    "\n",
    "    idx_replacements = idx_to_replace - seasonality\n",
    "\n",
    "    repeated = np.isin(idx_replacements, idx_to_replace)\n",
    "    repeated = idx_replacements[repeated]\n",
    "\n",
    "    for idx in repeated:\n",
    "        idx_replacements[idx_replacements == idx] = idx_replacements[idx_to_replace == idx]\n",
    "\n",
    "    y[idx_to_replace] = y[idx_replacements]\n",
    "\n",
    "    df['y'] = y\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cleanear_brc(ts: pd.DataFrame, seasonality: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans BRC dataset.\n",
    "    \"\"\"\n",
    "    fix_dates = ['2018-11-19', '2018-12-25',\n",
    "                 '2019-01-01', '2019-02-04', '2019-02-21',\n",
    "                 '2019-03-18', '2019-04-19', '2019-05-01',\n",
    "                 '2019-09-15', '2019-09-16', '2019-09-29',\n",
    "                 '2019-10-06', '2019-11-17', '2019-12-01',\n",
    "                 '2019-12-15', '2019-12-22', '2019-12-25',\n",
    "                 '2019-12-28', '2019-12-29', '2020-01-01',\n",
    "                 '2020-01-05', '2020-01-12', '2020-01-26',\n",
    "                 '2020-02-02', '2020-02-03', '2020-02-09',\n",
    "                 '2020-02-22', '2020-02-23']\n",
    "\n",
    "    seasonal_fix_dates = pd.to_datetime(fix_dates) - pd.Timedelta(days=seasonality)\n",
    "    seasonal_fix_dates = [date.strftime('%Y-%m-%d') for date in seasonal_fix_dates]\n",
    "\n",
    "    to_fix = [date for date in seasonal_fix_dates if date in fix_dates]\n",
    "    while to_fix:\n",
    "        seasonal_fix_dates = [(pd.to_datetime(date) - pd.Timedelta(days=seasonality)).strftime('%Y-%m-%d') \\\n",
    "                              if date in to_fix else date \\\n",
    "                              for date in seasonal_fix_dates]\n",
    "        to_fix = [date for date in seasonal_fix_dates if date in fix_dates]\n",
    "\n",
    "\n",
    "    def get_updated_ts(fix_date, date):\n",
    "        \"\"\"\n",
    "        fix_date: this date fixes date.\n",
    "        date: date to fix.\n",
    "        \"\"\"\n",
    "        fixed_date = ts.query('ds == @fix_date') \\\n",
    "                       .replace({fix_date: date})\n",
    "\n",
    "        to_fix = ts.query('ds == @date')\n",
    "        equal_order = np.array_equal(fixed_date['unique_id'].values,\n",
    "                                     to_fix['unique_id'].values)\n",
    "\n",
    "        assert equal_order\n",
    "\n",
    "        fixed_date.index = to_fix.index\n",
    "\n",
    "        return fixed_date\n",
    "\n",
    "    fixed_dates = [get_updated_ts(fix_date, date) \\\n",
    "                   for fix_date, date in zip(seasonal_fix_dates, fix_dates)]\n",
    "    fixed_dates = pd.concat(fixed_dates)\n",
    "    ts.update(fixed_dates)\n",
    "\n",
    "    ts = ts.groupby('unique_id') \\\n",
    "           .apply(remove_outliers, seasonality=7)\n",
    "\n",
    "    ts['ds'] = pd.to_datetime(ts['ds'])\n",
    "    ts = ts.query('ds >= \"2018-05-02\"').reset_index(drop=True)\n",
    "\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cleanear_glb(ts: pd.DataFrame, seasonality: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans GLB dataset.\n",
    "    \"\"\"\n",
    "    fix_dates = ['2019-04-30', '2019-05-01']\n",
    "    seasonality = 7\n",
    "\n",
    "    seasonal_fix_dates = pd.to_datetime(fix_dates) - pd.Timedelta(days=seasonality)\n",
    "    seasonal_fix_dates = [date.strftime('%Y-%m-%d') for date in seasonal_fix_dates]\n",
    "\n",
    "    fixed_dates = ts.query('ds in @seasonal_fix_dates') \\\n",
    "                    .replace(dict(zip(seasonal_fix_dates, fix_dates)))\n",
    "    fixed_dates.index = ts.query('ds in @fix_dates').index\n",
    "\n",
    "    ts.update(fixed_dates)\n",
    "\n",
    "    ts = ts.groupby('unique_id') \\\n",
    "           .apply(remove_outliers, seasonality=7)\n",
    "\n",
    "    ts['ds'] = pd.to_datetime(ts['ds'])\n",
    "    ts = ts.query('ds >= \"2018-04-01\"').reset_index(drop=True)\n",
    "\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class BRC:\n",
    "    seasonality: int = 7\n",
    "    horizon: int = 7\n",
    "    cleaner: Callable[[pd.DataFrame, int], pd.DataFrame] = cleanear_brc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class GLB:\n",
    "    seasonality: int = 7\n",
    "    horizon: int = 7\n",
    "    cleaner: Callable[[pd.DataFrame, int], pd.DataFrame] = cleanear_glb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "BusinessInfo = Info(groups=('BRC', 'GLB'),\n",
    "                    class_groups=(BRC, GLB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Business:\n",
    "\n",
    "    @staticmethod\n",
    "    def load(directory: str,\n",
    "             group: str,\n",
    "             cache: bool = True):\n",
    "        \"\"\"\n",
    "        Downloads and loads Tourism data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        directory: str\n",
    "            Directory where data will be downloaded.\n",
    "        group: str\n",
    "            Group name.\n",
    "            Allowed groups: 'GLB', 'BRC'.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        [1] Returns train+test sets.\n",
    "        \"\"\"\n",
    "        path = Path(directory) / 'business' / 'datasets'\n",
    "        file_cache = path / f'ts-{group.lower()}.p'\n",
    "\n",
    "        if file_cache.exists() and cache:\n",
    "            df, X, S = pd.read_pickle(file_cache)\n",
    "\n",
    "            return df, X, S\n",
    "\n",
    "        Business.download(directory, group)\n",
    "\n",
    "        class_group = BusinessInfo[group]\n",
    "\n",
    "        df = pd.read_csv(path / f'ts-{group.lower()}.csv')\n",
    "        df = class_group.cleaner(df, class_group.seasonality)\n",
    "        \n",
    "        if group == 'GLB':\n",
    "            stat = pd.read_csv(path / f'ts-{group.lower()}-statics.csv')\n",
    "            S = stat['unique_id'].drop_duplicates().to_frame()\n",
    "            S = S.merge(stat, how='left', on=['unique_id'])\n",
    "            \n",
    "        else:\n",
    "            S = None\n",
    "        \n",
    "        X = None\n",
    "\n",
    "        if cache:\n",
    "            data = (df, X, S)\n",
    "            pd.to_pickle(data, file_cache)\n",
    "\n",
    "        return df, X, S\n",
    "\n",
    "    @staticmethod\n",
    "    def download(directory: str, group: str) -> None:\n",
    "        \"\"\"Downloads Business Dataset.\"\"\"\n",
    "\n",
    "        fs = s3fs.S3FileSystem(key=os.environ['AWS_ACCES_KEY_ID'],\n",
    "                               secret=os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "\n",
    "        path = Path(directory) / 'business' / 'datasets'\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        download_file = path / f'ts-{group.lower()}.csv'\n",
    "        if not download_file.exists():\n",
    "            file = f'research-storage-orax/business-data/ts-{group.lower()}.csv'\n",
    "            fs.download(file, str(download_file))\n",
    "            \n",
    "        if group == 'GLB':\n",
    "            download_file = path / f'ts-{group.lower()}-statics.csv'\n",
    "            if not download_file.exists():\n",
    "                file = f'research-storage-orax/business-data/ts-{group.lower()}-statics.csv'\n",
    "                fs.download(file, str(download_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRC\n",
      "            unique_id         ds     y\n",
      "0  uid_12601_2_124692 2018-05-02  40.0\n",
      "1  uid_12601_2_124692 2018-05-03  79.0\n",
      "2  uid_12601_2_124692 2018-05-04  80.0\n",
      "3  uid_12601_2_124692 2018-05-05   0.0\n",
      "4  uid_12601_2_124692 2018-05-06   0.0\n",
      "GLB\n",
      "         unique_id         ds     y\n",
      "0  uid_3002_122693 2018-04-01   9.0\n",
      "1  uid_3002_122693 2018-04-02  12.0\n",
      "2  uid_3002_122693 2018-04-03  13.0\n",
      "3  uid_3002_122693 2018-04-04   7.0\n",
      "4  uid_3002_122693 2018-04-05  10.0\n"
     ]
    }
   ],
   "source": [
    "if os.environ.get('AWS_ACCES_KEY_ID'):\n",
    "    for group in BusinessInfo.groups:\n",
    "        print(group)\n",
    "        dataset, *_ = Business.load(directory='../data', group=group)\n",
    "        print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nixtla",
   "language": "python",
   "name": "nixtla"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
