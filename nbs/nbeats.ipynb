{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.8 64-bit ('nixtla': conda)",
   "display_name": "Python 3.7.8 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "52b0028e7074a0058398558ea661882eddbb489e66a28504cc0449d2f54d6290"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.nbeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch as t\n",
    "from torch import optim\n",
    "from pathlib import Path\n",
    "\n",
    "from nixtla.models.nbeats_model import NBeats, NBeatsBlock, IdentityBasis, TrendBasis, SeasonalityBasis, ExogenousBasisGenericA, ExogenousBasisInterpretable\n",
    "from nixtla.losses.pytorch import MAPELoss, MASELoss, SMAPELoss, MSELoss, MAELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Nbeats(object):\n",
    "    \"\"\"\n",
    "    Future documentation\n",
    "    \"\"\"\n",
    "    SEASONALITY_BLOCK = 'seasonality'\n",
    "    TREND_BLOCK = 'trend'\n",
    "    IDENTITY_BLOCK = 'identity'\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size_multiplier=2,\n",
    "                 output_size=1,\n",
    "                 shared_weights=True,\n",
    "                 stack_types=[TREND_BLOCK, SEASONALITY_BLOCK],\n",
    "                 n_blocks=[3,3],\n",
    "                 n_layers=[4,4],\n",
    "                 n_hidden=[256, 2048],\n",
    "                 n_harmonics=1,\n",
    "                 n_polynomials=2,\n",
    "                 exogenous_n_channels=1,\n",
    "                 exogenous_in_mlp=False,\n",
    "                 batch_normalization=False,\n",
    "                 dropout=0,\n",
    "                 x_s_n_hidden=1,\n",
    "                 learning_rate=0.001,\n",
    "                 lr_decay=1.0,\n",
    "                 n_lr_decay_steps=3,\n",
    "                 weight_decay = 0,\n",
    "                 n_iterations=300,\n",
    "                 early_stopping = None,\n",
    "                 loss='MAPE',\n",
    "                 frequency=None,\n",
    "                 seasonality=1,\n",
    "                 random_seed=1,\n",
    "                 device=None):\n",
    "        super(Nbeats, self).__init__()\n",
    "\n",
    "        self.input_size = int(input_size_multiplier*output_size)\n",
    "        self.output_size = output_size\n",
    "        self.shared_weights = shared_weights\n",
    "        self.stack_types = stack_types\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_harmonics = n_harmonics\n",
    "        self.n_polynomials = n_polynomials\n",
    "        self.exogenous_n_channels = exogenous_n_channels\n",
    "        self.exogenous_in_mlp = exogenous_in_mlp\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.dropout = dropout\n",
    "        self.x_s_n_hidden = x_s_n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.n_lr_decay_steps = n_lr_decay_steps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.n_iterations = n_iterations\n",
    "        self.early_stopping = early_stopping\n",
    "        self.loss = loss\n",
    "        self.frequency = frequency\n",
    "        self.seasonality = seasonality\n",
    "        self.random_seed = random_seed\n",
    "        if device is None:\n",
    "            device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
    "        self.device = device\n",
    "        self.l1_lambda = 1e-3\n",
    "\n",
    "        self._is_instantiated = False\n",
    "\n",
    "    def create_stack(self):\n",
    "        #print(f'| N-Beats')\n",
    "        if self.exogenous_in_mlp:\n",
    "            x_t_n_inputs = self.input_size + self.output_size * self.n_x_t\n",
    "        else:\n",
    "            x_t_n_inputs = self.input_size # y_lags\n",
    "        block_list = []\n",
    "        self.blocks_regularizer = []\n",
    "        for i in range(len(self.stack_types)):\n",
    "            #print(f'| --  Stack {self.stack_types[i]} (#{i})')\n",
    "            for block_id in range(self.n_blocks[i]):\n",
    "                self.blocks_regularizer += [0]\n",
    "                if self.shared_weights and block_id>0:\n",
    "                    nbeats_block = block_list[-1]\n",
    "                else:\n",
    "                    if self.stack_types[i] == 'seasonality':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_dim=4 * int(\n",
    "                                                        np.ceil(self.n_harmonics / 2 * self.output_size) - (self.n_harmonics - 1)),\n",
    "                                                   basis=SeasonalityBasis(harmonics=self.n_harmonics,\n",
    "                                                                                backcast_size=self.input_size,\n",
    "                                                                                forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   exogenous_in_mlp=self.exogenous_in_mlp,\n",
    "                                                   batch_normalization=self.batch_normalization,\n",
    "                                                   dropout=self.dropout)\n",
    "                    elif self.stack_types[i] == 'trend':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_dim=2 * (self.n_polynomials + 1),\n",
    "                                                   basis=TrendBasis(degree_of_polynomial=self.n_polynomials,\n",
    "                                                                            backcast_size=self.input_size,\n",
    "                                                                            forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   exogenous_in_mlp=self.exogenous_in_mlp,\n",
    "                                                   batch_normalization=self.batch_normalization,\n",
    "                                                   dropout=self.dropout)\n",
    "                    elif self.stack_types[i] == 'identity':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_dim=self.input_size + self.output_size,\n",
    "                                                   basis=IdentityBasis(backcast_size=self.input_size,\n",
    "                                                                       forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   exogenous_in_mlp=self.exogenous_in_mlp,\n",
    "                                                   batch_normalization=self.batch_normalization,\n",
    "                                                   dropout=self.dropout)\n",
    "                    elif self.stack_types[i] == 'exogenous':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_dim=2*self.n_x_t,\n",
    "                                                   basis=ExogenousBasisInterpretable(),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   exogenous_in_mlp=self.exogenous_in_mlp,\n",
    "                                                   batch_normalization=self.batch_normalization,\n",
    "                                                   dropout=self.dropout)\n",
    "                    elif self.stack_types[i] == 'exogenous_g_a':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_dim=2*(self.exogenous_n_channels),\n",
    "                                                   basis=ExogenousBasisGenericA(self.exogenous_n_channels, self.n_x_t),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   exogenous_in_mlp=self.exogenous_in_mlp,\n",
    "                                                   batch_normalization=self.batch_normalization,\n",
    "                                                   dropout=self.dropout)\n",
    "                        self.blocks_regularizer[-1] = 1                                                \n",
    "                #print(f'     | -- {nbeats_block}')\n",
    "                block_list.append(nbeats_block)\n",
    "        return block_list\n",
    "\n",
    "    def __loss_fn(self, loss_name: str):\n",
    "        def loss(x, freq, forecast, target, mask):\n",
    "            if loss_name == 'MAPE':\n",
    "                return MAPELoss(y=target, y_hat=forecast, mask=mask) + self.l1_regularization()\n",
    "            elif loss_name == 'MASE':\n",
    "                return MASELoss(y=target, y_hat=forecast, y_insample=x, seasonality=freq, mask=mask) + self.l1_regularization()\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return SMAPELoss(y=target, y_hat=forecast, mask=mask) + self.l1_regularization()\n",
    "            elif loss_name == 'MSE':\n",
    "                return MSELoss(y=target, y_hat=forecast, mask=mask) + self.l1_regularization()\n",
    "            elif loss_name == 'MAE':\n",
    "                return MAELoss(y=target, y_hat=forecast, mask=mask) + self.l1_regularization()\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "    \n",
    "    def l1_regularization(self):\n",
    "        l1_loss = 0\n",
    "        for i, indicator in enumerate(self.blocks_regularizer):\n",
    "            if indicator:\n",
    "                l1_loss +=  self.l1_lambda*t.sum(t.abs(self.model.blocks[i].basis.weight))\n",
    "        return l1_loss\n",
    "\n",
    "    def to_tensor(self, x: np.ndarray) -> t.Tensor:\n",
    "        tensor = t.as_tensor(x, dtype=t.float32).to(self.device)\n",
    "        return tensor\n",
    "\n",
    "    def evaluate_performance(self, ts_loader):\n",
    "        #TODO: mas opciones que mae\n",
    "        self.model.eval()\n",
    "\n",
    "        losses = []\n",
    "        with t.no_grad():\n",
    "            for batch in iter(ts_loader):\n",
    "                insample_y = self.to_tensor(batch['insample_y'])\n",
    "                insample_x_t = self.to_tensor(batch['insample_x_t'])\n",
    "                insample_mask = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x_t = self.to_tensor(batch['outsample_x_t'])\n",
    "                outsample_y = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                static_data = self.to_tensor(batch['static_data'])\n",
    "\n",
    "                forecast = self.model(insample_y, insample_x_t, insample_mask, outsample_x_t, static_data)\n",
    "                batch_loss = mae(y=forecast.cpu().data.numpy(),\n",
    "                                 y_hat=outsample_y.cpu().data.numpy(),\n",
    "                                 weights=outsample_mask.cpu().data.numpy())\n",
    "                losses.append(batch_loss)\n",
    "                break #TODO: remove this in future\n",
    "        loss = np.mean(losses)\n",
    "        return loss\n",
    "\n",
    "    def fit(self, train_ts_loader, val_ts_loader=None, n_iterations=None, verbose=True, eval_steps=1):\n",
    "        # TODO: Indexes hardcoded, information duplicated in train and val datasets\n",
    "\n",
    "        # Random Seeds (model initialization)\n",
    "        t.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed)\n",
    "\n",
    "        # Attributes of ts_dataset\n",
    "        self.n_x_t, self.n_x_s = train_ts_loader.get_n_variables()\n",
    "\n",
    "        # Instantiate model\n",
    "        if not self._is_instantiated:\n",
    "            block_list = self.create_stack()\n",
    "            self.model = NBeats(t.nn.ModuleList(block_list)).to(self.device)\n",
    "            self._is_instantiated = True\n",
    "\n",
    "        # Overwrite n_iterations and train datasets\n",
    "        if n_iterations is None:\n",
    "            n_iterations = self.n_iterations\n",
    "        \n",
    "        train_dataloader = iter(train_ts_loader)\n",
    "\n",
    "        lr_decay_steps = n_iterations // self.n_lr_decay_steps\n",
    "        if lr_decay_steps == 0:\n",
    "            lr_decay_steps = 1\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_steps, gamma=self.lr_decay)\n",
    "        training_loss_fn = self.__loss_fn(self.loss)\n",
    "\n",
    "        if verbose and (n_iterations > 0):\n",
    "            print('='*30+' Start fitting '+'='*30)\n",
    "            print(f'Number of exogenous variables: {self.n_x_t}')\n",
    "            print(f'Number of static variables: {self.n_x_s} , with dim_hidden: {self.x_s_n_hidden}')\n",
    "            print(f'Number of iterations: {n_iterations}')\n",
    "            print(f'Number of blocks: {len(self.model.blocks)}')\n",
    "\n",
    "        #self.loss_dict = {} # Restart self.loss_dict\n",
    "        start = time.time()\n",
    "        self.trajectories = {'step':[],'train_loss':[], 'val_loss':[]}\n",
    "        self.final_insample_loss = None\n",
    "        self.final_outsample_loss = None\n",
    "\n",
    "        # Training Loop\n",
    "        best_val_loss = np.inf\n",
    "        for step in range(n_iterations):\n",
    "            self.model.train()\n",
    "            train_ts_loader.train()\n",
    "\n",
    "            batch = next(train_dataloader)\n",
    "\n",
    "            insample_y = self.to_tensor(batch['insample_y'])\n",
    "            insample_x_t = self.to_tensor(batch['insample_x_t'])\n",
    "            insample_mask = self.to_tensor(batch['insample_mask'])\n",
    "\n",
    "            outsample_x_t = self.to_tensor(batch['outsample_x_t'])\n",
    "            outsample_y = self.to_tensor(batch['outsample_y'])\n",
    "            outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "\n",
    "            static_data = self.to_tensor(batch['static_data'])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            forecast = self.model(insample_y, insample_x_t, insample_mask, outsample_x_t, static_data)\n",
    "\n",
    "            training_loss = training_loss_fn(x=insample_y, freq=self.seasonality, forecast=forecast,\n",
    "                                            target=outsample_y, mask=outsample_mask)\n",
    "\n",
    "            if np.isnan(float(training_loss)):\n",
    "                break\n",
    "\n",
    "            training_loss.backward()\n",
    "            t.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            lr_scheduler.step()\n",
    "            if (step % eval_steps == 0):\n",
    "                string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(step,\n",
    "                                                                                time.time()-start,\n",
    "                                                                                self.loss,\n",
    "                                                                                training_loss.cpu().data.numpy())\n",
    "                self.trajectories['step'].append(step)\n",
    "                self.trajectories['train_loss'].append(training_loss.cpu().data.numpy())\n",
    "\n",
    "                if val_ts_loader is not None:\n",
    "                    loss = self.evaluate_performance(val_ts_loader)\n",
    "                    string += \", Outsample {}: {:.5f}\".format(self.loss, loss)\n",
    "                    self.trajectories['val_loss'].append(loss)\n",
    "\n",
    "                    if self.early_stopping:\n",
    "                        if loss < best_val_loss:\n",
    "                            counter = 0\n",
    "                            best_val_loss = loss\n",
    "                        else:\n",
    "                            counter += 1\n",
    "                        if counter >= self.early_stopping:\n",
    "                            print(10*'-',' Stopped training by early stopping', 10*'-')\n",
    "                            break\n",
    "                \n",
    "                print(string)\n",
    "\n",
    "                self.model.train()\n",
    "                train_ts_loader.train()\n",
    "\n",
    "        #End of fitting\n",
    "        if n_iterations >0:\n",
    "            self.final_insample_loss = training_loss.cpu().data.numpy() #TODO: this is batch!\n",
    "            string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(step,\n",
    "                                                                            time.time()-start,\n",
    "                                                                            self.loss,\n",
    "                                                                            self.final_insample_loss)\n",
    "            if val_ts_loader is not None:\n",
    "                self.final_outsample_loss = self.evaluate_performance(val_ts_loader)\n",
    "                string += \", Outsample {}: {:.5f}\".format(self.loss, self.final_outsample_loss)\n",
    "            print(string)\n",
    "            print('='*30+'End fitting '+'='*30)\n",
    "\n",
    "    def predict(self, ts_loader, X_test=None, eval_mode=False):\n",
    "\n",
    "        ts_loader.eval()\n",
    "        frequency = ts_loader.get_frequency()\n",
    "\n",
    "        # Build forecasts\n",
    "        unique_ids = ts_loader.get_meta_data_var('unique_id')\n",
    "        last_ds = ts_loader.get_meta_data_var('last_ds') #TODO: ajustar of offset\n",
    "\n",
    "        batch = next(iter(ts_loader))\n",
    "        insample_y = self.to_tensor(batch['insample_y'])\n",
    "        insample_x_t = self.to_tensor(batch['insample_x_t'])\n",
    "        insample_mask = self.to_tensor(batch['insample_mask'])\n",
    "        outsample_x_t = self.to_tensor(batch['outsample_x_t'])\n",
    "        outsample_y = self.to_tensor(batch['outsample_y'])\n",
    "        outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "        static_data = self.to_tensor(batch['static_data'])\n",
    "\n",
    "        self.model.eval()\n",
    "        with t.no_grad():\n",
    "            forecast = self.model(insample_y, insample_x_t, insample_mask, outsample_x_t, static_data)\n",
    "\n",
    "        if eval_mode:\n",
    "            return forecast, outsample_y, outsample_mask\n",
    "\n",
    "        # Predictions for panel\n",
    "        Y_hat_panel = pd.DataFrame(columns=['unique_id', 'ds'])\n",
    "        for i, unique_id in enumerate(unique_ids):\n",
    "            Y_hat_id = pd.DataFrame([unique_id]*self.output_size, columns=[\"unique_id\"])\n",
    "            ds = pd.date_range(start=last_ds[i], periods=self.output_size+1, freq=self.frequency)\n",
    "            Y_hat_id[\"ds\"] = ds[1:]\n",
    "            Y_hat_panel = Y_hat_panel.append(Y_hat_id, sort=False).reset_index(drop=True)\n",
    "\n",
    "        forecast = forecast.cpu().detach().numpy()\n",
    "        Y_hat_panel['y_hat'] = forecast.flatten()\n",
    "\n",
    "        if X_test is not None:\n",
    "            Y_hat_panel = X_test.merge(Y_hat_panel, on=['unique_id', 'ds'], how='left')\n",
    "        \n",
    "        return Y_hat_panel\n",
    "\n",
    "\n",
    "    def save(self, model_dir, model_id):\n",
    "    \n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "    \n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        print('Saving model to:\\n {}'.format(model_file)+'\\n')\n",
    "        t.save({'model_state_dict': self.model.state_dict()}, model_file)\n",
    "\n",
    "    def load(self, model_dir, model_id):\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        path = Path(model_file)\n",
    "\n",
    "        assert path.is_file(), 'No model_*.model file found in this path!'\n",
    "\n",
    "        print('Loading model from:\\n {}'.format(model_file)+'\\n')\n",
    "\n",
    "        checkpoint = t.load(model_file, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}