{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.tcn.tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from torch import optim\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "from nixtla.models.tcn.tcn_model import TCNModule\n",
    "from nixtla.losses.pytorch import MAPELoss, MASELoss, SMAPELoss, MSELoss, MAELoss\n",
    "from nixtla.losses.numpy import mae, mse, mape, smape, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_weights(module, initialization):\n",
    "    if type(module) == t.nn.Linear:\n",
    "        if initialization == 'orthogonal':\n",
    "            t.nn.init.orthogonal_(module.weight)\n",
    "        elif initialization == 'he_uniform':\n",
    "            t.nn.init.kaiming_uniform_(module.weight)\n",
    "        elif initialization == 'he_normal':\n",
    "            t.nn.init.kaiming_normal_(module.weight)\n",
    "        elif initialization == 'glorot_uniform':\n",
    "            t.nn.init.xavier_uniform_(module.weight)\n",
    "        elif initialization == 'glorot_normal':\n",
    "            t.nn.init.xavier_normal_(module.weight)\n",
    "        elif initialization == 'lecun_normal':\n",
    "            pass #t.nn.init.normal_(module.weight, 0.0, std=1/np.sqrt(module.weight.numel()))\n",
    "        else:\n",
    "            assert 1<0, f'Initialization {initialization} not found'\n",
    "\n",
    "class TCN(object):\n",
    "    \"\"\"\n",
    "    Future documentation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 output_size,\n",
    "                 n_channels,\n",
    "                 kernel_size,\n",
    "                 initialization,\n",
    "                 learning_rate,\n",
    "                 lr_decay,\n",
    "                 n_lr_decay_steps,\n",
    "                 weight_decay,\n",
    "                 l1_inputs,\n",
    "                 dropout_prob,\n",
    "                 max_epochs,\n",
    "                 early_stopping,\n",
    "                 loss,\n",
    "                 frequency,\n",
    "                 random_seed,\n",
    "                 seasonality,\n",
    "                 device=None):\n",
    "        super(TCN, self).__init__()\n",
    "\n",
    "        #------------------------ Model Attributes ------------------------#\n",
    "        # Architecture parameters\n",
    "        self.output_size = output_size\n",
    "        self.initialization = initialization\n",
    "        self.n_channels = n_channels\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # Regularization and optimization parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.n_lr_decay_steps = n_lr_decay_steps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.max_epochs = max_epochs\n",
    "        self.early_stopping = early_stopping\n",
    "        self.loss = loss\n",
    "        self.l1_inputs = l1_inputs\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        # Other\n",
    "        self.frequency = frequency\n",
    "        self.seasonality = seasonality\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        if device is None:\n",
    "            device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
    "        self.device = device\n",
    "\n",
    "        self._is_instantiated = False\n",
    "\n",
    "    def __loss_fn(self, loss_name: str):\n",
    "        def loss(x, freq, forecast, target, mask):\n",
    "            if loss_name == 'MAPE':\n",
    "                return MAPELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_inputs()\n",
    "            elif loss_name == 'MASE':\n",
    "                return MASELoss(y=target, y_hat=forecast, y_insample=x, seasonality=freq, mask=mask) + \\\n",
    "                       self.loss_l1_inputs()\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return SMAPELoss(y=target, y_hat=forecast, mask=mask) + self.loss_l1_inputs()\n",
    "            elif loss_name == 'MSE':\n",
    "                return MSELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_inputs()\n",
    "            elif loss_name == 'MAE':\n",
    "                return MAELoss(y=target, y_hat=forecast, mask=mask) + \\\n",
    "                       self.loss_l1_inputs()\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def __val_loss_fn(self, loss_name='MAE'):\n",
    "        #TODO: mase not implemented\n",
    "        def loss(forecast, target, weights):\n",
    "            if loss_name == 'MAPE':\n",
    "                return mape(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return smape(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MSE':\n",
    "                return mse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'RMSE':\n",
    "                return rmse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MAE':\n",
    "                return mae(y=target, y_hat=forecast, weights=weights)\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "    \n",
    "    def loss_l1_inputs(self):\n",
    "        loss_l1 = 0\n",
    "        # for i, indicator in enumerate(self.blocks_regularizer):\n",
    "        #     if indicator:\n",
    "        #         loss_l1 += self.l1_inputs * t.sum(t.abs(self.model.blocks[i].basis.weight))\n",
    "        return loss_l1\n",
    "\n",
    "    def to_tensor(self, x: np.ndarray) -> t.Tensor:\n",
    "        tensor = t.as_tensor(x, dtype=t.float32).to(self.device)\n",
    "        return tensor\n",
    "\n",
    "    def evaluate_performance(self, ts_loader, validation_loss_fn):\n",
    "        #TODO: mas opciones que mae\n",
    "        self.model.eval()\n",
    "\n",
    "        losses = []\n",
    "        with t.no_grad():\n",
    "            for batch in iter(ts_loader):\n",
    "                # Parse batch\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                forecast = self.model.predict(insample_y=insample_y, insample_x=insample_x,\n",
    "                                              s_matrix=s_matrix)\n",
    "\n",
    "                batch_loss = validation_loss_fn(target=forecast.cpu().data.numpy(),\n",
    "                                                forecast=outsample_y.cpu().data.numpy(),\n",
    "                                                weights=outsample_mask.cpu().data.numpy())\n",
    "                losses.append(batch_loss)\n",
    "        loss = np.mean(losses)\n",
    "        self.model.train()\n",
    "        return loss\n",
    "\n",
    "    def fit(self, train_ts_loader, val_ts_loader=None, max_epochs=None, verbose=True, eval_epochs=1):\n",
    "        # Random Seeds (model initialization)\n",
    "        t.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed) #TODO: interaccion rara con window_sampling de validacion\n",
    "\n",
    "        # Attributes of ts_dataset\n",
    "        self.n_x_t, self.n_s = train_ts_loader.get_n_variables()\n",
    "        self.n_x_t += 1 # add y\n",
    "\n",
    "        # Instantiate model\n",
    "        if not self._is_instantiated:\n",
    "            self.model = TCNModule(output_size=self.output_size, num_inputs = self.n_x_t,\n",
    "                                   num_channels=self.n_channels, num_static = self.n_s,\n",
    "                                   kernel_size=self.kernel_size, dropout=self.dropout_prob)\n",
    "            init_function = partial(init_weights, initialization=self.initialization)\n",
    "            self.model.apply(init_function)\n",
    "            self.model = self.model.to(self.device)\n",
    "            self._is_instantiated = True\n",
    "\n",
    "        # Overwrite n_iterations and train datasets\n",
    "        if max_epochs is None:\n",
    "            max_epochs = self.max_epochs\n",
    "\n",
    "        lr_decay_steps = max_epochs // self.n_lr_decay_steps\n",
    "        if lr_decay_steps == 0:\n",
    "            lr_decay_steps = 1\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_steps, gamma=self.lr_decay)\n",
    "        training_loss_fn = self.__loss_fn(self.loss)\n",
    "        validation_loss_fn = self.__val_loss_fn(self.loss) #Uses numpy losses\n",
    "\n",
    "        if verbose and (max_epochs > 0):\n",
    "            print('='*30+' Start fitting '+'='*30)\n",
    "            print(f'Number of exogenous variables: {self.n_x_t}')\n",
    "            #print(f'Number of static variables: {self.n_x_s} , with dim_hidden: {self.x_s_n_hidden}')\n",
    "            print(f'Number of epochs: {max_epochs}')\n",
    "\n",
    "        #self.loss_dict = {} # Restart self.loss_dict\n",
    "        start = time.time()\n",
    "        self.trajectories = {'epoch':[],'train_loss':[], 'val_loss':[]}\n",
    "        self.final_insample_loss = None\n",
    "        self.final_outsample_loss = None\n",
    "\n",
    "        # Training Loop\n",
    "        early_stopping_counter = 0\n",
    "        best_val_loss = np.inf\n",
    "        best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "        break_flag = False\n",
    "        epoch = 0\n",
    "        while (epoch < max_epochs) and (not break_flag):\n",
    "            epoch +=1\n",
    "            for batch in iter(train_ts_loader):\n",
    "                if (epoch > max_epochs) or (break_flag):\n",
    "                    continue\n",
    "\n",
    "                self.model.train()\n",
    "                train_ts_loader.train()\n",
    "                # Parse batch\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                forecast, outsample_y = self.model(insample_y=insample_y, insample_x=insample_x,\n",
    "                                                   s_matrix=s_matrix, outsample_y=outsample_y)\n",
    "\n",
    "                training_loss = training_loss_fn(x=insample_y, freq=self.seasonality, forecast=forecast,\n",
    "                                                target=outsample_y, mask=outsample_mask)\n",
    "\n",
    "                if np.isnan(float(training_loss)):\n",
    "                    break\n",
    "\n",
    "                training_loss.backward()\n",
    "                t.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            lr_scheduler.step()\n",
    "            if (epoch % eval_epochs == 0):\n",
    "                display_string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(epoch,\n",
    "                                                                                time.time()-start,\n",
    "                                                                                self.loss,\n",
    "                                                                                training_loss.cpu().data.numpy())\n",
    "                self.trajectories['epoch'].append(epoch)\n",
    "                self.trajectories['train_loss'].append(training_loss.cpu().data.numpy())\n",
    "\n",
    "                if val_ts_loader is not None:\n",
    "                    loss = self.evaluate_performance(ts_loader=val_ts_loader, \n",
    "                                                    validation_loss_fn=validation_loss_fn)\n",
    "                    display_string += \", Outsample {}: {:.5f}\".format(self.loss, loss)\n",
    "                    self.trajectories['val_loss'].append(loss)\n",
    "\n",
    "                    if self.early_stopping:\n",
    "                        if loss < best_val_loss:\n",
    "                            # Save current model if improves outsample loss\n",
    "                            best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "                            best_insample_loss = training_loss.cpu().data.numpy()\n",
    "                            early_stopping_counter = 0\n",
    "                            best_val_loss = loss\n",
    "                        else:\n",
    "                            early_stopping_counter += 1\n",
    "                        if early_stopping_counter >= self.early_stopping:\n",
    "                            break_flag = True\n",
    "                \n",
    "                print(display_string)\n",
    "\n",
    "                self.model.train()\n",
    "                train_ts_loader.train()\n",
    "\n",
    "            if break_flag:\n",
    "                print(10*'-',' Stopped training by early stopping', 10*'-')\n",
    "                self.model.load_state_dict(best_state_dict)\n",
    "                break\n",
    "\n",
    "        #End of fitting\n",
    "        if max_epochs >0:\n",
    "            self.final_insample_loss = training_loss.cpu().data.numpy() if not break_flag else best_insample_loss #This is batch!\n",
    "            string = 'epoch: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(epoch,\n",
    "                                                                            time.time()-start,\n",
    "                                                                            self.loss,\n",
    "                                                                            self.final_insample_loss)\n",
    "            if val_ts_loader is not None:\n",
    "                self.final_outsample_loss = self.evaluate_performance(ts_loader=val_ts_loader, \n",
    "                                                                      validation_loss_fn=validation_loss_fn)\n",
    "                string += \", Outsample {}: {:.5f}\".format(self.loss, self.final_outsample_loss)\n",
    "            print(string)\n",
    "            print('='*30+'End fitting '+'='*30)\n",
    "\n",
    "    def predict(self, ts_loader, X_test=None, eval_mode=False):\n",
    "\n",
    "        ts_loader.eval()\n",
    "        frequency = ts_loader.get_frequency()\n",
    "\n",
    "        # Build forecasts\n",
    "        unique_ids = ts_loader.get_meta_data_col('unique_id')\n",
    "        last_ds = ts_loader.get_meta_data_col('last_ds') #TODO: ajustar of offset\n",
    "\n",
    "        self.model.eval()\n",
    "        with t.no_grad():\n",
    "            forecasts = []\n",
    "            outsample_ys = []\n",
    "            outsample_masks = []\n",
    "            for batch in iter(ts_loader):\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                forecast = self.model.predict(insample_y=insample_y, insample_x=insample_x,\n",
    "                                              s_matrix=s_matrix)\n",
    "                forecasts += [forecast.cpu().data.numpy()]\n",
    "                outsample_ys += [outsample_y.cpu().data.numpy()]\n",
    "                outsample_masks += [outsample_mask.cpu().data.numpy()]\n",
    "        forecasts = np.vstack(forecasts)\n",
    "        outsample_ys = np.vstack(outsample_ys)\n",
    "        outsample_masks = np.vstack(outsample_masks)\n",
    "\n",
    "        if eval_mode:\n",
    "            return forecasts, outsample_ys, outsample_masks\n",
    "\n",
    "        # Predictions for panel\n",
    "        Y_hat_panel = pd.DataFrame(columns=['unique_id', 'ds'])\n",
    "        for i, unique_id in enumerate(unique_ids):\n",
    "            Y_hat_id = pd.DataFrame([unique_id]*self.output_size, columns=[\"unique_id\"])\n",
    "            ds = pd.date_range(start=last_ds[i], periods=self.output_size+1, freq=frequency)\n",
    "            Y_hat_id[\"ds\"] = ds[1:]\n",
    "            Y_hat_panel = Y_hat_panel.append(Y_hat_id, sort=False).reset_index(drop=True)\n",
    "\n",
    "        Y_hat_panel['y_hat'] = forecasts.flatten()\n",
    "\n",
    "        if X_test is not None:\n",
    "            Y_hat_panel = X_test.merge(Y_hat_panel, on=['unique_id', 'ds'], how='left')\n",
    "        \n",
    "        return Y_hat_panel\n",
    "\n",
    "\n",
    "    def save(self, model_dir, model_id, state_dict = None):\n",
    "    \n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        if state_dict is None:\n",
    "            state_dict = self.model.state_dict()\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        print('Saving model to:\\n {}'.format(model_file)+'\\n')\n",
    "        t.save({'model_state_dict': state_dict}, model_file)\n",
    "\n",
    "    def load(self, model_dir, model_id):\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        path = Path(model_file)\n",
    "\n",
    "        assert path.is_file(), 'No model_*.model file found in this path!'\n",
    "\n",
    "        print('Loading model from:\\n {}'.format(model_file)+'\\n')\n",
    "\n",
    "        checkpoint = t.load(model_file, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import random\n",
    "# import torch as t\n",
    "# import copy\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from nixtla.data.tsdataset import TimeSeriesDataset\n",
    "# from nixtla.data.tsloader_general import TimeSeriesLoader as TimeSeriesLoaderGeneral\n",
    "\n",
    "# from nixtla.data.datasets.epf import EPF, EPFInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_df, X_df, _ = EPF.load(directory='data', group=EPFInfo.groups[0])\n",
    "\n",
    "# std = X_df['Exogenous1'].std()\n",
    "# mean = X_df['Exogenous1'].mean()\n",
    "# X_df['Exogenous1'] = (X_df['Exogenous1']-mean)/std\n",
    "\n",
    "# std = X_df['Exogenous2'].std()\n",
    "# mean = X_df['Exogenous2'].mean()\n",
    "# X_df['Exogenous2'] = (X_df['Exogenous2']-mean)/std\n",
    "\n",
    "\n",
    "# epf_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=None, X_df=None, ts_in_test=365 * 24)\n",
    "\n",
    "# Y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = TimeSeriesLoaderGeneral(ts_dataset=epf_dataset,\n",
    "#                                        model='tcn',\n",
    "#                                        offset=0,\n",
    "#                                        window_sampling_limit=365*4*24, \n",
    "#                                        input_size=7*24,\n",
    "#                                        output_size=24,\n",
    "#                                        idx_to_sample_freq=1,\n",
    "#                                        batch_size=1024, \n",
    "#                                        complete_sample=True,\n",
    "#                                        complete_inputs=True,\n",
    "#                                        shuffle=False)\n",
    "\n",
    "# val_loader = TimeSeriesLoaderGeneral(ts_dataset=epf_dataset,\n",
    "#                                      model='tcn',\n",
    "#                                      offset=0,\n",
    "#                                      window_sampling_limit=365*4*24, \n",
    "#                                      input_size=7*24,\n",
    "#                                      output_size=24,\n",
    "#                                      idx_to_sample_freq=1,\n",
    "#                                      batch_size=1024,\n",
    "#                                      complete_sample=False,\n",
    "#                                      complete_inputs=False,\n",
    "#                                      shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tcn = TCN(output_size=24,\n",
    "#           n_channels=[1, 1, 1],\n",
    "#           kernel_size=4,\n",
    "#           initialization='lecun_normal',\n",
    "#           learning_rate=0.01,\n",
    "#           lr_decay=0.5,\n",
    "#           n_lr_decay_steps=3,\n",
    "#           weight_decay=0,\n",
    "#           l1_inputs=0,\n",
    "#           dropout_prob=0.3,\n",
    "#           max_epochs=500,\n",
    "#           early_stopping=10,\n",
    "#           loss='MAE',\n",
    "#           frequency='H',\n",
    "#           random_seed=1,\n",
    "#           seasonality=24,\n",
    "#           device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tcn.fit(train_ts_loader=train_loader, val_ts_loader=val_loader, eval_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_hat = tcn.predict(ts_loader=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_plot = Y_df.tail(24)['y']\n",
    "# y_hat_plot = y_hat['y_hat']\n",
    "# plt.plot(range(len(y_plot)), y_plot)\n",
    "# plt.plot(range(len(y_plot), len(y_plot)+len(y_hat_plot)), y_hat_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(Y_df['y'][-168:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nixtla",
   "language": "python",
   "name": "nixtla"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
