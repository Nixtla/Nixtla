{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.tcn.tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from torch import optim\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "from nixtla.models.tcn.tcn_model import TCNModule\n",
    "from nixtla.losses.pytorch import MAPELoss, MASELoss, SMAPELoss, MSELoss, MAELoss, PinballLoss\n",
    "from nixtla.losses.numpy import mae, mse, mape, smape, rmse, pinball_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_weights(module, initialization):\n",
    "    if type(module) == t.nn.Linear:\n",
    "        if initialization == 'orthogonal':\n",
    "            t.nn.init.orthogonal_(module.weight)\n",
    "        elif initialization == 'he_uniform':\n",
    "            t.nn.init.kaiming_uniform_(module.weight)\n",
    "        elif initialization == 'he_normal':\n",
    "            t.nn.init.kaiming_normal_(module.weight)\n",
    "        elif initialization == 'glorot_uniform':\n",
    "            t.nn.init.xavier_uniform_(module.weight)\n",
    "        elif initialization == 'glorot_normal':\n",
    "            t.nn.init.xavier_normal_(module.weight)\n",
    "        elif initialization == 'lecun_normal':\n",
    "            pass #t.nn.init.normal_(module.weight, 0.0, std=1/np.sqrt(module.weight.numel()))\n",
    "        else:\n",
    "            assert 1<0, f'Initialization {initialization} not found'\n",
    "\n",
    "class TCN(object):\n",
    "    \"\"\"\n",
    "    Future documentation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 output_size,\n",
    "                 n_channels,\n",
    "                 kernel_size,\n",
    "                 initialization,\n",
    "                 learning_rate,\n",
    "                 lr_decay,\n",
    "                 n_lr_decay_steps,\n",
    "                 weight_decay,\n",
    "                 dropout_prob,\n",
    "                 n_iterations,\n",
    "                 early_stopping,\n",
    "                 loss,\n",
    "                 val_loss,\n",
    "                 frequency,\n",
    "                 random_seed,\n",
    "                 seasonality,\n",
    "                 device=None):\n",
    "        super(TCN, self).__init__()\n",
    "\n",
    "        #------------------------ Model Attributes ------------------------#\n",
    "        # Architecture parameters\n",
    "        self.output_size = output_size\n",
    "        self.initialization = initialization\n",
    "        self.n_channels = n_channels\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # Regularization and optimization parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.n_lr_decay_steps = n_lr_decay_steps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.n_iterations = n_iterations\n",
    "        self.early_stopping = early_stopping\n",
    "        self.loss = loss\n",
    "        self.val_loss = val_loss\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        # Other\n",
    "        self.frequency = frequency\n",
    "        self.seasonality = seasonality\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        if device is None:\n",
    "            device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
    "        self.device = device\n",
    "\n",
    "        self._is_instantiated = False\n",
    "\n",
    "    def __loss_fn(self, loss_name: str):\n",
    "        def loss(x, freq, forecast, target, mask):\n",
    "            if loss_name == 'MAPE':\n",
    "                return MAPELoss(y=target, y_hat=forecast, mask=mask)\n",
    "            elif loss_name == 'MASE':\n",
    "                return MASELoss(y=target, y_hat=forecast, y_insample=x, seasonality=freq, mask=mask)\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return SMAPELoss(y=target, y_hat=forecast, mask=mask)\n",
    "            elif loss_name == 'MSE':\n",
    "                return MSELoss(y=target, y_hat=forecast, mask=mask)\n",
    "            elif loss_name == 'MAE':\n",
    "                return MAELoss(y=target, y_hat=forecast, mask=mask)\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def __val_loss_fn(self, loss_name='MAE'):\n",
    "        #TODO: mase not implemented\n",
    "        def loss(forecast, target, weights):\n",
    "            if loss_name == 'MAPE':\n",
    "                return mape(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return smape(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MSE':\n",
    "                return mse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'RMSE':\n",
    "                return rmse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MAE':\n",
    "                return mae(y=target, y_hat=forecast, weights=weights)\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def to_tensor(self, x: np.ndarray) -> t.Tensor:\n",
    "        tensor = t.as_tensor(x, dtype=t.float32).to(self.device)\n",
    "        return tensor\n",
    "\n",
    "    def evaluate_performance(self, ts_loader, validation_loss_fn):\n",
    "        self.model.eval()\n",
    "\n",
    "        target, forecast, outsample_mask = self.predict(ts_loader=ts_loader)\n",
    "\n",
    "        complete_loss = validation_loss_fn(target=target, forecast=forecast, weights=outsample_mask)\n",
    "\n",
    "        self.model.train()\n",
    "        return complete_loss\n",
    "\n",
    "    def fit(self, train_ts_loader, val_ts_loader=None, n_iterations=None, verbose=True, eval_freq=1):\n",
    "        # Random Seeds (model initialization)\n",
    "        t.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed) #TODO: interaccion rara con window_sampling de validacion\n",
    "\n",
    "        # Attributes of ts_dataset\n",
    "        self.n_x_t, self.n_s = train_ts_loader.get_n_variables()\n",
    "\n",
    "        # Instantiate model\n",
    "        if not self._is_instantiated:\n",
    "            self.model = TCNModule(output_size=self.output_size, num_inputs = self.n_x_t+1,\n",
    "                                   num_channels=self.n_channels, num_static = self.n_s,\n",
    "                                   kernel_size=self.kernel_size, dropout=self.dropout_prob)\n",
    "            init_function = partial(init_weights, initialization=self.initialization)\n",
    "            self.model.apply(init_function)\n",
    "            self.model = self.model.to(self.device)\n",
    "            self._is_instantiated = True\n",
    "\n",
    "        # Overwrite n_iterations and train datasets\n",
    "        if n_iterations is None:\n",
    "            n_iterations = self.n_iterations\n",
    "\n",
    "        lr_decay_steps = n_iterations // self.n_lr_decay_steps\n",
    "        if lr_decay_steps == 0:\n",
    "            lr_decay_steps = 1\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_steps, gamma=self.lr_decay)\n",
    "        training_loss_fn = self.__loss_fn(self.loss)\n",
    "        validation_loss_fn = self.__val_loss_fn(self.val_loss) #Uses numpy losses\n",
    "\n",
    "        print('\\n')\n",
    "        print('='*30+' Start fitting '+'='*30)\n",
    "\n",
    "        #self.loss_dict = {} # Restart self.loss_dict\n",
    "        start = time.time()\n",
    "        self.trajectories = {'iteration':[],'train_loss':[], 'val_loss':[]}\n",
    "        self.final_insample_loss = None\n",
    "        self.final_outsample_loss = None\n",
    "        \n",
    "        # Training Loop\n",
    "        early_stopping_counter = 0\n",
    "        best_val_loss = np.inf\n",
    "        best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "        break_flag = False\n",
    "        iteration = 0\n",
    "        epoch = 0\n",
    "        while (iteration < n_iterations) and (not break_flag):\n",
    "            epoch +=1\n",
    "            for batch in iter(train_ts_loader):\n",
    "                iteration += 1\n",
    "                if (iteration > n_iterations) or (break_flag):\n",
    "                    continue\n",
    "\n",
    "                self.model.train()\n",
    "                # Parse batch\n",
    "                S     = self.to_tensor(batch['S'])\n",
    "                Y     = self.to_tensor(batch['Y'])\n",
    "                X     = self.to_tensor(batch['X'])\n",
    "                available_mask  = self.to_tensor(batch['available_mask'])\n",
    "                outsample_mask = self.to_tensor(batch['sample_mask'])[:, -self.output_size:]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outsample_y, forecast = self.model(S=S, Y=Y, X=X)\n",
    "\n",
    "                training_loss = training_loss_fn(x=Y, freq=self.seasonality, forecast=forecast,\n",
    "                                                target=outsample_y, mask=outsample_mask)\n",
    "\n",
    "                if not np.isnan(float(training_loss)):\n",
    "                    training_loss.backward()\n",
    "                    t.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    early_stopping_counter = self.early_stopping\n",
    "\n",
    "                lr_scheduler.step()\n",
    "                if (iteration % eval_freq == 0):\n",
    "                    display_string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(iteration,\n",
    "                                                                                            time.time()-start,\n",
    "                                                                                            self.loss,\n",
    "                                                                                            training_loss.cpu().data.numpy())\n",
    "                    self.trajectories['iteration'].append(iteration)\n",
    "                    self.trajectories['train_loss'].append(training_loss.cpu().data.numpy())\n",
    "\n",
    "                    if val_ts_loader is not None:\n",
    "                        loss = self.evaluate_performance(ts_loader=val_ts_loader, \n",
    "                                                         validation_loss_fn=validation_loss_fn)\n",
    "                        display_string += \", Outsample {}: {:.5f}\".format(self.val_loss, loss)\n",
    "                        self.trajectories['val_loss'].append(loss)\n",
    "\n",
    "                        if self.early_stopping:\n",
    "                            if loss < best_val_loss:\n",
    "                                # Save current model if improves outsample loss\n",
    "                                best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "                                best_insample_loss = training_loss.cpu().data.numpy()\n",
    "                                early_stopping_counter = 0\n",
    "                                best_val_loss = loss\n",
    "                            else:\n",
    "                                early_stopping_counter += 1\n",
    "                            if early_stopping_counter >= self.early_stopping:\n",
    "                                break_flag = True\n",
    "                    \n",
    "                    print(display_string)\n",
    "\n",
    "                    self.model.train()\n",
    "\n",
    "                if break_flag:\n",
    "                    print('\\n')\n",
    "                    print(19*'-',' Stopped training by early stopping', 19*'-')\n",
    "                    self.model.load_state_dict(best_state_dict)\n",
    "                    break\n",
    "\n",
    "        #End of fitting\n",
    "        if n_iterations >0:\n",
    "            # This is batch loss!\n",
    "            self.final_insample_loss = training_loss.cpu().data.numpy() if not break_flag else best_insample_loss \n",
    "            string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(iteration,\n",
    "                                                                            time.time()-start,\n",
    "                                                                            self.loss,\n",
    "                                                                            self.final_insample_loss)\n",
    "            if val_ts_loader is not None:\n",
    "                self.final_outsample_loss = self.evaluate_performance(ts_loader=val_ts_loader, \n",
    "                                                                      validation_loss_fn=validation_loss_fn)\n",
    "                string += \", Outsample {}: {:.5f}\".format(self.val_loss, self.final_outsample_loss)\n",
    "            print(string)\n",
    "            print('='*30+'  End fitting  '+'='*30)\n",
    "            print('\\n')\n",
    "\n",
    "    def predict(self, ts_loader):\n",
    "        self.model.eval()\n",
    "        assert not ts_loader.shuffle, 'ts_loader must have shuffle as False.'\n",
    "\n",
    "        forecasts = []\n",
    "        outsample_ys = []\n",
    "        outsample_masks = []\n",
    "        with t.no_grad():\n",
    "            for batch in iter(ts_loader):\n",
    "                \n",
    "                # Parse batch\n",
    "                S     = self.to_tensor(batch['S'])\n",
    "                Y     = self.to_tensor(batch['Y'])\n",
    "                X     = self.to_tensor(batch['X'])\n",
    "                available_mask  = self.to_tensor(batch['available_mask'])\n",
    "                outsample_mask = batch['sample_mask'][:, -self.output_size:]\n",
    "\n",
    "                outsample_y, forecast = self.model(S=S, Y=Y, X=X)\n",
    "                outsample_ys.append(outsample_y.cpu().data.numpy())\n",
    "                forecasts.append(forecast.cpu().data.numpy())\n",
    "                outsample_masks.append(outsample_mask)\n",
    "\n",
    "        forecasts = np.vstack(forecasts)\n",
    "        outsample_ys = np.vstack(outsample_ys)\n",
    "        outsample_masks = np.vstack(outsample_masks)\n",
    "\n",
    "        n_series = ts_loader.ts_dataset.n_series\n",
    "        n_fcds = len(outsample_ys) // n_series\n",
    "        outsample_ys = outsample_ys.reshape(n_series, n_fcds, self.output_size)\n",
    "        forecasts = forecasts.reshape(n_series, n_fcds, self.output_size)\n",
    "        outsample_masks = outsample_masks.reshape(n_series, n_fcds, self.output_size)\n",
    "\n",
    "        self.model.train()\n",
    "        return outsample_ys, forecasts, outsample_masks\n",
    "\n",
    "\n",
    "    def save(self, model_dir, model_id, state_dict = None):\n",
    "    \n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        if state_dict is None:\n",
    "            state_dict = self.model.state_dict()\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        print('Saving model to:\\n {}'.format(model_file)+'\\n')\n",
    "        t.save({'model_state_dict': state_dict}, model_file)\n",
    "\n",
    "    def load(self, model_dir, model_id):\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        path = Path(model_file)\n",
    "\n",
    "        assert path.is_file(), 'No model_*.model file found in this path!'\n",
    "\n",
    "        print('Loading model from:\\n {}'.format(model_file)+'\\n')\n",
    "\n",
    "        checkpoint = t.load(model_file, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = {# Architecture parameters\n",
    "      'model': 'tcn',\n",
    "      'input_size_multiplier': 7,\n",
    "      'output_size': 24,\n",
    "      'kernel_size': 3,\n",
    "      'n_channels': [5, 5, 5, 5],\n",
    "      'initialization': 'lecun_normal',\n",
    "      'learning_rate': 0.01,\n",
    "      'lr_decay': 0.5,\n",
    "      'n_lr_decay_steps': 3,\n",
    "      'weight_decay': 0,\n",
    "      'dropout_prob': 0.2,\n",
    "      'n_iterations': 5,#_000,\n",
    "      'early_stopping': 10,\n",
    "      'loss': 'MAE',\n",
    "      'val_loss': 'MAE',\n",
    "      # Data parameters\n",
    "      'normalizer_y': None,\n",
    "      'normalizer_x': 'median',\n",
    "      'batch_size': 256,\n",
    "      'window_sampling_limit': 500_000,\n",
    "      'complete_inputs': False,\n",
    "      'frequency': 'H',\n",
    "      'idx_to_sample_freq': 24,\n",
    "      'random_seed': 1,\n",
    "      'seasonality': 24,\n",
    "      'random_seed': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TemporalConvNet' from 'nixtla.models.components.tcn' (/zfsauton2/home/kdgutier/anaconda3/envs/nixtla/lib/python3.7/site-packages/nixtla/models/components/tcn.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-af3fcb0391dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnixtla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEPF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPFInfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnixtla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsloader_general\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeSeriesLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnixtla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nixtla/lib/python3.7/site-packages/nixtla/experiments/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeSeriesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsloader_general\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeSeriesLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mesrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mesrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mESRNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mesrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmqesrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMQESRNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mesrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nixtla/lib/python3.7/site-packages/nixtla/models/esrnn/esrnn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStepLR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mesrnn_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_ESRNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m from ...losses.pytorch import (\n\u001b[1;32m     22\u001b[0m     \u001b[0mMAPELoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMASELoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSMAPELoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nixtla/lib/python3.7/site-packages/nixtla/models/esrnn/esrnn_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDRNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtcn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemporalConvNet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mTCN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TemporalConvNet' from 'nixtla.models.components.tcn' (/zfsauton2/home/kdgutier/anaconda3/envs/nixtla/lib/python3.7/site-packages/nixtla/models/components/tcn.py)"
     ]
    }
   ],
   "source": [
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "from nixtla.data.tsloader_general import TimeSeriesLoader\n",
    "from nixtla.experiments.utils import create_datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = ['NP']\n",
    "\n",
    "Y_df, X_df, S_df = EPF.load_groups(directory='../data', groups=dataset)\n",
    "\n",
    "plt.plot(Y_df.y.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ts_dataset, outsample_ts_dataset, scaler_y = create_datasets(mc, Y_df, X_df, S_df, 728*24, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ts_loader = TimeSeriesLoader(ts_dataset=train_ts_dataset,\n",
    "                                    model='tcn',\n",
    "                                    window_sampling_limit=int(mc['window_sampling_limit']),\n",
    "                                    input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                                    output_size=int(mc['output_size']),\n",
    "                                    idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                    batch_size=int(mc['batch_size']),\n",
    "                                    complete_inputs=mc['complete_inputs'],\n",
    "                                    complete_sample=False,\n",
    "                                    shuffle=True)\n",
    "\n",
    "val_ts_loader = TimeSeriesLoader(ts_dataset=outsample_ts_dataset,\n",
    "                                 model='tcn',\n",
    "                                 window_sampling_limit=int(mc['window_sampling_limit']),\n",
    "                                 input_size=int(mc['input_size_multiplier']*mc['output_size']),\n",
    "                                 output_size=int(mc['output_size']),\n",
    "                                 idx_to_sample_freq=24,\n",
    "                                 batch_size=int(mc['batch_size']),\n",
    "                                 complete_inputs=False,\n",
    "                                 complete_sample=False,\n",
    "                                 shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TCN(output_size=mc['output_size'],\n",
    "            n_channels=mc['n_channels'],\n",
    "            kernel_size=mc['kernel_size'],\n",
    "            initialization=mc['initialization'],\n",
    "            learning_rate=mc['learning_rate'],\n",
    "            lr_decay=mc['lr_decay'],\n",
    "            n_lr_decay_steps=mc['n_lr_decay_steps'],\n",
    "            weight_decay=mc['weight_decay'],\n",
    "            dropout_prob=mc['dropout_prob'],\n",
    "            n_iterations=mc['n_iterations'],\n",
    "            early_stopping=mc['early_stopping'],\n",
    "            loss=mc['loss'],\n",
    "            val_loss=mc['val_loss'],\n",
    "            frequency=mc['frequency'],\n",
    "            random_seed=mc['random_seed'],\n",
    "            seasonality=mc['seasonality'],\n",
    "            device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ts_loader=train_ts_loader, val_ts_loader=val_ts_loader, verbose=True, eval_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_split, y_hat_split, mask_split = model.predict(ts_loader=val_ts_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_true_split[0, 0:10, :].flatten(), label='true')\n",
    "plt.plot(y_hat_split[0, 0:10, :].flatten(), label='y_hat')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nixtla",
   "language": "python",
   "name": "nixtla"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
