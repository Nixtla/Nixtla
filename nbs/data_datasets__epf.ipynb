{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.datasets.epf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPF dataset\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from dataclasses import dataclass\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "from nixtla.data.datasets.utils import download_file, Info, TimeSeriesDataclass\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "SOURCE_URL = 'https://sandbox.zenodo.org/api/files/da5b2c6f-8418-4550-a7d0-7f2497b40f1b/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Electricity Price Forecasting meta information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class NP:\n",
    "    test_date: str = '2016-12-27'\n",
    "    name: str = 'NP'\n",
    "\n",
    "@dataclass\n",
    "class PJM:\n",
    "    test_date: str = '2016-12-27'\n",
    "    name: str = 'PJM'\n",
    "\n",
    "@dataclass\n",
    "class BE:\n",
    "    test_date: str = '2015-01-04'\n",
    "    name: str = 'BE'\n",
    "\n",
    "@dataclass\n",
    "class FR:\n",
    "    test_date: str = '2015-01-04'\n",
    "    name: str = 'FR'\n",
    "\n",
    "@dataclass\n",
    "class DE:\n",
    "    test_date: str = '2016-01-04'\n",
    "    name: str = 'DE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "EPFInfo = Info(groups=('NP', 'PJM', 'BE', 'FR', 'DE'),\n",
    "               class_groups=(NP, PJM, BE, FR, DE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EPF:\n",
    "\n",
    "    #@staticmethod\n",
    "    def load(directory: str,\n",
    "             group: str):\n",
    "             #is_training: bool,\n",
    "             #days_in_test: int,\n",
    "             #return_tensor: bool = True): # -> Union[TimeSeriesDataset, TimeSeriesDataclass]\n",
    "        \"\"\"\n",
    "        Downloads and loads EPF data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        directory: str\n",
    "            Directory where data will be downloaded.\n",
    "        group: str\n",
    "            Group name.\n",
    "            Allowed groups: 'NP', 'PJM', 'BE', 'FR', 'DE'.\n",
    "        training: bool\n",
    "            Wheter return training or testing data. Default True.\n",
    "        days_in_test: int\n",
    "            Number of days to consider in test.\n",
    "            Only used when training=True.\n",
    "        return_tensor: bool\n",
    "            Wheter return TimeSeriesDataset (tensors, True) or\n",
    "            TimeSeriesDataclass (dataframes)\n",
    "        \"\"\"\n",
    "        path = Path(directory) / 'epf' / 'datasets'\n",
    "\n",
    "        EPF.download(directory)\n",
    "\n",
    "        class_group = EPFInfo.get_group(group)\n",
    "\n",
    "        file = path / f'{group}.csv'\n",
    "\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        df.columns = ['ds', 'y'] + \\\n",
    "                     [f'Exogenous{i}' for i in range(1, len(df.columns) - 1)]\n",
    "\n",
    "        df['unique_id'] = group\n",
    "        df['ds'] = pd.to_datetime(df['ds'])\n",
    "        df['week_day'] = df['ds'].dt.dayofweek\n",
    "\n",
    "        dummies = pd.get_dummies(df['week_day'], prefix='day')\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "        dummies_cols = [col for col in df \\\n",
    "                        if (col.startswith('day') or col.startswith('hour_'))]\n",
    "\n",
    "        # last_date_test = pd.to_datetime(class_group.test_date) + \\\n",
    "        #                     timedelta(days=days_in_test)                        \n",
    "\n",
    "        # if is_training:\n",
    "        #     df = df.query('ds < @class_group.test_date')\n",
    "        # else:\n",
    "        #     last_date_test = pd.to_datetime(class_group.test_date) + \\\n",
    "        #                      timedelta(days=days_in_test)\n",
    "        #     df = df.query('ds >= @class_group.test_date')\n",
    "\n",
    "        Y = df.filter(items=['unique_id', 'ds', 'y'])\n",
    "        X = df.filter(items=['unique_id', 'ds', 'Exogenous1', 'Exogenous2', 'week_day'] + \\\n",
    "                      dummies_cols)\n",
    "        \n",
    "    #def get_data(self):\n",
    "        return Y, X\n",
    "        # if return_tensor:\n",
    "        #     return TimeSeriesDataset(y_df=Y, X_s_df=None, X_t_df=X, ts_train_mask=ts_train_mask)\n",
    "        # else:\n",
    "        #     return TimeSeriesDataclass(Y=Y, S=None, X=X, group=group)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_groups(directory: str,\n",
    "                    groups: List[str]): # = ['BE', 'FR'],\n",
    "                    #is_training: bool,\n",
    "                    #days_in_test: int,\n",
    "                    #return_tensor: bool = True): # -> Union[TimeSeriesDataset, TimeSeriesDataclass]:\n",
    "        \"\"\"\n",
    "        Downloads and loads panel of EPF data\n",
    "        according of groups.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        directory: str\n",
    "            Directory where data will be downloaded.\n",
    "        groups: List[str]\n",
    "            Group names.\n",
    "            Allowed groups: 'NP', 'PJM', 'BE', 'FR', 'DE'.\n",
    "        training: bool\n",
    "            Wheter return training or testing data. Default True.\n",
    "        days_in_test: int\n",
    "            Number of days to consider in test.\n",
    "            Only used when training=True.\n",
    "        return_tensor: bool\n",
    "            Wheter return TimeSeriesDataset (tensors, True) or\n",
    "            TimeSeriesDataclass (dataframes)\n",
    "        \"\"\"\n",
    "        Y = []\n",
    "        X = []\n",
    "        for group in groups:\n",
    "            Y_df, X_df = EPF.load(directory=directory, group=group)\n",
    "                                  #is_training=is_training, days_in_test=days_in_test,\n",
    "                                  #return_tensor=False)\n",
    "            Y.append(Y_df)\n",
    "            X.append(X_df)\n",
    "        Y = pd.concat(Y).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "        X = pd.concat(X).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "\n",
    "        S = Y[['unique_id']].drop_duplicates().reset_index(drop=True)\n",
    "        dummies = pd.get_dummies(S['unique_id'], prefix='static')\n",
    "        S = pd.concat([S, dummies], axis=1)\n",
    "        \n",
    "        # if return_tensor:\n",
    "        return Y, X, S\n",
    "        #     return TimeSeriesDataset(y_df=Y, X_s_df=None, X_t_df=X)\n",
    "        # else:\n",
    "        #     return TimeSeriesDataclass(Y=Y, S=S, X=X, group=groups)\n",
    "\n",
    "    @staticmethod\n",
    "    def download(directory: str) -> None:\n",
    "        \"\"\"Downloads EPF Dataset.\"\"\"\n",
    "        path = Path(directory) / 'epf' / 'datasets'\n",
    "        if not path.exists():\n",
    "            for group in EPFInfo.groups:\n",
    "                download_file(path, SOURCE_URL + f'{group}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load specific group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset: NP\nTrain mask percentage: 0.67\nX: time series features, of shape (#hours, #times,#features): \t(52416, 12)\nY: target series (in X), of shape (#hours, #times): \t \t(52416, 3)\nLast ds 2018-12-24 23:00:00\nTrain 17520.0 hours = 2.0 years\nValidation 34896.0 hours = 3.98 years\n\n\n"
     ]
    }
   ],
   "source": [
    "args = pd.Series({'dataset': 'NP'})\n",
    "\n",
    "Y_df, Xt_df = EPF.load(directory='data', group=args.dataset)\n",
    "\n",
    "# train_mask: 1 to keep, 0 to mask\n",
    "offset = 365 * 24 * 2\n",
    "train_outsample_mask = np.ones(len(Y_df))\n",
    "train_outsample_mask[-offset:] = 0\n",
    "\n",
    "print(f'Dataset: {args.dataset}')\n",
    "#print(\"Xt_df.columns\", Xt_df.columns)\n",
    "print(f'Train mask percentage: {np.round(np.sum(train_outsample_mask)/len(train_outsample_mask),2)}')\n",
    "print('X: time series features, of shape (#hours, #times,#features): \\t' + str(Xt_df.shape))\n",
    "print('Y: target series (in X), of shape (#hours, #times): \\t \\t' + str(Y_df.shape))\n",
    "print(f'Last ds {Y_df.ds.max()}')\n",
    "print(f'Train {sum(1-train_outsample_mask)} hours = {np.round(sum(1-train_outsample_mask)/(24*365),2)} years')\n",
    "print(f'Validation {sum(train_outsample_mask)} hours = {np.round(sum(train_outsample_mask)/(24*365),2)} years')\n",
    "# print('S: static features, of shape (#series,#features): \\t \\t' + str(S.shape))\n",
    "#Y_df.head()\n",
    "print('\\n')"
   ]
  },
  {
   "source": [
    "## Load all groups"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_df(Y_df, n_timestamps):\n",
    "    # Creates outsample_mask\n",
    "    # train 1 validation 0\n",
    "\n",
    "    last_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    last_df.sort_values(by=['unique_id', 'ds'], inplace=True, ascending=False)\n",
    "    last_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    last_df = last_df.groupby('unique_id').head(n_timestamps)\n",
    "    last_df['sample_mask'] = 0\n",
    "\n",
    "    last_df = last_df[['unique_id', 'ds', 'sample_mask']]\n",
    "\n",
    "    mask_df = Y_df.merge(last_df, on=['unique_id', 'ds'], how='left')\n",
    "    mask_df['sample_mask'] = mask_df['sample_mask'].fillna(1)    # The first len(Y)-n_hours used as train\n",
    "\n",
    "    mask_df = mask_df[['unique_id', 'ds', 'sample_mask']]\n",
    "    mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "    mask_df['available_mask'] = 1\n",
    "\n",
    "    assert len(mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "\n",
    "    return mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Validation splits\n                              ds                    \n                             min                 max\nunique_id sample_mask                               \nBE        0.0         2015-01-02 2016-12-31 23:00:00\n          1.0         2011-01-09 2015-01-01 23:00:00\nFR        0.0         2015-01-02 2016-12-31 23:00:00\n          1.0         2011-01-09 2015-01-01 23:00:00\nNP        0.0         2016-12-25 2018-12-24 23:00:00\n          1.0         2013-01-01 2016-12-24 23:00:00\nPJM       0.0         2016-12-25 2018-12-24 23:00:00\n          1.0         2013-01-01 2016-12-24 23:00:00\nTotal data \t\t\t209664 time stamps\nAvailable percentage=100.0, \t209664 time stamps\nTrain percentage=66.58, \t139584.0 time stamps\nOutsample percentage=33.42, \t70080.0 time stamps\n\n\n"
     ]
    }
   ],
   "source": [
    "val_ds = 2 * 365\n",
    "args = pd.Series({'dataset': ['NP', 'PJM', 'BE', 'FR']})\n",
    "\n",
    "Y_df, Xt_df, S_df = EPF.load_groups(directory='data', groups=args.dataset)\n",
    "\n",
    "# train_mask: 1 to keep, 0 to mask\n",
    "mask_df = get_mask_df(Y_df, n_timestamps=val_ds*24)\n",
    "train_outsample_mask = mask_df['sample_mask'].values\n",
    "\n",
    "print(\"Train Validation splits\")\n",
    "mask_df['train_mask'] = mask_df['available_mask'] * mask_df['sample_mask']\n",
    "n_tstamps = len(mask_df)\n",
    "n_avl = mask_df.available_mask.sum()        \n",
    "n_trn = mask_df.train_mask.sum()\n",
    "n_prd = len(mask_df)-mask_df.sample_mask.sum()\n",
    "\n",
    "avl_prc = np.round((100*n_avl)/n_tstamps,2)\n",
    "trn_prc = np.round((100*n_trn)/n_tstamps,2)\n",
    "prd_prc = np.round((100*n_prd)/n_tstamps,2)\n",
    "print(mask_df.groupby(['unique_id', 'sample_mask']).agg({'ds': ['min', 'max']}))\n",
    "print(f'Total data \\t\\t\\t{n_tstamps} time stamps')\n",
    "print(f'Available percentage={avl_prc}, \\t{n_avl} time stamps')\n",
    "print(f'Train percentage={trn_prc}, \\t{n_trn} time stamps')\n",
    "print(f'Outsample percentage={prd_prc}, \\t{n_prd} time stamps')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "nbeatsx",
   "display_name": "nbeatsx",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}