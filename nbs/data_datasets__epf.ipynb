{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.datasets.epf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPF dataset\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from dataclasses import dataclass\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "from nixtla.data.datasets.utils import download_file, Info, TimeSeriesDataclass\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "SOURCE_URL = 'https://sandbox.zenodo.org/api/files/da5b2c6f-8418-4550-a7d0-7f2497b40f1b/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tourism meta information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class NP:\n",
    "    test_date: str = '2016-12-27'\n",
    "    name: str = 'NP'\n",
    "\n",
    "@dataclass\n",
    "class PJM:\n",
    "    test_date: str = '2016-12-27'\n",
    "    name: str = 'PJM'\n",
    "\n",
    "@dataclass\n",
    "class BE:\n",
    "    test_date: str = '2015-01-04'\n",
    "    name: str = 'BE'\n",
    "\n",
    "@dataclass\n",
    "class FR:\n",
    "    test_date: str = '2015-01-04'\n",
    "    name: str = 'FR'\n",
    "\n",
    "@dataclass\n",
    "class DE:\n",
    "    test_date: str = '2016-01-04'\n",
    "    name: str = 'DE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "EPFInfo = Info(groups=('NP', 'PJM', 'BE', 'FR', 'DE'),\n",
    "               class_groups=(NP, PJM, BE, FR, DE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EPF:\n",
    "\n",
    "    #@staticmethod\n",
    "    def load(directory: str,\n",
    "             group: str,\n",
    "             training: bool = True,\n",
    "             days_in_test: int = 728,\n",
    "             return_tensor: bool = True): # -> Union[TimeSeriesDataset, TimeSeriesDataclass]\n",
    "        \"\"\"\n",
    "        Downloads and loads EPF data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        directory: str\n",
    "            Directory where data will be downloaded.\n",
    "        group: str\n",
    "            Group name.\n",
    "            Allowed groups: 'NP', 'PJM', 'BE', 'FR', 'DE'.\n",
    "        training: bool\n",
    "            Wheter return training or testing data. Default True.\n",
    "        days_in_test: int\n",
    "            Number of days to consider in test.\n",
    "            Only used when training=True.\n",
    "        return_tensor: bool\n",
    "            Wheter return TimeSeriesDataset (tensors, True) or\n",
    "            TimeSeriesDataclass (dataframes)\n",
    "        \"\"\"\n",
    "        path = Path(directory) / 'epf' / 'datasets'\n",
    "\n",
    "        EPF.download(directory)\n",
    "\n",
    "        class_group = EPFInfo.get_group(group)\n",
    "\n",
    "        file = path / f'{group}.csv'\n",
    "\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        df.columns = ['ds', 'y'] + \\\n",
    "                     [f'Exogenous{i}' for i in range(1, len(df.columns) - 1)]\n",
    "\n",
    "        df['unique_id'] = group\n",
    "        df['ds'] = pd.to_datetime(df['ds'])\n",
    "        df['week_day'] = df['ds'].dt.dayofweek\n",
    "\n",
    "        dummies = pd.get_dummies(df['week_day'], prefix='day')\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "        dummies_cols = [col for col in df \\\n",
    "                        if (col.startswith('day') or col.startswith('hour_'))]\n",
    "\n",
    "        if training:\n",
    "            df = df.query('ds < @class_group.test_date')\n",
    "        else:\n",
    "            last_date_test = pd.to_datetime(class_group.test_date) + \\\n",
    "                             timedelta(days=days_in_test)\n",
    "            df = df.query('ds >= @class_group.test_date')\n",
    "\n",
    "        Y = df.filter(items=['unique_id', 'ds', 'y'])\n",
    "        X = df.filter(items=['unique_id', 'ds', 'Exogenous1', 'Exogenous2', 'week_day'] + \\\n",
    "                      dummies_cols)\n",
    "        \n",
    "    #def get_data(self):\n",
    "        return Y, X\n",
    "        # if return_tensor:\n",
    "        #     return TimeSeriesDataset(y_df=Y, X_s_df=None, X_t_df=X, ts_train_mask=ts_train_mask)\n",
    "        # else:\n",
    "        #     return TimeSeriesDataclass(Y=Y, S=None, X=X, group=group)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_groups(directory: str,\n",
    "                    groups: List[str] = ['BE', 'FR'],\n",
    "                    training: bool = True,\n",
    "                    days_in_test: int = 728,\n",
    "                    return_tensor: bool = True) -> Union[TimeSeriesDataset, TimeSeriesDataclass]:\n",
    "        \"\"\"\n",
    "        Downloads and loads panel of EPF data\n",
    "        according of groups.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        directory: str\n",
    "            Directory where data will be downloaded.\n",
    "        groups: List[str]\n",
    "            Group names.\n",
    "            Allowed groups: 'NP', 'PJM', 'BE', 'FR', 'DE'.\n",
    "        training: bool\n",
    "            Wheter return training or testing data. Default True.\n",
    "        days_in_test: int\n",
    "            Number of days to consider in test.\n",
    "            Only used when training=True.\n",
    "        return_tensor: bool\n",
    "            Wheter return TimeSeriesDataset (tensors, True) or\n",
    "            TimeSeriesDataclass (dataframes)\n",
    "        \"\"\"\n",
    "        Y = []\n",
    "        X = []\n",
    "        for group in groups:\n",
    "            Y_df, X_df = EPF.load(directory, group,\n",
    "                                  training, days_in_test,\n",
    "                                  return_tensor=False)\n",
    "            Y.append(Y_df)\n",
    "            X.append(X_df)\n",
    "        Y = pd.concat(Y).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "        X = pd.concat(X).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "\n",
    "        S = Y[['unique_id']].drop_duplicates().reset_index(drop=True)\n",
    "        dummies = pd.get_dummies(S['unique_id'], prefix='static')\n",
    "        S = pd.concat([S, dummies], axis=1)\n",
    "        \n",
    "        # if return_tensor:\n",
    "        return Y, X, S\n",
    "        #     return TimeSeriesDataset(y_df=Y, X_s_df=None, X_t_df=X)\n",
    "        # else:\n",
    "        #     return TimeSeriesDataclass(Y=Y, S=S, X=X, group=groups)\n",
    "\n",
    "    @staticmethod\n",
    "    def download(directory: str) -> None:\n",
    "        \"\"\"Downloads EPF Dataset.\"\"\"\n",
    "        path = Path(directory) / 'epf' / 'datasets'\n",
    "        if not path.exists():\n",
    "            for group in EPFInfo.groups:\n",
    "                download_file(path, SOURCE_URL + f'{group}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load specific group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = pd.Series({'dataset': 'NP'})\n",
    "\n",
    "Y_df, Xt_df = EPF.load(directory='data', group=args.dataset)\n",
    "\n",
    "# train_mask: 1 to keep, 0 to mask\n",
    "offset = 365 * 24 * 2\n",
    "train_outsample_mask = np.ones(len(Y_df))\n",
    "train_outsample_mask[-offset:] = 0\n",
    "\n",
    "print(f'Dataset: {args.dataset}')\n",
    "#print(\"Xt_df.columns\", Xt_df.columns)\n",
    "print(f'Train mask percentage: {np.round(np.sum(train_outsample_mask)/len(train_outsample_mask),2)}')\n",
    "print('X: time series features, of shape (#hours, #times,#features): \\t' + str(Xt_df.shape))\n",
    "print('Y: target series (in X), of shape (#hours, #times): \\t \\t' + str(Y_df.shape))\n",
    "print(f'Last ds {Y_df.ds.max()}')\n",
    "print(f'Train {sum(1-train_outsample_mask)} hours = {np.round(sum(1-train_outsample_mask)/(24*365),2)} years')\n",
    "print(f'Validation {sum(train_outsample_mask)} hours = {np.round(sum(train_outsample_mask)/(24*365),2)} years')\n",
    "# print('S: static features, of shape (#series,#features): \\t \\t' + str(S.shape))\n",
    "#Y_df.head()\n",
    "print('\\n')"
   ]
  },
  {
   "source": [
    "## Load all groups"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_n_hours(Y_df, n_hours):\n",
    "    if 'last' in Y_df.columns:\n",
    "        del Y_df['last']\n",
    "\n",
    "    last_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    last_df.sort_values(by=['unique_id', 'ds'], inplace=True, ascending=False)\n",
    "    last_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    last_df = last_df.groupby('unique_id').head(n_hours)#.reset_index(drop=True)\n",
    "    last_df['last'] = 1\n",
    "\n",
    "    last_df = last_df[['unique_id', 'ds', 'last']]\n",
    "\n",
    "    Y_df = Y_df.merge(last_df, on=['unique_id', 'ds'], how='left')\n",
    "    Y_df['last'] = Y_df['last'].fillna(0)\n",
    "\n",
    "    Y_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "    \n",
    "    return Y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset: ['NP', 'PJM', 'BE', 'FR']\nX: time series features, of shape (#hours, #times,#features): \t(139776, 12)\nS: static features, of shape (#series,#features+unique_id): \t(4, 5)\nY: target series (in X), of shape (#hours, #times): \t \t(139776, 4)\n\n\nTrain Validation splits\nlast  unique_id\n0.0   BE          2013-01-03 23:00:00\n      FR          2013-01-03 23:00:00\n      NP          2014-12-27 23:00:00\n      PJM         2014-12-27 23:00:00\n1.0   BE          2015-01-03 23:00:00\n      FR          2015-01-03 23:00:00\n      NP          2016-12-26 23:00:00\n      PJM         2016-12-26 23:00:00\nName: ds, dtype: datetime64[ns]\nlast  unique_id\n0.0   BE          2011-01-09\n      FR          2011-01-09\n      NP          2013-01-01\n      PJM         2013-01-01\n1.0   BE          2013-01-04\n      FR          2013-01-04\n      NP          2014-12-28\n      PJM         2014-12-28\nName: ds, dtype: datetime64[ns]\nTrain insample percentage 0.5,         69696.0 hours = 7.96 years\nTrain outsample percentage 0.5,         70080.0 hours = 8.0 years\n\n\n"
     ]
    }
   ],
   "source": [
    "val_ds = 2 * 365\n",
    "args = pd.Series({'dataset': ['NP', 'PJM', 'BE', 'FR']})\n",
    "\n",
    "Y_df, Xt_df, S_df = EPF.load_groups(directory='data', groups=args.dataset)\n",
    "\n",
    "# train_mask: 1 to keep, 0 to mask\n",
    "# Y_df = get_last_n_days(Y_df, n_days=val_ds)\n",
    "Y_df = get_last_n_hours(Y_df, n_hours=val_ds*24)\n",
    "train_outsample_mask = Y_df['last'].values\n",
    "\n",
    "print(f'Dataset: {args.dataset}')\n",
    "#print(\"Xt_df.columns\", Xt_df.columns)\n",
    "print('X: time series features, of shape (#hours, #times,#features): \\t' + str(Xt_df.shape))\n",
    "print('S: static features, of shape (#series,#features+unique_id): \\t' + str(S_df.shape))\n",
    "print('Y: target series (in X), of shape (#hours, #times): \\t \\t' + str(Y_df.shape))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Train Validation splits\")\n",
    "print(last_df.groupby(['last', 'unique_id']).ds.max())\n",
    "print(last_df.groupby(['last', 'unique_id']).ds.min())\n",
    "print(f'Train insample percentage {np.round(sum(1-train_outsample_mask)/len(Y_df),2)}, \\\n",
    "        {sum(1-train_outsample_mask)} hours = {np.round(sum(1-train_outsample_mask)/(24*365),2)} years')\n",
    "print(f'Train outsample percentage {np.round(sum(train_outsample_mask)/len(Y_df),2)}, \\\n",
    "        {sum(train_outsample_mask)} hours = {np.round(sum(train_outsample_mask)/(24*365),2)} years')\n",
    "#Y_df.head()\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "nbeatsx",
   "display_name": "nbeatsx",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}