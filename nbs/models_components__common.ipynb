{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.components.common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-glucose",
   "metadata": {},
   "source": [
    "# Chomp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-exception",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Receives x input of dim nxcxt, and trims it so that only \n",
    "    'time available' information is used. Used for one dimensional \n",
    "    causal convolutions.\n",
    "    : param chomp_size: lenght of outsample values to skip.\n",
    "    \"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-swimming",
   "metadata": {},
   "source": [
    "# CausalConv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "ACTIVATIONS = ['ReLU',\n",
    "               'Softplus',\n",
    "               'Tanh',\n",
    "               'SELU',\n",
    "               'LeakyReLU',\n",
    "               'PReLU',\n",
    "               'Sigmoid']\n",
    "\n",
    "class CausalConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Receives x input of dim nxcxt, computes a unidimensional \n",
    "    causal convolution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels: int\n",
    "    out_channels: int\n",
    "    activation: str \n",
    "        https://discuss.pytorch.org/t/call-activation-function-from-string\n",
    "    padding: int\n",
    "    kernel_size: int\n",
    "    dilation: int\n",
    "    \n",
    "    Returns:\n",
    "    x: tesor\n",
    "        torch tensor of dim nxcxt \n",
    "        activation(conv1d(inputs, kernel) + bias)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride, padding, dilation, activation, with_weight_norm:bool):\n",
    "        super(CausalConv1d, self).__init__()\n",
    "        assert activation in ACTIVATIONS, f'{activation} is not in {ACTIVATIONS}'\n",
    "        \n",
    "        self.conv       = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, \n",
    "                                    kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                    dilation=dilation)\n",
    "        if with_weight_norm: self.conv = weight_norm(self.conv)\n",
    "        \n",
    "        self.chomp      = Chomp1d(padding)\n",
    "        self.activation = getattr(nn, activation)()\n",
    "        self.causalconv = nn.Sequential(self.conv, self.chomp, self.activation)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.causalconv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-sweden",
   "metadata": {},
   "source": [
    "# TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TimeDistributed(nn.Module):\n",
    "    \"\"\"\n",
    "    Receives x input of dim nxcxt, transforms it to txnxc\n",
    "    Collapses input of dim txnxc to (txn)xc and applies to a module.\n",
    "    Allows handling of variable sequence lengths and minibatch sizes.\n",
    "    : param module: Module to apply input to.\n",
    "    \"\"\"\n",
    "    def __init__(self, module):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, c, t = x.size(0), x.size(1), x.size(2)\n",
    "        x = x.permute(2, 0, 1).contiguous()\n",
    "        x = x.view(t * n, -1)\n",
    "        x = self.module(x)\n",
    "        x = x.view(t, n, -1)\n",
    "        x = x.permute(1, 2, 0).contiguous()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-appreciation",
   "metadata": {},
   "source": [
    "# FeaturewiseL1Regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-angle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturewiseL1Regularizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer meant to apply elementwise L1 regularization to a dimension.\n",
    "    Receives x input of dim (nxc) and returns the input (nxc).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, l1_coeff=1e-3):\n",
    "        super(FeaturewiseL1Regularizer).__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(t.Tensor(in_features, 1), requires_grad=True)\n",
    "        #nn.init.kaiming_uniform_(self.weight, a=math.sqrt(0.5))\n",
    "        nn.init.uniform_(self.weight, a=0., b=1.)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # xt_input.size() == [batch_size, in_features]\n",
    "        return x * self.weight # broadcasts batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-syracuse",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nixtla] *",
   "language": "python",
   "name": "conda-env-nixtla-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
