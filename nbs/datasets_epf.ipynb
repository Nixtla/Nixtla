{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.datasets.epf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPF dataset\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from dataclasses import dataclass\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "from nixtla.data.datasets.utils import download_file, Info, TimeSeriesDataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "SOURCE_URL = 'https://sandbox.zenodo.org/api/files/da5b2c6f-8418-4550-a7d0-7f2497b40f1b/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tourism meta information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class NP:\n",
    "    test_date: str = '2016-12-27'\n",
    "    name: str = 'NP'\n",
    "        \n",
    "@dataclass\n",
    "class PJM:\n",
    "    test_date: str = '2016-12-27'\n",
    "    name: str = 'PJM'\n",
    "        \n",
    "@dataclass\n",
    "class BE:\n",
    "    test_date: str = '2015-01-04'\n",
    "    name: str = 'BE'\n",
    "        \n",
    "@dataclass\n",
    "class FR:\n",
    "    test_date: str = '2015-01-04'\n",
    "    name: str = 'FR'\n",
    "        \n",
    "@dataclass\n",
    "class DE:\n",
    "    test_date: str = '2016-01-04'\n",
    "    name: str = 'DE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "EPFInfo = Info(groups=('NP', 'PJM', 'BE', 'FR', 'DE'),\n",
    "               class_groups=(NP, PJM, BE, FR, DE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EPF(TimeSeriesDataclass):\n",
    "\n",
    "    @staticmethod\n",
    "    def load(directory: str,\n",
    "             group: str,\n",
    "             training: bool = True,\n",
    "             days_in_test: int = 728) -> 'EPF':\n",
    "        \"\"\"\n",
    "        Downloads and loads EPF data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        directory: str\n",
    "            Directory where data will be downloaded.\n",
    "        group: str\n",
    "            Group name.\n",
    "            Allowed groups: 'NP', 'PJM', 'BE', 'FR', 'DE'.\n",
    "        training: bool\n",
    "            Wheter return training or testing data. Default True.\n",
    "        days_in_test: int\n",
    "            Number of days to consider in test.\n",
    "            Only used when training=True.\n",
    "        \"\"\"\n",
    "        path = Path(directory) / 'epf' / 'datasets'\n",
    "        \n",
    "        EPF.download(directory)\n",
    "\n",
    "        class_group = EPFInfo.get_group(group)\n",
    "\n",
    "        file = path / f'{group}.csv'\n",
    "\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        df.columns = ['ds', 'y'] + \\\n",
    "                     [f'Exogenous{i}' for i in range(1, len(df.columns) - 1)]\n",
    "        \n",
    "        df['unique_id'] = group\n",
    "        df['ds'] = pd.to_datetime(df['ds'])\n",
    "        df['week_day'] = df['ds'].dt.dayofweek\n",
    "        \n",
    "        dummies = pd.get_dummies(df['week_day'], prefix='day')\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        \n",
    "        dummies_cols = [col for col in df \\\n",
    "                        if (col.startswith('day') or col.startswith('hour_'))]\n",
    "        \n",
    "        if training:\n",
    "            df = df.query('ds < @class_group.test_date')\n",
    "        else:\n",
    "            last_date_test = pd.to_datetime(class_group.test_date) + \\\n",
    "                             timedelta(days=days_in_test)\n",
    "            df = df.query('ds >= @class_group.test_date')\n",
    "            \n",
    "        Y = df.filter(items=['unique_id', 'ds', 'y'])\n",
    "        X = df.filter(items=['unique_id', 'ds', 'Exogenous1', 'Exogenous2'] + \\\n",
    "                      dummies_cols)\n",
    "\n",
    "        return EPF(Y=Y, S=None, X=X, group=group)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_groups(directory: str,\n",
    "                    groups: List[str] = ['BE', 'FR'],\n",
    "                    training: bool = True,\n",
    "                    days_in_test: int = 728) -> 'EPF':\n",
    "        \"\"\"\n",
    "        Downloads and loads panel of EPF data\n",
    "        according of groups.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        directory: str\n",
    "            Directory where data will be downloaded.\n",
    "        groups: List[str]\n",
    "            Group names.\n",
    "            Allowed groups: 'NP', 'PJM', 'BE', 'FR', 'DE'.\n",
    "        training: bool\n",
    "            Wheter return training or testing data. Default True.\n",
    "        days_in_test: int\n",
    "            Number of days to consider in test.\n",
    "            Only used when training=True.\n",
    "        \"\"\"\n",
    "        Y = []\n",
    "        X = []\n",
    "        for group in groups:\n",
    "            data = EPF.load(directory, group, \n",
    "                            training, days_in_test)\n",
    "            Y.append(data.Y)\n",
    "            X.append(data.X)\n",
    "        Y = pd.concat(Y).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "        X = pd.concat(X).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "        \n",
    "        S = Y[['unique_id']].drop_duplicates().reset_index(drop=True)\n",
    "        dummies = pd.get_dummies(S['unique_id'], prefix='static')\n",
    "        S = pd.concat([S, dummies], axis=1)\n",
    "        \n",
    "        return EPF(Y=Y, X=X, S=S, group=groups)\n",
    "\n",
    "    @staticmethod\n",
    "    def download(directory: str) -> None:\n",
    "        \"\"\"Downloads EPF Dataset.\"\"\"\n",
    "        path = Path(directory) / 'epf' / 'datasets'\n",
    "        if not path.exists():\n",
    "            for group in EPFInfo.groups:\n",
    "                download_file(path, SOURCE_URL + f'{group}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar un grupo específico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pjm = EPF.load('data', group='PJM')\n",
    "pjm_test = EPF.load('data', group='PJM', training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y train: \n",
      "   unique_id                  ds          y\n",
      "0       PJM 2013-01-01 00:00:00  25.464211\n",
      "1       PJM 2013-01-01 01:00:00  23.554578\n",
      "\n",
      "X train: \n",
      "   unique_id                  ds  Exogenous1  Exogenous2  day_0  day_1  day_2  \\\n",
      "0       PJM 2013-01-01 00:00:00     85049.0     11509.0      0      1      0   \n",
      "1       PJM 2013-01-01 01:00:00     82128.0     10942.0      0      1      0   \n",
      "\n",
      "   day_3  day_4  day_5  day_6  \n",
      "0      0      0      0      0  \n",
      "1      0      0      0      0  \n"
     ]
    }
   ],
   "source": [
    "print('Y train: \\n', pjm.Y.head(2)) \n",
    "print('\\nX train: \\n', pjm.X.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y test: \n",
      "       unique_id                  ds          y\n",
      "34944       PJM 2016-12-27 00:00:00  19.113045\n",
      "34945       PJM 2016-12-27 01:00:00  18.042768\n",
      "\n",
      "X test: \n",
      "       unique_id                  ds  Exogenous1  Exogenous2  day_0  day_1  \\\n",
      "34944       PJM 2016-12-27 00:00:00     74616.0     10214.0      0      1   \n",
      "34945       PJM 2016-12-27 01:00:00     71821.0      9702.0      0      1   \n",
      "\n",
      "       day_2  day_3  day_4  day_5  day_6  \n",
      "34944      0      0      0      0      0  \n",
      "34945      0      0      0      0      0  \n"
     ]
    }
   ],
   "source": [
    "print('Y test: \\n', pjm_test.Y.head(2)) \n",
    "print('\\nX test: \\n', pjm_test.X.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar más de un grupo a la vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['PJM', 'NP', 'FR', 'BE', 'DE']\n",
    "pjm = EPF.load_groups('data', groups=groups)\n",
    "pjm_test = EPF.load_groups('data', groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y train: \n",
      "   unique_id                  ds      y\n",
      "0        BE 2011-01-09 00:00:00  32.54\n",
      "1        BE 2011-01-09 01:00:00  21.55\n",
      "\n",
      "X train: \n",
      "   unique_id                  ds  Exogenous1  Exogenous2  day_0  day_1  day_2  \\\n",
      "0        BE 2011-01-09 00:00:00     63065.0     63000.0      0      0      0   \n",
      "1        BE 2011-01-09 01:00:00     62715.0     58800.0      0      0      0   \n",
      "\n",
      "   day_3  day_4  day_5  day_6  \n",
      "0      0      0      0      1  \n",
      "1      0      0      0      1  \n",
      "\n",
      "S train: \n",
      "   unique_id  static_BE  static_DE  static_FR  static_NP  static_PJM\n",
      "0        BE          1          0          0          0           0\n",
      "1        DE          0          1          0          0           0\n"
     ]
    }
   ],
   "source": [
    "print('Y train: \\n', pjm.Y.head(2)) \n",
    "print('\\nX train: \\n', pjm.X.head(2))\n",
    "print('\\nS train: \\n', pjm.S.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y test: \n",
      "   unique_id                  ds      y\n",
      "0        BE 2011-01-09 00:00:00  32.54\n",
      "1        BE 2011-01-09 01:00:00  21.55\n",
      "\n",
      "X test: \n",
      "   unique_id                  ds  Exogenous1  Exogenous2  day_0  day_1  day_2  \\\n",
      "0        BE 2011-01-09 00:00:00     63065.0     63000.0      0      0      0   \n",
      "1        BE 2011-01-09 01:00:00     62715.0     58800.0      0      0      0   \n",
      "\n",
      "   day_3  day_4  day_5  day_6  \n",
      "0      0      0      0      1  \n",
      "1      0      0      0      1  \n",
      "\n",
      "S test: \n",
      "   unique_id  static_BE  static_DE  static_FR  static_NP  static_PJM\n",
      "0        BE          1          0          0          0           0\n",
      "1        DE          0          1          0          0           0\n"
     ]
    }
   ],
   "source": [
    "print('Y test: \\n', pjm_test.Y.head(2)) \n",
    "print('\\nX test: \\n', pjm_test.X.head(2))\n",
    "print('\\nS test: \\n', pjm_test.S.head(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
