{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.esrnn.utils.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESRNN utils losses\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class PinballLoss(nn.Module):\n",
    "    \"\"\" Pinball Loss\n",
    "    Computes the pinball loss between y and y_hat.\n",
    "\n",
    "    Parameters\n",
    "    ----------  \n",
    "    y: tensor\n",
    "        actual values in torch tensor.\n",
    "    y_hat: tensor (same shape as y)\n",
    "        predicted values in torch tensor.\n",
    "    tau: float, between 0 and 1\n",
    "        the slope of the pinball loss, in the context of \n",
    "        quantile regression, the value of tau determines the\n",
    "        conditional quantile level.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    pinball_loss:\n",
    "        average accuracy for the predicted quantile\n",
    "    \"\"\"\n",
    "    def __init__(self, tau=0.5):\n",
    "        super(PinballLoss, self).__init__()\n",
    "        self.tau = tau\n",
    "    \n",
    "    def forward(self, y, y_hat):\n",
    "        delta_y = torch.sub(y, y_hat)\n",
    "        pinball = torch.max(torch.mul(self.tau, delta_y), torch.mul((self.tau-1), delta_y))\n",
    "        pinball = pinball.mean()\n",
    "        return pinball\n",
    "\n",
    "class LevelVariabilityLoss(nn.Module):\n",
    "    \"\"\" Level Variability Loss\n",
    "    Computes the variability penalty for the level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    levels: tensor with shape (batch, n_time)\n",
    "        levels obtained from exponential smoothing component of ESRNN\n",
    "    level_variability_penalty: float\n",
    "        this parameter controls the strength of the penalization \n",
    "        to the wigglines of the level vector, induces smoothness\n",
    "        in the output\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    level_var_loss:\n",
    "        wiggliness loss for the level vector\n",
    "    \"\"\"\n",
    "    def __init__(self, level_variability_penalty):\n",
    "        super(LevelVariabilityLoss, self).__init__()\n",
    "        self.level_variability_penalty = level_variability_penalty\n",
    "\n",
    "    def forward(self, levels):\n",
    "        assert levels.shape[1] > 2\n",
    "        level_prev = torch.log(levels[:, :-1])\n",
    "        level_next = torch.log(levels[:, 1:])\n",
    "        log_diff_of_levels = torch.sub(level_prev, level_next)\n",
    "\n",
    "        log_diff_prev = log_diff_of_levels[:, :-1]\n",
    "        log_diff_next = log_diff_of_levels[:, 1:]\n",
    "        diff = torch.sub(log_diff_prev, log_diff_next)\n",
    "        level_var_loss = diff**2\n",
    "        level_var_loss = level_var_loss.mean() * self.level_variability_penalty\n",
    "        return level_var_loss\n",
    "\n",
    "class StateLoss(nn.Module):\n",
    "    pass\n",
    "\n",
    "class SmylLoss(nn.Module):\n",
    "    \"\"\"Computes the Smyl Loss that combines level variability with\n",
    "    with Pinball loss.\n",
    "    windows_y: tensor of actual values,\n",
    "                            shape (n_windows, batch_size, window_size).\n",
    "    windows_y_hat: tensor of predicted values,\n",
    "                                    shape (n_windows, batch_size, window_size).\n",
    "    levels: levels obtained from exponential smoothing component of ESRNN.\n",
    "                    tensor with shape (batch, n_time).\n",
    "    return: smyl_loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, tau, level_variability_penalty=0.0):\n",
    "        super(SmylLoss, self).__init__()\n",
    "        self.pinball_loss = PinballLoss(tau)\n",
    "        self.level_variability_loss = LevelVariabilityLoss(level_variability_penalty)\n",
    "\n",
    "    def forward(self, windows_y, windows_y_hat, levels):\n",
    "        smyl_loss = self.pinball_loss(windows_y, windows_y_hat)\n",
    "        if self.level_variability_loss.level_variability_penalty>0:\n",
    "            log_diff_of_levels = self.level_variability_loss(levels) \n",
    "            smyl_loss += log_diff_of_levels\n",
    "        return smyl_loss\n",
    "\n",
    "\n",
    "class DisaggregatedPinballLoss(nn.Module):\n",
    "    \"\"\" Pinball Loss\n",
    "    Computes the pinball loss between y and y_hat.\n",
    "\n",
    "    Parameters\n",
    "    ----------  \n",
    "    y: tensor\n",
    "        actual values in torch tensor.\n",
    "    y_hat: tensor (same shape as y)\n",
    "        predicted values in torch tensor.\n",
    "    tau: float, between 0 and 1\n",
    "        the slope of the pinball loss, in the context of \n",
    "        quantile regression, the value of tau determines the\n",
    "        conditional quantile level.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    pinball_loss:\n",
    "        average accuracy for the predicted quantile\n",
    "    \"\"\"\n",
    "    def __init__(self, tau=0.5):\n",
    "        super(DisaggregatedPinballLoss, self).__init__()\n",
    "        self.tau = tau\n",
    "    \n",
    "    def forward(self, y, y_hat):\n",
    "        delta_y = torch.sub(y, y_hat)\n",
    "        pinball = torch.max(torch.mul(self.tau, delta_y), torch.mul((self.tau-1), delta_y))\n",
    "        pinball = pinball.mean(axis=0).mean(axis=1)\n",
    "        return pinball\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nixtla",
   "language": "python",
   "name": "nixtla"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
