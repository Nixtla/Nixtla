{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c68c0f618c4f787b1e02bfee278e48d25b62407cb335aab5258cedc4db4ced"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.onnbeats.nbeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "import torch as t\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "from nixtla.models.onnbeats.nbeats_model import NBeats, NBeatsBlock, IdentityBasis\n",
    "from nixtla.models.onnbeats.nbeats_model import TrendBasis, SeasonalityBasis, ExogenousFutureBasis, ExogenousBasisInterpretable\n",
    "from nixtla.losses.pytorch import MAPELoss, MASELoss, SMAPELoss, MSELoss, MAELoss, RMSELoss\n",
    "from nixtla.losses.numpy import mae, mse, mape, smape, rmse#, accuracy\n",
    "\n",
    "def accuracy_logits(y: np.ndarray, y_hat: np.ndarray, weights=None, thr=0.5) -> np.ndarray:\n",
    "    \"\"\"Calculates the Accuracy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: numpy array\n",
    "      actual test values\n",
    "    y_hat: numpy array of len h (forecasting horizon)\n",
    "      predicted values\n",
    "    Return\n",
    "    ------\n",
    "    return accuracy    \n",
    "    \"\"\"\n",
    "    #print(\"y\", y)\n",
    "    #print(\"y_hat\", y_hat)\n",
    "    #print(\"y_hat_logits\", 1/(1 + np.exp(-y_hat)))\n",
    "    y_hat = ((1/(1 + np.exp(-y_hat))) > thr) * 1\n",
    "    #print(\"y.shape\", y.shape)\n",
    "    #print(\"y_hat.shape\", y_hat.shape)\n",
    "\n",
    "    #print(\"np.max(y)\", np.max(y))\n",
    "    #print(\"np.max(y_hat)\", np.max(y_hat))\n",
    "\n",
    "    #print(\"y\", y)\n",
    "    #print(\"y_hat\", y_hat)\n",
    "\n",
    "    #assert 1<0\n",
    "    \n",
    "    accuracy = np.average(y_hat==y, weights=weights) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Nbeats(object):\n",
    "    \"\"\"\n",
    "    Future documentation\n",
    "    \"\"\"\n",
    "    SEASONALITY_BLOCK = 'seasonality'\n",
    "    TREND_BLOCK = 'trend'\n",
    "    IDENTITY_BLOCK = 'identity'\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size_multiplier,\n",
    "                 output_size,\n",
    "                 shared_weights,\n",
    "                 stack_types,\n",
    "                 n_blocks,\n",
    "                 n_layers,\n",
    "                 n_hidden,\n",
    "                 n_harmonics,\n",
    "                 n_polynomials,\n",
    "                 exogenous_n_channels,\n",
    "                 f_cols,\n",
    "                 theta_with_exogenous,\n",
    "                 batch_normalization,\n",
    "                 dropout_prob,\n",
    "                 x_s_n_hidden,\n",
    "                 learning_rate,\n",
    "                 lr_decay,\n",
    "                 n_lr_decay_steps,\n",
    "                 l1_lambda_x,\n",
    "                 weight_decay,\n",
    "                 n_iterations,\n",
    "                 early_stopping,\n",
    "                 loss,\n",
    "                 val_loss,\n",
    "                 frequency,\n",
    "                 seasonality,\n",
    "                 random_seed,\n",
    "                 device=None):\n",
    "        super(Nbeats, self).__init__()\n",
    "\n",
    "        self.input_size = int(input_size_multiplier*output_size)\n",
    "        self.output_size = output_size\n",
    "        self.shared_weights = shared_weights\n",
    "        self.stack_types = stack_types\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_harmonics = n_harmonics\n",
    "        self.n_polynomials = n_polynomials\n",
    "        self.exogenous_n_channels = exogenous_n_channels\n",
    "        self.f_cols = f_cols\n",
    "        self.theta_with_exogenous = theta_with_exogenous\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.x_s_n_hidden = x_s_n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.n_lr_decay_steps = n_lr_decay_steps\n",
    "        self.l1_lambda_x = l1_lambda_x\n",
    "        self.weight_decay = weight_decay\n",
    "        self.n_iterations = n_iterations\n",
    "        self.early_stopping = early_stopping\n",
    "        self.loss = loss\n",
    "        self.val_loss = val_loss\n",
    "        self.frequency = frequency\n",
    "        self.seasonality = seasonality\n",
    "        self.random_seed = random_seed\n",
    "        if device is None:\n",
    "            device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
    "        self.device = device\n",
    "\n",
    "        self._is_instantiated = False\n",
    "\n",
    "    def create_stack(self):\n",
    "        # Declare parameter dimensions\n",
    "        if self.theta_with_exogenous:\n",
    "            x_t_n_inputs = self.input_size + self.n_x_t*self.output_size\n",
    "        else:\n",
    "            x_t_n_inputs = self.input_size # y_lags\n",
    "        \n",
    "        #------------------------ Model Definition ------------------------#\n",
    "        block_list = []\n",
    "        self.blocks_regularizer = []\n",
    "        for i in range(len(self.stack_types)):\n",
    "            #print(f'| --  Stack {self.stack_types[i]} (#{i})')\n",
    "            for block_id in range(self.n_blocks[i]):\n",
    "                # Batch norm only on first block\n",
    "                if (len(block_list)==0) and (self.batch_normalization):\n",
    "                    batch_normalization_block = True\n",
    "                else:\n",
    "                    batch_normalization_block = False\n",
    "\n",
    "                # Dummy of regularizer in block. Override with 1 if exogenous_block\n",
    "                #self.blocks_regularizer += [0]\n",
    "\n",
    "                # Shared weights\n",
    "                if self.shared_weights and block_id>0:\n",
    "                    nbeats_block = block_list[-1]\n",
    "\n",
    "                else:\n",
    "                    if self.stack_types[i] == 'seasonality':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=4 * int(\n",
    "                                                        np.ceil(self.n_harmonics / 2 * self.output_size) - (self.n_harmonics - 1)),\n",
    "                                                   basis=SeasonalityBasis(harmonics=self.n_harmonics,\n",
    "                                                                                backcast_size=self.input_size,\n",
    "                                                                                forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   theta_with_exogenous=self.theta_with_exogenous,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob)\n",
    "                    elif self.stack_types[i] == 'trend':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2 * (self.n_polynomials + 1),\n",
    "                                                   basis=TrendBasis(degree_of_polynomial=self.n_polynomials,\n",
    "                                                                            backcast_size=self.input_size,\n",
    "                                                                            forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   theta_with_exogenous=self.theta_with_exogenous,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob)\n",
    "                    elif self.stack_types[i] == 'identity':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=self.input_size + self.output_size,\n",
    "                                                   basis=IdentityBasis(backcast_size=self.input_size,\n",
    "                                                                       forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   theta_with_exogenous=self.theta_with_exogenous,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob)\n",
    "                    elif self.stack_types[i] == 'exogenous':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2*self.n_x_t,\n",
    "                                                   basis=ExogenousBasisInterpretable(),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   theta_with_exogenous=self.theta_with_exogenous,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob)\n",
    "                    elif self.stack_types[i] == 'exogenous_g_a':\n",
    "                        assert len(self.f_cols)>0, 'If ExogenousFutureBasis, provide x_f_cols hyperparameter'\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2*(self.exogenous_n_channels),\n",
    "                                                   basis=ExogenousFutureBasis(out_features=self.exogenous_n_channels,\n",
    "                                                                              f_idxs=self.f_idxs),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   theta_with_exogenous=self.theta_with_exogenous,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob)\n",
    "                #        self.blocks_regularizer[-1] = 1\n",
    "                #print(f'     | -- {nbeats_block}')\n",
    "                block_list.append(nbeats_block)\n",
    "        return block_list\n",
    "\n",
    "    def __loss_fn(self, loss_name: str):\n",
    "        def loss(x, freq, forecast, target, mask):\n",
    "            if loss_name == 'MAPE':\n",
    "                return MAPELoss(y=target, y_hat=forecast, mask=mask) + self.l1_regularization()\n",
    "            elif loss_name == 'MASE':\n",
    "                return MASELoss(y=target, y_hat=forecast, y_insample=x, seasonality=freq, mask=mask) + self.l1_regularization()\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return SMAPELoss(y=target, y_hat=forecast, mask=mask) + self.l1_regularization()\n",
    "            elif loss_name == 'MSE':\n",
    "                return MSELoss(y=target, y_hat=forecast, mask=mask) + self.l1_regularization()\n",
    "            elif loss_name == 'RMSE':\n",
    "                return RMSELoss(y=target, y_hat=forecast, mask=mask) + self.l1_regularization()\n",
    "            elif loss_name == 'MAE':\n",
    "                return MAELoss(y=target, y_hat=forecast, mask=mask) + self.l1_regularization()\n",
    "            elif loss_name == 'BCE':\n",
    "                return F.binary_cross_entropy_with_logits(input=forecast, target=target) + \\\n",
    "                       self.loss_l1_conv_layers() + self.loss_l1_theta()                \n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def __val_loss_fn(self, loss_name: str):\n",
    "        #TODO: mase not implemented\n",
    "        def loss(forecast, target, weights):\n",
    "            if loss_name == 'MAPE':\n",
    "                return mape(y=target, y_hat=forecast, weights=weights) #TODO: faltan weights\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return smape(y=target, y_hat=forecast, weights=weights) #TODO: faltan weights\n",
    "            elif loss_name == 'MSE':\n",
    "                return mse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'RMSE':\n",
    "                return rmse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MAE':\n",
    "                return mae(y=target, y_hat=forecast, weights=weights)\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def l1_regularization(self):\n",
    "        # l1_loss = 0\n",
    "        # for i, indicator in enumerate(self.blocks_regularizer):\n",
    "        #     if indicator:\n",
    "        #         l1_loss +=  self.l1_lambda*t.sum(t.abs(self.model.blocks[i].basis.weight))\n",
    "        # return l1_loss\n",
    "        return self.l1_lambda_x * t.sum(t.abs(self.model.l1_weight))\n",
    "\n",
    "    def to_tensor(self, x: np.ndarray) -> t.Tensor:\n",
    "        tensor = t.as_tensor(x, dtype=t.float32).to(self.device)\n",
    "        return tensor\n",
    "\n",
    "    def evaluate_performance(self, ts_loader, validation_loss_fn):\n",
    "        #TODO: mas opciones que mae\n",
    "        self.model.eval()\n",
    "\n",
    "        losses = []\n",
    "        with t.no_grad():\n",
    "            for batch in iter(ts_loader):\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                forecast = self.model(insample_y=insample_y, insample_x_t=insample_x,\n",
    "                                    insample_mask=insample_mask, outsample_x_t=outsample_x, x_s=s_matrix)\n",
    "                batch_loss = validation_loss_fn(target=outsample_y.cpu().data.numpy(),\n",
    "                                                forecast=forecast.cpu().data.numpy(),\n",
    "                                                weights=outsample_mask.cpu().data.numpy())\n",
    "                losses.append(batch_loss)\n",
    "        loss = np.mean(losses)\n",
    "        self.model.train()\n",
    "        return loss\n",
    "\n",
    "    def fit(self, train_ts_loader, val_ts_loader=None, n_iterations=None, verbose=True, eval_steps=1):\n",
    "        # Asserts\n",
    "        assert train_ts_loader.t_cols[0] == 'y', f'First variable must be y not {train_ts_loader.t_cols[0]}'\n",
    "        assert train_ts_loader.t_cols[1] == 'ejecutado', f'First exogenous variable must be ejecutado not {train_ts_loader.t_cols[1]}'\n",
    "        assert (self.input_size)==train_ts_loader.input_size, \\\n",
    "            f'model input_size {self.input_size} data input_size {train_ts_loader.input_size}'        \n",
    "\n",
    "        # Random Seeds (model initialization)\n",
    "        t.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed) #TODO: interaccion rara con window_sampling de validacion\n",
    "\n",
    "        # Attributes of ts_dataset\n",
    "        self.n_x_t, self.n_x_s = train_ts_loader.get_n_variables()\n",
    "        self.t_cols = train_ts_loader.t_cols\n",
    "        self.f_idxs = train_ts_loader.ts_dataset.get_f_idxs(self.f_cols)\n",
    "\n",
    "        # Instantiate model\n",
    "        if not self._is_instantiated:\n",
    "            block_list = self.create_stack()\n",
    "            self.model = NBeats(blocks=t.nn.ModuleList(block_list), in_features=self.n_x_t).to(self.device)\n",
    "            self._is_instantiated = True\n",
    "\n",
    "        # Overwrite n_iterations and train datasets\n",
    "        if n_iterations is None:\n",
    "            n_iterations = self.n_iterations\n",
    "\n",
    "        lr_decay_steps = n_iterations // self.n_lr_decay_steps\n",
    "        if lr_decay_steps == 0:\n",
    "            lr_decay_steps = 1\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_steps, gamma=self.lr_decay)\n",
    "\n",
    "        training_loss_fn = self.__loss_fn(self.loss)\n",
    "        validation_loss_fn = self.__val_loss_fn(self.loss) #Uses numpy losses\n",
    "\n",
    "        if verbose and (n_iterations > 0):\n",
    "            print('='*30+' Start fitting '+'='*30)\n",
    "            print(f'Number of exogenous variables: {self.n_x_t}')\n",
    "            print(f'Number of static variables: {self.n_x_s} , with dim_hidden: {self.x_s_n_hidden}')\n",
    "            print(f'Number of iterations: {n_iterations}')\n",
    "            print(f'Number of blocks: {len(self.model.blocks)}')\n",
    "\n",
    "        #self.loss_dict = {} # Restart self.loss_dict\n",
    "        start = time.time()\n",
    "        self.trajectories = {'iteration':[],'train_loss':[], 'val_loss':[]}\n",
    "        self.final_insample_loss = None\n",
    "        self.final_outsample_loss = None\n",
    "\n",
    "        # Training Loop\n",
    "        best_val_loss = np.inf\n",
    "        early_stopping_counter = 0\n",
    "        best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "        break_flag = False\n",
    "        iteration = 0\n",
    "        epoch = 0\n",
    "        while (iteration < n_iterations) and (not break_flag):\n",
    "            epoch +=1\n",
    "            for batch in iter(train_ts_loader):\n",
    "                iteration += 1\n",
    "                if (iteration > n_iterations) or (break_flag):\n",
    "                    continue\n",
    "                self.model.train()\n",
    "\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                forecast = self.model(insample_y=insample_y, insample_x_t=insample_x,\n",
    "                                    insample_mask=insample_mask, outsample_x_t=outsample_x, x_s=s_matrix)\n",
    "\n",
    "                training_loss = training_loss_fn(x=insample_y, freq=self.seasonality, forecast=forecast,\n",
    "                                                target=outsample_y, mask=outsample_mask)\n",
    "\n",
    "                if np.isnan(float(training_loss)):\n",
    "                    break\n",
    "\n",
    "                training_loss.backward()\n",
    "                t.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "                if (iteration % eval_steps == 0):\n",
    "                    display_string = 'Iteration: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(iteration,\n",
    "                                                                                    time.time()-start,\n",
    "                                                                                    self.loss,\n",
    "                                                                                    training_loss.cpu().data.numpy())\n",
    "                    self.trajectories['iteration'].append(iteration)\n",
    "                    self.trajectories['train_loss'].append(training_loss.cpu().data.numpy())\n",
    "\n",
    "                    if val_ts_loader is not None:\n",
    "                        loss = self.evaluate_performance(val_ts_loader, validation_loss_fn=validation_loss_fn)\n",
    "                        display_string += \", Outsample {}: {:.5f}\".format(self.loss, loss)\n",
    "                        self.trajectories['val_loss'].append(loss)\n",
    "\n",
    "                        if self.early_stopping:\n",
    "                            if loss < best_val_loss:\n",
    "                                # Save current model if improves outsample loss\n",
    "                                best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "                                best_insample_loss = training_loss.cpu().data.numpy()\n",
    "                                early_stopping_counter = 0\n",
    "                                best_val_loss = loss\n",
    "                            else:\n",
    "                                early_stopping_counter += 1\n",
    "                            if early_stopping_counter >= self.early_stopping:\n",
    "                                break_flag = True\n",
    "\n",
    "                    print(display_string)\n",
    "\n",
    "                    self.model.train()\n",
    "\n",
    "                if break_flag:\n",
    "                    print(10*'-',' Stopped training by early stopping', 10*'-')\n",
    "                    self.model.load_state_dict(best_state_dict)\n",
    "                    break\n",
    "\n",
    "        #End of fitting\n",
    "        if n_iterations >0:\n",
    "            self.final_insample_loss = training_loss.cpu().data.numpy() if not break_flag else best_insample_loss #This is batch!\n",
    "            string = 'Iteration: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(iteration,\n",
    "                                                                            time.time()-start,\n",
    "                                                                            self.loss,\n",
    "                                                                            self.final_insample_loss)\n",
    "            if val_ts_loader is not None:\n",
    "                self.final_outsample_loss = self.evaluate_performance(val_ts_loader, validation_loss_fn=validation_loss_fn)\n",
    "                string += \", Outsample {}: {:.5f}\".format(self.loss, self.final_outsample_loss)\n",
    "            print(string)\n",
    "            print('='*30+'End fitting '+'='*30)\n",
    "\n",
    "    def predict(self, ts_loader, y_insample_tensor=None, X_test=None, eval_mode=False):\n",
    "        self.model.eval()\n",
    "        assert not ts_loader.shuffle, 'ts_loader must have shuffle as False.'\n",
    "\n",
    "        forecasts = []\n",
    "        outsample_ys = []\n",
    "        outsample_masks = []\n",
    "        with t.no_grad():\n",
    "            counter = 0\n",
    "            for batch in iter(ts_loader):\n",
    "                if y_insample_tensor is None:\n",
    "                    insample_y     = batch['insample_y']\n",
    "                else:\n",
    "                    insample_y     = y_insample_tensor[counter*ts_loader.batch_size:(counter+1)*ts_loader.batch_size, :]\n",
    "                insample_y         = self.to_tensor(insample_y)\n",
    "                insample_x         = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask      = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x        = self.to_tensor(batch['outsample_x'])\n",
    "                s_matrix           = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                forecast = self.model(insample_y=insample_y, insample_x_t=insample_x,\n",
    "                                      insample_mask=insample_mask, outsample_x_t=outsample_x, x_s=s_matrix)\n",
    "                forecasts.append(forecast.cpu().data.numpy())\n",
    "                outsample_ys.append(batch['outsample_y'])\n",
    "                outsample_masks.append(batch['outsample_mask'])\n",
    "                counter += 1\n",
    "    \n",
    "        forecasts = np.vstack(forecasts)\n",
    "        outsample_ys = np.vstack(outsample_ys)\n",
    "        outsample_masks = np.vstack(outsample_masks)\n",
    "\n",
    "        self.model.train()\n",
    "        if eval_mode:\n",
    "            return outsample_ys, forecasts, outsample_masks\n",
    "\n",
    "        # Pandas wrangling\n",
    "        frequency = ts_loader.get_frequency()\n",
    "        unique_ids = ts_loader.get_meta_data_col('unique_id')\n",
    "        last_ds = ts_loader.get_meta_data_col('last_ds') #TODO: ajustar of offset\n",
    "\n",
    "        # Predictions for panel\n",
    "        Y_hat_panel = pd.DataFrame(columns=['unique_id', 'ds'])\n",
    "        for i, unique_id in enumerate(unique_ids):\n",
    "            Y_hat_id = pd.DataFrame([unique_id]*self.output_size, columns=[\"unique_id\"])\n",
    "            ds = pd.date_range(start=last_ds[i], periods=self.output_size+1, freq=frequency)\n",
    "            Y_hat_id[\"ds\"] = ds[1:]\n",
    "            Y_hat_panel = Y_hat_panel.append(Y_hat_id, sort=False).reset_index(drop=True)\n",
    "\n",
    "        Y_hat_panel['y_hat'] = forecasts.flatten()\n",
    "\n",
    "        if X_test is not None:\n",
    "            Y_hat_panel = X_test.merge(Y_hat_panel, on=['unique_id', 'ds'], how='left')\n",
    "        \n",
    "        return Y_hat_panel\n",
    "\n",
    "\n",
    "    def save(self, model_dir, model_id):\n",
    "\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        print('Saving model to:\\n {}'.format(model_file)+'\\n')\n",
    "        t.save({'model_state_dict': self.model.state_dict()}, model_file)\n",
    "\n",
    "    def load(self, model_dir, model_id):\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        path = Path(model_file)\n",
    "\n",
    "        assert path.is_file(), 'No model_*.model file found in this path!'\n",
    "\n",
    "        print('Loading model from:\\n {}'.format(model_file)+'\\n')\n",
    "\n",
    "        checkpoint = t.load(model_file, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch as t\n",
    "import copy\n",
    "from fastcore.foundation import patch\n",
    "from nixtla.data.ontsdataset import TimeSeriesDataset\n",
    "from nixtla.data.ontsloader_fast import TimeSeriesLoader as TimeSeriesLoaderFast\n",
    "# from nixtla.data.tsloader_pinche import TimeSeriesLoader as TimeSeriesLoaderPinche\n",
    "# from nixtla.data.tsloader_general import TimeSeriesLoader as TimeSeriesLoaderGeneral\n",
    "\n",
    "#from nixtla.models.nbeats.onnbeats import Nbeats\n",
    "from nixtla.data.datasets.on import load_on_data\n",
    "np.random.seed(1)\n",
    "t.manual_seed(1)\n",
    "\n",
    "def get_top6(Y_df):\n",
    "    if 'top' in Y_df.columns:\n",
    "        del Y_df['top']\n",
    "\n",
    "    Y_df['ds_day'] = Y_df['ds'].dt.date\n",
    "    n_days = len(Y_df.ds_day.unique())\n",
    "\n",
    "    print(\"Despues\")\n",
    "    print(\"n_days\", n_days)\n",
    "    print(\"Y_df.shape\", Y_df.shape)\n",
    "    print(\"np.tile(np.array(range(6)), n_days)\", len(np.tile(np.array(range(6)), n_days)))\n",
    "\n",
    "    Y_df.sort_values(by=[ 'y', 'unique_id', 'ds_day'], inplace=True, ascending=False)\n",
    "\n",
    "    top = Y_df.groupby(['unique_id', 'ds_day']).head(6)\n",
    "    top['top'] = np.tile(np.array(range(6)), n_days)\n",
    "\n",
    "    top = top[['unique_id', 'ds', 'top']]\n",
    "\n",
    "    Y_df = Y_df.merge(top, on=['unique_id', 'ds'], how='left')\n",
    "    Y_df['top'] = Y_df['top'].fillna(0)\n",
    "\n",
    "    Y_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "    del Y_df['ds_day']\n",
    "\n",
    "    #if one_hot_encoded:\n",
    "    # one_hot_encode\n",
    "    Y_df['top'] = (Y_df['top']>0) * 1\n",
    "    \n",
    "    Y_df['y'] = Y_df['top']\n",
    "    del Y_df['top']\n",
    "    \n",
    "    return Y_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Antes\n",
      "Y_insample_df.shape (101760, 3)\n",
      "Y_outsample_df.shape (2881, 3)\n",
      "\n",
      "\n",
      "\n",
      "n_days 1060\n",
      "Y_df.shape (101760, 4)\n",
      "np.tile(np.array(range(6)), n_days) 6360\n",
      "n_days 31\n",
      "Y_df.shape (2881, 4)\n",
      "np.tile(np.array(range(6)), n_days) 186\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Length of values (186) does not match length of index (181)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-93815384724d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#--------------------------------------------   One hot encode top 6 --------------------------------------------#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mY_insample_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_top6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_insample_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mY_outsample_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_top6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_outsample_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-332199794f56>\u001b[0m in \u001b[0;36mget_top6\u001b[0;34m(Y_df)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unique_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ds_day'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mtop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'top'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_days\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unique_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'top'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nixtla/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3042\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3043\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3046\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nixtla/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3118\u001b[0m         \"\"\"\n\u001b[1;32m   3119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3120\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3121\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nixtla/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3767\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3768\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3770\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nixtla/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         raise ValueError(\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0;34m\"does not match length of index \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (186) does not match length of index (181)"
     ]
    }
   ],
   "source": [
    "Y_insample_df, X_insample_df, Y_outsample_df, X_outsample_df, f_cols = load_on_data(root_dir='../data/on/', test_date='2020-09-01')\n",
    "\n",
    "print(\"Antes\")\n",
    "print(\"Y_insample_df.shape\", Y_insample_df.shape)\n",
    "print(\"Y_outsample_df.shape\", Y_outsample_df.shape)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "#--------------------------------------------   One hot encode top 6 --------------------------------------------#\n",
    "Y_insample_df = get_top6(Y_insample_df)\n",
    "Y_outsample_df = get_top6(Y_outsample_df)\n",
    "\n",
    "\n",
    "ts_train_mask = np.ones(len(Y_insample_df))\n",
    "ts_train_mask[-7*6*16:] = 0 # 16 fifteenminutales = 4 hours   (total = 1 week)\n",
    "\n",
    "loss_weights = 2*np.ones(len(Y_insample_df))\n",
    "\n",
    "dataset = TimeSeriesDataset(Y_df=Y_insample_df,\n",
    "                            S_df=None, X_df=X_insample_df,\n",
    "                            ts_train_mask=ts_train_mask,\n",
    "                            loss_weights=loss_weights,\n",
    "                            f_cols=f_cols)\n",
    "print('X: time series features, of shape (#series,#times,#features): \\t' + str(X_insample_df.shape))\n",
    "print('Y: target series (in X), of shape (#series,#times): \\t \\t' + str(Y_insample_df.shape))\n",
    "Y_insample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          unique_id                  ds           y      ds_day\n",
       "2351  demanda_final 2020-09-25 11:45:00  6653.13235  2020-09-25\n",
       "2255  demanda_final 2020-09-24 11:45:00  6642.04816  2020-09-24\n",
       "2352  demanda_final 2020-09-25 12:00:00  6636.62416  2020-09-25\n",
       "2256  demanda_final 2020-09-24 12:00:00  6625.53421  2020-09-24\n",
       "2257  demanda_final 2020-09-24 12:15:00  6621.96478  2020-09-24"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unique_id</th>\n      <th>ds</th>\n      <th>y</th>\n      <th>ds_day</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2351</th>\n      <td>demanda_final</td>\n      <td>2020-09-25 11:45:00</td>\n      <td>6653.13235</td>\n      <td>2020-09-25</td>\n    </tr>\n    <tr>\n      <th>2255</th>\n      <td>demanda_final</td>\n      <td>2020-09-24 11:45:00</td>\n      <td>6642.04816</td>\n      <td>2020-09-24</td>\n    </tr>\n    <tr>\n      <th>2352</th>\n      <td>demanda_final</td>\n      <td>2020-09-25 12:00:00</td>\n      <td>6636.62416</td>\n      <td>2020-09-25</td>\n    </tr>\n    <tr>\n      <th>2256</th>\n      <td>demanda_final</td>\n      <td>2020-09-24 12:00:00</td>\n      <td>6625.53421</td>\n      <td>2020-09-24</td>\n    </tr>\n    <tr>\n      <th>2257</th>\n      <td>demanda_final</td>\n      <td>2020-09-24 12:15:00</td>\n      <td>6621.96478</td>\n      <td>2020-09-24</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "Y_outsample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         unique_id                  ds           y      ds_day\n",
       "511  demanda_final 2020-09-06 07:45:00  4623.12018  2020-09-06\n",
       "510  demanda_final 2020-09-06 07:30:00  4605.87550  2020-09-06\n",
       "508  demanda_final 2020-09-06 07:00:00  4587.66098  2020-09-06\n",
       "507  demanda_final 2020-09-06 06:45:00  4570.11747  2020-09-06\n",
       "509  demanda_final 2020-09-06 07:15:00  4551.96620  2020-09-06"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unique_id</th>\n      <th>ds</th>\n      <th>y</th>\n      <th>ds_day</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>511</th>\n      <td>demanda_final</td>\n      <td>2020-09-06 07:45:00</td>\n      <td>4623.12018</td>\n      <td>2020-09-06</td>\n    </tr>\n    <tr>\n      <th>510</th>\n      <td>demanda_final</td>\n      <td>2020-09-06 07:30:00</td>\n      <td>4605.87550</td>\n      <td>2020-09-06</td>\n    </tr>\n    <tr>\n      <th>508</th>\n      <td>demanda_final</td>\n      <td>2020-09-06 07:00:00</td>\n      <td>4587.66098</td>\n      <td>2020-09-06</td>\n    </tr>\n    <tr>\n      <th>507</th>\n      <td>demanda_final</td>\n      <td>2020-09-06 06:45:00</td>\n      <td>4570.11747</td>\n      <td>2020-09-06</td>\n    </tr>\n    <tr>\n      <th>509</th>\n      <td>demanda_final</td>\n      <td>2020-09-06 07:15:00</td>\n      <td>4551.96620</td>\n      <td>2020-09-06</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "Y_outsample_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = TimeSeriesLoaderFast(ts_dataset=dataset,\n",
    "                                    model='nbeats',\n",
    "                                    offset=0,\n",
    "                                    window_sampling_limit=60*6*16, \n",
    "                                    input_size=3*16,\n",
    "                                    output_size=16,\n",
    "                                    idx_to_sample_freq=1,\n",
    "                                    batch_size=256,\n",
    "                                    shuffle=True,\n",
    "                                    is_train_loader=True)\n",
    "\n",
    "val_loader = TimeSeriesLoaderFast(ts_dataset=dataset,\n",
    "                                  model='nbeats',\n",
    "                                  offset=0,\n",
    "                                  window_sampling_limit=60*6*16,\n",
    "                                  input_size=3*16,\n",
    "                                  output_size=16,\n",
    "                                  idx_to_sample_freq=1,\n",
    "                                  batch_size=256,\n",
    "                                  shuffle=False,\n",
    "                                  is_train_loader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "dataloader = iter(train_loader)\n",
    "batch = next(dataloader)\n",
    "insample_y = batch['insample_y']\n",
    "insample_x = batch['insample_x']\n",
    "insample_mask = batch['insample_mask']\n",
    "outsample_x = batch['outsample_x']\n",
    "outsample_y = batch['outsample_y']\n",
    "outsample_mask = batch['outsample_mask']\n",
    "print(\"DataloaderGeneral batch time:\", time.time()-start)\n",
    "print(\"insample_y.shape\", insample_y.shape)\n",
    "print(\"insample_x.shape\", insample_x.shape)\n",
    "print(\"outsample_y.shape\", outsample_y.shape)\n",
    "print(\"outsample_x.shape\", outsample_x.shape)\n",
    "print(\"t.max(insample_y)\", t.max(insample_y))\n",
    "print(\"t.max(outsample_y)\", t.max(outsample_y * outsample_mask))\n",
    "insample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbeatsx = Nbeats(input_size_multiplier=3,\n",
    "                 output_size=16,\n",
    "                 shared_weights=False,\n",
    "                 stack_types=['exogenous_g_a']+3*['identity'],\n",
    "                 n_blocks=4*[1],\n",
    "                 n_layers=4*[2],\n",
    "                 n_hidden=4*[256],\n",
    "                 n_harmonics=1,\n",
    "                 n_polynomials=2,\n",
    "                 x_s_n_hidden=0,\n",
    "                 exogenous_n_channels=9,\n",
    "                 f_cols=f_cols,\n",
    "                 batch_normalization=False,\n",
    "                 dropout_prob=0.1,\n",
    "                 theta_with_exogenous=True,\n",
    "                 learning_rate=0.001,\n",
    "                 lr_decay=0.5,\n",
    "                 n_lr_decay_steps=3,\n",
    "                 weight_decay=0.0000001,\n",
    "                 l1_lambda_x=0.0001,\n",
    "                 n_iterations=100,\n",
    "                 early_stopping=3,\n",
    "                 loss='MAE',\n",
    "                 frequency=24,\n",
    "                 random_seed=1,\n",
    "                 seasonality='H')\n",
    "\n",
    "nbeatsx.fit(train_ts_loader=train_loader, val_ts_loader=val_loader, verbose=True, eval_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}