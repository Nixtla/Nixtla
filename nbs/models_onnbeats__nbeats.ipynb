{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "52b0028e7074a0058398558ea661882eddbb489e66a28504cc0449d2f54d6290"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.onnbeats.nbeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "import torch as t\n",
    "from torch import optim\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "from nixtla.models.nbeats.onnbeats_model import NBeats, NBeatsBlock, IdentityBasis\n",
    "from nixtla.models.nbeats.onnbeats_model import TrendBasis, SeasonalityBasis, ExogenousFutureBasis, ExogenousBasisInterpretable\n",
    "from nixtla.losses.pytorch import MAPELoss, MASELoss, SMAPELoss, MSELoss, MAELoss, RMSELoss\n",
    "from nixtla.losses.numpy import mae, mse, mape, smape, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Nbeats(object):\n",
    "    \"\"\"\n",
    "    Future documentation\n",
    "    \"\"\"\n",
    "    SEASONALITY_BLOCK = 'seasonality'\n",
    "    TREND_BLOCK = 'trend'\n",
    "    IDENTITY_BLOCK = 'identity'\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size_multiplier,\n",
    "                 output_size,\n",
    "                 shared_weights,\n",
    "                 stack_types,\n",
    "                 n_blocks,\n",
    "                 n_layers,\n",
    "                 n_hidden,\n",
    "                 n_harmonics,\n",
    "                 n_polynomials,\n",
    "                 exogenous_n_channels,\n",
    "                 f_cols,\n",
    "                 theta_with_exogenous,\n",
    "                 batch_normalization,\n",
    "                 dropout_prob,\n",
    "                 x_s_n_hidden,\n",
    "                 learning_rate,\n",
    "                 lr_decay,\n",
    "                 n_lr_decay_steps,\n",
    "                 l1_lambda_x,\n",
    "                 weight_decay,\n",
    "                 n_iterations,\n",
    "                 early_stopping,\n",
    "                 loss,\n",
    "                 frequency,\n",
    "                 seasonality,\n",
    "                 random_seed,\n",
    "                 device=None):\n",
    "        super(Nbeats, self).__init__()\n",
    "\n",
    "        self.input_size = int(input_size_multiplier*output_size)\n",
    "        self.output_size = output_size\n",
    "        self.shared_weights = shared_weights\n",
    "        self.stack_types = stack_types\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_harmonics = n_harmonics\n",
    "        self.n_polynomials = n_polynomials\n",
    "        self.exogenous_n_channels = exogenous_n_channels\n",
    "        self.f_cols = f_cols\n",
    "        self.theta_with_exogenous = theta_with_exogenous\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.x_s_n_hidden = x_s_n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.n_lr_decay_steps = n_lr_decay_steps\n",
    "        self.l1_lambda_x = l1_lambda_x\n",
    "        self.weight_decay = weight_decay\n",
    "        self.n_iterations = n_iterations\n",
    "        self.early_stopping = early_stopping\n",
    "        self.loss = loss\n",
    "        self.frequency = frequency\n",
    "        self.seasonality = seasonality\n",
    "        self.random_seed = random_seed\n",
    "        if device is None:\n",
    "            device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
    "        self.device = device\n",
    "\n",
    "        self._is_instantiated = False\n",
    "\n",
    "    def create_stack(self):\n",
    "        # Declare parameter dimensions\n",
    "        if self.theta_with_exogenous:\n",
    "            x_t_n_inputs = self.input_size + self.n_x_t\n",
    "        else:\n",
    "            x_t_n_inputs = self.input_size # y_lags\n",
    "        \n",
    "        #------------------------ Model Definition ------------------------#\n",
    "        block_list = []\n",
    "        self.blocks_regularizer = []\n",
    "        for i in range(len(self.stack_types)):\n",
    "            #print(f'| --  Stack {self.stack_types[i]} (#{i})')\n",
    "            for block_id in range(self.n_blocks[i]):\n",
    "                # Batch norm only on first block\n",
    "                if (len(block_list)==0) and (self.batch_normalization):\n",
    "                    batch_normalization_block = True\n",
    "                else:\n",
    "                    batch_normalization_block = False\n",
    "\n",
    "                # Dummy of regularizer in block. Override with 1 if exogenous_block\n",
    "                #self.blocks_regularizer += [0]\n",
    "\n",
    "                # Shared weights\n",
    "                if self.shared_weights and block_id>0:\n",
    "                    nbeats_block = block_list[-1]\n",
    "\n",
    "                else:\n",
    "                    if self.stack_types[i] == 'seasonality':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=4 * int(\n",
    "                                                        np.ceil(self.n_harmonics / 2 * self.output_size) - (self.n_harmonics - 1)),\n",
    "                                                   basis=SeasonalityBasis(harmonics=self.n_harmonics,\n",
    "                                                                                backcast_size=self.input_size,\n",
    "                                                                                forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   theta_with_exogenous=self.theta_with_exogenous,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob)\n",
    "                    elif self.stack_types[i] == 'trend':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2 * (self.n_polynomials + 1),\n",
    "                                                   basis=TrendBasis(degree_of_polynomial=self.n_polynomials,\n",
    "                                                                            backcast_size=self.input_size,\n",
    "                                                                            forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   theta_with_exogenous=self.theta_with_exogenous,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob)\n",
    "                    elif self.stack_types[i] == 'identity':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=self.input_size + self.output_size,\n",
    "                                                   basis=IdentityBasis(backcast_size=self.input_size,\n",
    "                                                                       forecast_size=self.output_size),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   theta_with_exogenous=self.theta_with_exogenous,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob)\n",
    "                    elif self.stack_types[i] == 'exogenous':\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2*self.n_x_t,\n",
    "                                                   basis=ExogenousBasisInterpretable(),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   theta_with_exogenous=self.theta_with_exogenous,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob)\n",
    "                    elif self.stack_types[i] == 'exogenous_g_a':\n",
    "                        assert len(self.f_cols)>0, 'If ExogenousFutureBasis, provide x_f_cols hyperparameter'\n",
    "                        nbeats_block = NBeatsBlock(x_t_n_inputs = x_t_n_inputs,\n",
    "                                                   x_s_n_inputs = self.n_x_s,\n",
    "                                                   x_s_n_hidden= self.x_s_n_hidden,\n",
    "                                                   theta_n_dim=2*(self.exogenous_n_channels),\n",
    "                                                   basis=ExogenousFutureBasis(out_features=self.exogenous_n_channels,\n",
    "                                                                              f_idxs=self.f_idxs),\n",
    "                                                   n_layers=self.n_layers[i],\n",
    "                                                   theta_n_hidden=self.n_hidden[i],\n",
    "                                                   theta_with_exogenous=self.theta_with_exogenous,\n",
    "                                                   batch_normalization=batch_normalization_block,\n",
    "                                                   dropout_prob=self.dropout_prob)\n",
    "                #        self.blocks_regularizer[-1] = 1\n",
    "                #print(f'     | -- {nbeats_block}')\n",
    "                block_list.append(nbeats_block)\n",
    "        return block_list\n",
    "\n",
    "    def __loss_fn(self, loss_name: str):\n",
    "        def loss(x, freq, forecast, target, mask):\n",
    "            if loss_name == 'MAPE':\n",
    "                return MAPELoss(y=target, y_hat=forecast, mask=mask) + self.l1_regularization()\n",
    "            elif loss_name == 'MASE':\n",
    "                return MASELoss(y=target, y_hat=forecast, y_insample=x, seasonality=freq, mask=mask) + self.l1_regularization()\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return SMAPELoss(y=target, y_hat=forecast, mask=mask) + self.l1_regularization()\n",
    "            elif loss_name == 'MSE':\n",
    "                return MSELoss(y=target, y_hat=forecast, mask=mask) + self.l1_regularization()\n",
    "            elif loss_name == 'RMSE':\n",
    "                return RMSELoss(y=target, y_hat=forecast, mask=mask) + self.l1_regularization()\n",
    "            elif loss_name == 'MAE':\n",
    "                return MAELoss(y=target, y_hat=forecast, mask=mask) + self.l1_regularization()\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def __val_loss_fn(self, loss_name: str):\n",
    "        #TODO: mase not implemented\n",
    "        def loss(forecast, target, weights):\n",
    "            if loss_name == 'MAPE':\n",
    "                return mape(y=target, y_hat=forecast, weights=weights) #TODO: faltan weights\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return smape(y=target, y_hat=forecast, weights=weights) #TODO: faltan weights\n",
    "            elif loss_name == 'MSE':\n",
    "                return mse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'RMSE':\n",
    "                return rmse(y=target, y_hat=forecast, weights=weights)\n",
    "            elif loss_name == 'MAE':\n",
    "                return mae(y=target, y_hat=forecast, weights=weights)\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def l1_regularization(self):\n",
    "        # l1_loss = 0\n",
    "        # for i, indicator in enumerate(self.blocks_regularizer):\n",
    "        #     if indicator:\n",
    "        #         l1_loss +=  self.l1_lambda*t.sum(t.abs(self.model.blocks[i].basis.weight))\n",
    "        # return l1_loss\n",
    "        return self.l1_lambda_x * t.sum(t.abs(self.model.l1_weight))\n",
    "\n",
    "    def to_tensor(self, x: np.ndarray) -> t.Tensor:\n",
    "        tensor = t.as_tensor(x, dtype=t.float32).to(self.device)\n",
    "        return tensor\n",
    "\n",
    "    def evaluate_performance(self, ts_loader, validation_loss_fn):\n",
    "        #TODO: mas opciones que mae\n",
    "        self.model.eval()\n",
    "\n",
    "        losses = []\n",
    "        with t.no_grad():\n",
    "            for batch in iter(ts_loader):\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                forecast = self.model(insample_y=insample_y, insample_x_t=insample_x,\n",
    "                                    insample_mask=insample_mask, outsample_x_t=outsample_x, x_s=s_matrix)\n",
    "                batch_loss = validation_loss_fn(target=forecast.cpu().data.numpy(),\n",
    "                                                forecast=outsample_y.cpu().data.numpy(),\n",
    "                                                weights=outsample_mask.cpu().data.numpy())\n",
    "                losses.append(batch_loss)\n",
    "        loss = np.mean(losses)\n",
    "        self.model.train()\n",
    "        return loss\n",
    "\n",
    "    def fit(self, train_ts_loader, val_ts_loader=None, n_iterations=None, verbose=True, eval_steps=1):\n",
    "        # Asserts\n",
    "        assert train_ts_loader.t_cols[0] == 'y', f'First variable must be y not {train_ts_loader.t_cols[0]}'\n",
    "        assert train_ts_loader.t_cols[1] == 'ejecutado', f'First exogenous variable must be ejecutado not {train_ts_loader.t_cols[1]}'\n",
    "        assert (self.input_size)==train_ts_loader.input_size, \\\n",
    "            f'model input_size {self.input_size} data input_size {train_ts_loader.input_size}'        \n",
    "\n",
    "        # Random Seeds (model initialization)\n",
    "        t.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed) #TODO: interaccion rara con window_sampling de validacion\n",
    "\n",
    "        # Attributes of ts_dataset\n",
    "        self.n_x_t, self.n_x_s = train_ts_loader.get_n_variables()\n",
    "        self.t_cols = train_ts_loader.t_cols\n",
    "        self.f_idxs = train_ts_loader.ts_dataset.get_f_idxs(self.f_cols)\n",
    "\n",
    "        # Instantiate model\n",
    "        if not self._is_instantiated:\n",
    "            block_list = self.create_stack()\n",
    "            self.model = NBeats(blocks=t.nn.ModuleList(block_list), in_features=self.n_x_t).to(self.device)\n",
    "            self._is_instantiated = True\n",
    "\n",
    "        # Overwrite n_iterations and train datasets\n",
    "        if n_iterations is None:\n",
    "            n_iterations = self.n_iterations\n",
    "\n",
    "        lr_decay_steps = n_iterations // self.n_lr_decay_steps\n",
    "        if lr_decay_steps == 0:\n",
    "            lr_decay_steps = 1\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_steps, gamma=self.lr_decay)\n",
    "\n",
    "        training_loss_fn = self.__loss_fn(self.loss)\n",
    "        validation_loss_fn = self.__val_loss_fn(self.loss) #Uses numpy losses\n",
    "\n",
    "        if verbose and (n_iterations > 0):\n",
    "            print('='*30+' Start fitting '+'='*30)\n",
    "            print(f'Number of exogenous variables: {self.n_x_t}')\n",
    "            print(f'Number of static variables: {self.n_x_s} , with dim_hidden: {self.x_s_n_hidden}')\n",
    "            print(f'Number of iterations: {n_iterations}')\n",
    "            print(f'Number of blocks: {len(self.model.blocks)}')\n",
    "\n",
    "        #self.loss_dict = {} # Restart self.loss_dict\n",
    "        start = time.time()\n",
    "        self.trajectories = {'iteration':[],'train_loss':[], 'val_loss':[]}\n",
    "        self.final_insample_loss = None\n",
    "        self.final_outsample_loss = None\n",
    "\n",
    "        # Training Loop\n",
    "        best_val_loss = np.inf\n",
    "        early_stopping_counter = 0\n",
    "        best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "        break_flag = False\n",
    "        iteration = 0\n",
    "        epoch = 0\n",
    "        while (iteration < n_iterations) and (not break_flag):\n",
    "            epoch +=1\n",
    "            for batch in iter(train_ts_loader):\n",
    "                iteration += 1\n",
    "                if (iteration > n_iterations) or (break_flag):\n",
    "                    continue\n",
    "                self.model.train()\n",
    "                train_ts_loader.train()\n",
    "\n",
    "                insample_y     = self.to_tensor(batch['insample_y'])\n",
    "                insample_x     = self.to_tensor(batch['insample_x'])\n",
    "                insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "                outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "                outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "                outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "                s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                forecast = self.model(insample_y=insample_y, insample_x_t=insample_x,\n",
    "                                    insample_mask=insample_mask, outsample_x_t=outsample_x, x_s=s_matrix)\n",
    "\n",
    "                training_loss = training_loss_fn(x=insample_y, freq=self.seasonality, forecast=forecast,\n",
    "                                                target=outsample_y, mask=outsample_mask)\n",
    "\n",
    "                if np.isnan(float(training_loss)):\n",
    "                    break\n",
    "\n",
    "                training_loss.backward()\n",
    "                t.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "                if (iteration % eval_steps == 0):\n",
    "                    display_string = 'Iteration: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(iteration,\n",
    "                                                                                    time.time()-start,\n",
    "                                                                                    self.loss,\n",
    "                                                                                    training_loss.cpu().data.numpy())\n",
    "                    self.trajectories['iteration'].append(iteration)\n",
    "                    self.trajectories['train_loss'].append(training_loss.cpu().data.numpy())\n",
    "\n",
    "                    if val_ts_loader is not None:\n",
    "                        loss = self.evaluate_performance(val_ts_loader, validation_loss_fn=validation_loss_fn)\n",
    "                        display_string += \", Outsample {}: {:.5f}\".format(self.loss, loss)\n",
    "                        self.trajectories['val_loss'].append(loss)\n",
    "\n",
    "                        if self.early_stopping:\n",
    "                            if loss < best_val_loss:\n",
    "                                # Save current model if improves outsample loss\n",
    "                                best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "                                best_insample_loss = training_loss.cpu().data.numpy()\n",
    "                                early_stopping_counter = 0\n",
    "                                best_val_loss = loss\n",
    "                            else:\n",
    "                                early_stopping_counter += 1\n",
    "                            if early_stopping_counter >= self.early_stopping:\n",
    "                                break_flag = True\n",
    "\n",
    "                    print(display_string)\n",
    "\n",
    "                    self.model.train()\n",
    "                    train_ts_loader.train()\n",
    "\n",
    "                if break_flag:\n",
    "                    print(10*'-',' Stopped training by early stopping', 10*'-')\n",
    "                    self.model.load_state_dict(best_state_dict)\n",
    "                    break\n",
    "\n",
    "        #End of fitting\n",
    "        if n_iterations >0:\n",
    "            self.final_insample_loss = training_loss.cpu().data.numpy() if not break_flag else best_insample_loss #This is batch!\n",
    "            string = 'Iteration: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(iteration,\n",
    "                                                                            time.time()-start,\n",
    "                                                                            self.loss,\n",
    "                                                                            self.final_insample_loss)\n",
    "            if val_ts_loader is not None:\n",
    "                self.final_outsample_loss = self.evaluate_performance(val_ts_loader, validation_loss_fn=validation_loss_fn)\n",
    "                string += \", Outsample {}: {:.5f}\".format(self.loss, self.final_outsample_loss)\n",
    "            print(string)\n",
    "            print('='*30+'End fitting '+'='*30)\n",
    "\n",
    "    #TODO: predict podria no funcionar con muchas series, para on ahora no importa\n",
    "    def predict(self, ts_loader, X_test=None, eval_mode=False):\n",
    "\n",
    "        ts_loader.eval()\n",
    "        frequency = ts_loader.get_frequency()\n",
    "\n",
    "        # Build forecasts\n",
    "        unique_ids = ts_loader.get_meta_data_col('unique_id')\n",
    "        last_ds = ts_loader.get_meta_data_col('last_ds') #TODO: ajustar of offset\n",
    "\n",
    "        batch = next(iter(ts_loader))\n",
    "        insample_y     = self.to_tensor(batch['insample_y'])\n",
    "        insample_x     = self.to_tensor(batch['insample_x'])\n",
    "        insample_mask  = self.to_tensor(batch['insample_mask'])\n",
    "        outsample_x    = self.to_tensor(batch['outsample_x'])\n",
    "        outsample_y    = self.to_tensor(batch['outsample_y'])\n",
    "        outsample_mask = self.to_tensor(batch['outsample_mask'])\n",
    "        s_matrix       = self.to_tensor(batch['s_matrix'])\n",
    "\n",
    "        self.model.eval()\n",
    "        with t.no_grad():\n",
    "            forecast = self.model(insample_y=insample_y, insample_x_t=insample_x,\n",
    "                                  insample_mask=insample_mask, outsample_x_t=outsample_x, x_s=s_matrix)\n",
    "\n",
    "        if eval_mode:\n",
    "            return forecast, outsample_y, outsample_mask\n",
    "\n",
    "        # Predictions for panel\n",
    "        Y_hat_panel = pd.DataFrame(columns=['unique_id', 'ds'])\n",
    "        for i, unique_id in enumerate(unique_ids):\n",
    "            Y_hat_id = pd.DataFrame([unique_id]*self.output_size, columns=[\"unique_id\"])\n",
    "            ds = pd.date_range(start=last_ds[i], periods=self.output_size+1, freq=self.frequency)\n",
    "            Y_hat_id[\"ds\"] = ds[1:]\n",
    "            Y_hat_panel = Y_hat_panel.append(Y_hat_id, sort=False).reset_index(drop=True)\n",
    "\n",
    "        forecast = forecast.cpu().detach().numpy()\n",
    "        Y_hat_panel['y_hat'] = forecast.flatten()\n",
    "\n",
    "        if X_test is not None:\n",
    "            Y_hat_panel = X_test.merge(Y_hat_panel, on=['unique_id', 'ds'], how='left')\n",
    "\n",
    "        return Y_hat_panel\n",
    "\n",
    "\n",
    "    def save(self, model_dir, model_id):\n",
    "\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        print('Saving model to:\\n {}'.format(model_file)+'\\n')\n",
    "        t.save({'model_state_dict': self.model.state_dict()}, model_file)\n",
    "\n",
    "    def load(self, model_dir, model_id):\n",
    "\n",
    "        model_file = os.path.join(model_dir, f\"model_{model_id}.model\")\n",
    "        path = Path(model_file)\n",
    "\n",
    "        assert path.is_file(), 'No model_*.model file found in this path!'\n",
    "\n",
    "        print('Loading model from:\\n {}'.format(model_file)+'\\n')\n",
    "\n",
    "        checkpoint = t.load(model_file, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing dataframes ...\n",
      "Creating ts tensor ...\n",
      "X: time series features, of shape (#series,#times,#features): \t(101760, 331)\n",
      "Y: target series (in X), of shape (#series,#times): \t \t(101760, 3)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       unique_id                  ds           y\n",
       "0  demanda_final 2017-10-07 00:00:00  5545.98338\n",
       "1  demanda_final 2017-10-07 00:15:00  5448.66753\n",
       "2  demanda_final 2017-10-07 00:30:00  5392.92989\n",
       "3  demanda_final 2017-10-07 00:45:00  5371.67098\n",
       "4  demanda_final 2017-10-07 01:00:00  5296.58436"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unique_id</th>\n      <th>ds</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>demanda_final</td>\n      <td>2017-10-07 00:00:00</td>\n      <td>5545.98338</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>demanda_final</td>\n      <td>2017-10-07 00:15:00</td>\n      <td>5448.66753</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>demanda_final</td>\n      <td>2017-10-07 00:30:00</td>\n      <td>5392.92989</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>demanda_final</td>\n      <td>2017-10-07 00:45:00</td>\n      <td>5371.67098</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>demanda_final</td>\n      <td>2017-10-07 01:00:00</td>\n      <td>5296.58436</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch as t\n",
    "import copy\n",
    "from fastcore.foundation import patch\n",
    "from nixtla.data.ontsdataset import TimeSeriesDataset\n",
    "from nixtla.data.ontsloader_fast import TimeSeriesLoader as TimeSeriesLoaderFast\n",
    "# from nixtla.data.tsloader_pinche import TimeSeriesLoader as TimeSeriesLoaderPinche\n",
    "# from nixtla.data.tsloader_general import TimeSeriesLoader as TimeSeriesLoaderGeneral\n",
    "\n",
    "#from nixtla.models.nbeats.onnbeats import Nbeats\n",
    "from nixtla.data.datasets.on import load_on_data\n",
    "np.random.seed(1)\n",
    "t.manual_seed(1)\n",
    "\n",
    "Y_insample_df, X_insample_df, Y_outsample_df, X_outsample_df, f_cols = load_on_data(root_dir='../data', test_date='2020-09-01')\n",
    "\n",
    "ts_train_mask = np.ones(len(Y_insample_df))\n",
    "ts_train_mask[-7*6*16:] = 0 # 16 fifteenminutales = 4 hours   (total = 1 week)\n",
    "dataset = TimeSeriesDataset(Y_df=Y_insample_df, S_df=None, X_df=X_insample_df, ts_train_mask=ts_train_mask, f_cols=f_cols)\n",
    "print('X: time series features, of shape (#series,#times,#features): \\t' + str(X_insample_df.shape))\n",
    "print('Y: target series (in X), of shape (#series,#times): \\t \\t' + str(Y_insample_df.shape))\n",
    "Y_insample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            unique_id                  ds           y\n",
       "101755  demanda_final 2020-08-31 22:45:00  5911.97322\n",
       "101756  demanda_final 2020-08-31 23:00:00  5819.01473\n",
       "101757  demanda_final 2020-08-31 23:15:00  5750.59348\n",
       "101758  demanda_final 2020-08-31 23:30:00  5639.96684\n",
       "101759  demanda_final 2020-08-31 23:45:00  5589.45439"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unique_id</th>\n      <th>ds</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>101755</th>\n      <td>demanda_final</td>\n      <td>2020-08-31 22:45:00</td>\n      <td>5911.97322</td>\n    </tr>\n    <tr>\n      <th>101756</th>\n      <td>demanda_final</td>\n      <td>2020-08-31 23:00:00</td>\n      <td>5819.01473</td>\n    </tr>\n    <tr>\n      <th>101757</th>\n      <td>demanda_final</td>\n      <td>2020-08-31 23:15:00</td>\n      <td>5750.59348</td>\n    </tr>\n    <tr>\n      <th>101758</th>\n      <td>demanda_final</td>\n      <td>2020-08-31 23:30:00</td>\n      <td>5639.96684</td>\n    </tr>\n    <tr>\n      <th>101759</th>\n      <td>demanda_final</td>\n      <td>2020-08-31 23:45:00</td>\n      <td>5589.45439</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "Y_insample_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = TimeSeriesLoaderFast(ts_dataset=dataset,\n",
    "                                    model='nbeats',\n",
    "                                    offset=0,\n",
    "                                    window_sampling_limit=60*6*16, \n",
    "                                    input_size=3*16,\n",
    "                                    output_size=16,\n",
    "                                    idx_to_sample_freq=1,\n",
    "                                    batch_size=256,\n",
    "                                    shuffle=True,\n",
    "                                    is_train_loader=True)\n",
    "\n",
    "val_loader = TimeSeriesLoaderFast(ts_dataset=dataset,\n",
    "                                  model='nbeats',\n",
    "                                  offset=0,\n",
    "                                  window_sampling_limit=60*6*16,\n",
    "                                  input_size=3*16,\n",
    "                                  output_size=16,\n",
    "                                  idx_to_sample_freq=1,\n",
    "                                  batch_size=256,\n",
    "                                  shuffle=False,\n",
    "                                  is_train_loader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DataloaderGeneral batch time: 0.004068851470947266\ninsample_y.shape torch.Size([256, 48])\ninsample_x.shape torch.Size([256, 328, 48])\noutsample_y.shape torch.Size([256, 16])\noutsample_x.shape torch.Size([256, 328, 16])\nt.max(insample_y) tensor(6632.9390)\nt.max(outsample_y) tensor(6552.0488)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[5081.6943, 5111.7661, 5111.7661,  ..., 5899.2358, 5899.2358,\n",
       "         5892.8955],\n",
       "        [5769.2231, 5769.2231, 5696.2783,  ..., 5907.2275, 5875.2837,\n",
       "         5875.2837],\n",
       "        [4957.3071, 4957.3071, 5053.0923,  ..., 5938.2847, 5905.4185,\n",
       "         5905.4185],\n",
       "        ...,\n",
       "        [5151.8848, 5176.2510, 5176.2510,  ..., 6131.1484, 6131.1484,\n",
       "         6131.5996],\n",
       "        [6234.8438, 6234.8438, 6180.6270,  ..., 5199.1909, 5338.4956,\n",
       "         5338.4956],\n",
       "        [5970.6201, 5970.6201, 5973.8721,  ..., 5133.4688, 5194.5215,\n",
       "         5194.5215]])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "start = time.time()\n",
    "dataloader = iter(train_loader)\n",
    "batch = next(dataloader)\n",
    "insample_y = batch['insample_y']\n",
    "insample_x = batch['insample_x']\n",
    "insample_mask = batch['insample_mask']\n",
    "outsample_x = batch['outsample_x']\n",
    "outsample_y = batch['outsample_y']\n",
    "outsample_mask = batch['outsample_mask']\n",
    "print(\"DataloaderGeneral batch time:\", time.time()-start)\n",
    "print(\"insample_y.shape\", insample_y.shape)\n",
    "print(\"insample_x.shape\", insample_x.shape)\n",
    "print(\"outsample_y.shape\", outsample_y.shape)\n",
    "print(\"outsample_x.shape\", outsample_x.shape)\n",
    "print(\"t.max(insample_y)\", t.max(insample_y))\n",
    "print(\"t.max(outsample_y)\", t.max(outsample_y * outsample_mask))\n",
    "insample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================== Start fitting ==============================\n",
      "Number of exogenous variables: 328\n",
      "Number of static variables: 0 , with dim_hidden: 0\n",
      "Number of iterations: 100\n",
      "Number of blocks: 4\n",
      "Iteration: 5, Time: 1.046, Insample MAE: 312.11322, Outsample MAE: 315.07239\n",
      "Iteration: 10, Time: 2.230, Insample MAE: 270.41895, Outsample MAE: 275.35861\n",
      "Iteration: 15, Time: 3.415, Insample MAE: 264.43765, Outsample MAE: 284.36130\n",
      "Iteration: 20, Time: 4.569, Insample MAE: 272.50696, Outsample MAE: 260.23141\n",
      "Iteration: 25, Time: 5.743, Insample MAE: 225.10390, Outsample MAE: 241.25958\n",
      "Iteration: 30, Time: 6.933, Insample MAE: 226.68974, Outsample MAE: 236.84113\n",
      "Iteration: 35, Time: 8.105, Insample MAE: 230.91516, Outsample MAE: 220.43462\n",
      "Iteration: 40, Time: 9.274, Insample MAE: 209.47870, Outsample MAE: 218.16832\n",
      "Iteration: 45, Time: 10.453, Insample MAE: 199.12753, Outsample MAE: 204.60783\n",
      "Iteration: 50, Time: 11.646, Insample MAE: 201.74980, Outsample MAE: 190.58318\n",
      "Iteration: 55, Time: 12.826, Insample MAE: 216.44344, Outsample MAE: 182.10768\n",
      "Iteration: 60, Time: 13.983, Insample MAE: 172.57915, Outsample MAE: 177.38623\n",
      "Iteration: 65, Time: 15.166, Insample MAE: 172.17442, Outsample MAE: 175.66461\n",
      "Iteration: 70, Time: 16.347, Insample MAE: 174.90288, Outsample MAE: 160.52332\n",
      "Iteration: 75, Time: 17.536, Insample MAE: 159.52051, Outsample MAE: 161.17323\n",
      "Iteration: 80, Time: 18.716, Insample MAE: 154.22656, Outsample MAE: 154.23320\n",
      "Iteration: 85, Time: 19.892, Insample MAE: 155.16862, Outsample MAE: 150.06020\n",
      "Iteration: 90, Time: 21.102, Insample MAE: 140.20018, Outsample MAE: 148.53416\n",
      "Iteration: 95, Time: 22.284, Insample MAE: 159.87202, Outsample MAE: 158.63817\n",
      "Iteration: 100, Time: 23.451, Insample MAE: 159.87039, Outsample MAE: 144.66306\n",
      "Iteration: 100, Time: 23.601, Insample MAE: 159.87039, Outsample MAE: 144.66306\n",
      "==============================End fitting ==============================\n"
     ]
    }
   ],
   "source": [
    "nbeatsx = Nbeats(input_size_multiplier=3,\n",
    "                 output_size=16,\n",
    "                 shared_weights=False,\n",
    "                 stack_types=['exogenous_g_a']+3*['identity'],\n",
    "                 n_blocks=4*[1],\n",
    "                 n_layers=4*[2],\n",
    "                 n_hidden=4*[256],\n",
    "                 n_harmonics=1,\n",
    "                 n_polynomials=2,\n",
    "                 x_s_n_hidden=0,\n",
    "                 exogenous_n_channels=9,\n",
    "                 f_cols=f_cols,\n",
    "                 batch_normalization=False,\n",
    "                 dropout_prob=0.1,\n",
    "                 theta_with_exogenous=True,\n",
    "                 learning_rate=0.001,\n",
    "                 lr_decay=0.5,\n",
    "                 n_lr_decay_steps=3,\n",
    "                 weight_decay=0.0000001,\n",
    "                 l1_lambda_x=0.0001,\n",
    "                 n_iterations=100,\n",
    "                 early_stopping=3,\n",
    "                 loss='MAE',\n",
    "                 frequency=24,\n",
    "                 random_seed=1,\n",
    "                 seasonality='H')\n",
    "\n",
    "nbeatsx.fit(train_ts_loader=train_loader, val_ts_loader=val_loader, verbose=True, eval_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}