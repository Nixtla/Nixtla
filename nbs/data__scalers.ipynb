{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from statsmodels.robust import mad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class MedianScaler(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, data):\n",
    "\n",
    "        if len(data.shape)!=2:\n",
    "            raise IndexError('Error: Provide 2-D array. First dimension is datapoints and' + \n",
    "                  ' second features')\n",
    "            return -1\n",
    "\n",
    "        self.median = np.median(data, axis=0)\n",
    "        self.mad = mad(data, axis=0)\n",
    "        self.fitted = True\n",
    "        \n",
    "    def fit_transform(self, data):\n",
    "\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "    \n",
    "    def transform(self, data):\n",
    "\n",
    "        if not self.fitted:\n",
    "            print('Error: The scaler has not been yet fitted. Called fit or fit_transform')\n",
    "            return -1\n",
    "\n",
    "        if len(data.shape)!=2:\n",
    "            raise IndexError('Error: Provide 2-D array. First dimension is datapoints and' + \n",
    "                  ' second features')\n",
    "\n",
    "        transformed_data = np.zeros(shape=data.shape)\n",
    "\n",
    "        for i in range(data.shape[1]):\n",
    "\n",
    "            transformed_data[:, i] = (data[:, i] - self.median[i]) / self.mad[i]\n",
    "            \n",
    "        return transformed_data\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "\n",
    "        if not self.fitted:\n",
    "            print('Error: The scaler has not been yet fitted. Called fit or fit_transform')\n",
    "            return -1\n",
    "\n",
    "        if len(data.shape)!=2:\n",
    "            raise IndexError('Error: Provide 2-D array. First dimension is datapoints and' + \n",
    "                  ' second features')\n",
    "\n",
    "        transformed_data = np.zeros(shape=data.shape)\n",
    "\n",
    "        for i in range(data.shape[1]):\n",
    "\n",
    "            transformed_data[:, i] = data[:, i] * self.mad[i] + self.median[i] \n",
    "\n",
    "        return transformed_data\n",
    "\n",
    "\n",
    "class InvariantScaler(MedianScaler):\n",
    "\n",
    "    def __init__(self):\n",
    "        super()\n",
    "\n",
    "    def fit(self, data):\n",
    "\n",
    "        super().fit(data)\n",
    "        \n",
    "    def fit_transform(self, data):\n",
    "\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "    \n",
    "    def transform(self, data):\n",
    "\n",
    "        transformed_data = super().transform(data)\n",
    "        transformed_data = np.arcsinh(transformed_data)\n",
    "\n",
    "        return transformed_data\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "\n",
    "        transformed_data = np.sinh(data)\n",
    "        transformed_data = super().inverse_transform(transformed_data)\n",
    "\n",
    "        return transformed_data\n",
    "\n",
    "\n",
    "class DataScaler(object):\n",
    "\n",
    "    \"\"\"Class to perform data scaling operations\n",
    "\n",
    "    The scaling technique is defined by the ``normalize`` parameter which takes one of the \n",
    "    following values: \n",
    "\n",
    "    - ``'Norm'`` for normalizing the data to the interval [0, 1].\n",
    "\n",
    "    - ``'Norm1'`` for normalizing the data to the interval [-1, 1]. \n",
    "\n",
    "    - ``'Std'`` for standarizing the data to follow a normal distribution. \n",
    "\n",
    "    - ``'Median'`` for normalizing the data based on the median as defined in as defined in `here <https://doi.org/10.1109/TPWRS.2017.2734563>`_.\n",
    "\n",
    "    - ``'Invariant'`` for scaling the data based on the asinh transformation (a variance stabilizing transformations) as defined in `here <https://doi.org/10.1109/TPWRS.2017.2734563>`_. \n",
    "    \n",
    "    This class follows the same syntax of the scalers defined in the \n",
    "    `sklearn.preprocessing <https://scikit-learn.org/stable/modules/preprocessing.html>`_ module of the \n",
    "    scikit-learn library\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    normalize : str\n",
    "        Type of scaling to be performed. Possible values are ``'Norm'``, ``'Norm1'``, ``'Std'``, \n",
    "        ``'Median'``, or ``'Invariant'``\n",
    "\n",
    "    Example\n",
    "    --------\n",
    "    >>> from epftoolbox.data import read_data\n",
    "    >>> from epftoolbox.data import DataScaler\n",
    "    >>> df_train, df_test = read_data(path='.', dataset='PJM', begin_test_date='01-01-2016', end_test_date='01-02-2016')\n",
    "    Test datasets: 2016-01-01 00:00:00 - 2016-02-01 23:00:00\n",
    "    >>> df_train.tail()\n",
    "                             Price  Exogenous 1  Exogenous 2\n",
    "    Date                                                    \n",
    "    2015-12-31 19:00:00  29.513832     100700.0      13015.0\n",
    "    2015-12-31 20:00:00  28.440134      99832.0      12858.0\n",
    "    2015-12-31 21:00:00  26.701700      97033.0      12626.0\n",
    "    2015-12-31 22:00:00  23.262253      92022.0      12176.0\n",
    "    2015-12-31 23:00:00  22.262431      86295.0      11434.0\n",
    "    >>> df_test.head()\n",
    "                             Price  Exogenous 1  Exogenous 2\n",
    "    Date                                                    \n",
    "    2016-01-01 00:00:00  20.341321      76840.0      10406.0\n",
    "    2016-01-01 01:00:00  19.462741      74819.0      10075.0\n",
    "    2016-01-01 02:00:00  17.172706      73182.0       9795.0\n",
    "    2016-01-01 03:00:00  16.963876      72300.0       9632.0\n",
    "    2016-01-01 04:00:00  17.403722      72535.0       9566.0\n",
    "    >>> Xtrain = df_train.values\n",
    "    >>> Xtest = df_train.values\n",
    "    >>> scaler = DataScaler('Norm')\n",
    "    >>> Xtrain_scaled = scaler.fit_transform(Xtrain)\n",
    "    >>> Xtest_scaled = scaler.transform(Xtest)\n",
    "    >>> Xtrain_inverse = scaler.inverse_transform(Xtrain_scaled)\n",
    "    >>> Xtest_inverse = scaler.inverse_transform(Xtest_scaled)\n",
    "    >>> Xtrain[:3,:]\n",
    "    array([[2.5464211e+01, 8.5049000e+04, 1.1509000e+04],\n",
    "           [2.3554578e+01, 8.2128000e+04, 1.0942000e+04],\n",
    "           [2.2122277e+01, 8.0729000e+04, 1.0639000e+04]])\n",
    "    >>> Xtrain_scaled[:3,:]\n",
    "    array([[0.03833877, 0.2736787 , 0.28415155],\n",
    "           [0.03608228, 0.24425597, 0.24633138],\n",
    "           [0.03438982, 0.23016409, 0.2261206 ]])\n",
    "    >>> Xtrain_inverse[:3,:]\n",
    "    array([[2.5464211e+01, 8.5049000e+04, 1.1509000e+04],\n",
    "           [2.3554578e+01, 8.2128000e+04, 1.0942000e+04],\n",
    "           [2.2122277e+01, 8.0729000e+04, 1.0639000e+04]])\n",
    "    >>> Xtest[:3,:]\n",
    "    array([[2.5464211e+01, 8.5049000e+04, 1.1509000e+04],\n",
    "           [2.3554578e+01, 8.2128000e+04, 1.0942000e+04],\n",
    "           [2.2122277e+01, 8.0729000e+04, 1.0639000e+04]])\n",
    "    >>> Xtest_scaled[:3,:]\n",
    "    array([[0.03833877, 0.2736787 , 0.28415155],\n",
    "           [0.03608228, 0.24425597, 0.24633138],\n",
    "           [0.03438982, 0.23016409, 0.2261206 ]])\n",
    "    >>> Xtest_inverse[:3,:]\n",
    "    array([[2.5464211e+01, 8.5049000e+04, 1.1509000e+04],\n",
    "           [2.3554578e+01, 8.2128000e+04, 1.0942000e+04],\n",
    "           [2.2122277e+01, 8.0729000e+04, 1.0639000e+04]])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, normalize):\n",
    "\n",
    "        if normalize == 'Norm':\n",
    "            self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        elif normalize == 'Norm1':\n",
    "            self.scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        elif normalize == 'Std':\n",
    "            self.scaler = StandardScaler()\n",
    "        elif normalize == 'Median':\n",
    "            self.scaler = MedianScaler()\n",
    "        elif normalize == 'Invariant':\n",
    "            self.scaler = InvariantScaler()        \n",
    "\n",
    "    def fit_transform(self, dataset):\n",
    "        \"\"\"Method that estimates an scaler object using the data in ``dataset`` and scales the data in  ``dataset``\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : numpy.array\n",
    "            Dataset used to estimate the scaler\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.array\n",
    "            Scaled data\n",
    "        \"\"\"\n",
    "\n",
    "        return self.scaler.fit_transform(dataset)\n",
    "\n",
    "    def transform(self, dataset):\n",
    "        \"\"\"Method that scales the data in ``dataset``\n",
    "        \n",
    "        It must be called after calling the :class:`fit_transform` method for estimating the scaler\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : numpy.array\n",
    "            Dataset to be scaled\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.array\n",
    "            Scaled data\n",
    "        \"\"\"\n",
    "\n",
    "        return self.scaler.transform(dataset)\n",
    "\n",
    "    def inverse_transform(self, dataset):\n",
    "        \"\"\"Method that inverse-scale the data in ``dataset``\n",
    "        \n",
    "        It must be called after calling the :class:`fit_transform` method for estimating the scaler\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : numpy.array\n",
    "            Dataset to be scaled\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.array\n",
    "            Inverse-scaled data\n",
    "        \"\"\"\n",
    "\n",
    "        return self.scaler.inverse_transform(dataset)\n",
    "\n",
    "def scaling(datasets, normalize):\n",
    "    \"\"\"Function that scales data and returns the scaled data and the :class:`DataScaler` used for scaling.\n",
    "\n",
    "    It rescales all the datasets contained in the list ``datasets`` using the first dataset as reference. \n",
    "    For example, if ``datasets=[X_1, X_2, X_3]``, the function estimates a :class:`DataScaler` object using the array ``X_1``, \n",
    "    and transform ``X_1``, ``X_2``, and ``X_3`` using the :class:`DataScaler` object.\n",
    "\n",
    "    Each dataset must be a numpy.array and it should have the same column-dimensions. For example, if\n",
    "    ``datasets=[X_1, X_2, X_3]``, ``X_1`` must be a numpy.array of size ``[n_1, m]``,\n",
    "    ``X_2`` of size ``[n_2, m]``, and ``X_3`` of size ``[n_3, m]``, where ``n_1``, ``n_2``, ``n_3`` can be\n",
    "    different.\n",
    "\n",
    "    The scaling technique is defined by the ``normalize`` parameter which takes one of the \n",
    "    following values: \n",
    "\n",
    "    - ``'Norm'`` for normalizing the data to the interval [0, 1].\n",
    "\n",
    "    - ``'Norm1'`` for normalizing the data to the interval [-1, 1]. \n",
    "\n",
    "    - ``'Std'`` for standarizing the data to follow a normal distribution. \n",
    "\n",
    "    - ``'Median'`` for normalizing the data based on the median as defined in as defined in `here <https://doi.org/10.1109/TPWRS.2017.2734563>`_.\n",
    "\n",
    "    - ``'Invariant'`` for scaling the data based on the asinh transformation (a variance stabilizing transformations) as defined in `here <https://doi.org/10.1109/TPWRS.2017.2734563>`_. \n",
    "\n",
    "\n",
    "    The function returns the scaled data together with a :class:`DataScaler` object representing the scaling. \n",
    "    This object can be used to scale other dataset using the same rules or to inverse-transform the data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets : list\n",
    "        List of numpy.array objects to be scaled.\n",
    "    normalize : str\n",
    "        Type of scaling to be performed. Possible values are ``'Norm'``, ``'Norm1'``, ``'Std'``, \n",
    "        ``'Median'``, or ``'Invariant'``\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List, :class:`DataScaler`\n",
    "        List of scaled datasets and the :class:`DataScaler` object used for scaling. Each dataset in the \n",
    "        list is a numpy.array.\n",
    "    \n",
    "    Example\n",
    "    --------\n",
    "    >>> from epftoolbox.data import read_data\n",
    "    >>> from epftoolbox.data import scaling\n",
    "    >>> df_train, df_test = read_data(path='.', dataset='PJM', begin_test_date='01-01-2016', end_test_date='01-02-2016')\n",
    "    Test datasets: 2016-01-01 00:00:00 - 2016-02-01 23:00:00\n",
    "    >>> df_train.tail()\n",
    "                             Price  Exogenous 1  Exogenous 2\n",
    "    Date                                                    \n",
    "    2015-12-31 19:00:00  29.513832     100700.0      13015.0\n",
    "    2015-12-31 20:00:00  28.440134      99832.0      12858.0\n",
    "    2015-12-31 21:00:00  26.701700      97033.0      12626.0\n",
    "    2015-12-31 22:00:00  23.262253      92022.0      12176.0\n",
    "    2015-12-31 23:00:00  22.262431      86295.0      11434.0\n",
    "    >>> df_test.head()\n",
    "                             Price  Exogenous 1  Exogenous 2\n",
    "    Date                                                    \n",
    "    2016-01-01 00:00:00  20.341321      76840.0      10406.0\n",
    "    2016-01-01 01:00:00  19.462741      74819.0      10075.0\n",
    "    2016-01-01 02:00:00  17.172706      73182.0       9795.0\n",
    "    2016-01-01 03:00:00  16.963876      72300.0       9632.0\n",
    "    2016-01-01 04:00:00  17.403722      72535.0       9566.0\n",
    "    >>> Xtrain = df_train.values\n",
    "    >>> Xtest = df_train.values\n",
    "    >>> [Xtrain_scaled, Xtest_scaled], scaler = scaling([Xtrain,Xtest],'Norm')\n",
    "    >>> Xtrain[:3,:]\n",
    "    array([[2.5464211e+01, 8.5049000e+04, 1.1509000e+04],\n",
    "           [2.3554578e+01, 8.2128000e+04, 1.0942000e+04],\n",
    "           [2.2122277e+01, 8.0729000e+04, 1.0639000e+04]])\n",
    "    >>> Xtrain_scaled[:3,:]\n",
    "    array([[0.03833877, 0.2736787 , 0.28415155],\n",
    "           [0.03608228, 0.24425597, 0.24633138],\n",
    "           [0.03438982, 0.23016409, 0.2261206 ]])\n",
    "    >>> Xtest[:3,:]\n",
    "    array([[2.5464211e+01, 8.5049000e+04, 1.1509000e+04],\n",
    "           [2.3554578e+01, 8.2128000e+04, 1.0942000e+04],\n",
    "           [2.2122277e+01, 8.0729000e+04, 1.0639000e+04]])           \n",
    "    >>> Xtest_scaled[:3,:]\n",
    "    array([[0.03833877, 0.2736787 , 0.28415155],\n",
    "           [0.03608228, 0.24425597, 0.24633138],\n",
    "           [0.03438982, 0.23016409, 0.2261206 ]])\n",
    "    >>> type(scaler)\n",
    "    <class 'epftoolbox.data._wrangling.DataScaler'>\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    scaler = DataScaler(normalize)\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        if i == 0:\n",
    "            dataset = scaler.fit_transform(dataset)\n",
    "        else:\n",
    "            dataset = scaler.transform(dataset)\n",
    "\n",
    "        datasets[i] = dataset\n",
    "\n",
    "    return datasets, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}