{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c68c0f618c4f787b1e02bfee278e48d25b62407cb335aab5258cedc4db4ced"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.nbeats.onnbeats_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import math\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from typing import Tuple\n",
    "from nixtla.models.component import TemporalConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _StaticFeaturesEncoder(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(_StaticFeaturesEncoder, self).__init__()\n",
    "        layers = [nn.Dropout(p=0.5),\n",
    "                  nn.Linear(in_features=in_features, out_features=out_features),\n",
    "                  nn.ReLU()]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "class NBeatsBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    N-BEATS block which takes a basis function as an argument.\n",
    "    \"\"\"\n",
    "    def __init__(self, x_t_n_inputs: int, x_s_n_inputs: int, x_s_n_hidden: int, theta_n_dim: int, basis: nn.Module, # n_static:int\n",
    "                 n_layers: int, theta_n_hidden: int, theta_with_exogenous: bool, batch_normalization: bool, dropout_prob: float):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.x_s_n_inputs = x_s_n_inputs\n",
    "        self.x_s_n_hidden = x_s_n_hidden\n",
    "        self.theta_with_exogenous = theta_with_exogenous\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        if x_s_n_inputs == 0:\n",
    "            x_s_n_hidden = 0\n",
    "        input_layer = [nn.Linear(in_features=x_t_n_inputs + x_s_n_hidden, out_features=theta_n_hidden), nn.ReLU()]\n",
    "\n",
    "        hidden_layers = []\n",
    "        for _ in range(n_layers-1):\n",
    "            hidden_layers.append(nn.Linear(in_features=theta_n_hidden, out_features=theta_n_hidden))\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "\n",
    "            if self.batch_normalization:\n",
    "                hidden_layers.append(nn.BatchNorm1d(theta_n_hidden))\n",
    "\n",
    "            if self.dropout_prob>0:\n",
    "                hidden_layers.append(nn.Dropout(p=self.dropout_prob))\n",
    "\n",
    "        output_layer = [nn.Linear(in_features=theta_n_hidden, out_features=theta_n_dim)]\n",
    "        layers = input_layer + hidden_layers + output_layer\n",
    "\n",
    "        # x_s_n_inputs is computed with data, x_s_n_hidden is provided by user, if 0 no statics are used\n",
    "        if (self.x_s_n_inputs > 0) and (self.x_s_n_hidden > 0):\n",
    "            self.static_encoder = _StaticFeaturesEncoder(in_features=x_s_n_inputs, out_features=x_s_n_hidden)\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.basis = basis\n",
    "\n",
    "    def forward(self, insample_y: t.Tensor, insample_x_t: t.Tensor,\n",
    "                outsample_x_t: t.Tensor, x_s: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "\n",
    "        # Static exogenous\n",
    "        if (self.x_s_n_inputs > 0) and (self.x_s_n_hidden > 0):\n",
    "            x_s = self.static_encoder(x_s)\n",
    "            insample_y = t.cat((insample_y, x_s), 1)\n",
    "\n",
    "        # Temporal exogenous, only forecasted exogenous are used\n",
    "        # TODO: for epf not wavenet, include wavenet encoder in the future\n",
    "        if (self.theta_with_exogenous) and (len(outsample_x_t)>0):\n",
    "            #outsample_x_t_flatten = outsample_x_t.reshape(len(outsample_x_t), -1)\n",
    "            insample_x_t_last_flatten = insample_x_t[:,:,-1].reshape(len(insample_x_t), -1)\n",
    "            insample_y = t.cat((insample_y, insample_x_t_last_flatten), 1)\n",
    "\n",
    "        # Compute local projection weights and projection\n",
    "        theta = self.layers(insample_y)\n",
    "        backcast, forecast = self.basis(theta, insample_x_t, outsample_x_t)\n",
    "\n",
    "        return backcast, forecast\n",
    "\n",
    "class NBeats(nn.Module):\n",
    "    \"\"\"\n",
    "    N-Beats Model.\n",
    "    \"\"\"\n",
    "    def __init__(self, blocks: nn.ModuleList, in_features):\n",
    "        super().__init__()\n",
    "        self.blocks = blocks\n",
    "        self.l1_weight = nn.Parameter(t.Tensor(1, in_features, 1), requires_grad=True)\n",
    "        nn.init.kaiming_uniform_(self.l1_weight, a=math.sqrt(0.5))\n",
    "\n",
    "    def forward(self, insample_y: t.Tensor, insample_x_t: t.Tensor, insample_mask: t.Tensor,\n",
    "                outsample_x_t: t.Tensor, x_s: t.Tensor) -> t.Tensor:\n",
    "\n",
    "        insample_x_t = insample_x_t * self.l1_weight # Element-wise multiplication, broadcasted on b and t.\n",
    "        outsample_x_t = outsample_x_t * self.l1_weight # Element-wise multiplication, broadcasted on b and t.\n",
    "\n",
    "        residuals = insample_y.flip(dims=(-1,))\n",
    "        insample_x_t = insample_x_t.flip(dims=(-1,))\n",
    "        insample_mask = insample_mask.flip(dims=(-1,))\n",
    "\n",
    "        forecast = insample_y[:, -1:] # Level with Naive1\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            backcast, block_forecast = block(insample_y=residuals, insample_x_t=insample_x_t,\n",
    "                                             outsample_x_t=outsample_x_t, x_s=x_s)\n",
    "            residuals = (residuals - backcast) * insample_mask\n",
    "            forecast = forecast + block_forecast\n",
    "        return forecast\n",
    "\n",
    "    def decomposed_prediction(self, insample_y: t.Tensor, insample_x_t: t.Tensor, insample_mask: t.Tensor,\n",
    "                              outsample_x_t: t.Tensor):\n",
    "\n",
    "        residuals = insample_y.flip(dims=(-1,))\n",
    "        insample_x_t = insample_x_t.flip(dims=(-1,))\n",
    "        insample_mask = insample_mask.flip(dims=(-1,))\n",
    "\n",
    "        forecast = insample_y[:, -1:] # Level with Naive1\n",
    "        forecast_components = []\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            backcast, block_forecast = block(residuals, insample_x_t, outsample_x_t)\n",
    "            residuals = (residuals - backcast) * insample_mask\n",
    "            forecast = forecast + block_forecast\n",
    "            forecast_components.append(block_forecast)\n",
    "        return forecast, forecast_components\n",
    "\n",
    "class IdentityBasis(nn.Module):\n",
    "    def __init__(self, backcast_size: int, forecast_size: int):\n",
    "        super().__init__()\n",
    "        self.forecast_size = forecast_size\n",
    "        self.backcast_size = backcast_size\n",
    "\n",
    "    def forward(self, theta: t.Tensor, insample_x_t: t.Tensor, outsample_x_t: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "        backcast = theta[:, :self.backcast_size]\n",
    "        forecast = theta[:, -self.forecast_size:]\n",
    "        return backcast, forecast\n",
    "\n",
    "class TrendBasis(nn.Module):\n",
    "    def __init__(self, degree_of_polynomial: int, backcast_size: int, forecast_size: int):\n",
    "        super().__init__()\n",
    "        polynomial_size = degree_of_polynomial + 1\n",
    "        self.backcast_basis = nn.Parameter(\n",
    "            t.tensor(np.concatenate([np.power(np.arange(backcast_size, dtype=np.float) / backcast_size, i)[None, :]\n",
    "                                    for i in range(polynomial_size)]), dtype=t.float32), requires_grad=False)\n",
    "        self.forecast_basis = nn.Parameter(\n",
    "            t.tensor(np.concatenate([np.power(np.arange(forecast_size, dtype=np.float) / forecast_size, i)[None, :]\n",
    "                                    for i in range(polynomial_size)]), dtype=t.float32), requires_grad=False)\n",
    "\n",
    "    def forward(self, theta: t.Tensor, insample_x_t: t.Tensor, outsample_x_t: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "        cut_point = self.forecast_basis.shape[0]\n",
    "        backcast = t.einsum('bp,pt->bt', theta[:, cut_point:], self.backcast_basis)\n",
    "        forecast = t.einsum('bp,pt->bt', theta[:, :cut_point], self.forecast_basis)\n",
    "        return backcast, forecast\n",
    "\n",
    "class SeasonalityBasis(nn.Module):\n",
    "    def __init__(self, harmonics: int, backcast_size: int, forecast_size: int):\n",
    "        super().__init__()\n",
    "        frequency = np.append(np.zeros(1, dtype=np.float32),\n",
    "                                        np.arange(harmonics, harmonics / 2 * forecast_size,\n",
    "                                                    dtype=np.float32) / harmonics)[None, :]\n",
    "        backcast_grid = -2 * np.pi * (\n",
    "                np.arange(backcast_size, dtype=np.float32)[:, None] / forecast_size) * frequency\n",
    "        forecast_grid = 2 * np.pi * (\n",
    "                np.arange(forecast_size, dtype=np.float32)[:, None] / forecast_size) * frequency\n",
    "\n",
    "        backcast_cos_template = t.tensor(np.transpose(np.cos(backcast_grid)), dtype=t.float32)\n",
    "        backcast_sin_template = t.tensor(np.transpose(np.sin(backcast_grid)), dtype=t.float32)\n",
    "        backcast_template = t.cat([backcast_cos_template, backcast_sin_template], dim=0)\n",
    "\n",
    "        forecast_cos_template = t.tensor(np.transpose(np.cos(forecast_grid)), dtype=t.float32)\n",
    "        forecast_sin_template = t.tensor(np.transpose(np.sin(forecast_grid)), dtype=t.float32)\n",
    "        forecast_template = t.cat([forecast_cos_template, forecast_sin_template], dim=0)\n",
    "\n",
    "        self.backcast_basis = nn.Parameter(backcast_template, requires_grad=False)\n",
    "        self.forecast_basis = nn.Parameter(forecast_template, requires_grad=False)\n",
    "\n",
    "    def forward(self, theta: t.Tensor, insample_x_t: t.Tensor, outsample_x_t: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "        cut_point = self.forecast_basis.shape[0]\n",
    "        backcast = t.einsum('bp,pt->bt', theta[:, cut_point:], self.backcast_basis)\n",
    "        forecast = t.einsum('bp,pt->bt', theta[:, :cut_point], self.forecast_basis)\n",
    "        return backcast, forecast\n",
    "\n",
    "class ExogenousBasisInterpretable(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, theta: t.Tensor, insample_x_t: t.Tensor, outsample_x_t: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "        backcast_basis = insample_x_t\n",
    "        forecast_basis = outsample_x_t\n",
    "\n",
    "        cut_point = forecast_basis.shape[1]\n",
    "        backcast = t.einsum('bp,bpt->bt', theta[:, cut_point:], backcast_basis)\n",
    "        forecast = t.einsum('bp,bpt->bt', theta[:, :cut_point], forecast_basis)\n",
    "        return backcast, forecast\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "class ExogenousFutureBasis(nn.Module):\n",
    "    def __init__(self, out_features, f_idxs, num_levels = 4, kernel_size=3):\n",
    "        super().__init__()\n",
    "        # Shape of (1, in_features, 1) to broadcast over b and t\n",
    "        self.f_idxs = f_idxs\n",
    "        in_features = len(self.f_idxs)\n",
    "        #self.weight = nn.Parameter(t.Tensor(1, in_features, 1), requires_grad=True)\n",
    "        #nn.init.kaiming_uniform_(self.weight, a=math.sqrt(0.5))\n",
    "\n",
    "        padding = (kernel_size - 1) * (2**0)\n",
    "        input_layer = [nn.Conv1d(in_channels=in_features, out_channels=out_features,\n",
    "                                 kernel_size=kernel_size, padding=padding, dilation=2**0),\n",
    "                                 Chomp1d(padding),\n",
    "                                 nn.ReLU()]\n",
    "        conv_layers = []\n",
    "        for i in range(1, num_levels):\n",
    "            dilation = 2**i\n",
    "            padding = (kernel_size - 1) * dilation\n",
    "            conv_layers.append(nn.Conv1d(in_channels=out_features, out_channels=out_features,\n",
    "                                         padding=padding, kernel_size=3, dilation=dilation))\n",
    "            conv_layers.append(Chomp1d(padding))\n",
    "            conv_layers.append(nn.ReLU())\n",
    "        conv_layers = input_layer + conv_layers\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*conv_layers)\n",
    "\n",
    "    def transform(self, insample_x_t, outsample_x_t):\n",
    "\n",
    "        input_size = insample_x_t.shape[2]\n",
    "\n",
    "        x_t = t.cat([insample_x_t[:,self.f_idxs,:], outsample_x_t[:,self.f_idxs,:]], dim=2)\n",
    "\n",
    "        #x_t = x_t * self.weight # Element-wise multiplication, broadcasted on b and t.\n",
    "        x_t = self.conv_layers(x_t)[:]\n",
    "\n",
    "        backcast_basis = x_t[:,:, :input_size]\n",
    "        forecast_basis = x_t[:,:, input_size:]\n",
    "\n",
    "        return backcast_basis, forecast_basis\n",
    "\n",
    "    def forward(self, theta: t.Tensor, insample_x_t: t.Tensor, outsample_x_t: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "        backcast_basis, forecast_basis = self.transform(insample_x_t, outsample_x_t)\n",
    "\n",
    "        cut_point = forecast_basis.shape[1]\n",
    "        backcast = t.einsum('bp,bpt->bt', theta[:, cut_point:], backcast_basis)\n",
    "        forecast = t.einsum('bp,bpt->bt', theta[:, :cut_point], forecast_basis)\n",
    "        return backcast, forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}