{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.esrnn.esrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESRNN model\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "from nixtla.models.esrnn.utils.config import ModelConfig\n",
    "from nixtla.models.esrnn.utils.esrnn import _ESRNN\n",
    "from nixtla.models.esrnn.utils.losses import SmylLoss, PinballLoss\n",
    "from nixtla.models.esrnn.utils.data import Iterator\n",
    "from nixtla.models.esrnn.utils.evaluation import owa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ESRNN(object):\n",
    "    \"\"\" Exponential Smoothing Recurrent Neural Network\n",
    "\n",
    "    Pytorch Implementation of the M4 time series forecasting competition winner.\n",
    "    Proposed by Smyl. The model uses a hybrid approach of Machine Learning and\n",
    "    statistical methods by combining recurrent neural networks to model a common\n",
    "    trend with shared parameters across series, and multiplicative Holt-Winter\n",
    "    exponential smoothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_epochs: int\n",
    "        maximum number of complete passes to train data during fit\n",
    "    freq_of_test: int\n",
    "        period for the diagnostic evaluation of the model.\n",
    "    learning_rate: float\n",
    "        size of the stochastic gradient descent steps\n",
    "    lr_scheduler_step_size: int\n",
    "        this step_size is the period for each learning rate decay\n",
    "    per_series_lr_multip: float\n",
    "        multiplier for per-series parameters smoothing and initial\n",
    "        seasonalities learning rate (default 1.0)\n",
    "    gradient_eps: float\n",
    "        term added to the Adam optimizer denominator to improve\n",
    "        numerical stability (default: 1e-8)\n",
    "    gradient_clipping_threshold: float\n",
    "        max norm of gradient vector, with all parameters treated\n",
    "        as a single vector\n",
    "    rnn_weight_decay: float\n",
    "        parameter to control classic L2/Tikhonov regularization\n",
    "        of the rnn parameters\n",
    "    noise_std: float\n",
    "        standard deviation of white noise added to input during\n",
    "        fit to avoid the model from memorizing the train data\n",
    "    level_variability_penalty: float\n",
    "        this parameter controls the strength of the penalization\n",
    "        to the wigglines of the level vector, induces smoothness\n",
    "        in the output\n",
    "    testing_percentile: float\n",
    "        This value is only for diagnostic evaluation.\n",
    "        In case of percentile predictions this parameter controls\n",
    "        for the value predicted, when forecasting point value,\n",
    "        the forecast is the median, so percentile=50.\n",
    "    training_percentile: float\n",
    "        To reduce the model's tendency to over estimate, the\n",
    "        training_percentile can be set to fit a smaller value\n",
    "        through the Pinball Loss.\n",
    "    batch_size: int\n",
    "        number of training examples for the stochastic gradient steps\n",
    "    seasonality: int list\n",
    "        list of seasonalities of the time series\n",
    "        Hourly [24, 168], Daily [7], Weekly [52], Monthly [12],\n",
    "        Quarterly [4], Yearly [].\n",
    "    input_size: int\n",
    "        input size of the recurrent neural network, usually a\n",
    "        multiple of seasonality\n",
    "    output_size: int\n",
    "        output_size or forecast horizon of the recurrent neural\n",
    "        network, usually multiple of seasonality\n",
    "    random_seed: int\n",
    "        random_seed for pseudo random pytorch initializer and\n",
    "        numpy random generator\n",
    "    exogenous_size: int\n",
    "        size of one hot encoded categorical variable, invariannt\n",
    "        per time series of the panel\n",
    "    min_inp_seq_length: int\n",
    "        description\n",
    "    max_periods: int\n",
    "        Parameter to chop longer series, to last max_periods,\n",
    "        max e.g. 40 years\n",
    "    cell_type: str\n",
    "        Type of RNN cell, available GRU, LSTM, RNN, ResidualLSTM.\n",
    "    state_hsize: int\n",
    "        dimension of hidden state of the recurrent neural network\n",
    "    dilations: int list\n",
    "        each list represents one chunk of Dilated LSTMS, connected in\n",
    "        standard ResNet fashion\n",
    "    add_nl_layer: bool\n",
    "        whether to insert a tanh() layer between the RNN stack and the\n",
    "        linear adaptor (output) layers\n",
    "    device: str\n",
    "        pytorch device either 'cpu' or 'cuda'\n",
    "    Notes\n",
    "    -----\n",
    "    **References:**\n",
    "    `M4 Competition Conclusions\n",
    "    <https://rpubs.com/fotpetr/m4competition>`__\n",
    "    `Original Dynet Implementation of ESRNN\n",
    "    <https://github.com/M4Competition/M4-methods/tree/master/118%20-%20slaweks17>`__\n",
    "    \"\"\"\n",
    "    def __init__(self, max_epochs=15, batch_size=1, batch_size_test=64, freq_of_test=-1,\n",
    "                learning_rate=1e-3, lr_scheduler_step_size=9, lr_decay=0.9,\n",
    "                per_series_lr_multip=1.0, gradient_eps=1e-8, gradient_clipping_threshold=20,\n",
    "                rnn_weight_decay=0, noise_std=0.001,\n",
    "                level_variability_penalty=80,\n",
    "                testing_percentile=50, training_percentile=50, ensemble=False,\n",
    "                cell_type='LSTM',\n",
    "                state_hsize=40, dilations=[[1, 2], [4, 8]],\n",
    "                add_nl_layer=False, seasonality=[4], input_size=4, output_size=8,\n",
    "                frequency=None, max_periods=20, random_seed=1,\n",
    "                device='cpu', root_dir='./'):\n",
    "        super(ESRNN, self).__init__()\n",
    "        self.mc = ModelConfig(max_epochs=max_epochs, batch_size=batch_size, batch_size_test=batch_size_test,\n",
    "                            freq_of_test=freq_of_test, learning_rate=learning_rate,\n",
    "                            lr_scheduler_step_size=lr_scheduler_step_size, lr_decay=lr_decay,\n",
    "                            per_series_lr_multip=per_series_lr_multip,\n",
    "                            gradient_eps=gradient_eps, gradient_clipping_threshold=gradient_clipping_threshold,\n",
    "                            rnn_weight_decay=rnn_weight_decay, noise_std=noise_std,\n",
    "                            level_variability_penalty=level_variability_penalty,\n",
    "                            testing_percentile=testing_percentile, training_percentile=training_percentile,\n",
    "                            ensemble=ensemble,\n",
    "                            cell_type=cell_type,\n",
    "                            state_hsize=state_hsize, dilations=dilations, add_nl_layer=add_nl_layer,\n",
    "                            seasonality=seasonality, input_size=input_size, output_size=output_size,\n",
    "                            frequency=frequency, max_periods=max_periods, random_seed=random_seed,\n",
    "                            device=device, root_dir=root_dir)\n",
    "        self._fitted = False\n",
    "\n",
    "    def train(self, dataloader, max_epochs,\n",
    "                        warm_start=False, shuffle=True, verbose=True):\n",
    "        \"\"\"\n",
    "        Auxiliary function, pytorch train procedure for the ESRNN model\n",
    "\n",
    "        Parameters:\n",
    "        -------\n",
    "        dataloader: pytorch dataloader\n",
    "        max_epochs: int\n",
    "        warm_start: bool\n",
    "        shuffle: bool\n",
    "        verbose: bool\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.mc.ensemble:\n",
    "         self.esrnn_ensemble = [deepcopy(self.esrnn).to(self.mc.device)] * 5\n",
    "\n",
    "        if verbose: print(15*'='+' Training ESRNN  ' + 15*'=' + '\\n')\n",
    "\n",
    "        # Optimizers\n",
    "        if not warm_start:\n",
    "            self.es_optimizer = optim.Adam(params=self.esrnn.es.parameters(),\n",
    "                                            lr=self.mc.learning_rate*self.mc.per_series_lr_multip,\n",
    "                                            betas=(0.9, 0.999), eps=self.mc.gradient_eps)\n",
    "\n",
    "            self.es_scheduler = StepLR(optimizer=self.es_optimizer,\n",
    "                                        step_size=self.mc.lr_scheduler_step_size,\n",
    "                                        gamma=0.9)\n",
    "\n",
    "            self.rnn_optimizer = optim.Adam(params=self.esrnn.rnn.parameters(),\n",
    "                                            lr=self.mc.learning_rate,\n",
    "                                            betas=(0.9, 0.999), eps=self.mc.gradient_eps,\n",
    "                                            weight_decay=self.mc.rnn_weight_decay)\n",
    "\n",
    "            self.rnn_scheduler = StepLR(optimizer=self.rnn_optimizer,\n",
    "                                        step_size=self.mc.lr_scheduler_step_size,\n",
    "                                        gamma=self.mc.lr_decay)\n",
    "\n",
    "        # Loss Functions\n",
    "        train_tau = self.mc.training_percentile / 100\n",
    "        train_loss = SmylLoss(tau=train_tau,\n",
    "                              level_variability_penalty=self.mc.level_variability_penalty)\n",
    "\n",
    "        eval_tau = self.mc.testing_percentile / 100\n",
    "        eval_loss = PinballLoss(tau=eval_tau)\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            self.esrnn.train()\n",
    "            start = time.time()\n",
    "            if shuffle:\n",
    "                dataloader.shuffle_dataset(random_seed=epoch)\n",
    "            losses = []\n",
    "            for j in range(dataloader.n_batches):\n",
    "                self.es_optimizer.zero_grad()\n",
    "                self.rnn_optimizer.zero_grad()\n",
    "\n",
    "                batch = dataloader.get_batch()\n",
    "                windows_y, windows_y_hat, levels = self.esrnn(batch)\n",
    "\n",
    "                # Pinball loss on normalized values\n",
    "                loss = train_loss(windows_y, windows_y_hat, levels)\n",
    "                losses.append(loss.data.cpu().numpy())\n",
    "                #print(\"loss\", loss)\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(self.esrnn.rnn.parameters(),\n",
    "                                                self.mc.gradient_clipping_threshold)\n",
    "                torch.nn.utils.clip_grad_norm_(self.esrnn.es.parameters(),\n",
    "                                                self.mc.gradient_clipping_threshold)\n",
    "                self.rnn_optimizer.step()\n",
    "                self.es_optimizer.step()\n",
    "\n",
    "            # Decay learning rate\n",
    "            self.es_scheduler.step()\n",
    "            self.rnn_scheduler.step()\n",
    "\n",
    "            if self.mc.ensemble:\n",
    "                copy_esrnn = deepcopy(self.esrnn)\n",
    "                copy_esrnn.eval()\n",
    "                self.esrnn_ensemble.pop(0)\n",
    "                self.esrnn_ensemble.append(copy_esrnn)\n",
    "\n",
    "\n",
    "            # Evaluation\n",
    "            self.train_loss = np.mean(losses)\n",
    "            if verbose:\n",
    "                print(\"========= Epoch {} finished =========\".format(epoch))\n",
    "                print(\"Training time: {}\".format(round(time.time()-start, 5)))\n",
    "                print(\"Training loss ({} prc): {:.5f}\".format(self.mc.training_percentile,\n",
    "                                                              self.train_loss))\n",
    "\n",
    "            if (epoch % self.mc.freq_of_test == 0) and (self.mc.freq_of_test > 0):\n",
    "                if self.y_test_df is not None:\n",
    "                    self.test_loss = self.model_evaluation(dataloader, eval_loss)\n",
    "                    print(\"Testing loss  ({} prc): {:.5f}\".format(self.mc.testing_percentile,\n",
    "                                                                  self.test_loss))\n",
    "                    self.evaluate_model_prediction(self.y_train_df, self.X_test_df,\n",
    "                                                   self.y_test_df, self.y_hat_benchmark, epoch=epoch)\n",
    "                    self.esrnn.train()\n",
    "\n",
    "        if verbose: print('Train finished! \\n')\n",
    "\n",
    "    def per_series_evaluation(self, dataloader, criterion):\n",
    "        \"\"\"\n",
    "        Auxiliary function, evaluate ESRNN model for training\n",
    "        procedure supervision.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataloader: pytorch dataloader\n",
    "        criterion: pytorch test criterion\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Create fast dataloader\n",
    "            if self.mc.n_series < self.mc.batch_size_test: new_batch_size = self.mc.n_series\n",
    "            else: new_batch_size = self.mc.batch_size_test\n",
    "            dataloader.update_batch_size(new_batch_size)\n",
    "\n",
    "            per_series_losses = []\n",
    "            for j in range(dataloader.n_batches):\n",
    "                batch = dataloader.get_batch()\n",
    "                windows_y, windows_y_hat, _ = self.esrnn(batch)\n",
    "                loss = criterion(windows_y, windows_y_hat)\n",
    "                per_series_losses += loss.data.cpu().numpy().tolist()\n",
    "\n",
    "            dataloader.update_batch_size(self.mc.batch_size)\n",
    "        return per_series_losses\n",
    "\n",
    "    def model_evaluation(self, dataloader, criterion):\n",
    "        \"\"\"\n",
    "        Auxiliary function, evaluate ESRNN model for training\n",
    "        procedure supervision.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataloader: pytorch dataloader\n",
    "        criterion: pytorch test criterion\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model_loss: float\n",
    "            loss for train supervision purpose.\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Create fast dataloader\n",
    "            if self.mc.n_series < self.mc.batch_size_test: new_batch_size = self.mc.n_series\n",
    "            else: new_batch_size = self.mc.batch_size_test\n",
    "            dataloader.update_batch_size(new_batch_size)\n",
    "\n",
    "            model_loss = 0.0\n",
    "            for j in range(dataloader.n_batches):\n",
    "                batch = dataloader.get_batch()\n",
    "                windows_y, windows_y_hat, _ = self.esrnn(batch)\n",
    "                loss = criterion(windows_y, windows_y_hat)\n",
    "                model_loss += loss.data.cpu().numpy()\n",
    "\n",
    "            model_loss /= dataloader.n_batches\n",
    "            dataloader.update_batch_size(self.mc.batch_size)\n",
    "        return model_loss\n",
    "\n",
    "    def evaluate_model_prediction(self, y_train_df, X_test_df, y_test_df, y_hat_benchmark='y_hat_naive2', epoch=None):\n",
    "        \"\"\"\n",
    "        Evaluate ESRNN model against benchmark in y_test_df\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_train_df: pandas dataframe\n",
    "            panel with columns 'unique_id', 'ds', 'y'\n",
    "        X_test_df: pandas dataframe\n",
    "            panel with columns 'unique_id', 'ds', 'x'\n",
    "        y_test_df: pandas dataframe\n",
    "            panel with columns 'unique_id', 'ds', 'y' and a column\n",
    "            y_hat_benchmark identifying benchmark predictions\n",
    "        y_hat_benchmark: str\n",
    "            column name of benchmark predictions, default y_hat_naive2\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model_owa : float\n",
    "            relative improvement of model with respect to benchmark, measured with\n",
    "            the M4 overall weighted average.\n",
    "        smape: float\n",
    "            relative improvement of model with respect to benchmark, measured with\n",
    "            the symmetric mean absolute percentage error.\n",
    "        mase: float\n",
    "            relative improvement of model with respect to benchmark, measured with\n",
    "            the M4 mean absolute scaled error.\n",
    "        \"\"\"\n",
    "\n",
    "        assert self._fitted, \"Model not fitted yet\"\n",
    "\n",
    "        y_panel = y_test_df.filter(['unique_id', 'ds', 'y'])\n",
    "        y_benchmark_panel = y_test_df.filter(['unique_id', 'ds', y_hat_benchmark])\n",
    "        y_benchmark_panel.rename(columns={y_hat_benchmark: 'y_hat'}, inplace=True)\n",
    "        y_hat_panel = self.predict(X_test_df)\n",
    "        y_insample = y_train_df.filter(['unique_id', 'ds', 'y'])\n",
    "\n",
    "        model_owa, model_mase, model_smape = owa(y_panel, y_hat_panel,\n",
    "                                                 y_benchmark_panel, y_insample,\n",
    "                                                 seasonality=self.mc.naive_seasonality)\n",
    "\n",
    "        if self.min_owa > model_owa:\n",
    "            self.min_owa = model_owa\n",
    "            if epoch is not None:\n",
    "                self.min_epoch = epoch\n",
    "\n",
    "        print('OWA: {} '.format(np.round(model_owa, 3)))\n",
    "        print('SMAPE: {} '.format(np.round(model_smape, 3)))\n",
    "        print('MASE: {} '.format(np.round(model_mase, 3)))\n",
    "\n",
    "        return model_owa, model_mase, model_smape\n",
    "\n",
    "    def fit(self, X_df, y_df, X_test_df=None, y_test_df=None, y_hat_benchmark='y_hat_naive2',\n",
    "                    warm_start=False, shuffle=True, verbose=True):\n",
    "        \"\"\"\n",
    "        Fit ESRNN model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_df : pandas dataframe\n",
    "            Train dataframe in long format with columns 'unique_id', 'ds'\n",
    "            and 'x'.\n",
    "            - 'unique_id' an identifier of each independent time series.\n",
    "            - 'ds' is a datetime column\n",
    "            - 'x' is a single exogenous variable\n",
    "        y_df : pandas dataframe\n",
    "            Train dataframe in long format with columns 'unique_id', 'ds' and 'y'.\n",
    "            - 'unique_id' an identifier of each independent time series.\n",
    "            - 'ds' is a datetime column\n",
    "            - 'y' is the column with the target values\n",
    "        X_test_df: pandas dataframe\n",
    "            Optional test dataframe with columns 'unique_id', 'ds' and 'x'.\n",
    "            If provided the fit procedure will evaluate the intermediate\n",
    "            performance within training epochs.\n",
    "        y_test_df: pandas dataframe\n",
    "            Optional test dataframe with columns 'unique_id', 'ds' and 'x' and\n",
    "            y_hat_benchmark column.\n",
    "            If provided the fit procedure will evaluate the intermediate\n",
    "            performance within training epochs.\n",
    "        y_hat_benchmark: str\n",
    "            Name of the benchmark model for the comparison of the relative\n",
    "            improvement of the model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "\n",
    "        # Transform long dfs to wide numpy\n",
    "        assert type(X_df) == pd.core.frame.DataFrame\n",
    "        assert type(y_df) == pd.core.frame.DataFrame\n",
    "        assert all([(col in X_df) for col in ['unique_id', 'ds', 'x']])\n",
    "        assert all([(col in y_df) for col in ['unique_id', 'ds', 'y']])\n",
    "        if y_test_df is not None:\n",
    "                assert y_hat_benchmark in y_test_df.columns, 'benchmark is not present in y_test_df, use y_hat_benchmark to define it'\n",
    "\n",
    "        # Storing dfs for OWA evaluation, initializing min_owa\n",
    "        self.y_train_df = y_df\n",
    "        self.X_test_df = X_test_df\n",
    "        self.y_test_df = y_test_df\n",
    "        self.min_owa = 4.0\n",
    "        self.min_epoch = 0\n",
    "\n",
    "        self.int_ds = isinstance(self.y_train_df['ds'][0], (int, np.int, np.int64))\n",
    "\n",
    "        self.y_hat_benchmark = y_hat_benchmark\n",
    "\n",
    "        X, y = self.long_to_wide(X_df, y_df)\n",
    "        assert len(X)==len(y)\n",
    "        assert X.shape[1]>=3\n",
    "\n",
    "        # Exogenous variables\n",
    "        unique_categories = np.unique(X[:, 1])\n",
    "        self.mc.category_to_idx = dict((word, index) for index, word in enumerate(unique_categories))\n",
    "        exogenous_size = len(unique_categories)\n",
    "\n",
    "        # Create batches (device in mc)\n",
    "        self.train_dataloader = Iterator(mc=self.mc, X=X, y=y)\n",
    "\n",
    "        # Random Seeds (model initialization)\n",
    "        torch.manual_seed(self.mc.random_seed)\n",
    "        np.random.seed(self.mc.random_seed)\n",
    "\n",
    "        # Initialize model\n",
    "        n_series = self.train_dataloader.n_series\n",
    "        self.instantiate_esrnn(exogenous_size, n_series)\n",
    "\n",
    "        # Validating frequencies\n",
    "        X_train_frequency = pd.infer_freq(X_df.head()['ds'])\n",
    "        y_train_frequency = pd.infer_freq(y_df.head()['ds'])\n",
    "        self.frequencies = [X_train_frequency, y_train_frequency]\n",
    "\n",
    "        if (X_test_df is not None) and (y_test_df is not None):\n",
    "                X_test_frequency = pd.infer_freq(X_test_df.head()['ds'])\n",
    "                y_test_frequency = pd.infer_freq(y_test_df.head()['ds'])\n",
    "                self.frequencies += [X_test_frequency, y_test_frequency]\n",
    "\n",
    "        assert len(set(self.frequencies)) <= 1, \\\n",
    "            \"Match the frequencies of the dataframes {}\".format(self.frequencies)\n",
    "\n",
    "        self.mc.frequency = self.frequencies[0]\n",
    "        print(\"Infered frequency: {}\".format(self.mc.frequency))\n",
    "\n",
    "        # Train model\n",
    "        self._fitted = True\n",
    "        self.train(dataloader=self.train_dataloader, max_epochs=self.mc.max_epochs,\n",
    "                             warm_start=warm_start, shuffle=shuffle, verbose=verbose)\n",
    "\n",
    "    def instantiate_esrnn(self, exogenous_size, n_series):\n",
    "        \"\"\"Auxiliary function used at beginning of train to instantiate ESRNN\"\"\"\n",
    "\n",
    "        self.mc.exogenous_size = exogenous_size\n",
    "        self.mc.n_series = n_series\n",
    "        self.esrnn = _ESRNN(self.mc).to(self.mc.device)\n",
    "\n",
    "    def predict(self, X_df, decomposition=False):\n",
    "        \"\"\"\n",
    "        Predict using the ESRNN model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_df : pandas dataframe\n",
    "            Dataframe in LONG format with columns 'unique_id', 'ds'\n",
    "            and 'x'.\n",
    "            - 'unique_id' an identifier of each independent time series.\n",
    "            - 'ds' is a datetime column\n",
    "            - 'x' is a single exogenous variable\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Y_hat_panel : pandas dataframe\n",
    "            Dataframe in LONG format with columns 'unique_id', 'ds'\n",
    "            and 'x'.\n",
    "            - 'unique_id' an identifier of each independent time series.\n",
    "            - 'ds' datetime columnn that matches the dates in X_df\n",
    "            - 'y_hat' is the column with the predicted target values\n",
    "        \"\"\"\n",
    "\n",
    "        #print(9*'='+' Predicting ESRNN ' + 9*'=' + '\\n')\n",
    "        assert type(X_df) == pd.core.frame.DataFrame\n",
    "        assert 'unique_id' in X_df\n",
    "        assert self._fitted, \"Model not fitted yet\"\n",
    "\n",
    "        self.esrnn.eval()\n",
    "\n",
    "        # Create fast dataloader\n",
    "        if self.mc.n_series < self.mc.batch_size_test: new_batch_size = self.mc.n_series\n",
    "        else: new_batch_size = self.mc.batch_size_test\n",
    "        self.train_dataloader.update_batch_size(new_batch_size)\n",
    "        dataloader = self.train_dataloader\n",
    "\n",
    "        # Create Y_hat_panel placeholders\n",
    "        output_size = self.mc.output_size\n",
    "        n_unique_id = len(dataloader.sort_key['unique_id'])\n",
    "        panel_unique_id = pd.Series(dataloader.sort_key['unique_id']).repeat(output_size)\n",
    "\n",
    "        #access column with last train date\n",
    "        panel_last_ds = pd.Series(dataloader.X[:, 2])\n",
    "        panel_ds = []\n",
    "        for i in range(len(panel_last_ds)):\n",
    "            ranges = pd.date_range(start=panel_last_ds[i], periods=output_size+1, freq=self.mc.frequency)\n",
    "            panel_ds += list(ranges[1:])\n",
    "\n",
    "        panel_y_hat= np.zeros((output_size * n_unique_id))\n",
    "\n",
    "        # Predict\n",
    "        count = 0\n",
    "        for j in range(dataloader.n_batches):\n",
    "            batch = dataloader.get_batch()\n",
    "            batch_size = batch.y.shape[0]\n",
    "\n",
    "            if self.mc.ensemble:\n",
    "                y_hat = torch.zeros((5,batch_size,output_size))\n",
    "                for i in range(5):\n",
    "                    y_hat[i,:,:] = self.esrnn_ensemble[i].predict(batch)\n",
    "                y_hat = torch.mean(y_hat,0)\n",
    "            else:\n",
    "                y_hat = self.esrnn.predict(batch)\n",
    "\n",
    "            y_hat = y_hat.data.cpu().numpy()\n",
    "\n",
    "            panel_y_hat[count:count+output_size*batch_size] = y_hat.flatten()\n",
    "            count += output_size*batch_size\n",
    "\n",
    "        Y_hat_panel_dict = {'unique_id': panel_unique_id,\n",
    "                                                'ds': panel_ds,\n",
    "                                                'y_hat': panel_y_hat}\n",
    "\n",
    "        assert len(panel_ds) == len(panel_y_hat) == len(panel_unique_id)\n",
    "\n",
    "        Y_hat_panel = pd.DataFrame.from_dict(Y_hat_panel_dict)\n",
    "\n",
    "        if 'ds' in X_df:\n",
    "            Y_hat_panel = X_df.merge(Y_hat_panel, on=['unique_id', 'ds'], how='left')\n",
    "        else:\n",
    "            Y_hat_panel = X_df.merge(Y_hat_panel, on=['unique_id'], how='left')\n",
    "\n",
    "        self.train_dataloader.update_batch_size(self.mc.batch_size)\n",
    "        return Y_hat_panel\n",
    "\n",
    "    def long_to_wide(self, X_df, y_df):\n",
    "        \"\"\"\n",
    "        Auxiliary function to wrangle LONG format dataframes\n",
    "        to a wide format compatible with ESRNN inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_df : pandas dataframe\n",
    "            Dataframe in long format with columns 'unique_id', 'ds'\n",
    "            and 'x'.\n",
    "            - 'unique_id' an identifier of each independent time series.\n",
    "            - 'ds' is a datetime column\n",
    "            - 'x' is a single exogenous variable\n",
    "        y_df : pandas dataframe\n",
    "            Dataframe in long format with columns 'unique_id', 'ds' and 'y'.\n",
    "            - 'unique_id' an identifier of each independent time series.\n",
    "            - 'ds' is a datetime column\n",
    "            - 'y' is the column with the target values\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X: numpy array, shape (n_unique_ids, n_time)\n",
    "        y: numpy array, shape (n_unique_ids, n_time)\n",
    "        \"\"\"\n",
    "        data = X_df.copy()\n",
    "        data['y'] = y_df['y'].copy()\n",
    "        sorted_ds = np.sort(data['ds'].unique())\n",
    "        ds_map = {}\n",
    "        for dmap, t in enumerate(sorted_ds):\n",
    "                ds_map[t] = dmap\n",
    "        data['ds_map'] = data['ds'].map(ds_map)\n",
    "        data = data.sort_values(by=['ds_map','unique_id'])\n",
    "        df_wide = data.pivot(index='unique_id', columns='ds_map')['y']\n",
    "\n",
    "        x_unique = data[['unique_id', 'x']].groupby('unique_id').first()\n",
    "        last_ds =  data[['unique_id', 'ds']].groupby('unique_id').last()\n",
    "        assert len(x_unique)==len(data.unique_id.unique())\n",
    "        df_wide['x'] = x_unique\n",
    "        df_wide['last_ds'] = last_ds\n",
    "        df_wide = df_wide.reset_index().rename_axis(None, axis=1)\n",
    "\n",
    "        ds_cols = data.ds_map.unique().tolist()\n",
    "        X = df_wide.filter(items=['unique_id', 'x', 'last_ds']).values\n",
    "        y = df_wide.filter(items=ds_cols).values\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def get_dir_name(self, root_dir=None):\n",
    "        \"\"\"Auxiliary function to save ESRNN model\"\"\"\n",
    "        if not root_dir:\n",
    "            assert self.mc.root_dir\n",
    "            root_dir = self.mc.root_dir\n",
    "\n",
    "        data_dir = self.mc.dataset_name\n",
    "        model_parent_dir = os.path.join(root_dir, data_dir)\n",
    "        model_path = ['esrnn_{}'.format(str(self.mc.copy))]\n",
    "        model_dir = os.path.join(model_parent_dir, '_'.join(model_path))\n",
    "        return model_dir\n",
    "\n",
    "    def save(self, model_dir=None, copy=None):\n",
    "        \"\"\"Auxiliary function to save ESRNN model\"\"\"\n",
    "        if copy is not None:\n",
    "                self.mc.copy = copy\n",
    "\n",
    "        if not model_dir:\n",
    "            assert self.mc.root_dir\n",
    "            model_dir = self.get_dir_name()\n",
    "\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        rnn_filepath = os.path.join(model_dir, \"rnn.model\")\n",
    "        es_filepath = os.path.join(model_dir, \"es.model\")\n",
    "\n",
    "        print('Saving model to:\\n {}'.format(model_dir)+'\\n')\n",
    "        torch.save({'model_state_dict': self.es.state_dict()}, es_filepath)\n",
    "        torch.save({'model_state_dict': self.rnn.state_dict()}, rnn_filepath)\n",
    "\n",
    "    def load(self, model_dir=None, copy=None):\n",
    "        \"\"\"Auxiliary function to load ESRNN model\"\"\"\n",
    "        if copy is not None:\n",
    "            self.mc.copy = copy\n",
    "\n",
    "        if not model_dir:\n",
    "            assert self.mc.root_dir\n",
    "            model_dir = self.get_dir_name()\n",
    "\n",
    "        rnn_filepath = os.path.join(model_dir, \"rnn.model\")\n",
    "        es_filepath = os.path.join(model_dir, \"es.model\")\n",
    "        path = Path(es_filepath)\n",
    "\n",
    "        if path.is_file():\n",
    "            print('Loading model from:\\n {}'.format(model_dir)+'\\n')\n",
    "\n",
    "            checkpoint = torch.load(es_filepath, map_location=self.mc.device)\n",
    "            self.es.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.es.to(self.mc.device)\n",
    "\n",
    "            checkpoint = torch.load(rnn_filepath, map_location=self.mc.device)\n",
    "            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.rnn.to(self.mc.device)\n",
    "        else:\n",
    "            print('Model path {} does not exist'.format(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nixtla",
   "language": "python",
   "name": "nixtla"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
