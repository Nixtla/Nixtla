{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.tsdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from fastcore.foundation import patch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# TODO: resolver t_cols y X_cols duplicados t_cols se usa en dataloader X_cols para indexar con f_cols\n",
    "#.      idea mantenemos solo X_cols y en el dataloader corregimos con 'y' y 'insample_mask' \n",
    "# TODO: paralelizar y mejorar _df_to_lists, probablemente Pool de multiprocessing\n",
    "#.      si est√° balanceado el panel np reshape hace el truco <- pensar\n",
    "# TODO: definir defaults para sample_mask, calculo de availabitly al interior con ds\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 Y_df: pd.DataFrame,\n",
    "                 X_df: pd.DataFrame=None,\n",
    "                 S_df: pd.DataFrame=None,\n",
    "                 mask_df: pd.DataFrame=None,\n",
    "                 ds_in_test: int=0,\n",
    "                 is_test: bool=False,\n",
    "                 f_cols: list=None,\n",
    "                 verbose: bool=False):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        assert type(Y_df) == pd.core.frame.DataFrame\n",
    "        assert all([(col in Y_df) for col in ['unique_id', 'ds', 'y']])\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if X_df is not None:\n",
    "            assert type(X_df) == pd.core.frame.DataFrame\n",
    "            assert all([(col in X_df) for col in ['unique_id', 'ds']])\n",
    "            assert len(Y_df)==len(X_df), f'The dimensions of Y_df and X_df are not the same'\n",
    "\n",
    "        if mask_df is not None:\n",
    "            assert len(Y_df)==len(mask_df), f'The dimensions of Y_df and mask_df are not the same'\n",
    "            assert all([(col in mask_df) for col in ['unique_id', 'ds', 'sample_mask']])\n",
    "            if 'available_mask' not in mask_df.columns:\n",
    "                self.verbose: print('Available mask not provided, defaulted with 1s.')\n",
    "                mask_df['available_mask'] = 1\n",
    "            assert np.sum(np.isnan(mask_df.available_mask.values))==0\n",
    "            assert np.sum(np.isnan(mask_df.sample_mask.values))==0\n",
    "        else:\n",
    "            mask_df = self.get_default_mask_df(Y_df=Y_df, is_test=is_test,\n",
    "                                               ds_in_test=ds_in_test)\n",
    "\n",
    "        if self.verbose: print(\"Train Validation splits\")\n",
    "        mask_df['train_mask'] = mask_df['available_mask'] * mask_df['sample_mask']\n",
    "        self.n_ds  = len(mask_df)\n",
    "        self.n_avl = mask_df.available_mask.sum()        \n",
    "        self.n_ins = mask_df.sample_mask.sum()\n",
    "        self.n_out = len(mask_df)-mask_df.sample_mask.sum()\n",
    "\n",
    "        avl_prc = np.round((100*self.n_avl)/self.n_ds,2)\n",
    "        ins_prc = np.round((100*self.n_ins)/self.n_ds,2)\n",
    "        out_prc = np.round((100*self.n_out)/self.n_ds,2)\n",
    "        if self.verbose:\n",
    "            print(mask_df.groupby(['unique_id', 'sample_mask']).agg({'ds': ['min', 'max']}))\n",
    "            print(f'Total data \\t\\t\\t{self.n_ds} time stamps')\n",
    "            print(f'Available percentage={avl_prc}, \\t{self.n_avl} time stamps')\n",
    "            print(f'Insample  percentage={ins_prc}, \\t{self.n_ins} time stamps')\n",
    "            print(f'Outsample percentage={out_prc}, \\t{self.n_out} time stamps')\n",
    "            print('\\n')\n",
    "\n",
    "        ts_data, s_data, self.meta_data, self.t_cols, self.X_cols \\\n",
    "                         = self._df_to_lists(Y_df=Y_df, S_df=S_df, X_df=X_df, mask_df=mask_df)\n",
    "\n",
    "        # Dataset attributes\n",
    "        self.n_series   = len(ts_data)\n",
    "        self.max_len    = max([len(ts['y']) for ts in ts_data])\n",
    "        self.n_channels = len(self.t_cols) # y, X_cols, insample_mask and outsample_mask\n",
    "        self.frequency  = pd.infer_freq(Y_df.head()['ds']) #TODO: improve, can die with head\n",
    "        self.f_cols     = f_cols\n",
    "\n",
    "        # Number of X and S features\n",
    "        self.n_x = 0 if X_df is None else len(self.X_cols)\n",
    "        self.n_s = 0 if S_df is None else S_df.shape[1]-1 # -1 for unique_id\n",
    "\n",
    "        # print('Creating ts tensor ...')\n",
    "        # Balances panel and creates \n",
    "        # numpy  s_matrix of shape (n_series, n_s)\n",
    "        # numpy ts_tensor of shape (n_series, n_channels, max_len) n_channels = y + X_cols + masks\n",
    "        self.ts_tensor, self.s_matrix, self.len_series = self._create_tensor(ts_data, s_data)\n",
    "\n",
    "    def get_default_mask_df(self, Y_df, ds_in_test, is_test):\n",
    "        # Creates outsample_mask\n",
    "        # train 1 validation 0\n",
    "        last_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "        last_df.sort_values(by=['unique_id', 'ds'], inplace=True, ascending=False)\n",
    "        last_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        last_df = last_df.groupby('unique_id').head(ds_in_test)\n",
    "        last_df['sample_mask'] = 0\n",
    "\n",
    "        last_df = last_df[['unique_id', 'ds', 'sample_mask']]\n",
    "\n",
    "        mask_df = Y_df.merge(last_df, on=['unique_id', 'ds'], how='left')\n",
    "        mask_df['sample_mask'] = mask_df['sample_mask'].fillna(1)\n",
    "\n",
    "        mask_df = mask_df[['unique_id', 'ds', 'sample_mask']]\n",
    "        mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "        mask_df['available_mask'] = 1\n",
    "\n",
    "        assert len(mask_df)==len(Y_df), \\\n",
    "            f'The mask_df length {len(mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "        \n",
    "        if is_test:\n",
    "            mask_df['sample_mask'] = 1 - mask_df['sample_mask']\n",
    "\n",
    "        return mask_df\n",
    "\n",
    "    def _df_to_lists(self, Y_df, S_df, X_df, mask_df):\n",
    "        \"\"\" TODO: Comment on unbalanced panels\n",
    "        \"\"\"\n",
    "        unique_ids = Y_df['unique_id'].unique()\n",
    "\n",
    "        if X_df is not None:\n",
    "            X_cols = [col for col in X_df.columns if col not in ['unique_id','ds']]\n",
    "        else:\n",
    "            X_cols = []\n",
    "\n",
    "        if S_df is not None:\n",
    "            S_cols = [col for col in S_df.columns if col not in ['unique_id']]\n",
    "        else:\n",
    "            S_cols = []\n",
    "\n",
    "        ts_data = []\n",
    "        s_data = []\n",
    "        meta_data = []\n",
    "        for i, u_id in enumerate(unique_ids):\n",
    "            #top_row    = np.asscalar(Y_df['unique_id'].searchsorted(u_id, 'left'))\n",
    "            #bottom_row = np.asscalar(Y_df['unique_id'].searchsorted(u_id, 'right'))\n",
    "            top_row    = Y_df['unique_id'].searchsorted(u_id, 'left').item()\n",
    "            bottom_row = Y_df['unique_id'].searchsorted(u_id, 'right').item()\n",
    "            \n",
    "            # Y values\n",
    "            y_true = Y_df[top_row:bottom_row]['y'].values\n",
    "            ts_data_i = {'y': y_true}\n",
    "            \n",
    "            # X values\n",
    "            for X_col in X_cols:\n",
    "                serie =  X_df[top_row:bottom_row][X_col].values\n",
    "                ts_data_i[X_col] = serie\n",
    "\n",
    "            # Mask values\n",
    "            available_mask = mask_df[top_row:bottom_row]['available_mask'].values\n",
    "            sample_mask = mask_df[top_row:bottom_row]['sample_mask'].values            \n",
    "            ts_data_i['available_mask'] = available_mask\n",
    "            ts_data_i['sample_mask']  = sample_mask\n",
    "            ts_data.append(ts_data_i)\n",
    "\n",
    "            # S values\n",
    "            s_data_i = defaultdict(list)\n",
    "            for S_col in S_cols:\n",
    "                s_data_i[S_col] = S_df.loc[S_df['unique_id']==u_id, S_col].values\n",
    "            s_data.append(s_data_i)\n",
    "\n",
    "            # Metadata\n",
    "            last_ds_i  = Y_df[top_row:bottom_row]['ds']\n",
    "            meta_data_i = {'unique_id': u_id,\n",
    "                           'last_ds': last_ds_i}\n",
    "            meta_data.append(meta_data_i)\n",
    "\n",
    "        t_cols = ['y'] + X_cols + ['available_mask', 'sample_mask']\n",
    "\n",
    "        return ts_data, s_data, meta_data, t_cols, X_cols\n",
    "\n",
    "    def _create_tensor(self, ts_data, s_data):\n",
    "        \"\"\"\n",
    "        s_matrix of shape (n_series, n_s)\n",
    "        ts_tensor of shape (n_series, n_channels, max_len) n_channels = y + X_cols + masks\n",
    "        \"\"\"\n",
    "        s_matrix  = np.zeros((self.n_series, self.n_s))\n",
    "        ts_tensor = np.zeros((self.n_series, self.n_channels, self.max_len))\n",
    "\n",
    "        len_series = []\n",
    "        for idx in range(self.n_series):\n",
    "            # Left padded time series tensor\n",
    "            # TODO: Maybe we can place according to ds\n",
    "            ts_idx = np.array(list(ts_data[idx].values()))\n",
    "\n",
    "            ts_tensor[idx, :, -ts_idx.shape[1]:] = ts_idx\n",
    "            s_matrix[idx, :] = list(s_data[idx].values())\n",
    "            len_series.append(ts_idx.shape[1])\n",
    "        \n",
    "        return ts_tensor, s_matrix, np.array(len_series)\n",
    "\n",
    "    def get_meta_data_col(self, col):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        col_values = [x[col] for x in self.meta_data]\n",
    "        return col_values\n",
    "\n",
    "    def get_filtered_ts_tensor(self, offset, output_size, window_sampling_limit, ts_idxs=None):\n",
    "        \"\"\"\n",
    "        Esto te da todo lo que tenga el tensor, el futuro incluido esto orque se usa exogenoas del futuro\n",
    "        La mascara se hace despues\n",
    "        \"\"\"\n",
    "        last_outsample_ds = self.max_len - offset + output_size\n",
    "        first_ds = max(last_outsample_ds - window_sampling_limit - output_size, 0)\n",
    "        if ts_idxs is None:\n",
    "            filtered_ts_tensor = self.ts_tensor[:, :, first_ds:last_outsample_ds]\n",
    "        else:\n",
    "            filtered_ts_tensor = self.ts_tensor[ts_idxs, :, first_ds:last_outsample_ds]\n",
    "        right_padding = max(last_outsample_ds - self.max_len, 0) #To padd with zeros if there is \"nothing\" to the right\n",
    "\n",
    "        #assert np.sum(np.isnan(filtered_ts_tensor))<1.0, \\\n",
    "        #   f'The balanced balanced filtered_tensor has {np.sum(np.isnan(filtered_ts_tensor))} nan values'\n",
    "        \n",
    "        return filtered_ts_tensor, right_padding #ANTES, ts_train_mask\n",
    "\n",
    "    def get_f_idxs(self, cols):\n",
    "        # Check if cols are available f_cols and return the idxs\n",
    "        assert all(col in self.f_cols for col in cols), f'Some variables in {cols} are not available in f_cols.'\n",
    "        f_idxs = [self.X_cols.index(col) for col in cols]\n",
    "        return f_idxs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MASK example and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_default_mask_df(Y_df, ds_in_test, is_test):\n",
    "    # Creates outsample_mask\n",
    "    # train 1 validation 0\n",
    "    last_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    last_df.sort_values(by=['unique_id', 'ds'], inplace=True, ascending=False)\n",
    "    last_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    last_df = last_df.groupby('unique_id').head(ds_in_test)\n",
    "    last_df['sample_mask'] = 0\n",
    "\n",
    "    last_df = last_df[['unique_id', 'ds', 'sample_mask']]\n",
    "\n",
    "    mask_df = Y_df.merge(last_df, on=['unique_id', 'ds'], how='left')\n",
    "    mask_df['sample_mask'] = mask_df['sample_mask'].fillna(1)\n",
    "\n",
    "    mask_df = mask_df[['unique_id', 'ds', 'sample_mask']]\n",
    "    mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "    mask_df['available_mask'] = 1\n",
    "\n",
    "    assert len(mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "\n",
    "    if is_test:\n",
    "        mask_df['sample_mask'] = 1 - mask_df['sample_mask']\n",
    "\n",
    "    return mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f05133f8a50>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "\n",
    "args = pd.Series({'dataset': ['NP']})\n",
    "\n",
    "Y_df, Xt_df, S_df = EPF.load_groups(directory='../data', groups=args.dataset)\n",
    "\n",
    "mask_df = get_default_mask_df(Y_df=Y_df, ds_in_test=728*24, is_test=False)\n",
    "\n",
    "plt.plot(mask_df.sample_mask.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zfsauton2/home/kdgutier/anaconda3/envs/nixtla/lib/python3.7/site-packages/ipykernel_launcher.py:176: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.\n"
     ]
    }
   ],
   "source": [
    "dataset = TimeSeriesDataset(Y_df=Y_df, S_df=S_df, X_df=Xt_df, mask_df=mask_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y',\n",
       " 'Exogenous1',\n",
       " 'Exogenous2',\n",
       " 'week_day',\n",
       " 'day_0',\n",
       " 'day_1',\n",
       " 'day_2',\n",
       " 'day_3',\n",
       " 'day_4',\n",
       " 'day_5',\n",
       " 'day_6',\n",
       " 'available_mask',\n",
       " 'sample_mask']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.t_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.05 30.47 28.92 ... 49.09 49.02 48.1 ]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.ts_tensor[0, dataset.t_cols.index('y'), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.ts_tensor.shape (1, 13, 52416)\n"
     ]
    }
   ],
   "source": [
    "print(\"dataset.ts_tensor.shape\", dataset.ts_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered, right_padding = dataset.get_filtered_ts_tensor(offset=10, output_size=12, window_sampling_limit=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nixtla",
   "language": "python",
   "name": "nixtla"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
