{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.tsdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch as t\n",
    "\n",
    "from fastcore.foundation import patch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# TODO: resolver t_cols y X_cols duplicados t_cols se usa en dataloader X_cols para indexar con f_cols\n",
    "#.      idea mantenemos solo X_cols y en el dataloader corregimos con 'y' y 'insample_mask' \n",
    "# TODO: paralelizar y mejorar _df_to_lists, probablemente Pool de multiprocessing\n",
    "#.      si est√° balanceado el panel np reshape hace el truco <- pensar\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 Y_df: pd.DataFrame,\n",
    "                 X_df: pd.DataFrame=None,\n",
    "                 S_df: pd.DataFrame=None,\n",
    "                 mask_df: list=None,\n",
    "                 f_cols: list=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        assert type(Y_df) == pd.core.frame.DataFrame\n",
    "        assert all([(col in Y_df) for col in ['unique_id', 'ds', 'y']])\n",
    "        if X_df is not None:\n",
    "            assert type(X_df) == pd.core.frame.DataFrame\n",
    "            assert all([(col in X_df) for col in ['unique_id', 'ds']])\n",
    "\n",
    "        print('Processing dataframes ...')\n",
    "        # Pandas dataframes to data lists\n",
    "        ts_data, s_data, self.meta_data, self.t_cols, self.X_cols \\\n",
    "                         = self._df_to_lists(Y_df=Y_df, S_df=S_df, X_df=X_df, mask_df=mask_df)\n",
    "\n",
    "        # Dataset attributes\n",
    "        self.n_series   = len(ts_data)\n",
    "        self.max_len    = max([len(ts['y']) for ts in ts_data])\n",
    "        self.n_channels = len(self.t_cols) # y, X_cols, insample_mask and outsample_mask\n",
    "        self.frequency  = pd.infer_freq(Y_df.head()['ds']) #TODO: improve, can die with head\n",
    "        self.f_cols     = f_cols\n",
    "\n",
    "        # Number of X and S features\n",
    "        self.n_x = 0 if X_df is None else len(self.X_cols)\n",
    "        self.n_s = 0 if S_df is None else S_df.shape[1]-1 # -1 for unique_id\n",
    "\n",
    "        print('Creating ts tensor ...')\n",
    "        # Balances panel and creates \n",
    "        # numpy  s_matrix of shape (n_series, n_s)\n",
    "        # numpy ts_tensor of shape (n_series, n_channels, max_len) n_channels = y + X_cols + masks\n",
    "        self.ts_tensor, self.s_matrix, self.len_series = self._create_tensor(ts_data, s_data)\n",
    "\n",
    "    def _df_to_lists(self, Y_df, S_df, X_df, mask_df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        unique_ids = Y_df['unique_id'].unique()\n",
    "\n",
    "        if X_df is not None:\n",
    "            X_cols = [col for col in X_df.columns if col not in ['unique_id','ds']]\n",
    "        else:\n",
    "            X_cols = []\n",
    "\n",
    "        if S_df is not None:\n",
    "            S_cols = [col for col in S_df.columns if col not in ['unique_id']]\n",
    "        else:\n",
    "            S_cols = []\n",
    "\n",
    "        ts_data = []\n",
    "        s_data = []\n",
    "        meta_data = []\n",
    "        for i, u_id in enumerate(unique_ids):\n",
    "            top_row    = np.asscalar(Y_df['unique_id'].searchsorted(u_id, 'left'))\n",
    "            bottom_row = np.asscalar(Y_df['unique_id'].searchsorted(u_id, 'right'))\n",
    "            \n",
    "            # Y values\n",
    "            y_true = Y_df[top_row:bottom_row]['y'].values\n",
    "            ts_data_i = {'y': y_true}\n",
    "            \n",
    "            # X values\n",
    "            for X_col in X_cols:\n",
    "                serie =  X_df[top_row:bottom_row][X_col].values\n",
    "                ts_data_i[X_col] = serie\n",
    "\n",
    "            # Mask values\n",
    "            outsample_mask = mask_df[top_row:bottom_row]['mask'].values\n",
    "            ts_data_i['insample_mask']  = np.ones(len(y_true))\n",
    "            ts_data_i['outsample_mask'] = outsample_mask\n",
    "            ts_data.append(ts_data_i)\n",
    "\n",
    "            # S values\n",
    "            s_data_i = defaultdict(list)\n",
    "            for S_col in S_cols:\n",
    "                s_data_i[S_col] = S_df.loc[S_df['unique_id']==u_id, S_col].values\n",
    "            s_data.append(s_data_i)\n",
    "\n",
    "            # Metadata\n",
    "            last_ds_i  = Y_df[top_row:bottom_row]['ds'].max()\n",
    "            meta_data_i = {'unique_id': u_id,\n",
    "                           'last_ds': last_ds_i}\n",
    "            meta_data.append(meta_data_i)\n",
    "\n",
    "        t_cols = ['y'] + X_cols + ['insample_mask', 'outsample_mask']\n",
    "\n",
    "        return ts_data, s_data, meta_data, t_cols, X_cols\n",
    "\n",
    "    def _create_tensor(self, ts_data, s_data):\n",
    "        \"\"\"\n",
    "        s_matrix of shape (n_series, n_s)\n",
    "        ts_tensor of shape (n_series, n_channels, max_len) n_channels = y + X_cols + masks\n",
    "        \"\"\"\n",
    "        s_matrix  = np.zeros((self.n_series, self.n_s))\n",
    "        ts_tensor = np.zeros((self.n_series, self.n_channels, self.max_len))\n",
    "\n",
    "        len_series = []\n",
    "        for idx in range(self.n_series):\n",
    "            # Left padded time series tensor\n",
    "            # TODO: Maybe we can place according to ds\n",
    "            ts_idx = np.array(list(ts_data[idx].values()))\n",
    "\n",
    "            # ANTES\n",
    "            #ts_tensor[idx, :self.t_cols.index('outsample_mask'), -ts_idx.shape[1]:] = ts_idx\n",
    "            #ts_tensor[idx,  self.t_cols.index('insample_mask'), -ts_idx.shape[1]:] = 1\n",
    "            \n",
    "            # To avoid sampling windows without inputs available to predict we shift -1\n",
    "            # outsample_mask will be completed with the train_mask, this ensures available data\n",
    "            #ts_tensor[idx,  self.t_cols.index('outsample_mask'), -(ts_idx.shape[1]-1):] = 1\n",
    "\n",
    "            # AHORA\n",
    "            ts_tensor[idx, :, -ts_idx.shape[1]:] = ts_idx\n",
    "            s_matrix[idx, :] = list(s_data[idx].values())\n",
    "            len_series.append(ts_idx.shape[1])\n",
    "\n",
    "        return ts_tensor, s_matrix, np.array(len_series)\n",
    "\n",
    "    def get_meta_data_col(self, col):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        col_values = [x[col] for x in self.meta_data]\n",
    "        return col_values\n",
    "\n",
    "    def get_filtered_ts_tensor(self, offset, output_size, window_sampling_limit, ts_idxs=None):\n",
    "        \"\"\"\n",
    "        Esto te da todo lo que tenga el tensor, el futuro incluido esto orque se usa exogenoas del futuro\n",
    "        La mascara se hace despues\n",
    "        \"\"\"\n",
    "        last_outsample_ds = self.max_len - offset + output_size\n",
    "        first_ds = max(last_outsample_ds - window_sampling_limit - output_size, 0)\n",
    "        if ts_idxs is None:\n",
    "            filtered_ts_tensor = self.ts_tensor[:, :, first_ds:last_outsample_ds]\n",
    "        else:\n",
    "            filtered_ts_tensor = self.ts_tensor[ts_idxs, :, first_ds:last_outsample_ds]\n",
    "        right_padding = max(last_outsample_ds - self.max_len, 0) #To padd with zeros if there is \"nothing\" to the right\n",
    "        \n",
    "        # ANTES\n",
    "        #ts_train_mask = self.ts_train_mask[ts_idxs, first_ds:last_outsample_ds]\n",
    "\n",
    "        assert np.sum(np.isnan(filtered_ts_tensor))<1.0, \\\n",
    "            f'The balanced balanced filtered_tensor has {np.sum(np.isnan(filtered_ts_tensor))} nan values'\n",
    "        return filtered_ts_tensor, right_padding #ANTES, ts_train_mask\n",
    "\n",
    "    def get_f_idxs(self, cols):\n",
    "        # Check if cols are available f_cols and return the idxs\n",
    "        assert all(col in self.f_cols for col in cols), f'Some variables in {cols} are not available in f_cols.'\n",
    "        f_idxs = [self.X_cols.index(col) for col in cols]\n",
    "        return f_idxs\n"
   ]
  },
  {
   "source": [
    "# MASK example and test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X: time series features, of shape (#hours, #times,#features): \t(139776, 12)\nS: static features, of shape (#series,#features+unique_id): \t(4, 5)\nY: target series (in X), of shape (#hours, #times): \t \t(139776, 3)\n\n\nTrain Validation splits\nmask  unique_id\n0.0   BE          2015-01-03 23:00:00\n      FR          2015-01-03 23:00:00\n      NP          2016-12-26 23:00:00\n      PJM         2016-12-26 23:00:00\n1.0   BE          2013-01-03 23:00:00\n      FR          2013-01-03 23:00:00\n      NP          2014-12-27 23:00:00\n      PJM         2014-12-27 23:00:00\nName: ds, dtype: datetime64[ns]\nmask  unique_id\n0.0   BE          2013-01-04\n      FR          2013-01-04\n      NP          2014-12-28\n      PJM         2014-12-28\n1.0   BE          2011-01-09\n      FR          2011-01-09\n      NP          2013-01-01\n      PJM         2013-01-01\nName: ds, dtype: datetime64[ns]\nTrain insample percentage 0.5,         69696.0 hours = 7.96 years\nTrain outsample percentage 0.5,         70080.0 hours = 8.0 years\n\n\nProcessing dataframes ...\nCreating ts tensor ...\n"
     ]
    }
   ],
   "source": [
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "\n",
    "def get_last_n_hours_mask_df(Y_df, n_hours):\n",
    "    # Creates outsample_mask\n",
    "    # train 1 validation 0\n",
    "\n",
    "    last_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    last_df.sort_values(by=['unique_id', 'ds'], inplace=True, ascending=False)\n",
    "    last_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    last_df = last_df.groupby('unique_id').head(n_hours)\n",
    "    last_df['mask'] = 0\n",
    "\n",
    "    last_df = last_df[['unique_id', 'ds', 'mask']]\n",
    "\n",
    "    mask_df = Y_df.merge(last_df, on=['unique_id', 'ds'], how='left')\n",
    "    mask_df['mask'] = mask_df['mask'].fillna(1)    # The first len(Y)-n_hours used as train\n",
    "\n",
    "    mask_df = mask_df[['unique_id', 'ds', 'mask']]\n",
    "    mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "\n",
    "    assert len(mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "\n",
    "    return mask_df\n",
    "\n",
    "val_ds = 2 * 365\n",
    "args = pd.Series({'dataset': ['NP', 'PJM', 'BE', 'FR']})\n",
    "\n",
    "Y_df, Xt_df, S_df = EPF.load_groups(directory='data', groups=args.dataset)\n",
    "\n",
    "mask_df = get_last_n_hours_mask_df(Y_df, n_hours=val_ds*24)\n",
    "mask = mask_df['mask'].values\n",
    "\n",
    "#print(f'Dataset: {args.dataset}')\n",
    "#print(\"Xt_df.columns\", Xt_df.columns)\n",
    "print('X: time series features, of shape (#hours, #times,#features): \\t' + str(Xt_df.shape))\n",
    "if S_df is not None:\n",
    "    print('S: static features, of shape (#series,#features+unique_id): \\t' + str(S_df.shape))\n",
    "print('Y: target series (in X), of shape (#hours, #times): \\t \\t' + str(Y_df.shape))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Train Validation splits\")\n",
    "print(mask_df.groupby(['mask', 'unique_id']).ds.max())\n",
    "print(mask_df.groupby(['mask', 'unique_id']).ds.min())\n",
    "print(f'Train insample percentage {np.round(sum(mask)/len(Y_df),2)}, \\\n",
    "        {sum(mask)} hours = {np.round(sum(mask)/(24*365),2)} years')\n",
    "print(f'Train outsample percentage {np.round(sum(1-mask)/len(Y_df),2)}, \\\n",
    "        {sum(1-mask)} hours = {np.round(sum(1-mask)/(24*365),2)} years')\n",
    "#Y_df.head()\n",
    "print('\\n')\n",
    "\n",
    "dataset = TimeSeriesDataset(Y_df=Y_df, S_df=S_df, X_df=Xt_df, mask_df=mask_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['y',\n",
       " 'Exogenous1',\n",
       " 'Exogenous2',\n",
       " 'week_day',\n",
       " 'day_0',\n",
       " 'day_1',\n",
       " 'day_2',\n",
       " 'day_3',\n",
       " 'day_4',\n",
       " 'day_5',\n",
       " 'day_6',\n",
       " 'insample_mask',\n",
       " 'outsample_mask']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "dataset.t_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "dataset.ts_tensor[0, dataset.t_cols.index('outsample_mask'), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Checking the insample_mask of series 0\n",
    "dataset.ts_tensor[0, dataset.t_cols.index('insample_mask'), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([32.54, 21.55, 15.71, ..., 42.44, 42.03, 40.91])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "dataset.ts_tensor[0, dataset.t_cols.index('y'), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataset.ts_tensor.shape (4, 13, 34944)\n"
     ]
    }
   ],
   "source": [
    "print(\"dataset.ts_tensor.shape\", dataset.ts_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[[3.3520000e+01, 2.7500000e+01, 2.2840000e+01, ...,\n",
       "          4.2440000e+01, 4.2030000e+01, 4.0910000e+01],\n",
       "         [7.1196000e+04, 6.8045000e+04, 6.6880000e+04, ...,\n",
       "          7.2883000e+04, 7.2926000e+04, 7.3070000e+04],\n",
       "         [6.6725000e+04, 6.3084000e+04, 6.1337000e+04, ...,\n",
       "          6.4515000e+04, 6.2554000e+04, 6.7342000e+04],\n",
       "         ...,\n",
       "         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, ...,\n",
       "          1.0000000e+00, 1.0000000e+00, 1.0000000e+00],\n",
       "         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n",
       " \n",
       "        [[3.3520000e+01, 2.7500000e+01, 2.2840000e+01, ...,\n",
       "          4.2440000e+01, 4.2030000e+01, 4.0910000e+01],\n",
       "         [7.1196000e+04, 6.8045000e+04, 6.6880000e+04, ...,\n",
       "          7.2883000e+04, 7.2926000e+04, 7.3070000e+04],\n",
       "         [6.6725000e+04, 6.3084000e+04, 6.1337000e+04, ...,\n",
       "          6.4515000e+04, 6.2554000e+04, 6.7342000e+04],\n",
       "         ...,\n",
       "         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, ...,\n",
       "          1.0000000e+00, 1.0000000e+00, 1.0000000e+00],\n",
       "         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n",
       " \n",
       "        [[1.8750000e+01, 1.8620000e+01, 1.8800000e+01, ...,\n",
       "          2.6820000e+01, 2.6650000e+01, 2.5680000e+01],\n",
       "         [3.9770000e+04, 3.9600000e+04, 3.9764000e+04, ...,\n",
       "          4.5471000e+04, 4.4386000e+04, 4.3017000e+04],\n",
       "         [2.4590000e+03, 2.3650000e+03, 2.2830000e+03, ...,\n",
       "          2.1290000e+03, 1.8270000e+03, 1.6890000e+03],\n",
       "         ...,\n",
       "         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, ...,\n",
       "          1.0000000e+00, 1.0000000e+00, 1.0000000e+00],\n",
       "         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n",
       " \n",
       "        [[2.0815025e+01, 2.0600587e+01, 2.0785209e+01, ...,\n",
       "          2.4525406e+01, 2.2834593e+01, 2.0454542e+01],\n",
       "         [7.6603000e+04, 7.5992000e+04, 7.6437000e+04, ...,\n",
       "          9.3346000e+04, 8.7603000e+04, 8.1626000e+04],\n",
       "         [9.6540000e+03, 9.4410000e+03, 9.3590000e+03, ...,\n",
       "          1.2129000e+04, 1.1642000e+04, 1.0924000e+04],\n",
       "         ...,\n",
       "         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, ...,\n",
       "          1.0000000e+00, 1.0000000e+00, 1.0000000e+00],\n",
       "         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]]]),\n",
       " 2)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "dataset.get_filtered_ts_tensor(offset=10, output_size=12, window_sampling_limit=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c68c0f618c4f787b1e02bfee278e48d25b62407cb335aab5258cedc4db4ced"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}