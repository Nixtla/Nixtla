{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.tsdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import time\n",
    "import gc\n",
    "import logging\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import Collection, Dict, Iterable, List, Optional, Tuple\n",
    "from typing_extensions import Literal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from fastcore.foundation import patch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# TODO: paralelizar y mejorar _df_to_lists, probablemente Pool de multiprocessing\n",
    "#.      si est√° balanceado el panel np reshape hace el truco <- pensar\n",
    "# TODO: definir defaults para sample_mask, calculo de availabitly al interior con ds\n",
    "# TODO: define future of f_cols.\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 Y_df: pd.DataFrame,\n",
    "                 X_df: pd.DataFrame = None,\n",
    "                 S_df: pd.DataFrame = None,\n",
    "                 mask_df: Optional[pd.DataFrame] = None,\n",
    "                 ds_in_test: int = 0,\n",
    "                 is_test: bool = False,\n",
    "                 f_cols: Optional[list] = None,\n",
    "                 verbose: bool = False) -> 'TimeSeriesDataset':\n",
    "        \"\"\"\n",
    "        TimeSeriesDataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Y_df: pd.DataFrame\n",
    "            Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "        X_df: pd.DataFrame\n",
    "            Exogenous time series with columns ['unique_id', 'ds', 'y'].\n",
    "        S_df: pd.DataFrame\n",
    "            Static exogenous variables with columns ['unique_id', 'ds'] \n",
    "            and static variables.\n",
    "        mask_df: pd.DataFrame\n",
    "            Outsample mask with columns ['unique_id', 'ds', 'sample_mask']\n",
    "            and optionally 'available_mask'.\n",
    "            Default None: constructs default mask based on ds_in_test.\n",
    "        ds_in_test: int\n",
    "            Only used when mask_df = None.\n",
    "            Numer of datestamps to use as outsample.\n",
    "        is_test: bool\n",
    "            Only used when mask_df = None.\n",
    "            Wheter target time series belongs to test set.\n",
    "        verbose: bool\n",
    "            Wheter log outputs.\n",
    "        \"\"\"\n",
    "        assert type(Y_df) == pd.core.frame.DataFrame\n",
    "        assert all([(col in Y_df) for col in ['unique_id', 'ds']])\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if X_df is not None:\n",
    "            assert type(X_df) == pd.core.frame.DataFrame\n",
    "            assert all([(col in X_df) for col in ['unique_id', 'ds']])\n",
    "            assert len(Y_df)==len(X_df), f'The dimensions of Y_df and X_df are not the same'\n",
    "\n",
    "        if mask_df is not None:\n",
    "            assert len(Y_df)==len(mask_df), f'The dimensions of Y_df and mask_df are not the same'\n",
    "            assert all([(col in mask_df) for col in ['unique_id', 'ds', 'sample_mask']])\n",
    "            if 'available_mask' not in mask_df.columns:\n",
    "                self.verbose: logging.info('Available mask not provided, defaulted with 1s.')\n",
    "                mask_df['available_mask'] = 1\n",
    "            assert np.sum(np.isnan(mask_df.available_mask.values)) == 0\n",
    "            assert np.sum(np.isnan(mask_df.sample_mask.values)) == 0\n",
    "        else:\n",
    "            mask_df = self.get_default_mask_df(Y_df=Y_df, \n",
    "                                               is_test=is_test,\n",
    "                                               ds_in_test=ds_in_test)\n",
    "        \n",
    "        # Logging\n",
    "        #start = time.time()\n",
    "        mask_df['train_mask'] = mask_df['available_mask'] * mask_df['sample_mask']\n",
    "        n_ds  = len(mask_df)\n",
    "        n_avl = mask_df.available_mask.sum()        \n",
    "        n_ins = mask_df.sample_mask.sum()\n",
    "        n_out = len(mask_df)-mask_df.sample_mask.sum()\n",
    "\n",
    "        avl_prc = np.round((100*n_avl)/n_ds,2)\n",
    "        ins_prc = np.round((100*n_ins)/n_ds,2)\n",
    "        out_prc = np.round((100*n_out)/n_ds,2)\n",
    "        if self.verbose:\n",
    "            logging.info('Train Validation splits\\n')\n",
    "            if len(mask_df.unique_id.unique()) < 10:\n",
    "                logging.info(mask_df.groupby(['unique_id', 'sample_mask']).agg({'ds': ['min', 'max']}))\n",
    "            else:\n",
    "                logging.info(mask_df.groupby(['sample_mask']).agg({'ds': ['min', 'max']}))\n",
    "            dataset_info  = f'\\nTotal data \\t\\t\\t{n_ds} time stamps \\n'\n",
    "            dataset_info += f'Available percentage={avl_prc}, \\t{n_avl} time stamps \\n'\n",
    "            dataset_info += f'Insample  percentage={ins_prc}, \\t{n_ins} time stamps \\n'\n",
    "            dataset_info += f'Outsample percentage={out_prc}, \\t{n_out} time stamps \\n'\n",
    "            logging.info(dataset_info)\n",
    "        #print(f\"logging time {time.time()-start}\")\n",
    "\n",
    "        #start = time.time()\n",
    "        ts_data, s_data, self.meta_data, self.y_cols, self.t_cols, self.s_cols \\\n",
    "                                            = self._df_to_lists(Y_df=Y_df, S_df=S_df, X_df=X_df, mask_df=mask_df)\n",
    "        #print(f\"df2list time {time.time()-start}\")\n",
    "\n",
    "        # Dataset attributes\n",
    "        self.n_y        = len(self.y_cols)\n",
    "        self.n_series   = len(ts_data)\n",
    "        self.max_len    = max([len(ts) for ts in ts_data])\n",
    "        self.n_channels = len(self.t_cols) # t_cols insample_mask and outsample_mask\n",
    "        self.frequency  = pd.infer_freq(Y_df.head()['ds']) #TODO: improve, can die with head\n",
    "        self.f_cols     = f_cols\n",
    "\n",
    "        # Number of X and S features\n",
    "        self.n_x = 0 if X_df is None else X_df.shape[1]-2 # -2 for unique_id and ds\n",
    "        self.n_s = 0 if S_df is None else S_df.shape[1]-1 # -1 for unique_id\n",
    "\n",
    "        # print('Creating ts tensor ...')\n",
    "        # Balances panel and creates \n",
    "        # numpy  s_matrix of shape (n_series, n_s)\n",
    "        # numpy ts_tensor of shape (n_series, n_channels, max_len) n_channels = t_cols + masks\n",
    "        self.ts_tensor, self.s_matrix, self.len_series = self._create_tensor(ts_data, s_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def get_default_mask_df(self: TimeSeriesDataset, \n",
    "                        Y_df: pd.DataFrame, \n",
    "                        ds_in_test: int, \n",
    "                        is_test: bool) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Constructs default mask df.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    ds_in_test: int\n",
    "        Numer of datestamps to use as outsample.\n",
    "    is_test: bool\n",
    "        Wheter target time series belongs to test set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Mask DataFrame with columns \n",
    "    ['unique_id', 'ds', 'available_mask', 'sample_mask'].\n",
    "    \"\"\"\n",
    "    last_df = Y_df[['unique_id', 'ds']].copy()\n",
    "    last_df.sort_values(by=['unique_id', 'ds'], inplace=True, ascending=False)\n",
    "    last_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    last_df = last_df.groupby('unique_id').head(ds_in_test)\n",
    "    last_df['sample_mask'] = 0\n",
    "\n",
    "    last_df = last_df[['unique_id', 'ds', 'sample_mask']]\n",
    "\n",
    "    mask_df = Y_df.merge(last_df, on=['unique_id', 'ds'], how='left')\n",
    "    mask_df['sample_mask'] = mask_df['sample_mask'].fillna(1)\n",
    "\n",
    "    mask_df = mask_df[['unique_id', 'ds', 'sample_mask']]\n",
    "    mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "    mask_df['available_mask'] = 1\n",
    "\n",
    "    assert len(mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "\n",
    "    if is_test:\n",
    "        mask_df['sample_mask'] = 1 - mask_df['sample_mask']\n",
    "\n",
    "    return mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def _df_to_lists(self: TimeSeriesDataset, \n",
    "                 S_df: pd.DataFrame,\n",
    "                 Y_df: pd.DataFrame,\n",
    "                 X_df: pd.DataFrame, \n",
    "                 mask_df: pd.DataFrame) -> Tuple[List[Dict[str, np.ndarray]], \n",
    "                                                 List[Dict[str, np.ndarray]],\n",
    "                                                 List[Dict[str, np.ndarray]], \n",
    "                                                 List[str],\n",
    "                                                 List[str]]:\n",
    "    \"\"\"\n",
    "    Transforms input dataframes to lists.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    S_df: pd.DataFrame\n",
    "        Static exogenous variables with columns ['unique_id', 'ds'] \n",
    "        and static variables.    \n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    X_df: pd.DataFrame\n",
    "        Exogenous time series with columns ['unique_id', 'ds', 'y'].\n",
    "    mask_df: pd.DataFrame\n",
    "        Outsample mask with columns ['unique_id', 'ds', 'sample_mask']\n",
    "        and optionally 'available_mask'.\n",
    "        Default None: constructs default mask based on ds_in_test.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple of five lists:\n",
    "        - Each element of the list is a dictionary with target, exogenous\n",
    "          and mask values.\n",
    "          e.g [{'y': array([1., 2.]), 'Exogenous1': array([0., 1.])}]\n",
    "        - Each element of the list is a dictionary with static variables.\n",
    "          e.g [{'static_NP': array([1], dtype=uint8),\n",
    "                'static_PJM': array([0], dtype=uint8)}]\n",
    "        - Each element of the list is a dictionary with two keys:\n",
    "          unique_id and last_ds.\n",
    "          e.g [{'unique_id': array(['uid_1', 'uid_2']), \n",
    "                'last_ds': array(['2020-01-01', '2020-01-02'])}]\n",
    "        - List of variables. \n",
    "        - List of exogenous variables.\n",
    "    \"\"\"\n",
    "\n",
    "    y_cols = [col for col in Y_df.columns if col not in ['unique_id', 'ds']]\n",
    "\n",
    "    # None protections\n",
    "    if X_df is None:\n",
    "        X_df = Y_df[['unique_id', 'ds']]\n",
    "    \n",
    "    if S_df is None:\n",
    "        S_df = Y_df[['unique_id']].drop_duplicates()\n",
    "    \n",
    "    # Protect order of data\n",
    "    Y = Y_df.sort_values(by=['unique_id', 'ds']).copy()\n",
    "    X = X_df.sort_values(by=['unique_id', 'ds']).copy()\n",
    "    M = mask_df.sort_values(by=['unique_id', 'ds']).copy()\n",
    "    assert np.array_equal(X.unique_id.values, Y.unique_id.values), f'Mismatch in X, Y unique_ids'\n",
    "    assert np.array_equal(X.ds.values, Y.ds.values), f'Mismatch in X, Y ds'\n",
    "    assert np.array_equal(M.unique_id.values, Y.unique_id.values), f'Mismatch in M, Y unique_ids'\n",
    "    assert np.array_equal(M.ds.values, Y.ds.values), f'Mismatch in M, Y ds'\n",
    "    \n",
    "    # Create bigger grouped by dataframe G to parse\n",
    "    M = M[['available_mask', 'sample_mask']]\n",
    "    X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
    "    G = pd.concat([Y, X, M], axis=1)\n",
    "    S = S_df.sort_values(by=['unique_id']).copy()\n",
    "    \n",
    "    # time columns and static columns for future indexing\n",
    "    t_cols = list(G.columns[2:]) # avoid unique_id and ds\n",
    "    s_cols = list(S.columns[1:]) # avoid unique_id\n",
    "    \n",
    "    G = G.groupby(['unique_id'])\n",
    "    S = S.groupby(['unique_id'])\n",
    "    \n",
    "    ts_data = []\n",
    "    meta_data = []\n",
    "    for idx, group in G:\n",
    "        group = group.reset_index(drop=True)\n",
    "        meta_data.append(group.values[:, :2]) # save unique_id and ds\n",
    "        ts_data.append(group.values[:, 2:]) # avoid unique_id and ds\n",
    "        \n",
    "    s_data = []\n",
    "    for idx, group in S:\n",
    "        s_data.append(group.iloc[:, 1:].values) # avoid unique_id\n",
    "        assert len(s_data[-1])==1, 'Check repetitions of unique_ids'\n",
    "    \n",
    "    del S, Y, X, M, G\n",
    "    gc.collect()\n",
    "    \n",
    "    return ts_data, s_data, meta_data, y_cols, t_cols, s_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def _create_tensor(self: TimeSeriesDataset, \n",
    "                   ts_data: List[Dict[str, np.ndarray]], \n",
    "                   s_data: List[Dict[str, np.ndarray]]) -> Tuple[np.ndarray, \n",
    "                                                                 np.ndarray,\n",
    "                                                                 np.ndarray]:\n",
    "    \"\"\"\n",
    "    Transforms outputs from self._df_to_lists to numpy arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ts_data: List[Dict[str, np.ndarray]]\n",
    "        Each element of the list is a dictionary with target, exogenous\n",
    "        and mask values.\n",
    "    s_data: List[Dict[str, np.ndarray]]\n",
    "        Each element of the list is a dictionary with static variables.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple of three elements:\n",
    "        - ts_tensor of shape (n_series, n_channels, max_len) n_channels = t_cols + masks\n",
    "        - s_matrix of shape (n_series, n_s)\n",
    "        - len_series: Array with series lenghts.\n",
    "    \"\"\"\n",
    "    ts_tensor = np.zeros((self.n_series, self.n_channels, self.max_len))\n",
    "\n",
    "    len_series = []\n",
    "    for idx in range(self.n_series):\n",
    "        # Left padded time series tensor\n",
    "        ts_idx = ts_data[idx]\n",
    "        ts_tensor[idx, :, -ts_idx.shape[0]:] = ts_idx.T\n",
    "        len_series.append(ts_idx.shape[0])\n",
    "    \n",
    "    s_matrix = np.vstack(s_data)\n",
    "    len_series = np.array(len_series)\n",
    "    return ts_tensor, s_matrix, len_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def get_meta_data_col(self: TimeSeriesDataset, \n",
    "                      col: Literal['unique_id', 'last_ds']) -> List:\n",
    "    \"\"\"\n",
    "    Based on col returns a list of values for each time series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    col: Literal['unique_id', 'last_ds']\n",
    "        Interest column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List of values for each time series.\n",
    "    \"\"\"\n",
    "    allowed_cols = ['unique_id', 'last_ds']\n",
    "\n",
    "    if col not in allowed_cols:\n",
    "        raise Exception(f'Unknown col {col}')\n",
    "\n",
    "    col_values = [x[col] for x in self.meta_data]\n",
    "\n",
    "    return col_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def get_filtered_ts_tensor(self: TimeSeriesDataset, \n",
    "                           output_size: int, \n",
    "                           window_sampling_limit: int, \n",
    "                           ts_idxs: Optional[Collection[int]] = None) -> Tuple[t.Tensor, int]:\n",
    "    \"\"\"\n",
    "    Get ts tensor filtered based on output_size, window_sampling_limit and\n",
    "    ts_idxs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_size: int\n",
    "        Forecast horizon.\n",
    "    window_sampling_limit: int\n",
    "        Max size of observations to consider, including output_size.\n",
    "    ts_idxs: Collection\n",
    "        Indexes of time series to consider.\n",
    "        Default None: returns all ts.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple of filtered tensor and right_padding size.\n",
    "    \"\"\"\n",
    "    #TODO:\n",
    "    # - Check last_outsample_ds, unnecesary because size of ts_tensor is (n_ts, max_len).\n",
    "    last_outsample_ds = self.max_len + output_size\n",
    "    first_ds = max(self.max_len - window_sampling_limit, 0)\n",
    "\n",
    "    idxs = range(self.n_series) if ts_idxs is None else ts_idxs\n",
    "    filtered_ts_tensor = self.ts_tensor[idxs, :, first_ds:last_outsample_ds]\n",
    "    right_padding = max(last_outsample_ds - self.max_len, 0) #To padd with zeros if there is \"nothing\" to the right\n",
    "\n",
    "    return filtered_ts_tensor, right_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def get_f_idxs(self: TimeSeriesDataset, \n",
    "               cols: List[str]):\n",
    "    \"\"\"\n",
    "    Gets indexes of exogenous variables.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cols: List[str]\n",
    "        Interest exogenous variables.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Indexes of cols variables.\n",
    "    \"\"\"\n",
    "    # Check if cols are available f_cols and return the idxs\n",
    "    if not all(col in self.f_cols for col in cols):\n",
    "        str_cols = ', '.join(cols)\n",
    "        raise Exception(f'Some variables in {str_cols} are not available in f_cols.')\n",
    "    \n",
    "    # Avoid 'y', 'available_mask', 'sample_mask'\n",
    "    f_idxs = [self.t_cols[1:-2].index(col) for col in cols]\n",
    "\n",
    "    return f_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_default_mask_df(Y_df, ds_in_test, is_test):\n",
    "    # Creates outsample_mask\n",
    "    # train 1 validation 0\n",
    "    last_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    last_df.sort_values(by=['unique_id', 'ds'], inplace=True, ascending=False)\n",
    "    last_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    last_df = last_df.groupby('unique_id').head(ds_in_test)\n",
    "    last_df['sample_mask'] = 0\n",
    "\n",
    "    last_df = last_df[['unique_id', 'ds', 'sample_mask']]\n",
    "\n",
    "    mask_df = Y_df.merge(last_df, on=['unique_id', 'ds'], how='left')\n",
    "    mask_df['sample_mask'] = mask_df['sample_mask'].fillna(1)\n",
    "\n",
    "    mask_df = mask_df[['unique_id', 'ds', 'sample_mask']]\n",
    "    mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "    mask_df['available_mask'] = 1\n",
    "\n",
    "    assert len(mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "\n",
    "    if is_test:\n",
    "        mask_df['sample_mask'] = 1 - mask_df['sample_mask']\n",
    "\n",
    "    return mask_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MASK example and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa2668c4fd0>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "\n",
    "args = pd.Series({'dataset': ['NP']})\n",
    "\n",
    "Y_df, X_df, S_df = EPF.load_groups(directory='../data', groups=['NP', 'PJM'])\n",
    "\n",
    "mask_df = get_default_mask_df(Y_df=Y_df, ds_in_test=728*24, is_test=False)\n",
    "\n",
    "plt.plot(mask_df.sample_mask.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0.0         2016-12-27 2018-12-24 23:00:00\n",
      "          1.0         2013-01-01 2016-12-26 23:00:00\n",
      "PJM       0.0         2016-12-27 2018-12-24 23:00:00\n",
      "          1.0         2013-01-01 2016-12-26 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t104832 time stamps \n",
      "Available percentage=100.0, \t104832 time stamps \n",
      "Insample  percentage=66.67, \t69888.0 time stamps \n",
      "Outsample percentage=33.33, \t34944.0 time stamps \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = TimeSeriesDataset(Y_df=Y_df, S_df=S_df, X_df=X_df, mask_df=mask_df, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.05 30.47 28.92 ... 49.09 49.02 48.1 ]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.ts_tensor[0, dataset.t_cols.index('y'), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.ts_tensor.shape (2, 13, 52416)\n"
     ]
    }
   ],
   "source": [
    "print(\"dataset.ts_tensor.shape\", dataset.ts_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered, right_padding = dataset.get_filtered_ts_tensor(output_size=12, window_sampling_limit=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yearly\n"
     ]
    }
   ],
   "source": [
    "from nixtla.data.datasets.tourism import Tourism, TourismInfo\n",
    "print(TourismInfo.groups[0])\n",
    "Y_df, *_ = Tourism.load(directory='../data', group=TourismInfo.groups[0])\n",
    "\n",
    "train_ts_dataset = TimeSeriesDataset(Y_df=Y_df, ds_in_test=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:             ds     \n",
      "            min  max\n",
      "sample_mask         \n",
      "0.0          14  841\n",
      "1.0           1  835\n",
      "INFO:root:\n",
      "Total data \t\t\t858458 time stamps \n",
      "Available percentage=100.0, \t858458 time stamps \n",
      "Insample  percentage=83.92, \t720458.0 time stamps \n",
      "Outsample percentage=16.08, \t138000.0 time stamps \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nixtla.data.datasets.m4 import M4, M4Info\n",
    "\n",
    "Y_df, *_ = M4.load(directory='../data', group=M4Info.groups[0])\n",
    "\n",
    "train_ts_dataset = TimeSeriesDataset(Y_df=Y_df, ds_in_test=6, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nixtla",
   "language": "python",
   "name": "nixtla"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}