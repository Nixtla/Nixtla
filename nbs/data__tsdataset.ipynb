{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.tsdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch as t\n",
    "\n",
    "from fastcore.foundation import patch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# TODO: resolver t_cols y X_cols duplicados t_cols se usa en dataloader X_cols para indexar con f_cols\n",
    "#.      idea mantenemos solo X_cols y en el dataloader corregimos con 'y' y 'insample_mask' \n",
    "# TODO: paralelizar y mejorar _df_to_lists, probablemente Pool de multiprocessing\n",
    "#.      si est√° balanceado el panel np reshape hace el truco <- pensar\n",
    "# TODO: definir defaults para sample_mask, calculo de availabitly al interior con ds\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 Y_df: pd.DataFrame,\n",
    "                 X_df: pd.DataFrame=None,\n",
    "                 S_df: pd.DataFrame=None,\n",
    "                 mask_df: pd.DataFrame=None,\n",
    "                 f_cols: list=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        assert type(Y_df) == pd.core.frame.DataFrame\n",
    "        assert all([(col in Y_df) for col in ['unique_id', 'ds', 'y']])\n",
    "\n",
    "        if X_df is not None:\n",
    "            assert type(X_df) == pd.core.frame.DataFrame\n",
    "            assert all([(col in X_df) for col in ['unique_id', 'ds']])\n",
    "            assert len(Y_df)==len(X_df), f'The dimensions of Y_df and X_df are not the same'\n",
    "\n",
    "        if mask_df is not None:\n",
    "            assert len(Y_df)==len(mask_df), f'The dimensions of Y_df and mask_df are not the same'\n",
    "            assert all([(col in mask_df) for col in ['unique_id', 'ds', 'sample_mask']])\n",
    "            if 'available_mask' not in mask_df.columns:\n",
    "                print('Available mask not provided, defaulted with 1s.')\n",
    "                mask_df['available_mask'] = 1\n",
    "            assert np.sum(np.isnan(mask_df.available_mask.values))==0\n",
    "            assert np.sum(np.isnan(mask_df.sample_mask.values))==0\n",
    "        else:\n",
    "            mask_df = Y_df[['unique_id', 'ds']]\n",
    "            mask_df['available_mask'] = 1\n",
    "            mask_df['sample_mask'] = 1\n",
    "\n",
    "        print(\"Train Validation splits\")\n",
    "        mask_df['train_mask'] = mask_df['available_mask'] * mask_df['sample_mask']\n",
    "        self.n_tstamps = len(mask_df)\n",
    "        self.n_avl = mask_df.available_mask.sum()        \n",
    "        self.n_trn = mask_df.train_mask.sum()\n",
    "        self.n_prd = len(mask_df)-mask_df.sample_mask.sum()\n",
    "\n",
    "        avl_prc = np.round((100*self.n_avl)/self.n_tstamps,2)\n",
    "        trn_prc = np.round((100*self.n_trn)/self.n_tstamps,2)\n",
    "        prd_prc = np.round((100*self.n_prd)/self.n_tstamps,2)\n",
    "        print(mask_df.groupby(['unique_id', 'sample_mask']).agg({'ds': ['min', 'max']}))\n",
    "        print(f'Total data \\t\\t\\t{self.n_tstamps} time stamps')\n",
    "        print(f'Available percentage={avl_prc}, \\t{self.n_avl} time stamps')\n",
    "        print(f'Train percentage={trn_prc}, \\t{self.n_trn} time stamps')\n",
    "        print(f'Outsample percentage={prd_prc}, \\t{self.n_prd} time stamps')\n",
    "        print('\\n')\n",
    "\n",
    "        ts_data, s_data, self.meta_data, self.t_cols, self.X_cols \\\n",
    "                         = self._df_to_lists(Y_df=Y_df, S_df=S_df, X_df=X_df, mask_df=mask_df)\n",
    "\n",
    "        # Dataset attributes\n",
    "        self.n_series   = len(ts_data)\n",
    "        self.max_len    = max([len(ts['y']) for ts in ts_data])\n",
    "        self.n_channels = len(self.t_cols) # y, X_cols, insample_mask and outsample_mask\n",
    "        self.frequency  = pd.infer_freq(Y_df.head()['ds']) #TODO: improve, can die with head\n",
    "        self.f_cols     = f_cols\n",
    "\n",
    "        # Number of X and S features\n",
    "        self.n_x = 0 if X_df is None else len(self.X_cols)\n",
    "        self.n_s = 0 if S_df is None else S_df.shape[1]-1 # -1 for unique_id\n",
    "\n",
    "        # print('Creating ts tensor ...')\n",
    "        # Balances panel and creates \n",
    "        # numpy  s_matrix of shape (n_series, n_s)\n",
    "        # numpy ts_tensor of shape (n_series, n_channels, max_len) n_channels = y + X_cols + masks\n",
    "        self.ts_tensor, self.s_matrix, self.len_series = self._create_tensor(ts_data, s_data)\n",
    "\n",
    "    def _df_to_lists(self, Y_df, S_df, X_df, mask_df):\n",
    "        \"\"\" TODO: Comment on unbalanced panels\n",
    "        \"\"\"\n",
    "        unique_ids = Y_df['unique_id'].unique()\n",
    "\n",
    "        if X_df is not None:\n",
    "            X_cols = [col for col in X_df.columns if col not in ['unique_id','ds']]\n",
    "        else:\n",
    "            X_cols = []\n",
    "\n",
    "        if S_df is not None:\n",
    "            S_cols = [col for col in S_df.columns if col not in ['unique_id']]\n",
    "        else:\n",
    "            S_cols = []\n",
    "\n",
    "        ts_data = []\n",
    "        s_data = []\n",
    "        meta_data = []\n",
    "        for i, u_id in enumerate(unique_ids):\n",
    "            top_row    = np.asscalar(Y_df['unique_id'].searchsorted(u_id, 'left'))\n",
    "            bottom_row = np.asscalar(Y_df['unique_id'].searchsorted(u_id, 'right'))\n",
    "            \n",
    "            # Y values\n",
    "            y_true = Y_df[top_row:bottom_row]['y'].values\n",
    "            ts_data_i = {'y': y_true}\n",
    "            \n",
    "            # X values\n",
    "            for X_col in X_cols:\n",
    "                serie =  X_df[top_row:bottom_row][X_col].values\n",
    "                ts_data_i[X_col] = serie\n",
    "\n",
    "            # Mask values\n",
    "            available_mask = mask_df[top_row:bottom_row]['available_mask'].values\n",
    "            sample_mask = mask_df[top_row:bottom_row]['sample_mask'].values            \n",
    "            ts_data_i['available_mask'] = available_mask\n",
    "            ts_data_i['sample_mask']  = sample_mask\n",
    "            ts_data.append(ts_data_i)\n",
    "\n",
    "            # S values\n",
    "            s_data_i = defaultdict(list)\n",
    "            for S_col in S_cols:\n",
    "                s_data_i[S_col] = S_df.loc[S_df['unique_id']==u_id, S_col].values\n",
    "            s_data.append(s_data_i)\n",
    "\n",
    "            # Metadata\n",
    "            last_ds_i  = Y_df[top_row:bottom_row]['ds'].max()\n",
    "            meta_data_i = {'unique_id': u_id,\n",
    "                           'last_ds': last_ds_i}\n",
    "            meta_data.append(meta_data_i)\n",
    "\n",
    "        t_cols = ['y'] + X_cols + ['available_mask', 'sample_mask']\n",
    "\n",
    "        return ts_data, s_data, meta_data, t_cols, X_cols\n",
    "\n",
    "    def _create_tensor(self, ts_data, s_data):\n",
    "        \"\"\"\n",
    "        s_matrix of shape (n_series, n_s)\n",
    "        ts_tensor of shape (n_series, n_channels, max_len) n_channels = y + X_cols + masks\n",
    "        \"\"\"\n",
    "        s_matrix  = np.zeros((self.n_series, self.n_s))\n",
    "        ts_tensor = np.zeros((self.n_series, self.n_channels, self.max_len))\n",
    "\n",
    "        len_series = []\n",
    "        for idx in range(self.n_series):\n",
    "            # Left padded time series tensor\n",
    "            # TODO: Maybe we can place according to ds\n",
    "            ts_idx = np.array(list(ts_data[idx].values()))\n",
    "\n",
    "            ts_tensor[idx, :, -ts_idx.shape[1]:] = ts_idx\n",
    "            s_matrix[idx, :] = list(s_data[idx].values())\n",
    "            len_series.append(ts_idx.shape[1])\n",
    "        \n",
    "        return ts_tensor, s_matrix, np.array(len_series)\n",
    "\n",
    "    def get_meta_data_col(self, col):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        col_values = [x[col] for x in self.meta_data]\n",
    "        return col_values\n",
    "\n",
    "    def get_filtered_ts_tensor(self, offset, output_size, window_sampling_limit, ts_idxs=None):\n",
    "        \"\"\"\n",
    "        Esto te da todo lo que tenga el tensor, el futuro incluido esto orque se usa exogenoas del futuro\n",
    "        La mascara se hace despues\n",
    "        \"\"\"\n",
    "        last_outsample_ds = self.max_len - offset + output_size\n",
    "        first_ds = max(last_outsample_ds - window_sampling_limit - output_size, 0)\n",
    "        if ts_idxs is None:\n",
    "            filtered_ts_tensor = self.ts_tensor[:, :, first_ds:last_outsample_ds]\n",
    "        else:\n",
    "            filtered_ts_tensor = self.ts_tensor[ts_idxs, :, first_ds:last_outsample_ds]\n",
    "        right_padding = max(last_outsample_ds - self.max_len, 0) #To padd with zeros if there is \"nothing\" to the right\n",
    "\n",
    "        #assert np.sum(np.isnan(filtered_ts_tensor))<1.0, \\\n",
    "        #   f'The balanced balanced filtered_tensor has {np.sum(np.isnan(filtered_ts_tensor))} nan values'\n",
    "        \n",
    "        return filtered_ts_tensor, right_padding #ANTES, ts_train_mask\n",
    "\n",
    "    def get_f_idxs(self, cols):\n",
    "        # Check if cols are available f_cols and return the idxs\n",
    "        assert all(col in self.f_cols for col in cols), f'Some variables in {cols} are not available in f_cols.'\n",
    "        f_idxs = [self.X_cols.index(col) for col in cols]\n",
    "        return f_idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TimeSeriesDatasetViejo(Dataset):\n",
    "    def __init__(self,\n",
    "                 Y_df: pd.DataFrame,\n",
    "                 X_df: pd.DataFrame=None,\n",
    "                 S_df: pd.DataFrame=None,\n",
    "                 f_cols: list=None,\n",
    "                 ts_train_mask: list=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        assert type(Y_df) == pd.core.frame.DataFrame\n",
    "        assert all([(col in Y_df) for col in ['unique_id', 'ds', 'y']])\n",
    "        if X_df is not None:\n",
    "            assert type(X_df) == pd.core.frame.DataFrame\n",
    "            assert all([(col in X_df) for col in ['unique_id', 'ds']])\n",
    "\n",
    "        print('Processing dataframes ...')\n",
    "        # Pandas dataframes to data lists\n",
    "        ts_data, s_data, self.meta_data, self.t_cols, self.X_cols = self._df_to_lists(Y_df=Y_df, S_df=S_df, X_df=X_df)\n",
    "\n",
    "        # Dataset attributes\n",
    "        self.n_series   = len(ts_data)\n",
    "        self.max_len    = max([len(ts['y']) for ts in ts_data])\n",
    "        self.n_channels = len(self.t_cols) # y, X_cols, insample_mask and outsample_mask\n",
    "        self.frequency  = pd.infer_freq(Y_df.head()['ds']) #TODO: improve, can die with head\n",
    "        self.f_cols     = f_cols\n",
    "\n",
    "        # Number of X and S features\n",
    "        self.n_x = 0 if X_df is None else len(self.X_cols)\n",
    "        self.n_s = 0 if S_df is None else S_df.shape[1]-1 # -1 for unique_id\n",
    "\n",
    "        print('Creating ts tensor ...')\n",
    "        # Balances panel and creates\n",
    "        # numpy  s_matrix of shape (n_series, n_s)\n",
    "        # numpy ts_tensor of shape (n_series, n_channels, max_len) n_channels = y + X_cols + masks\n",
    "        self.ts_tensor, self.s_matrix, self.len_series = self._create_tensor(ts_data, s_data)\n",
    "        if ts_train_mask is None: ts_train_mask = np.ones(self.max_len)\n",
    "        assert len(ts_train_mask)==self.max_len, f'Outsample mask must have {self.max_len} length'\n",
    "\n",
    "        self._declare_outsample_train_mask(ts_train_mask)\n",
    "\n",
    "\n",
    "    def _df_to_lists(self, Y_df, S_df, X_df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        unique_ids = Y_df['unique_id'].unique()\n",
    "\n",
    "        if X_df is not None:\n",
    "            X_cols = [col for col in X_df.columns if col not in ['unique_id','ds']]\n",
    "        else:\n",
    "            X_cols = []\n",
    "\n",
    "        if S_df is not None:\n",
    "            S_cols = [col for col in S_df.columns if col not in ['unique_id']]\n",
    "        else:\n",
    "            S_cols = []\n",
    "\n",
    "        ts_data = []\n",
    "        s_data = []\n",
    "        meta_data = []\n",
    "        for i, u_id in enumerate(unique_ids):\n",
    "            top_row = np.asscalar(Y_df['unique_id'].searchsorted(u_id, 'left'))\n",
    "            bottom_row = np.asscalar(Y_df['unique_id'].searchsorted(u_id, 'right'))\n",
    "            serie = Y_df[top_row:bottom_row]['y'].values\n",
    "            last_ds_i = Y_df[top_row:bottom_row]['ds'].max()\n",
    "\n",
    "            # Y values\n",
    "            ts_data_i = {'y': serie}\n",
    "\n",
    "            # X values\n",
    "            for X_col in X_cols:\n",
    "                serie =  X_df[top_row:bottom_row][X_col].values\n",
    "                ts_data_i[X_col] = serie\n",
    "            ts_data.append(ts_data_i)\n",
    "\n",
    "            # S values\n",
    "            s_data_i = defaultdict(list)\n",
    "            for S_col in S_cols:\n",
    "                s_data_i[S_col] = S_df.loc[S_df['unique_id']==u_id, S_col].values\n",
    "            s_data.append(s_data_i)\n",
    "\n",
    "            # Metadata\n",
    "            meta_data_i = {'unique_id': u_id,\n",
    "                           'last_ds': last_ds_i}\n",
    "            meta_data.append(meta_data_i)\n",
    "\n",
    "        t_cols = ['y'] + X_cols + ['insample_mask', 'outsample_mask']\n",
    "    \n",
    "        return ts_data, s_data, meta_data, t_cols, X_cols\n",
    "\n",
    "    def _create_tensor(self, ts_data, s_data):\n",
    "        \"\"\"\n",
    "        s_matrix of shape (n_series, n_s)\n",
    "        ts_tensor of shape (n_series, n_channels, max_len) n_channels = y + X_cols + masks\n",
    "        \"\"\"\n",
    "        s_matrix  = np.zeros((self.n_series, self.n_s))\n",
    "        ts_tensor = np.zeros((self.n_series, self.n_channels, self.max_len))\n",
    "\n",
    "        len_series = []\n",
    "        for idx in range(self.n_series):\n",
    "            ts_idx = np.array(list(ts_data[idx].values()))\n",
    "            \n",
    "            ts_tensor[idx, :self.t_cols.index('insample_mask'), -ts_idx.shape[1]:] = ts_idx\n",
    "            ts_tensor[idx,  self.t_cols.index('insample_mask'), -ts_idx.shape[1]:] = 1\n",
    "\n",
    "            # outsample_mask will be completed with the train_mask, this ensures available data\n",
    "            ts_tensor[idx,  self.t_cols.index('outsample_mask'), -(ts_idx.shape[1]):] = 1\n",
    "            s_matrix[idx, :] = list(s_data[idx].values())\n",
    "            len_series.append(ts_idx.shape[1])\n",
    "\n",
    "        return ts_tensor, s_matrix, np.array(len_series)\n",
    "\n",
    "    def _declare_outsample_train_mask(self, ts_train_mask):\n",
    "        # Update attribute and ts_tensor\n",
    "        self.ts_train_mask = ts_train_mask\n",
    "\n",
    "    def get_meta_data_col(self, col):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        col_values = [x[col] for x in self.meta_data]\n",
    "        return col_values\n",
    "\n",
    "    def get_filtered_ts_tensor(self, offset, output_size, window_sampling_limit, ts_idxs=None):\n",
    "        \"\"\"\n",
    "        Esto te da todo lo que tenga el tensor, el futuro incluido esto orque se usa exogenoas del futuro\n",
    "        La mascara se hace despues\n",
    "        \"\"\"\n",
    "        last_outsample_ds = self.max_len - offset + output_size\n",
    "        first_ds = max(last_outsample_ds - window_sampling_limit - output_size, 0)\n",
    "        if ts_idxs is None:\n",
    "            filtered_ts_tensor = self.ts_tensor[:, :, first_ds:last_outsample_ds]\n",
    "        else:\n",
    "            filtered_ts_tensor = self.ts_tensor[ts_idxs, :, first_ds:last_outsample_ds]\n",
    "        right_padding = max(last_outsample_ds - self.max_len, 0) #To padd with zeros if there is \"nothing\" to the right\n",
    "        ts_train_mask = self.ts_train_mask[first_ds:last_outsample_ds]\n",
    "\n",
    "        assert np.sum(np.isnan(filtered_ts_tensor))<1.0, \\\n",
    "            f'The balanced balanced filtered_tensor has {np.sum(np.isnan(filtered_ts_tensor))} nan values'\n",
    "        return filtered_ts_tensor, right_padding, ts_train_mask\n",
    "\n",
    "    def get_f_idxs(self, cols):\n",
    "        # Check if cols are available f_cols and return the idxs\n",
    "        assert all(col in self.f_cols for col in cols), f'Some variables in {cols} are not available in f_cols.'\n",
    "        f_idxs = [self.X_cols.index(col) for col in cols]\n",
    "        return f_idxs"
   ]
  },
  {
   "source": [
    "# MASK example and test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random validation de-activated\nTrain 17424 hours = 1.99 years\nValidation 17520 hours = 2.0 years\nX: time series features, of shape (#hours, #times,#features): \t(34944, 12)\nS: static features, of shape (#series,#features+unique_id): \t(1, 2)\nY: target series (in X), of shape (#hours, #times): \t \t(34944, 3)\n\n\n"
     ]
    }
   ],
   "source": [
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "\n",
    "def declare_mask_df_Cristian(Y_df, offset):\n",
    "    # train_mask: 1 to keep, 0 to hide\n",
    "    train_outsample_mask = np.ones(len(Y_df), dtype=int)\n",
    "\n",
    "    print('Random validation de-activated')\n",
    "    train_outsample_mask[-offset:] = 0\n",
    "\n",
    "    print(f'Train {sum(train_outsample_mask)} hours = {np.round(sum(train_outsample_mask)/(24*365),2)} years')\n",
    "    print(f'Validation {sum(1-train_outsample_mask)} hours = {np.round(sum(1-train_outsample_mask)/(24*365),2)} years')\n",
    "\n",
    "    mask_df = Y_df[['unique_id', 'ds', 'y']].copy()\n",
    "    mask_df['available_mask'] = np.ones(len(Y_df))\n",
    "    mask_df['sample_mask'] = train_outsample_mask\n",
    "    return mask_df, train_outsample_mask\n",
    "\n",
    "val_ds = 2 * 365\n",
    "args = pd.Series({'dataset': ['NP']})\n",
    "\n",
    "Y_df, Xt_df, S_df = EPF.load_groups(directory='data', groups=args.dataset)\n",
    "\n",
    "mask_df, train_outsample_mask = declare_mask_df_Cristian(Y_df=Y_df, offset=val_ds*24)\n",
    "\n",
    "mask = mask_df['sample_mask'].values\n",
    "\n",
    "#print(f'Dataset: {args.dataset}')\n",
    "#print(\"Xt_df.columns\", Xt_df.columns)\n",
    "print('X: time series features, of shape (#hours, #times,#features): \\t' + str(Xt_df.shape))\n",
    "if S_df is not None:\n",
    "    print('S: static features, of shape (#series,#features+unique_id): \\t' + str(S_df.shape))\n",
    "print('Y: target series (in X), of shape (#hours, #times): \\t \\t' + str(Y_df.shape))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Validation splits\n                              ds                    \n                             min                 max\nunique_id sample_mask                               \nNP        0           2014-12-28 2016-12-26 23:00:00\n          1           2013-01-01 2014-12-27 23:00:00\nTotal data \t\t\t34944 time stamps\nAvailable percentage=100.0, \t34944.0 time stamps\nTrain percentage=49.86, \t17424.0 time stamps\nPredict percentage=50.14, \t17520 time stamps\n\n\nProcessing dataframes ...\nCreating ts tensor ...\n"
     ]
    }
   ],
   "source": [
    "dataset = TimeSeriesDataset(Y_df=Y_df, S_df=S_df, X_df=Xt_df, mask_df=mask_df)\n",
    "dataset_viejo = TimeSeriesDatasetViejo(Y_df=Y_df, S_df=S_df, X_df=Xt_df, ts_train_mask=train_outsample_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['y',\n",
       " 'Exogenous1',\n",
       " 'Exogenous2',\n",
       " 'week_day',\n",
       " 'day_0',\n",
       " 'day_1',\n",
       " 'day_2',\n",
       " 'day_3',\n",
       " 'day_4',\n",
       " 'day_5',\n",
       " 'day_6',\n",
       " 'available_mask',\n",
       " 'sample_mask']"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "dataset.t_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['y',\n",
       " 'Exogenous1',\n",
       " 'Exogenous2',\n",
       " 'week_day',\n",
       " 'day_0',\n",
       " 'day_1',\n",
       " 'day_2',\n",
       " 'day_3',\n",
       " 'day_4',\n",
       " 'day_5',\n",
       " 'day_6',\n",
       " 'insample_mask',\n",
       " 'outsample_mask']"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "dataset_viejo.t_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1. 1. 1. ... 1. 1. 1.]\n[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.ts_tensor[0, dataset.t_cols.index('available_mask'), :])\n",
    "print(dataset_viejo.ts_tensor[0, dataset_viejo.t_cols.index('insample_mask'), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1. 1. 1. ... 0. 0. 0.]\n[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Checking the insample_mask of series 0\n",
    "print(dataset.ts_tensor[0, dataset.t_cols.index('sample_mask'), :])\n",
    "print(dataset_viejo.ts_tensor[0, dataset_viejo.t_cols.index('outsample_mask'), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[31.05 30.47 28.92 ... 26.82 26.65 25.68]\n[31.05 30.47 28.92 ... 26.82 26.65 25.68]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.ts_tensor[0, dataset.t_cols.index('y'), :])\n",
    "print(dataset_viejo.ts_tensor[0, dataset_viejo.t_cols.index('y'), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataset.ts_tensor.shape (1, 13, 34944)\ndataset_viejo.ts_tensor.shape (1, 13, 34944)\n"
     ]
    }
   ],
   "source": [
    "print(\"dataset.ts_tensor.shape\", dataset.ts_tensor.shape)\n",
    "print(\"dataset_viejo.ts_tensor.shape\", dataset_viejo.ts_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_1, right_padding_1 = dataset.get_filtered_ts_tensor(offset=10, output_size=12, window_sampling_limit=36)\n",
    "filtered_2, right_padding_2, ts_mask = dataset_viejo.get_filtered_ts_tensor(offset=10, output_size=12, window_sampling_limit=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[ True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True]\n  [ True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True]\n  [ True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True]\n  [ True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True]\n  [ True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True]\n  [ True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True]\n  [ True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True]\n  [ True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True]\n  [ True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True]\n  [ True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True]\n  [ True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True]\n  [ True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True  True  True  True  True  True  True  True  True  True\n    True  True]\n  [False False False False False False False False False False False\n   False False False False False False False False False False False\n   False False False False False False False False False False False\n   False False False False False False False False False False False\n   False False]]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "print(filtered_1 == filtered_2)\n",
    "right_padding_1 == right_padding_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "52b0028e7074a0058398558ea661882eddbb489e66a28504cc0449d2f54d6290"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}