{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.tsloader_pinche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch as t\n",
    "import copy\n",
    "from fastcore.foundation import patch\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#TODO: pensar como mandar batches por epochs en \n",
    "class TimeSeriesLoader(object):\n",
    "    def __init__(self,\n",
    "                 ts_dataset:TimeSeriesDataset,\n",
    "                 model:str,\n",
    "                 offset:int,\n",
    "                 window_sampling_limit: int, \n",
    "                 input_size: int,\n",
    "                 output_size: int,\n",
    "                 idx_to_sample_freq: int,\n",
    "                 batch_size: int,\n",
    "                 is_train_loader: bool):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Dataloader attributes\n",
    "        self.model = model\n",
    "        self.window_sampling_limit = window_sampling_limit\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "        self.idx_to_sample_freq = idx_to_sample_freq\n",
    "        self.offset = offset\n",
    "        self.ts_dataset = ts_dataset\n",
    "        self.t_cols = self.ts_dataset.t_cols        \n",
    "        self.is_train_loader = is_train_loader\n",
    "        self._is_train = True\n",
    "\n",
    "        # Dataloader protections\n",
    "        assert offset < self.ts_dataset.max_len, \\\n",
    "            f'Offset {offset} must be smaller than max_len {self.ts_dataset.max_len}'\n",
    "\n",
    "        self.window_sampling_idx = self._update_windows_sampling_idx()\n",
    "        \n",
    "        #TODO: cambiar estos prints\n",
    "        # print('X: time series features, of shape (#series,#times,#features): \\t' + str(X.shape))\n",
    "        # print('Y: target series (in X), of shape (#series,#times): \\t \\t' + str(Y.shape))\n",
    "        # print('S: static features, of shape (#series,#features): \\t \\t' + str(S.shape))\n",
    "\n",
    "    def _update_windows_sampling_idx(self):\n",
    "        # Filter sampling_mask with offset and window_sampling_limit\n",
    "        last_ds = self.ts_dataset.max_len - self.offset\n",
    "        first_ds = max(last_ds - self.window_sampling_limit, 0)\n",
    "        filtered_outsample_mask = self.ts_dataset.ts_tensor[:, self.ts_dataset.t_cols.index('outsample_mask'), first_ds:last_ds]\n",
    "        filtered_ts_train_mask = self.ts_dataset.ts_train_mask[first_ds:last_ds]\n",
    "\n",
    "        # Get indices of train/validation windows\n",
    "        if self.is_train_loader:\n",
    "            train_mask =  filtered_outsample_mask * filtered_ts_train_mask\n",
    "            indices = np.argwhere(train_mask > 0)\n",
    "        else:\n",
    "            val_mask = filtered_outsample_mask * (1-filtered_ts_train_mask)\n",
    "            indices = np.argwhere(val_mask > 0)\n",
    "\n",
    "        #To change relative position of filtered tensor to global position\n",
    "        indices[:, 1] += first_ds\n",
    "\n",
    "        #Loop for each serie to extract window_sampling_idx\n",
    "        window_sampling_idx = []\n",
    "        for i in range(self.ts_dataset.n_series):\n",
    "            ts_idx = indices[indices[:, 0] == i]\n",
    "            window_sampling_idx.append(list(ts_idx[:, 1]))\n",
    "        return window_sampling_idx\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            if self._is_train:\n",
    "                sampled_ts_indices = np.random.randint(self.ts_dataset.n_series, size=self.batch_size)\n",
    "            else:\n",
    "                sampled_ts_indices = range(self.ts_dataset.n_series)\n",
    "\n",
    "            batch_dict = defaultdict(list)\n",
    "            for index in sampled_ts_indices:\n",
    "                batch_i = self.__get_item__(index)\n",
    "                for key in batch_i:\n",
    "                    batch_dict[key].append(batch_i[key])\n",
    "\n",
    "            batch = defaultdict(list)\n",
    "            for key in batch_dict:\n",
    "                batch[key] = t.Tensor(np.stack(batch_dict[key]))\n",
    "\n",
    "            yield batch\n",
    "\n",
    "    def __get_item__(self, index):\n",
    "        if self.model == 'nbeats':\n",
    "            return self._nbeats_batch(index)\n",
    "        elif self.model == 'esrnn':\n",
    "            assert 1<0, 'hacer esrnn'\n",
    "        else:\n",
    "            assert 1<0, 'error'\n",
    "\n",
    "    def _nbeats_batch(self, index):\n",
    "        # y, X_cols, insample_mask and outsample_mask - 2 masks\n",
    "        insample = np.zeros((self.ts_dataset.n_channels-2, self.input_size), dtype=float)\n",
    "        insample_mask = np.zeros(self.input_size)\n",
    "        outsample = np.zeros((self.ts_dataset.n_channels-2, self.output_size), dtype=float)\n",
    "        outsample_mask = np.zeros(self.output_size)\n",
    "            \n",
    "        ts = self.ts_dataset.ts_tensor[index]\n",
    "        len_ts = self.ts_dataset.len_series[index]\n",
    "        init_ts = max(self.ts_dataset.max_len-len_ts, self.ts_dataset.max_len-self.offset-self.window_sampling_limit) #TODO: precomputar en loader\n",
    "\n",
    "        assert self.ts_dataset.max_len-self.offset > init_ts, f'Offset too big for serie {index}'\n",
    "        if self._is_train:\n",
    "            cut_point = np.random.choice(self.window_sampling_idx[index],1)[0] # Sampling from available cuts for ts \"index\"\n",
    "        else:\n",
    "            cut_point = max(self.ts_dataset.max_len-self.offset, self.input_size)\n",
    "        \n",
    "        insample_window = ts[:self.t_cols.index('insample_mask'), max(0, cut_point - self.input_size):cut_point] # se saca mask channel del final\n",
    "        insample_mask_start = min(self.input_size, cut_point - init_ts) #In case cut_point is close to init_ts, because series are padded\n",
    "        # print('ts', ts)\n",
    "        # print('insample', insample)\n",
    "        # print('insample_window', insample_window)\n",
    "        # print('self.window_sampling_idx[index]',self.window_sampling_idx[index])\n",
    "        # print('cut_point', cut_point)\n",
    "        # print('----')\n",
    "        insample[:, -insample_window.shape[1]:] = insample_window\n",
    "        insample_mask[-insample_mask_start:] = 1.0\n",
    "\n",
    "        if self._is_train:\n",
    "            #se saca mask channel del final\n",
    "            outsample_window = ts[:self.t_cols.index('insample_mask'), cut_point:min(self.ts_dataset.max_len - self.offset, cut_point + self.output_size)]\n",
    "        else:\n",
    "            #se saca mask channel del final\n",
    "            outsample_window = ts[:self.t_cols.index('insample_mask'), cut_point:min(self.ts_dataset.max_len, cut_point + self.output_size)]\n",
    "\n",
    "        # First mask is to filter after offset, second mask to filter ts validation\n",
    "        outsample[:, :outsample_window.shape[1]] = outsample_window \n",
    "        outsample_mask[:outsample_window.shape[1]] = 1.0\n",
    "        outsample_mask[:outsample_window.shape[1]] = outsample_mask[:outsample_window.shape[1]] * \\\n",
    "                                                     self.ts_dataset.ts_train_mask[cut_point:(cut_point+outsample_window.shape[1])]\n",
    "\n",
    "        insample_y = insample[self.t_cols.index('y'), :]\n",
    "        insample_x = insample[1:, :]\n",
    "\n",
    "        outsample_y = outsample[self.t_cols.index('y'), :]\n",
    "        outsample_x = outsample[1:, :]\n",
    "\n",
    "        s_matrix = self.ts_dataset.s_matrix[index, :]\n",
    "\n",
    "        batch = {'s_matrix': s_matrix,\n",
    "                 'insample_y': insample_y, 'insample_x':insample_x, 'insample_mask':insample_mask,\n",
    "                 'outsample_y': outsample_y, 'outsample_x':outsample_x, 'outsample_mask':outsample_mask}\n",
    "        return batch\n",
    "\n",
    "    def update_offset(self, offset):\n",
    "        if offset == self.offset:\n",
    "            return # Avoid extra computation\n",
    "        self.offset = offset\n",
    "        self._create_train_data()\n",
    "\n",
    "    def get_meta_data_col(self, col):\n",
    "        return self.ts_dataset.get_meta_data_col(col)\n",
    "\n",
    "    def get_n_variables(self):\n",
    "        return self.ts_dataset.n_x, self.ts_dataset.n_s\n",
    "\n",
    "    def get_n_series(self):\n",
    "        return self.ts_dataset.n_series\n",
    "\n",
    "    def get_max_len(self):\n",
    "        return self.ts_dataset.max_len\n",
    "\n",
    "    def get_n_channels(self):\n",
    "        return self.ts_dataset.n_channels\n",
    "\n",
    "    def get_X_cols(self):\n",
    "        return self.ts_dataset.X_cols\n",
    "\n",
    "    def get_frequency(self):\n",
    "        return self.ts_dataset.frequency\n",
    "\n",
    "    def train(self):\n",
    "        self._is_train = True\n",
    "\n",
    "    def eval(self):\n",
    "        self._is_train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EPFInfo.groups[0] NP\nY_df.shape (34944, 3)\nX_df.shape (34944, 11)\n"
     ]
    }
   ],
   "source": [
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "Y_df, X_df = EPF.load(directory='data', group=EPFInfo.groups[0])\n",
    "print(\"EPFInfo.groups[0]\", EPFInfo.groups[0])\n",
    "print(\"Y_df.shape\", Y_df.shape)\n",
    "print(\"X_df.shape\", X_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sum(train_outsample_mask) 26184.0\nProcessing dataframes ...\nCreating ts tensor ...\n"
     ]
    }
   ],
   "source": [
    "train_outsample_mask = np.ones(len(Y_df))\n",
    "train_outsample_mask[-365 * 24:] = 0\n",
    "print(\"sum(train_outsample_mask)\", sum(train_outsample_mask))\n",
    "epf_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=None, X_df=X_df, ts_train_mask=train_outsample_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_loader = TimeSeriesLoader(ts_dataset=epf_dataset,\n",
    "                             model='nbeats',\n",
    "                             offset=0,\n",
    "                             window_sampling_limit=365*4*24, \n",
    "                             input_size=3*24,\n",
    "                             output_size=24,\n",
    "                             idx_to_sample_freq=1,\n",
    "                             batch_size= 2048,\n",
    "                             is_train_loader=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DataloaderPinche batch time: 4.033509969711304\nepf_dataset.t_cols\n ['y', 'Exogenous1', 'Exogenous2', 'day_0', 'day_1', 'day_2', 'day_3', 'day_4', 'day_5', 'day_6', 'insample_mask', 'outsample_mask']\nts_loader.input_size 72\nts_loader.output_size 24\ninsample_y.shape torch.Size([2048, 72])\ninsample_x.shape torch.Size([2048, 9, 72])\noutsample_y.shape torch.Size([2048, 24])\noutsample_x.shape torch.Size([2048, 9, 24])\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "start = time.time()\n",
    "dataloader = iter(ts_loader)\n",
    "batch = next(dataloader)\n",
    "insample_y = batch['insample_y']\n",
    "insample_x = batch['insample_x']\n",
    "insample_mask = batch['insample_mask']\n",
    "outsample_y = batch['outsample_y']\n",
    "outsample_x = batch['outsample_x']\n",
    "outsample_mask = batch['outsample_mask']\n",
    "\n",
    "print(\"DataloaderPinche batch time:\", time.time()-start)\n",
    "print(\"epf_dataset.t_cols\\n\", epf_dataset.t_cols)\n",
    "print(\"ts_loader.input_size\", ts_loader.input_size)\n",
    "print(\"ts_loader.output_size\", ts_loader.output_size)\n",
    "print(\"insample_y.shape\", insample_y.shape)\n",
    "print(\"insample_x.shape\", insample_x.shape)\n",
    "print(\"outsample_y.shape\", outsample_y.shape)\n",
    "print(\"outsample_x.shape\", outsample_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outsample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outsample_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outsample_mask.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('nixtla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c68c0f618c4f787b1e02bfee278e48d25b62407cb335aab5258cedc4db4ced"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}