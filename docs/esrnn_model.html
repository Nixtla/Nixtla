---

title: ESRNN model


keywords: fastai
sidebar: home_sidebar

summary: "API details."
description: "API details."
nb_path: "nbs/esrnn_model.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/esrnn_model.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ESRNN" class="doc_header"><code>class</code> <code>ESRNN</code><a href="https://github.com/Grupo-Abraxas/nixtla/tree/master/nixtla/models/esrnn/esrnn.py#L27" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ESRNN</code>(<strong><code>max_epochs</code></strong>=<em><code>15</code></em>, <strong><code>batch_size</code></strong>=<em><code>1</code></em>, <strong><code>batch_size_test</code></strong>=<em><code>64</code></em>, <strong><code>freq_of_test</code></strong>=<em><code>-1</code></em>, <strong><code>learning_rate</code></strong>=<em><code>0.001</code></em>, <strong><code>lr_scheduler_step_size</code></strong>=<em><code>9</code></em>, <strong><code>lr_decay</code></strong>=<em><code>0.9</code></em>, <strong><code>per_series_lr_multip</code></strong>=<em><code>1.0</code></em>, <strong><code>gradient_eps</code></strong>=<em><code>1e-08</code></em>, <strong><code>gradient_clipping_threshold</code></strong>=<em><code>20</code></em>, <strong><code>rnn_weight_decay</code></strong>=<em><code>0</code></em>, <strong><code>noise_std</code></strong>=<em><code>0.001</code></em>, <strong><code>level_variability_penalty</code></strong>=<em><code>80</code></em>, <strong><code>testing_percentile</code></strong>=<em><code>50</code></em>, <strong><code>training_percentile</code></strong>=<em><code>50</code></em>, <strong><code>ensemble</code></strong>=<em><code>False</code></em>, <strong><code>cell_type</code></strong>=<em><code>'LSTM'</code></em>, <strong><code>state_hsize</code></strong>=<em><code>40</code></em>, <strong><code>dilations</code></strong>=<em><code>[[1, 2], [4, 8]]</code></em>, <strong><code>add_nl_layer</code></strong>=<em><code>False</code></em>, <strong><code>seasonality</code></strong>=<em><code>[4]</code></em>, <strong><code>input_size</code></strong>=<em><code>4</code></em>, <strong><code>output_size</code></strong>=<em><code>8</code></em>, <strong><code>frequency</code></strong>=<em><code>None</code></em>, <strong><code>max_periods</code></strong>=<em><code>20</code></em>, <strong><code>random_seed</code></strong>=<em><code>1</code></em>, <strong><code>device</code></strong>=<em><code>'cpu'</code></em>, <strong><code>root_dir</code></strong>=<em><code>'./'</code></em>)</p>
</blockquote>
<p>Exponential Smoothing Recurrent Neural Network</p>
<p>Pytorch Implementation of the M4 time series forecasting competition winner.
Proposed by Smyl. The model uses a hybrid approach of Machine Learning and
statistical methods by combining recurrent neural networks to model a common
trend with shared parameters across series, and multiplicative Holt-Winter
exponential smoothing.</p>
<h2 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h2><p>max_epochs: int
    maximum number of complete passes to train data during fit
freq_of_test: int
    period for the diagnostic evaluation of the model.
learning_rate: float
    size of the stochastic gradient descent steps
lr_scheduler_step_size: int
    this step_size is the period for each learning rate decay
per_series_lr_multip: float
    multiplier for per-series parameters smoothing and initial
    seasonalities learning rate (default 1.0)
gradient_eps: float
    term added to the Adam optimizer denominator to improve
    numerical stability (default: 1e-8)
gradient_clipping_threshold: float
    max norm of gradient vector, with all parameters treated
    as a single vector
rnn_weight_decay: float
    parameter to control classic L2/Tikhonov regularization
    of the rnn parameters
noise_std: float
    standard deviation of white noise added to input during
    fit to avoid the model from memorizing the train data
level_variability_penalty: float
    this parameter controls the strength of the penalization
    to the wigglines of the level vector, induces smoothness
    in the output
testing_percentile: float
    This value is only for diagnostic evaluation.
    In case of percentile predictions this parameter controls
    for the value predicted, when forecasting point value,
    the forecast is the median, so percentile=50.
training_percentile: float
    To reduce the model's tendency to over estimate, the
    training_percentile can be set to fit a smaller value
    through the Pinball Loss.
batch_size: int
    number of training examples for the stochastic gradient steps
seasonality: int list
    list of seasonalities of the time series
    Hourly [24, 168], Daily [7], Weekly [52], Monthly [12],
    Quarterly [4], Yearly [].
input_size: int
    input size of the recurrent neural network, usually a
    multiple of seasonality
output_size: int
    output_size or forecast horizon of the recurrent neural
    network, usually multiple of seasonality
random_seed: int
    random_seed for pseudo random pytorch initializer and
    numpy random generator
exogenous_size: int
    size of one hot encoded categorical variable, invariannt
    per time series of the panel
min_inp_seq_length: int
    description
max_periods: int
    Parameter to chop longer series, to last max_periods,
    max e.g. 40 years
cell_type: str
    Type of RNN cell, available GRU, LSTM, RNN, ResidualLSTM.
state_hsize: int
    dimension of hidden state of the recurrent neural network
dilations: int list
    each list represents one chunk of Dilated LSTMS, connected in
    standard ResNet fashion
add_nl_layer: bool
    whether to insert a tanh() layer between the RNN stack and the
    linear adaptor (output) layers
device: str
    pytorch device either 'cpu' or 'cuda'</p>
<h2 id="Notes">Notes<a class="anchor-link" href="#Notes"> </a></h2><p><strong>References:</strong>
<code>M4 Competition Conclusions
&lt;https://rpubs.com/fotpetr/m4competition&gt;</code><strong>
<code>Original Dynet Implementation of ESRNN
&lt;https://github.com/M4Competition/M4-methods/tree/master/118%20-%20slaweks17&gt;</code></strong></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

</div>
 

